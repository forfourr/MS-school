{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression Test\n",
    ": 각 이미지는 28x28/ gray/ 10 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Label\n",
    "0: 티셔츠\n",
    "1: 바지\n",
    "2: 스웨터\n",
    "3: 드레스\n",
    "4: 코트\n",
    "5: 샌들\n",
    "6: 셔츠\n",
    "7: 운동화\n",
    "8: 가방\n",
    "9: 앵클 부츠\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download dataset / dataloader 생성\n",
    "train_dataset = dataset.FashionMNIST(root = './data', train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "test_dataset = dataset.FashionMNIST(root = './data', train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# create train/test Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = 100,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = 1,\n",
    "                                          shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LosigiticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LosigiticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        #sigmoid X : 이진분류가 아니라 다중분류이기 때문이다.\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 *28     #이미지 크기가 28*28/ 흑백 아니라면 28*28*3\n",
    "num_classes = 10        #label이 10개\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "total_step = len(train_loader)\n",
    "print(total_step)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Loss, Optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LosigiticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LosigiticRegression(input_size=input_size,\n",
    "                            num_classes=num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device >> cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LosigiticRegression(\n",
       "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "#device setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device >>', device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/600], Loss: 2.3927, Train Acc:0.1300\n",
      "Epoch [1/10], Step [2/600], Loss: 2.3523, Train Acc:0.1300\n",
      "Epoch [1/10], Step [3/600], Loss: 2.3563, Train Acc:0.1367\n",
      "Epoch [1/10], Step [4/600], Loss: 2.3610, Train Acc:0.1300\n",
      "Epoch [1/10], Step [5/600], Loss: 2.3271, Train Acc:0.1300\n",
      "Epoch [1/10], Step [6/600], Loss: 2.3504, Train Acc:0.1383\n",
      "Epoch [1/10], Step [7/600], Loss: 2.3135, Train Acc:0.1486\n",
      "Epoch [1/10], Step [8/600], Loss: 2.3142, Train Acc:0.1487\n",
      "Epoch [1/10], Step [9/600], Loss: 2.2781, Train Acc:0.1533\n",
      "Epoch [1/10], Step [10/600], Loss: 2.2783, Train Acc:0.1580\n",
      "Epoch [1/10], Step [11/600], Loss: 2.2940, Train Acc:0.1591\n",
      "Epoch [1/10], Step [12/600], Loss: 2.2104, Train Acc:0.1650\n",
      "Epoch [1/10], Step [13/600], Loss: 2.2258, Train Acc:0.1685\n",
      "Epoch [1/10], Step [14/600], Loss: 2.2025, Train Acc:0.1736\n",
      "Epoch [1/10], Step [15/600], Loss: 2.1535, Train Acc:0.1780\n",
      "Epoch [1/10], Step [16/600], Loss: 2.1640, Train Acc:0.1819\n",
      "Epoch [1/10], Step [17/600], Loss: 2.1327, Train Acc:0.1900\n",
      "Epoch [1/10], Step [18/600], Loss: 2.1195, Train Acc:0.1978\n",
      "Epoch [1/10], Step [19/600], Loss: 2.1060, Train Acc:0.2026\n",
      "Epoch [1/10], Step [20/600], Loss: 2.1016, Train Acc:0.2090\n",
      "Epoch [1/10], Step [21/600], Loss: 2.0914, Train Acc:0.2152\n",
      "Epoch [1/10], Step [22/600], Loss: 2.0683, Train Acc:0.2218\n",
      "Epoch [1/10], Step [23/600], Loss: 1.9894, Train Acc:0.2322\n",
      "Epoch [1/10], Step [24/600], Loss: 2.0507, Train Acc:0.2371\n",
      "Epoch [1/10], Step [25/600], Loss: 2.0351, Train Acc:0.2444\n",
      "Epoch [1/10], Step [26/600], Loss: 2.0207, Train Acc:0.2492\n",
      "Epoch [1/10], Step [27/600], Loss: 2.0011, Train Acc:0.2570\n",
      "Epoch [1/10], Step [28/600], Loss: 1.9833, Train Acc:0.2618\n",
      "Epoch [1/10], Step [29/600], Loss: 1.9141, Train Acc:0.2707\n",
      "Epoch [1/10], Step [30/600], Loss: 1.8899, Train Acc:0.2800\n",
      "Epoch [1/10], Step [31/600], Loss: 1.9029, Train Acc:0.2871\n",
      "Epoch [1/10], Step [32/600], Loss: 1.8957, Train Acc:0.2934\n",
      "Epoch [1/10], Step [33/600], Loss: 1.9174, Train Acc:0.3021\n",
      "Epoch [1/10], Step [34/600], Loss: 1.8718, Train Acc:0.3085\n",
      "Epoch [1/10], Step [35/600], Loss: 1.8426, Train Acc:0.3160\n",
      "Epoch [1/10], Step [36/600], Loss: 1.8531, Train Acc:0.3233\n",
      "Epoch [1/10], Step [37/600], Loss: 1.8181, Train Acc:0.3305\n",
      "Epoch [1/10], Step [38/600], Loss: 1.8306, Train Acc:0.3358\n",
      "Epoch [1/10], Step [39/600], Loss: 1.7819, Train Acc:0.3426\n",
      "Epoch [1/10], Step [40/600], Loss: 1.8025, Train Acc:0.3473\n",
      "Epoch [1/10], Step [41/600], Loss: 1.8035, Train Acc:0.3524\n",
      "Epoch [1/10], Step [42/600], Loss: 1.7410, Train Acc:0.3583\n",
      "Epoch [1/10], Step [43/600], Loss: 1.7785, Train Acc:0.3653\n",
      "Epoch [1/10], Step [44/600], Loss: 1.7953, Train Acc:0.3686\n",
      "Epoch [1/10], Step [45/600], Loss: 1.7930, Train Acc:0.3740\n",
      "Epoch [1/10], Step [46/600], Loss: 1.7947, Train Acc:0.3770\n",
      "Epoch [1/10], Step [47/600], Loss: 1.7831, Train Acc:0.3791\n",
      "Epoch [1/10], Step [48/600], Loss: 1.7483, Train Acc:0.3833\n",
      "Epoch [1/10], Step [49/600], Loss: 1.6463, Train Acc:0.3890\n",
      "Epoch [1/10], Step [50/600], Loss: 1.6667, Train Acc:0.3924\n",
      "Epoch [1/10], Step [51/600], Loss: 1.6489, Train Acc:0.3969\n",
      "Epoch [1/10], Step [52/600], Loss: 1.6507, Train Acc:0.4013\n",
      "Epoch [1/10], Step [53/600], Loss: 1.7127, Train Acc:0.4047\n",
      "Epoch [1/10], Step [54/600], Loss: 1.6296, Train Acc:0.4085\n",
      "Epoch [1/10], Step [55/600], Loss: 1.6370, Train Acc:0.4120\n",
      "Epoch [1/10], Step [56/600], Loss: 1.7438, Train Acc:0.4134\n",
      "Epoch [1/10], Step [57/600], Loss: 1.6094, Train Acc:0.4167\n",
      "Epoch [1/10], Step [58/600], Loss: 1.7403, Train Acc:0.4174\n",
      "Epoch [1/10], Step [59/600], Loss: 1.6094, Train Acc:0.4205\n",
      "Epoch [1/10], Step [60/600], Loss: 1.6116, Train Acc:0.4232\n",
      "Epoch [1/10], Step [61/600], Loss: 1.6133, Train Acc:0.4262\n",
      "Epoch [1/10], Step [62/600], Loss: 1.4835, Train Acc:0.4310\n",
      "Epoch [1/10], Step [63/600], Loss: 1.6392, Train Acc:0.4335\n",
      "Epoch [1/10], Step [64/600], Loss: 1.5295, Train Acc:0.4372\n",
      "Epoch [1/10], Step [65/600], Loss: 1.5380, Train Acc:0.4405\n",
      "Epoch [1/10], Step [66/600], Loss: 1.5218, Train Acc:0.4438\n",
      "Epoch [1/10], Step [67/600], Loss: 1.5501, Train Acc:0.4463\n",
      "Epoch [1/10], Step [68/600], Loss: 1.5201, Train Acc:0.4496\n",
      "Epoch [1/10], Step [69/600], Loss: 1.4637, Train Acc:0.4528\n",
      "Epoch [1/10], Step [70/600], Loss: 1.5983, Train Acc:0.4544\n",
      "Epoch [1/10], Step [71/600], Loss: 1.5488, Train Acc:0.4570\n",
      "Epoch [1/10], Step [72/600], Loss: 1.4841, Train Acc:0.4599\n",
      "Epoch [1/10], Step [73/600], Loss: 1.4186, Train Acc:0.4629\n",
      "Epoch [1/10], Step [74/600], Loss: 1.4866, Train Acc:0.4651\n",
      "Epoch [1/10], Step [75/600], Loss: 1.4924, Train Acc:0.4680\n",
      "Epoch [1/10], Step [76/600], Loss: 1.5333, Train Acc:0.4704\n",
      "Epoch [1/10], Step [77/600], Loss: 1.5103, Train Acc:0.4718\n",
      "Epoch [1/10], Step [78/600], Loss: 1.5291, Train Acc:0.4731\n",
      "Epoch [1/10], Step [79/600], Loss: 1.5837, Train Acc:0.4733\n",
      "Epoch [1/10], Step [80/600], Loss: 1.5022, Train Acc:0.4751\n",
      "Epoch [1/10], Step [81/600], Loss: 1.4727, Train Acc:0.4777\n",
      "Epoch [1/10], Step [82/600], Loss: 1.4593, Train Acc:0.4800\n",
      "Epoch [1/10], Step [83/600], Loss: 1.4523, Train Acc:0.4819\n",
      "Epoch [1/10], Step [84/600], Loss: 1.4373, Train Acc:0.4838\n",
      "Epoch [1/10], Step [85/600], Loss: 1.3871, Train Acc:0.4859\n",
      "Epoch [1/10], Step [86/600], Loss: 1.3470, Train Acc:0.4884\n",
      "Epoch [1/10], Step [87/600], Loss: 1.4540, Train Acc:0.4898\n",
      "Epoch [1/10], Step [88/600], Loss: 1.4337, Train Acc:0.4915\n",
      "Epoch [1/10], Step [89/600], Loss: 1.3911, Train Acc:0.4931\n",
      "Epoch [1/10], Step [90/600], Loss: 1.4884, Train Acc:0.4942\n",
      "Epoch [1/10], Step [91/600], Loss: 1.4446, Train Acc:0.4959\n",
      "Epoch [1/10], Step [92/600], Loss: 1.4763, Train Acc:0.4964\n",
      "Epoch [1/10], Step [93/600], Loss: 1.4649, Train Acc:0.4969\n",
      "Epoch [1/10], Step [94/600], Loss: 1.3578, Train Acc:0.4978\n",
      "Epoch [1/10], Step [95/600], Loss: 1.3750, Train Acc:0.4992\n",
      "Epoch [1/10], Step [96/600], Loss: 1.3868, Train Acc:0.5005\n",
      "Epoch [1/10], Step [97/600], Loss: 1.4009, Train Acc:0.5015\n",
      "Epoch [1/10], Step [98/600], Loss: 1.3645, Train Acc:0.5027\n",
      "Epoch [1/10], Step [99/600], Loss: 1.3521, Train Acc:0.5042\n",
      "Epoch [1/10], Step [100/600], Loss: 1.3846, Train Acc:0.5054\n",
      "Epoch [1/10], Step [101/600], Loss: 1.3836, Train Acc:0.5067\n",
      "Epoch [1/10], Step [102/600], Loss: 1.2677, Train Acc:0.5088\n",
      "Epoch [1/10], Step [103/600], Loss: 1.2623, Train Acc:0.5109\n",
      "Epoch [1/10], Step [104/600], Loss: 1.3921, Train Acc:0.5122\n",
      "Epoch [1/10], Step [105/600], Loss: 1.1833, Train Acc:0.5146\n",
      "Epoch [1/10], Step [106/600], Loss: 1.3086, Train Acc:0.5159\n",
      "Epoch [1/10], Step [107/600], Loss: 1.4242, Train Acc:0.5167\n",
      "Epoch [1/10], Step [108/600], Loss: 1.3031, Train Acc:0.5178\n",
      "Epoch [1/10], Step [109/600], Loss: 1.2231, Train Acc:0.5194\n",
      "Epoch [1/10], Step [110/600], Loss: 1.2747, Train Acc:0.5206\n",
      "Epoch [1/10], Step [111/600], Loss: 1.3280, Train Acc:0.5214\n",
      "Epoch [1/10], Step [112/600], Loss: 1.1667, Train Acc:0.5231\n",
      "Epoch [1/10], Step [113/600], Loss: 1.2921, Train Acc:0.5240\n",
      "Epoch [1/10], Step [114/600], Loss: 1.3732, Train Acc:0.5243\n",
      "Epoch [1/10], Step [115/600], Loss: 1.2156, Train Acc:0.5262\n",
      "Epoch [1/10], Step [116/600], Loss: 1.2735, Train Acc:0.5270\n",
      "Epoch [1/10], Step [117/600], Loss: 1.3497, Train Acc:0.5274\n",
      "Epoch [1/10], Step [118/600], Loss: 1.2543, Train Acc:0.5282\n",
      "Epoch [1/10], Step [119/600], Loss: 1.3044, Train Acc:0.5290\n",
      "Epoch [1/10], Step [120/600], Loss: 1.3156, Train Acc:0.5302\n",
      "Epoch [1/10], Step [121/600], Loss: 1.2885, Train Acc:0.5311\n",
      "Epoch [1/10], Step [122/600], Loss: 1.3029, Train Acc:0.5315\n",
      "Epoch [1/10], Step [123/600], Loss: 1.3262, Train Acc:0.5320\n",
      "Epoch [1/10], Step [124/600], Loss: 1.1555, Train Acc:0.5333\n",
      "Epoch [1/10], Step [125/600], Loss: 1.2621, Train Acc:0.5338\n",
      "Epoch [1/10], Step [126/600], Loss: 1.2636, Train Acc:0.5346\n",
      "Epoch [1/10], Step [127/600], Loss: 1.1937, Train Acc:0.5360\n",
      "Epoch [1/10], Step [128/600], Loss: 1.2226, Train Acc:0.5370\n",
      "Epoch [1/10], Step [129/600], Loss: 1.2076, Train Acc:0.5381\n",
      "Epoch [1/10], Step [130/600], Loss: 1.1731, Train Acc:0.5392\n",
      "Epoch [1/10], Step [131/600], Loss: 1.1973, Train Acc:0.5405\n",
      "Epoch [1/10], Step [132/600], Loss: 1.2924, Train Acc:0.5411\n",
      "Epoch [1/10], Step [133/600], Loss: 1.2317, Train Acc:0.5417\n",
      "Epoch [1/10], Step [134/600], Loss: 1.2986, Train Acc:0.5422\n",
      "Epoch [1/10], Step [135/600], Loss: 1.1734, Train Acc:0.5430\n",
      "Epoch [1/10], Step [136/600], Loss: 1.1913, Train Acc:0.5437\n",
      "Epoch [1/10], Step [137/600], Loss: 1.2146, Train Acc:0.5443\n",
      "Epoch [1/10], Step [138/600], Loss: 1.1844, Train Acc:0.5453\n",
      "Epoch [1/10], Step [139/600], Loss: 1.1537, Train Acc:0.5465\n",
      "Epoch [1/10], Step [140/600], Loss: 1.1956, Train Acc:0.5474\n",
      "Epoch [1/10], Step [141/600], Loss: 1.3294, Train Acc:0.5478\n",
      "Epoch [1/10], Step [142/600], Loss: 1.2430, Train Acc:0.5481\n",
      "Epoch [1/10], Step [143/600], Loss: 1.1691, Train Acc:0.5491\n",
      "Epoch [1/10], Step [144/600], Loss: 1.1566, Train Acc:0.5503\n",
      "Epoch [1/10], Step [145/600], Loss: 1.1975, Train Acc:0.5511\n",
      "Epoch [1/10], Step [146/600], Loss: 1.1533, Train Acc:0.5518\n",
      "Epoch [1/10], Step [147/600], Loss: 1.1274, Train Acc:0.5529\n",
      "Epoch [1/10], Step [148/600], Loss: 1.1955, Train Acc:0.5539\n",
      "Epoch [1/10], Step [149/600], Loss: 1.2411, Train Acc:0.5543\n",
      "Epoch [1/10], Step [150/600], Loss: 1.2701, Train Acc:0.5546\n",
      "Epoch [1/10], Step [151/600], Loss: 1.2607, Train Acc:0.5549\n",
      "Epoch [1/10], Step [152/600], Loss: 1.1650, Train Acc:0.5556\n",
      "Epoch [1/10], Step [153/600], Loss: 0.9767, Train Acc:0.5570\n",
      "Epoch [1/10], Step [154/600], Loss: 1.2346, Train Acc:0.5573\n",
      "Epoch [1/10], Step [155/600], Loss: 1.1921, Train Acc:0.5579\n",
      "Epoch [1/10], Step [156/600], Loss: 1.1284, Train Acc:0.5588\n",
      "Epoch [1/10], Step [157/600], Loss: 1.1566, Train Acc:0.5591\n",
      "Epoch [1/10], Step [158/600], Loss: 1.0799, Train Acc:0.5599\n",
      "Epoch [1/10], Step [159/600], Loss: 1.1559, Train Acc:0.5608\n",
      "Epoch [1/10], Step [160/600], Loss: 1.1696, Train Acc:0.5616\n",
      "Epoch [1/10], Step [161/600], Loss: 1.1162, Train Acc:0.5624\n",
      "Epoch [1/10], Step [162/600], Loss: 1.1217, Train Acc:0.5631\n",
      "Epoch [1/10], Step [163/600], Loss: 1.1918, Train Acc:0.5634\n",
      "Epoch [1/10], Step [164/600], Loss: 1.1835, Train Acc:0.5637\n",
      "Epoch [1/10], Step [165/600], Loss: 1.1331, Train Acc:0.5645\n",
      "Epoch [1/10], Step [166/600], Loss: 1.1211, Train Acc:0.5654\n",
      "Epoch [1/10], Step [167/600], Loss: 1.2357, Train Acc:0.5661\n",
      "Epoch [1/10], Step [168/600], Loss: 1.1492, Train Acc:0.5668\n",
      "Epoch [1/10], Step [169/600], Loss: 1.1916, Train Acc:0.5670\n",
      "Epoch [1/10], Step [170/600], Loss: 0.9931, Train Acc:0.5680\n",
      "Epoch [1/10], Step [171/600], Loss: 1.1139, Train Acc:0.5685\n",
      "Epoch [1/10], Step [172/600], Loss: 1.0356, Train Acc:0.5694\n",
      "Epoch [1/10], Step [173/600], Loss: 1.0724, Train Acc:0.5702\n",
      "Epoch [1/10], Step [174/600], Loss: 1.0306, Train Acc:0.5711\n",
      "Epoch [1/10], Step [175/600], Loss: 1.1057, Train Acc:0.5718\n",
      "Epoch [1/10], Step [176/600], Loss: 1.1765, Train Acc:0.5723\n",
      "Epoch [1/10], Step [177/600], Loss: 1.1268, Train Acc:0.5731\n",
      "Epoch [1/10], Step [178/600], Loss: 1.0311, Train Acc:0.5740\n",
      "Epoch [1/10], Step [179/600], Loss: 1.0882, Train Acc:0.5746\n",
      "Epoch [1/10], Step [180/600], Loss: 1.1540, Train Acc:0.5748\n",
      "Epoch [1/10], Step [181/600], Loss: 1.0000, Train Acc:0.5756\n",
      "Epoch [1/10], Step [182/600], Loss: 1.0821, Train Acc:0.5761\n",
      "Epoch [1/10], Step [183/600], Loss: 1.0840, Train Acc:0.5767\n",
      "Epoch [1/10], Step [184/600], Loss: 1.0829, Train Acc:0.5775\n",
      "Epoch [1/10], Step [185/600], Loss: 1.0161, Train Acc:0.5783\n",
      "Epoch [1/10], Step [186/600], Loss: 1.1007, Train Acc:0.5788\n",
      "Epoch [1/10], Step [187/600], Loss: 1.1178, Train Acc:0.5795\n",
      "Epoch [1/10], Step [188/600], Loss: 1.1429, Train Acc:0.5798\n",
      "Epoch [1/10], Step [189/600], Loss: 1.0345, Train Acc:0.5805\n",
      "Epoch [1/10], Step [190/600], Loss: 0.9448, Train Acc:0.5813\n",
      "Epoch [1/10], Step [191/600], Loss: 1.1666, Train Acc:0.5812\n",
      "Epoch [1/10], Step [192/600], Loss: 1.0656, Train Acc:0.5816\n",
      "Epoch [1/10], Step [193/600], Loss: 0.9357, Train Acc:0.5826\n",
      "Epoch [1/10], Step [194/600], Loss: 1.1301, Train Acc:0.5827\n",
      "Epoch [1/10], Step [195/600], Loss: 1.0490, Train Acc:0.5833\n",
      "Epoch [1/10], Step [196/600], Loss: 1.1851, Train Acc:0.5836\n",
      "Epoch [1/10], Step [197/600], Loss: 1.0344, Train Acc:0.5840\n",
      "Epoch [1/10], Step [198/600], Loss: 1.0463, Train Acc:0.5846\n",
      "Epoch [1/10], Step [199/600], Loss: 1.0931, Train Acc:0.5849\n",
      "Epoch [1/10], Step [200/600], Loss: 1.0169, Train Acc:0.5854\n",
      "Epoch [1/10], Step [201/600], Loss: 0.9889, Train Acc:0.5859\n",
      "Epoch [1/10], Step [202/600], Loss: 0.9236, Train Acc:0.5866\n",
      "Epoch [1/10], Step [203/600], Loss: 1.1058, Train Acc:0.5870\n",
      "Epoch [1/10], Step [204/600], Loss: 1.1136, Train Acc:0.5873\n",
      "Epoch [1/10], Step [205/600], Loss: 1.0331, Train Acc:0.5877\n",
      "Epoch [1/10], Step [206/600], Loss: 1.0231, Train Acc:0.5882\n",
      "Epoch [1/10], Step [207/600], Loss: 1.0359, Train Acc:0.5887\n",
      "Epoch [1/10], Step [208/600], Loss: 1.0728, Train Acc:0.5889\n",
      "Epoch [1/10], Step [209/600], Loss: 1.0182, Train Acc:0.5893\n",
      "Epoch [1/10], Step [210/600], Loss: 1.1091, Train Acc:0.5896\n",
      "Epoch [1/10], Step [211/600], Loss: 1.1068, Train Acc:0.5898\n",
      "Epoch [1/10], Step [212/600], Loss: 1.1186, Train Acc:0.5901\n",
      "Epoch [1/10], Step [213/600], Loss: 0.9340, Train Acc:0.5906\n",
      "Epoch [1/10], Step [214/600], Loss: 1.0299, Train Acc:0.5913\n",
      "Epoch [1/10], Step [215/600], Loss: 1.0520, Train Acc:0.5916\n",
      "Epoch [1/10], Step [216/600], Loss: 0.9512, Train Acc:0.5922\n",
      "Epoch [1/10], Step [217/600], Loss: 0.9458, Train Acc:0.5929\n",
      "Epoch [1/10], Step [218/600], Loss: 1.0044, Train Acc:0.5933\n",
      "Epoch [1/10], Step [219/600], Loss: 1.1099, Train Acc:0.5934\n",
      "Epoch [1/10], Step [220/600], Loss: 1.0579, Train Acc:0.5940\n",
      "Epoch [1/10], Step [221/600], Loss: 1.1209, Train Acc:0.5941\n",
      "Epoch [1/10], Step [222/600], Loss: 1.0441, Train Acc:0.5944\n",
      "Epoch [1/10], Step [223/600], Loss: 0.9805, Train Acc:0.5950\n",
      "Epoch [1/10], Step [224/600], Loss: 1.0051, Train Acc:0.5955\n",
      "Epoch [1/10], Step [225/600], Loss: 1.0288, Train Acc:0.5959\n",
      "Epoch [1/10], Step [226/600], Loss: 0.9065, Train Acc:0.5965\n",
      "Epoch [1/10], Step [227/600], Loss: 1.1271, Train Acc:0.5968\n",
      "Epoch [1/10], Step [228/600], Loss: 0.9537, Train Acc:0.5975\n",
      "Epoch [1/10], Step [229/600], Loss: 1.0305, Train Acc:0.5981\n",
      "Epoch [1/10], Step [230/600], Loss: 1.0353, Train Acc:0.5983\n",
      "Epoch [1/10], Step [231/600], Loss: 1.0002, Train Acc:0.5988\n",
      "Epoch [1/10], Step [232/600], Loss: 1.0317, Train Acc:0.5993\n",
      "Epoch [1/10], Step [233/600], Loss: 1.0193, Train Acc:0.5997\n",
      "Epoch [1/10], Step [234/600], Loss: 0.9821, Train Acc:0.6003\n",
      "Epoch [1/10], Step [235/600], Loss: 1.0168, Train Acc:0.6006\n",
      "Epoch [1/10], Step [236/600], Loss: 1.0883, Train Acc:0.6009\n",
      "Epoch [1/10], Step [237/600], Loss: 1.0659, Train Acc:0.6013\n",
      "Epoch [1/10], Step [238/600], Loss: 1.0783, Train Acc:0.6015\n",
      "Epoch [1/10], Step [239/600], Loss: 1.0149, Train Acc:0.6020\n",
      "Epoch [1/10], Step [240/600], Loss: 0.9852, Train Acc:0.6024\n",
      "Epoch [1/10], Step [241/600], Loss: 1.0174, Train Acc:0.6028\n",
      "Epoch [1/10], Step [242/600], Loss: 1.0161, Train Acc:0.6031\n",
      "Epoch [1/10], Step [243/600], Loss: 1.0191, Train Acc:0.6036\n",
      "Epoch [1/10], Step [244/600], Loss: 1.0157, Train Acc:0.6037\n",
      "Epoch [1/10], Step [245/600], Loss: 1.0987, Train Acc:0.6039\n",
      "Epoch [1/10], Step [246/600], Loss: 0.9845, Train Acc:0.6045\n",
      "Epoch [1/10], Step [247/600], Loss: 1.0228, Train Acc:0.6051\n",
      "Epoch [1/10], Step [248/600], Loss: 0.8979, Train Acc:0.6058\n",
      "Epoch [1/10], Step [249/600], Loss: 1.0190, Train Acc:0.6061\n",
      "Epoch [1/10], Step [250/600], Loss: 0.9733, Train Acc:0.6066\n",
      "Epoch [1/10], Step [251/600], Loss: 1.0506, Train Acc:0.6069\n",
      "Epoch [1/10], Step [252/600], Loss: 0.9339, Train Acc:0.6076\n",
      "Epoch [1/10], Step [253/600], Loss: 0.9752, Train Acc:0.6083\n",
      "Epoch [1/10], Step [254/600], Loss: 1.0494, Train Acc:0.6086\n",
      "Epoch [1/10], Step [255/600], Loss: 1.0432, Train Acc:0.6089\n",
      "Epoch [1/10], Step [256/600], Loss: 0.8898, Train Acc:0.6097\n",
      "Epoch [1/10], Step [257/600], Loss: 0.8932, Train Acc:0.6102\n",
      "Epoch [1/10], Step [258/600], Loss: 1.0510, Train Acc:0.6107\n",
      "Epoch [1/10], Step [259/600], Loss: 1.0850, Train Acc:0.6110\n",
      "Epoch [1/10], Step [260/600], Loss: 0.9237, Train Acc:0.6115\n",
      "Epoch [1/10], Step [261/600], Loss: 1.0413, Train Acc:0.6118\n",
      "Epoch [1/10], Step [262/600], Loss: 0.9171, Train Acc:0.6124\n",
      "Epoch [1/10], Step [263/600], Loss: 0.9627, Train Acc:0.6127\n",
      "Epoch [1/10], Step [264/600], Loss: 0.9940, Train Acc:0.6129\n",
      "Epoch [1/10], Step [265/600], Loss: 0.9991, Train Acc:0.6132\n",
      "Epoch [1/10], Step [266/600], Loss: 0.9839, Train Acc:0.6135\n",
      "Epoch [1/10], Step [267/600], Loss: 1.0293, Train Acc:0.6137\n",
      "Epoch [1/10], Step [268/600], Loss: 0.9694, Train Acc:0.6141\n",
      "Epoch [1/10], Step [269/600], Loss: 0.9986, Train Acc:0.6143\n",
      "Epoch [1/10], Step [270/600], Loss: 0.9047, Train Acc:0.6149\n",
      "Epoch [1/10], Step [271/600], Loss: 0.9572, Train Acc:0.6154\n",
      "Epoch [1/10], Step [272/600], Loss: 1.0278, Train Acc:0.6155\n",
      "Epoch [1/10], Step [273/600], Loss: 0.8923, Train Acc:0.6162\n",
      "Epoch [1/10], Step [274/600], Loss: 0.9022, Train Acc:0.6168\n",
      "Epoch [1/10], Step [275/600], Loss: 1.0374, Train Acc:0.6170\n",
      "Epoch [1/10], Step [276/600], Loss: 0.9835, Train Acc:0.6171\n",
      "Epoch [1/10], Step [277/600], Loss: 0.9180, Train Acc:0.6175\n",
      "Epoch [1/10], Step [278/600], Loss: 0.9520, Train Acc:0.6179\n",
      "Epoch [1/10], Step [279/600], Loss: 0.9242, Train Acc:0.6184\n",
      "Epoch [1/10], Step [280/600], Loss: 1.0600, Train Acc:0.6184\n",
      "Epoch [1/10], Step [281/600], Loss: 0.8573, Train Acc:0.6189\n",
      "Epoch [1/10], Step [282/600], Loss: 0.9696, Train Acc:0.6189\n",
      "Epoch [1/10], Step [283/600], Loss: 0.8805, Train Acc:0.6194\n",
      "Epoch [1/10], Step [284/600], Loss: 0.9266, Train Acc:0.6200\n",
      "Epoch [1/10], Step [285/600], Loss: 0.9670, Train Acc:0.6204\n",
      "Epoch [1/10], Step [286/600], Loss: 1.0237, Train Acc:0.6207\n",
      "Epoch [1/10], Step [287/600], Loss: 0.9815, Train Acc:0.6209\n",
      "Epoch [1/10], Step [288/600], Loss: 1.0242, Train Acc:0.6211\n",
      "Epoch [1/10], Step [289/600], Loss: 0.9763, Train Acc:0.6215\n",
      "Epoch [1/10], Step [290/600], Loss: 0.8768, Train Acc:0.6218\n",
      "Epoch [1/10], Step [291/600], Loss: 0.9202, Train Acc:0.6221\n",
      "Epoch [1/10], Step [292/600], Loss: 0.9879, Train Acc:0.6223\n",
      "Epoch [1/10], Step [293/600], Loss: 1.1037, Train Acc:0.6223\n",
      "Epoch [1/10], Step [294/600], Loss: 0.8761, Train Acc:0.6226\n",
      "Epoch [1/10], Step [295/600], Loss: 0.9448, Train Acc:0.6228\n",
      "Epoch [1/10], Step [296/600], Loss: 0.8665, Train Acc:0.6234\n",
      "Epoch [1/10], Step [297/600], Loss: 0.9827, Train Acc:0.6236\n",
      "Epoch [1/10], Step [298/600], Loss: 0.9083, Train Acc:0.6239\n",
      "Epoch [1/10], Step [299/600], Loss: 0.8718, Train Acc:0.6242\n",
      "Epoch [1/10], Step [300/600], Loss: 0.9724, Train Acc:0.6245\n",
      "Epoch [1/10], Step [301/600], Loss: 0.8348, Train Acc:0.6250\n",
      "Epoch [1/10], Step [302/600], Loss: 0.9744, Train Acc:0.6251\n",
      "Epoch [1/10], Step [303/600], Loss: 0.9512, Train Acc:0.6253\n",
      "Epoch [1/10], Step [304/600], Loss: 1.0654, Train Acc:0.6253\n",
      "Epoch [1/10], Step [305/600], Loss: 1.0861, Train Acc:0.6252\n",
      "Epoch [1/10], Step [306/600], Loss: 0.9679, Train Acc:0.6254\n",
      "Epoch [1/10], Step [307/600], Loss: 0.9555, Train Acc:0.6256\n",
      "Epoch [1/10], Step [308/600], Loss: 1.0143, Train Acc:0.6258\n",
      "Epoch [1/10], Step [309/600], Loss: 1.0124, Train Acc:0.6261\n",
      "Epoch [1/10], Step [310/600], Loss: 0.9629, Train Acc:0.6263\n",
      "Epoch [1/10], Step [311/600], Loss: 0.9065, Train Acc:0.6267\n",
      "Epoch [1/10], Step [312/600], Loss: 0.7919, Train Acc:0.6272\n",
      "Epoch [1/10], Step [313/600], Loss: 0.8416, Train Acc:0.6275\n",
      "Epoch [1/10], Step [314/600], Loss: 0.9590, Train Acc:0.6277\n",
      "Epoch [1/10], Step [315/600], Loss: 0.8216, Train Acc:0.6281\n",
      "Epoch [1/10], Step [316/600], Loss: 0.9851, Train Acc:0.6281\n",
      "Epoch [1/10], Step [317/600], Loss: 0.9247, Train Acc:0.6283\n",
      "Epoch [1/10], Step [318/600], Loss: 0.9435, Train Acc:0.6287\n",
      "Epoch [1/10], Step [319/600], Loss: 0.9930, Train Acc:0.6288\n",
      "Epoch [1/10], Step [320/600], Loss: 1.0952, Train Acc:0.6286\n",
      "Epoch [1/10], Step [321/600], Loss: 0.7689, Train Acc:0.6291\n",
      "Epoch [1/10], Step [322/600], Loss: 0.9707, Train Acc:0.6291\n",
      "Epoch [1/10], Step [323/600], Loss: 0.9721, Train Acc:0.6294\n",
      "Epoch [1/10], Step [324/600], Loss: 0.9963, Train Acc:0.6296\n",
      "Epoch [1/10], Step [325/600], Loss: 0.9852, Train Acc:0.6297\n",
      "Epoch [1/10], Step [326/600], Loss: 1.0088, Train Acc:0.6300\n",
      "Epoch [1/10], Step [327/600], Loss: 0.8479, Train Acc:0.6305\n",
      "Epoch [1/10], Step [328/600], Loss: 0.9337, Train Acc:0.6308\n",
      "Epoch [1/10], Step [329/600], Loss: 0.9325, Train Acc:0.6311\n",
      "Epoch [1/10], Step [330/600], Loss: 0.8694, Train Acc:0.6314\n",
      "Epoch [1/10], Step [331/600], Loss: 0.9032, Train Acc:0.6317\n",
      "Epoch [1/10], Step [332/600], Loss: 0.9073, Train Acc:0.6320\n",
      "Epoch [1/10], Step [333/600], Loss: 0.8945, Train Acc:0.6324\n",
      "Epoch [1/10], Step [334/600], Loss: 0.9279, Train Acc:0.6327\n",
      "Epoch [1/10], Step [335/600], Loss: 0.9057, Train Acc:0.6330\n",
      "Epoch [1/10], Step [336/600], Loss: 0.7493, Train Acc:0.6334\n",
      "Epoch [1/10], Step [337/600], Loss: 0.7983, Train Acc:0.6339\n",
      "Epoch [1/10], Step [338/600], Loss: 0.7826, Train Acc:0.6344\n",
      "Epoch [1/10], Step [339/600], Loss: 0.9463, Train Acc:0.6347\n",
      "Epoch [1/10], Step [340/600], Loss: 0.8617, Train Acc:0.6351\n",
      "Epoch [1/10], Step [341/600], Loss: 0.8797, Train Acc:0.6353\n",
      "Epoch [1/10], Step [342/600], Loss: 0.9493, Train Acc:0.6356\n",
      "Epoch [1/10], Step [343/600], Loss: 0.8289, Train Acc:0.6359\n",
      "Epoch [1/10], Step [344/600], Loss: 0.8601, Train Acc:0.6362\n",
      "Epoch [1/10], Step [345/600], Loss: 0.8908, Train Acc:0.6364\n",
      "Epoch [1/10], Step [346/600], Loss: 0.9532, Train Acc:0.6366\n",
      "Epoch [1/10], Step [347/600], Loss: 0.8875, Train Acc:0.6369\n",
      "Epoch [1/10], Step [348/600], Loss: 0.9301, Train Acc:0.6372\n",
      "Epoch [1/10], Step [349/600], Loss: 0.8928, Train Acc:0.6375\n",
      "Epoch [1/10], Step [350/600], Loss: 0.9009, Train Acc:0.6377\n",
      "Epoch [1/10], Step [351/600], Loss: 0.9451, Train Acc:0.6379\n",
      "Epoch [1/10], Step [352/600], Loss: 0.8922, Train Acc:0.6383\n",
      "Epoch [1/10], Step [353/600], Loss: 0.9655, Train Acc:0.6383\n",
      "Epoch [1/10], Step [354/600], Loss: 0.9490, Train Acc:0.6385\n",
      "Epoch [1/10], Step [355/600], Loss: 0.8813, Train Acc:0.6387\n",
      "Epoch [1/10], Step [356/600], Loss: 1.0986, Train Acc:0.6386\n",
      "Epoch [1/10], Step [357/600], Loss: 0.7915, Train Acc:0.6390\n",
      "Epoch [1/10], Step [358/600], Loss: 0.9117, Train Acc:0.6393\n",
      "Epoch [1/10], Step [359/600], Loss: 0.9785, Train Acc:0.6393\n",
      "Epoch [1/10], Step [360/600], Loss: 0.8681, Train Acc:0.6396\n",
      "Epoch [1/10], Step [361/600], Loss: 0.7908, Train Acc:0.6401\n",
      "Epoch [1/10], Step [362/600], Loss: 0.8889, Train Acc:0.6403\n",
      "Epoch [1/10], Step [363/600], Loss: 0.9051, Train Acc:0.6405\n",
      "Epoch [1/10], Step [364/600], Loss: 0.8583, Train Acc:0.6409\n",
      "Epoch [1/10], Step [365/600], Loss: 0.8170, Train Acc:0.6412\n",
      "Epoch [1/10], Step [366/600], Loss: 0.8684, Train Acc:0.6415\n",
      "Epoch [1/10], Step [367/600], Loss: 0.8288, Train Acc:0.6417\n",
      "Epoch [1/10], Step [368/600], Loss: 0.8502, Train Acc:0.6419\n",
      "Epoch [1/10], Step [369/600], Loss: 0.9001, Train Acc:0.6421\n",
      "Epoch [1/10], Step [370/600], Loss: 0.9329, Train Acc:0.6422\n",
      "Epoch [1/10], Step [371/600], Loss: 1.0076, Train Acc:0.6423\n",
      "Epoch [1/10], Step [372/600], Loss: 0.9036, Train Acc:0.6424\n",
      "Epoch [1/10], Step [373/600], Loss: 0.9031, Train Acc:0.6427\n",
      "Epoch [1/10], Step [374/600], Loss: 0.9990, Train Acc:0.6428\n",
      "Epoch [1/10], Step [375/600], Loss: 0.8862, Train Acc:0.6431\n",
      "Epoch [1/10], Step [376/600], Loss: 0.8796, Train Acc:0.6435\n",
      "Epoch [1/10], Step [377/600], Loss: 0.9160, Train Acc:0.6438\n",
      "Epoch [1/10], Step [378/600], Loss: 0.8479, Train Acc:0.6440\n",
      "Epoch [1/10], Step [379/600], Loss: 0.8230, Train Acc:0.6444\n",
      "Epoch [1/10], Step [380/600], Loss: 0.7666, Train Acc:0.6447\n",
      "Epoch [1/10], Step [381/600], Loss: 0.9096, Train Acc:0.6450\n",
      "Epoch [1/10], Step [382/600], Loss: 0.8943, Train Acc:0.6451\n",
      "Epoch [1/10], Step [383/600], Loss: 0.7580, Train Acc:0.6455\n",
      "Epoch [1/10], Step [384/600], Loss: 0.9052, Train Acc:0.6458\n",
      "Epoch [1/10], Step [385/600], Loss: 0.8478, Train Acc:0.6461\n",
      "Epoch [1/10], Step [386/600], Loss: 0.9091, Train Acc:0.6462\n",
      "Epoch [1/10], Step [387/600], Loss: 0.9329, Train Acc:0.6464\n",
      "Epoch [1/10], Step [388/600], Loss: 0.8972, Train Acc:0.6465\n",
      "Epoch [1/10], Step [389/600], Loss: 0.7988, Train Acc:0.6467\n",
      "Epoch [1/10], Step [390/600], Loss: 0.9280, Train Acc:0.6468\n",
      "Epoch [1/10], Step [391/600], Loss: 0.9056, Train Acc:0.6471\n",
      "Epoch [1/10], Step [392/600], Loss: 1.0024, Train Acc:0.6470\n",
      "Epoch [1/10], Step [393/600], Loss: 0.9179, Train Acc:0.6470\n",
      "Epoch [1/10], Step [394/600], Loss: 0.9586, Train Acc:0.6472\n",
      "Epoch [1/10], Step [395/600], Loss: 0.8153, Train Acc:0.6475\n",
      "Epoch [1/10], Step [396/600], Loss: 0.8188, Train Acc:0.6478\n",
      "Epoch [1/10], Step [397/600], Loss: 0.8897, Train Acc:0.6480\n",
      "Epoch [1/10], Step [398/600], Loss: 0.9673, Train Acc:0.6481\n",
      "Epoch [1/10], Step [399/600], Loss: 0.7606, Train Acc:0.6485\n",
      "Epoch [1/10], Step [400/600], Loss: 0.9772, Train Acc:0.6486\n",
      "Epoch [1/10], Step [401/600], Loss: 0.9103, Train Acc:0.6488\n",
      "Epoch [1/10], Step [402/600], Loss: 0.8798, Train Acc:0.6491\n",
      "Epoch [1/10], Step [403/600], Loss: 0.8780, Train Acc:0.6492\n",
      "Epoch [1/10], Step [404/600], Loss: 0.8980, Train Acc:0.6494\n",
      "Epoch [1/10], Step [405/600], Loss: 0.8934, Train Acc:0.6496\n",
      "Epoch [1/10], Step [406/600], Loss: 0.8674, Train Acc:0.6498\n",
      "Epoch [1/10], Step [407/600], Loss: 0.8321, Train Acc:0.6501\n",
      "Epoch [1/10], Step [408/600], Loss: 0.8872, Train Acc:0.6502\n",
      "Epoch [1/10], Step [409/600], Loss: 0.8281, Train Acc:0.6505\n",
      "Epoch [1/10], Step [410/600], Loss: 0.8978, Train Acc:0.6507\n",
      "Epoch [1/10], Step [411/600], Loss: 0.8419, Train Acc:0.6510\n",
      "Epoch [1/10], Step [412/600], Loss: 0.8369, Train Acc:0.6512\n",
      "Epoch [1/10], Step [413/600], Loss: 0.8685, Train Acc:0.6513\n",
      "Epoch [1/10], Step [414/600], Loss: 0.8083, Train Acc:0.6517\n",
      "Epoch [1/10], Step [415/600], Loss: 0.8248, Train Acc:0.6520\n",
      "Epoch [1/10], Step [416/600], Loss: 0.8891, Train Acc:0.6521\n",
      "Epoch [1/10], Step [417/600], Loss: 0.8372, Train Acc:0.6524\n",
      "Epoch [1/10], Step [418/600], Loss: 0.8286, Train Acc:0.6527\n",
      "Epoch [1/10], Step [419/600], Loss: 0.9011, Train Acc:0.6528\n",
      "Epoch [1/10], Step [420/600], Loss: 0.8933, Train Acc:0.6531\n",
      "Epoch [1/10], Step [421/600], Loss: 0.8556, Train Acc:0.6533\n",
      "Epoch [1/10], Step [422/600], Loss: 0.8883, Train Acc:0.6533\n",
      "Epoch [1/10], Step [423/600], Loss: 0.8449, Train Acc:0.6535\n",
      "Epoch [1/10], Step [424/600], Loss: 0.9338, Train Acc:0.6537\n",
      "Epoch [1/10], Step [425/600], Loss: 0.8288, Train Acc:0.6539\n",
      "Epoch [1/10], Step [426/600], Loss: 0.8020, Train Acc:0.6541\n",
      "Epoch [1/10], Step [427/600], Loss: 0.8679, Train Acc:0.6543\n",
      "Epoch [1/10], Step [428/600], Loss: 0.9781, Train Acc:0.6543\n",
      "Epoch [1/10], Step [429/600], Loss: 0.7228, Train Acc:0.6547\n",
      "Epoch [1/10], Step [430/600], Loss: 0.8797, Train Acc:0.6549\n",
      "Epoch [1/10], Step [431/600], Loss: 1.0132, Train Acc:0.6549\n",
      "Epoch [1/10], Step [432/600], Loss: 0.8938, Train Acc:0.6551\n",
      "Epoch [1/10], Step [433/600], Loss: 0.8950, Train Acc:0.6553\n",
      "Epoch [1/10], Step [434/600], Loss: 0.9190, Train Acc:0.6554\n",
      "Epoch [1/10], Step [435/600], Loss: 0.8629, Train Acc:0.6556\n",
      "Epoch [1/10], Step [436/600], Loss: 0.9110, Train Acc:0.6558\n",
      "Epoch [1/10], Step [437/600], Loss: 0.6834, Train Acc:0.6561\n",
      "Epoch [1/10], Step [438/600], Loss: 0.7921, Train Acc:0.6564\n",
      "Epoch [1/10], Step [439/600], Loss: 0.8648, Train Acc:0.6567\n",
      "Epoch [1/10], Step [440/600], Loss: 0.8887, Train Acc:0.6567\n",
      "Epoch [1/10], Step [441/600], Loss: 0.8445, Train Acc:0.6569\n",
      "Epoch [1/10], Step [442/600], Loss: 0.7840, Train Acc:0.6572\n",
      "Epoch [1/10], Step [443/600], Loss: 0.8758, Train Acc:0.6574\n",
      "Epoch [1/10], Step [444/600], Loss: 0.8731, Train Acc:0.6576\n",
      "Epoch [1/10], Step [445/600], Loss: 0.8557, Train Acc:0.6577\n",
      "Epoch [1/10], Step [446/600], Loss: 0.7970, Train Acc:0.6579\n",
      "Epoch [1/10], Step [447/600], Loss: 0.8777, Train Acc:0.6581\n",
      "Epoch [1/10], Step [448/600], Loss: 0.9304, Train Acc:0.6583\n",
      "Epoch [1/10], Step [449/600], Loss: 0.8504, Train Acc:0.6584\n",
      "Epoch [1/10], Step [450/600], Loss: 0.7609, Train Acc:0.6587\n",
      "Epoch [1/10], Step [451/600], Loss: 0.7276, Train Acc:0.6588\n",
      "Epoch [1/10], Step [452/600], Loss: 0.8672, Train Acc:0.6590\n",
      "Epoch [1/10], Step [453/600], Loss: 0.9351, Train Acc:0.6591\n",
      "Epoch [1/10], Step [454/600], Loss: 0.9232, Train Acc:0.6591\n",
      "Epoch [1/10], Step [455/600], Loss: 0.8404, Train Acc:0.6592\n",
      "Epoch [1/10], Step [456/600], Loss: 0.8340, Train Acc:0.6594\n",
      "Epoch [1/10], Step [457/600], Loss: 0.8699, Train Acc:0.6596\n",
      "Epoch [1/10], Step [458/600], Loss: 0.8965, Train Acc:0.6596\n",
      "Epoch [1/10], Step [459/600], Loss: 0.9278, Train Acc:0.6597\n",
      "Epoch [1/10], Step [460/600], Loss: 0.8695, Train Acc:0.6599\n",
      "Epoch [1/10], Step [461/600], Loss: 0.8800, Train Acc:0.6600\n",
      "Epoch [1/10], Step [462/600], Loss: 0.8481, Train Acc:0.6602\n",
      "Epoch [1/10], Step [463/600], Loss: 0.8989, Train Acc:0.6603\n",
      "Epoch [1/10], Step [464/600], Loss: 0.8283, Train Acc:0.6605\n",
      "Epoch [1/10], Step [465/600], Loss: 0.8718, Train Acc:0.6605\n",
      "Epoch [1/10], Step [466/600], Loss: 0.8225, Train Acc:0.6607\n",
      "Epoch [1/10], Step [467/600], Loss: 0.9576, Train Acc:0.6607\n",
      "Epoch [1/10], Step [468/600], Loss: 0.8806, Train Acc:0.6609\n",
      "Epoch [1/10], Step [469/600], Loss: 0.9050, Train Acc:0.6610\n",
      "Epoch [1/10], Step [470/600], Loss: 0.8703, Train Acc:0.6611\n",
      "Epoch [1/10], Step [471/600], Loss: 0.8408, Train Acc:0.6613\n",
      "Epoch [1/10], Step [472/600], Loss: 0.9317, Train Acc:0.6613\n",
      "Epoch [1/10], Step [473/600], Loss: 0.7599, Train Acc:0.6615\n",
      "Epoch [1/10], Step [474/600], Loss: 0.8760, Train Acc:0.6617\n",
      "Epoch [1/10], Step [475/600], Loss: 0.8480, Train Acc:0.6619\n",
      "Epoch [1/10], Step [476/600], Loss: 0.8003, Train Acc:0.6621\n",
      "Epoch [1/10], Step [477/600], Loss: 0.9212, Train Acc:0.6623\n",
      "Epoch [1/10], Step [478/600], Loss: 0.8331, Train Acc:0.6626\n",
      "Epoch [1/10], Step [479/600], Loss: 0.8371, Train Acc:0.6628\n",
      "Epoch [1/10], Step [480/600], Loss: 0.9468, Train Acc:0.6629\n",
      "Epoch [1/10], Step [481/600], Loss: 0.8224, Train Acc:0.6632\n",
      "Epoch [1/10], Step [482/600], Loss: 0.7687, Train Acc:0.6635\n",
      "Epoch [1/10], Step [483/600], Loss: 0.7121, Train Acc:0.6638\n",
      "Epoch [1/10], Step [484/600], Loss: 0.8618, Train Acc:0.6639\n",
      "Epoch [1/10], Step [485/600], Loss: 0.9380, Train Acc:0.6639\n",
      "Epoch [1/10], Step [486/600], Loss: 0.8073, Train Acc:0.6641\n",
      "Epoch [1/10], Step [487/600], Loss: 0.8130, Train Acc:0.6643\n",
      "Epoch [1/10], Step [488/600], Loss: 0.9700, Train Acc:0.6644\n",
      "Epoch [1/10], Step [489/600], Loss: 0.8087, Train Acc:0.6645\n",
      "Epoch [1/10], Step [490/600], Loss: 0.9167, Train Acc:0.6647\n",
      "Epoch [1/10], Step [491/600], Loss: 0.8062, Train Acc:0.6648\n",
      "Epoch [1/10], Step [492/600], Loss: 0.8579, Train Acc:0.6650\n",
      "Epoch [1/10], Step [493/600], Loss: 0.8604, Train Acc:0.6652\n",
      "Epoch [1/10], Step [494/600], Loss: 0.8608, Train Acc:0.6654\n",
      "Epoch [1/10], Step [495/600], Loss: 0.8341, Train Acc:0.6655\n",
      "Epoch [1/10], Step [496/600], Loss: 0.7694, Train Acc:0.6657\n",
      "Epoch [1/10], Step [497/600], Loss: 0.7967, Train Acc:0.6660\n",
      "Epoch [1/10], Step [498/600], Loss: 0.8596, Train Acc:0.6662\n",
      "Epoch [1/10], Step [499/600], Loss: 0.9336, Train Acc:0.6663\n",
      "Epoch [1/10], Step [500/600], Loss: 0.8128, Train Acc:0.6664\n",
      "Epoch [1/10], Step [501/600], Loss: 0.9301, Train Acc:0.6664\n",
      "Epoch [1/10], Step [502/600], Loss: 0.8668, Train Acc:0.6665\n",
      "Epoch [1/10], Step [503/600], Loss: 0.8444, Train Acc:0.6667\n",
      "Epoch [1/10], Step [504/600], Loss: 0.8060, Train Acc:0.6669\n",
      "Epoch [1/10], Step [505/600], Loss: 0.7512, Train Acc:0.6671\n",
      "Epoch [1/10], Step [506/600], Loss: 0.7838, Train Acc:0.6673\n",
      "Epoch [1/10], Step [507/600], Loss: 0.8667, Train Acc:0.6674\n",
      "Epoch [1/10], Step [508/600], Loss: 0.7792, Train Acc:0.6675\n",
      "Epoch [1/10], Step [509/600], Loss: 0.8104, Train Acc:0.6677\n",
      "Epoch [1/10], Step [510/600], Loss: 0.8657, Train Acc:0.6678\n",
      "Epoch [1/10], Step [511/600], Loss: 0.7672, Train Acc:0.6680\n",
      "Epoch [1/10], Step [512/600], Loss: 0.8180, Train Acc:0.6682\n",
      "Epoch [1/10], Step [513/600], Loss: 0.8050, Train Acc:0.6683\n",
      "Epoch [1/10], Step [514/600], Loss: 0.8880, Train Acc:0.6684\n",
      "Epoch [1/10], Step [515/600], Loss: 0.8617, Train Acc:0.6685\n",
      "Epoch [1/10], Step [516/600], Loss: 0.7140, Train Acc:0.6688\n",
      "Epoch [1/10], Step [517/600], Loss: 0.7760, Train Acc:0.6691\n",
      "Epoch [1/10], Step [518/600], Loss: 0.8095, Train Acc:0.6693\n",
      "Epoch [1/10], Step [519/600], Loss: 0.7693, Train Acc:0.6696\n",
      "Epoch [1/10], Step [520/600], Loss: 0.7842, Train Acc:0.6698\n",
      "Epoch [1/10], Step [521/600], Loss: 0.8560, Train Acc:0.6699\n",
      "Epoch [1/10], Step [522/600], Loss: 0.8104, Train Acc:0.6701\n",
      "Epoch [1/10], Step [523/600], Loss: 0.7888, Train Acc:0.6703\n",
      "Epoch [1/10], Step [524/600], Loss: 0.9048, Train Acc:0.6703\n",
      "Epoch [1/10], Step [525/600], Loss: 0.8168, Train Acc:0.6704\n",
      "Epoch [1/10], Step [526/600], Loss: 0.9900, Train Acc:0.6704\n",
      "Epoch [1/10], Step [527/600], Loss: 0.7247, Train Acc:0.6707\n",
      "Epoch [1/10], Step [528/600], Loss: 0.7351, Train Acc:0.6708\n",
      "Epoch [1/10], Step [529/600], Loss: 0.8384, Train Acc:0.6709\n",
      "Epoch [1/10], Step [530/600], Loss: 0.8493, Train Acc:0.6710\n",
      "Epoch [1/10], Step [531/600], Loss: 0.8333, Train Acc:0.6711\n",
      "Epoch [1/10], Step [532/600], Loss: 0.9579, Train Acc:0.6711\n",
      "Epoch [1/10], Step [533/600], Loss: 0.8356, Train Acc:0.6712\n",
      "Epoch [1/10], Step [534/600], Loss: 0.7281, Train Acc:0.6715\n",
      "Epoch [1/10], Step [535/600], Loss: 0.7978, Train Acc:0.6716\n",
      "Epoch [1/10], Step [536/600], Loss: 0.8209, Train Acc:0.6718\n",
      "Epoch [1/10], Step [537/600], Loss: 0.9337, Train Acc:0.6718\n",
      "Epoch [1/10], Step [538/600], Loss: 0.8118, Train Acc:0.6720\n",
      "Epoch [1/10], Step [539/600], Loss: 0.8175, Train Acc:0.6722\n",
      "Epoch [1/10], Step [540/600], Loss: 0.7837, Train Acc:0.6724\n",
      "Epoch [1/10], Step [541/600], Loss: 0.7610, Train Acc:0.6726\n",
      "Epoch [1/10], Step [542/600], Loss: 0.8767, Train Acc:0.6727\n",
      "Epoch [1/10], Step [543/600], Loss: 0.8127, Train Acc:0.6728\n",
      "Epoch [1/10], Step [544/600], Loss: 0.7747, Train Acc:0.6730\n",
      "Epoch [1/10], Step [545/600], Loss: 0.9272, Train Acc:0.6730\n",
      "Epoch [1/10], Step [546/600], Loss: 0.8759, Train Acc:0.6731\n",
      "Epoch [1/10], Step [547/600], Loss: 0.7131, Train Acc:0.6733\n",
      "Epoch [1/10], Step [548/600], Loss: 0.8578, Train Acc:0.6734\n",
      "Epoch [1/10], Step [549/600], Loss: 0.8311, Train Acc:0.6734\n",
      "Epoch [1/10], Step [550/600], Loss: 0.8160, Train Acc:0.6735\n",
      "Epoch [1/10], Step [551/600], Loss: 0.8031, Train Acc:0.6737\n",
      "Epoch [1/10], Step [552/600], Loss: 0.8173, Train Acc:0.6738\n",
      "Epoch [1/10], Step [553/600], Loss: 0.7750, Train Acc:0.6739\n",
      "Epoch [1/10], Step [554/600], Loss: 0.8722, Train Acc:0.6740\n",
      "Epoch [1/10], Step [555/600], Loss: 0.8483, Train Acc:0.6741\n",
      "Epoch [1/10], Step [556/600], Loss: 0.9164, Train Acc:0.6741\n",
      "Epoch [1/10], Step [557/600], Loss: 0.8451, Train Acc:0.6742\n",
      "Epoch [1/10], Step [558/600], Loss: 0.7389, Train Acc:0.6744\n",
      "Epoch [1/10], Step [559/600], Loss: 0.8286, Train Acc:0.6745\n",
      "Epoch [1/10], Step [560/600], Loss: 0.7080, Train Acc:0.6748\n",
      "Epoch [1/10], Step [561/600], Loss: 0.8672, Train Acc:0.6749\n",
      "Epoch [1/10], Step [562/600], Loss: 0.7810, Train Acc:0.6752\n",
      "Epoch [1/10], Step [563/600], Loss: 0.8717, Train Acc:0.6752\n",
      "Epoch [1/10], Step [564/600], Loss: 0.7854, Train Acc:0.6754\n",
      "Epoch [1/10], Step [565/600], Loss: 0.8321, Train Acc:0.6755\n",
      "Epoch [1/10], Step [566/600], Loss: 0.7345, Train Acc:0.6757\n",
      "Epoch [1/10], Step [567/600], Loss: 0.8670, Train Acc:0.6758\n",
      "Epoch [1/10], Step [568/600], Loss: 0.8500, Train Acc:0.6759\n",
      "Epoch [1/10], Step [569/600], Loss: 0.8904, Train Acc:0.6759\n",
      "Epoch [1/10], Step [570/600], Loss: 0.7213, Train Acc:0.6761\n",
      "Epoch [1/10], Step [571/600], Loss: 0.7091, Train Acc:0.6763\n",
      "Epoch [1/10], Step [572/600], Loss: 0.8092, Train Acc:0.6765\n",
      "Epoch [1/10], Step [573/600], Loss: 0.9009, Train Acc:0.6765\n",
      "Epoch [1/10], Step [574/600], Loss: 0.7084, Train Acc:0.6767\n",
      "Epoch [1/10], Step [575/600], Loss: 0.6279, Train Acc:0.6770\n",
      "Epoch [1/10], Step [576/600], Loss: 0.9441, Train Acc:0.6771\n",
      "Epoch [1/10], Step [577/600], Loss: 0.7799, Train Acc:0.6773\n",
      "Epoch [1/10], Step [578/600], Loss: 0.6539, Train Acc:0.6776\n",
      "Epoch [1/10], Step [579/600], Loss: 0.7946, Train Acc:0.6777\n",
      "Epoch [1/10], Step [580/600], Loss: 0.8181, Train Acc:0.6778\n",
      "Epoch [1/10], Step [581/600], Loss: 0.7190, Train Acc:0.6779\n",
      "Epoch [1/10], Step [582/600], Loss: 0.8629, Train Acc:0.6780\n",
      "Epoch [1/10], Step [583/600], Loss: 0.7403, Train Acc:0.6781\n",
      "Epoch [1/10], Step [584/600], Loss: 0.8242, Train Acc:0.6782\n",
      "Epoch [1/10], Step [585/600], Loss: 0.8096, Train Acc:0.6784\n",
      "Epoch [1/10], Step [586/600], Loss: 0.8084, Train Acc:0.6786\n",
      "Epoch [1/10], Step [587/600], Loss: 0.7701, Train Acc:0.6787\n",
      "Epoch [1/10], Step [588/600], Loss: 0.7838, Train Acc:0.6789\n",
      "Epoch [1/10], Step [589/600], Loss: 0.8239, Train Acc:0.6790\n",
      "Epoch [1/10], Step [590/600], Loss: 0.8057, Train Acc:0.6791\n",
      "Epoch [1/10], Step [591/600], Loss: 0.7534, Train Acc:0.6793\n",
      "Epoch [1/10], Step [592/600], Loss: 0.7929, Train Acc:0.6794\n",
      "Epoch [1/10], Step [593/600], Loss: 0.8133, Train Acc:0.6795\n",
      "Epoch [1/10], Step [594/600], Loss: 0.8061, Train Acc:0.6796\n",
      "Epoch [1/10], Step [595/600], Loss: 0.7823, Train Acc:0.6797\n",
      "Epoch [1/10], Step [596/600], Loss: 0.8881, Train Acc:0.6797\n",
      "Epoch [1/10], Step [597/600], Loss: 0.8359, Train Acc:0.6799\n",
      "Epoch [1/10], Step [598/600], Loss: 0.7829, Train Acc:0.6800\n",
      "Epoch [1/10], Step [599/600], Loss: 0.8448, Train Acc:0.6801\n",
      "Epoch [1/10], Step [600/600], Loss: 0.8906, Train Acc:0.6801\n",
      "Epoch [2/10], Step [1/600], Loss: 0.8507, Train Acc:0.7100\n",
      "Epoch [2/10], Step [2/600], Loss: 0.7294, Train Acc:0.7250\n",
      "Epoch [2/10], Step [3/600], Loss: 0.8542, Train Acc:0.7200\n",
      "Epoch [2/10], Step [4/600], Loss: 0.8188, Train Acc:0.7200\n",
      "Epoch [2/10], Step [5/600], Loss: 0.6539, Train Acc:0.7420\n",
      "Epoch [2/10], Step [6/600], Loss: 0.7839, Train Acc:0.7533\n",
      "Epoch [2/10], Step [7/600], Loss: 0.7110, Train Acc:0.7657\n",
      "Epoch [2/10], Step [8/600], Loss: 0.8716, Train Acc:0.7588\n",
      "Epoch [2/10], Step [9/600], Loss: 0.8231, Train Acc:0.7544\n",
      "Epoch [2/10], Step [10/600], Loss: 0.8653, Train Acc:0.7460\n",
      "Epoch [2/10], Step [11/600], Loss: 0.8291, Train Acc:0.7409\n",
      "Epoch [2/10], Step [12/600], Loss: 0.8033, Train Acc:0.7367\n",
      "Epoch [2/10], Step [13/600], Loss: 0.7745, Train Acc:0.7377\n",
      "Epoch [2/10], Step [14/600], Loss: 0.7956, Train Acc:0.7400\n",
      "Epoch [2/10], Step [15/600], Loss: 0.8494, Train Acc:0.7380\n",
      "Epoch [2/10], Step [16/600], Loss: 0.7406, Train Acc:0.7375\n",
      "Epoch [2/10], Step [17/600], Loss: 0.9052, Train Acc:0.7341\n",
      "Epoch [2/10], Step [18/600], Loss: 0.7614, Train Acc:0.7344\n",
      "Epoch [2/10], Step [19/600], Loss: 0.8385, Train Acc:0.7363\n",
      "Epoch [2/10], Step [20/600], Loss: 0.7528, Train Acc:0.7400\n",
      "Epoch [2/10], Step [21/600], Loss: 0.7861, Train Acc:0.7414\n",
      "Epoch [2/10], Step [22/600], Loss: 0.8283, Train Acc:0.7386\n",
      "Epoch [2/10], Step [23/600], Loss: 0.8524, Train Acc:0.7400\n",
      "Epoch [2/10], Step [24/600], Loss: 0.7738, Train Acc:0.7408\n",
      "Epoch [2/10], Step [25/600], Loss: 0.6948, Train Acc:0.7420\n",
      "Epoch [2/10], Step [26/600], Loss: 0.8333, Train Acc:0.7431\n",
      "Epoch [2/10], Step [27/600], Loss: 0.8096, Train Acc:0.7444\n",
      "Epoch [2/10], Step [28/600], Loss: 0.8554, Train Acc:0.7429\n",
      "Epoch [2/10], Step [29/600], Loss: 0.7464, Train Acc:0.7455\n",
      "Epoch [2/10], Step [30/600], Loss: 0.6030, Train Acc:0.7487\n",
      "Epoch [2/10], Step [31/600], Loss: 0.7520, Train Acc:0.7503\n",
      "Epoch [2/10], Step [32/600], Loss: 0.6928, Train Acc:0.7519\n",
      "Epoch [2/10], Step [33/600], Loss: 0.8448, Train Acc:0.7500\n",
      "Epoch [2/10], Step [34/600], Loss: 0.7700, Train Acc:0.7509\n",
      "Epoch [2/10], Step [35/600], Loss: 0.6357, Train Acc:0.7529\n",
      "Epoch [2/10], Step [36/600], Loss: 0.6988, Train Acc:0.7539\n",
      "Epoch [2/10], Step [37/600], Loss: 0.7021, Train Acc:0.7538\n",
      "Epoch [2/10], Step [38/600], Loss: 0.8725, Train Acc:0.7516\n",
      "Epoch [2/10], Step [39/600], Loss: 0.7411, Train Acc:0.7518\n",
      "Epoch [2/10], Step [40/600], Loss: 0.6968, Train Acc:0.7528\n",
      "Epoch [2/10], Step [41/600], Loss: 0.9114, Train Acc:0.7512\n",
      "Epoch [2/10], Step [42/600], Loss: 0.6959, Train Acc:0.7536\n",
      "Epoch [2/10], Step [43/600], Loss: 0.8995, Train Acc:0.7521\n",
      "Epoch [2/10], Step [44/600], Loss: 0.8595, Train Acc:0.7518\n",
      "Epoch [2/10], Step [45/600], Loss: 0.7651, Train Acc:0.7520\n",
      "Epoch [2/10], Step [46/600], Loss: 0.7145, Train Acc:0.7533\n",
      "Epoch [2/10], Step [47/600], Loss: 0.7799, Train Acc:0.7534\n",
      "Epoch [2/10], Step [48/600], Loss: 0.7677, Train Acc:0.7542\n",
      "Epoch [2/10], Step [49/600], Loss: 0.8238, Train Acc:0.7531\n",
      "Epoch [2/10], Step [50/600], Loss: 0.7761, Train Acc:0.7534\n",
      "Epoch [2/10], Step [51/600], Loss: 0.7512, Train Acc:0.7541\n",
      "Epoch [2/10], Step [52/600], Loss: 0.9291, Train Acc:0.7527\n",
      "Epoch [2/10], Step [53/600], Loss: 0.7538, Train Acc:0.7515\n",
      "Epoch [2/10], Step [54/600], Loss: 1.0234, Train Acc:0.7494\n",
      "Epoch [2/10], Step [55/600], Loss: 0.7609, Train Acc:0.7498\n",
      "Epoch [2/10], Step [56/600], Loss: 0.8821, Train Acc:0.7491\n",
      "Epoch [2/10], Step [57/600], Loss: 0.6825, Train Acc:0.7498\n",
      "Epoch [2/10], Step [58/600], Loss: 0.7399, Train Acc:0.7509\n",
      "Epoch [2/10], Step [59/600], Loss: 0.6918, Train Acc:0.7520\n",
      "Epoch [2/10], Step [60/600], Loss: 0.8606, Train Acc:0.7507\n",
      "Epoch [2/10], Step [61/600], Loss: 0.7190, Train Acc:0.7520\n",
      "Epoch [2/10], Step [62/600], Loss: 0.7924, Train Acc:0.7518\n",
      "Epoch [2/10], Step [63/600], Loss: 0.8064, Train Acc:0.7516\n",
      "Epoch [2/10], Step [64/600], Loss: 0.8585, Train Acc:0.7508\n",
      "Epoch [2/10], Step [65/600], Loss: 0.7321, Train Acc:0.7511\n",
      "Epoch [2/10], Step [66/600], Loss: 0.6944, Train Acc:0.7514\n",
      "Epoch [2/10], Step [67/600], Loss: 0.8291, Train Acc:0.7512\n",
      "Epoch [2/10], Step [68/600], Loss: 0.8968, Train Acc:0.7509\n",
      "Epoch [2/10], Step [69/600], Loss: 0.7232, Train Acc:0.7517\n",
      "Epoch [2/10], Step [70/600], Loss: 0.8189, Train Acc:0.7510\n",
      "Epoch [2/10], Step [71/600], Loss: 0.8036, Train Acc:0.7515\n",
      "Epoch [2/10], Step [72/600], Loss: 0.7197, Train Acc:0.7522\n",
      "Epoch [2/10], Step [73/600], Loss: 0.8133, Train Acc:0.7516\n",
      "Epoch [2/10], Step [74/600], Loss: 0.7364, Train Acc:0.7520\n",
      "Epoch [2/10], Step [75/600], Loss: 0.7722, Train Acc:0.7523\n",
      "Epoch [2/10], Step [76/600], Loss: 0.8097, Train Acc:0.7524\n",
      "Epoch [2/10], Step [77/600], Loss: 0.7280, Train Acc:0.7529\n",
      "Epoch [2/10], Step [78/600], Loss: 0.7537, Train Acc:0.7533\n",
      "Epoch [2/10], Step [79/600], Loss: 0.8670, Train Acc:0.7535\n",
      "Epoch [2/10], Step [80/600], Loss: 0.7896, Train Acc:0.7541\n",
      "Epoch [2/10], Step [81/600], Loss: 0.8954, Train Acc:0.7542\n",
      "Epoch [2/10], Step [82/600], Loss: 0.8153, Train Acc:0.7539\n",
      "Epoch [2/10], Step [83/600], Loss: 0.7555, Train Acc:0.7539\n",
      "Epoch [2/10], Step [84/600], Loss: 0.7643, Train Acc:0.7542\n",
      "Epoch [2/10], Step [85/600], Loss: 0.7935, Train Acc:0.7536\n",
      "Epoch [2/10], Step [86/600], Loss: 0.7127, Train Acc:0.7537\n",
      "Epoch [2/10], Step [87/600], Loss: 0.6525, Train Acc:0.7548\n",
      "Epoch [2/10], Step [88/600], Loss: 0.9934, Train Acc:0.7541\n",
      "Epoch [2/10], Step [89/600], Loss: 0.8219, Train Acc:0.7539\n",
      "Epoch [2/10], Step [90/600], Loss: 0.8044, Train Acc:0.7531\n",
      "Epoch [2/10], Step [91/600], Loss: 0.8426, Train Acc:0.7520\n",
      "Epoch [2/10], Step [92/600], Loss: 0.7631, Train Acc:0.7523\n",
      "Epoch [2/10], Step [93/600], Loss: 0.6742, Train Acc:0.7525\n",
      "Epoch [2/10], Step [94/600], Loss: 0.7212, Train Acc:0.7528\n",
      "Epoch [2/10], Step [95/600], Loss: 0.6593, Train Acc:0.7537\n",
      "Epoch [2/10], Step [96/600], Loss: 0.6716, Train Acc:0.7544\n",
      "Epoch [2/10], Step [97/600], Loss: 0.6660, Train Acc:0.7553\n",
      "Epoch [2/10], Step [98/600], Loss: 0.6995, Train Acc:0.7557\n",
      "Epoch [2/10], Step [99/600], Loss: 0.7800, Train Acc:0.7557\n",
      "Epoch [2/10], Step [100/600], Loss: 0.6846, Train Acc:0.7559\n",
      "Epoch [2/10], Step [101/600], Loss: 0.9018, Train Acc:0.7552\n",
      "Epoch [2/10], Step [102/600], Loss: 0.7217, Train Acc:0.7555\n",
      "Epoch [2/10], Step [103/600], Loss: 0.7715, Train Acc:0.7553\n",
      "Epoch [2/10], Step [104/600], Loss: 0.7780, Train Acc:0.7551\n",
      "Epoch [2/10], Step [105/600], Loss: 0.7562, Train Acc:0.7557\n",
      "Epoch [2/10], Step [106/600], Loss: 0.7909, Train Acc:0.7558\n",
      "Epoch [2/10], Step [107/600], Loss: 0.7098, Train Acc:0.7559\n",
      "Epoch [2/10], Step [108/600], Loss: 0.7515, Train Acc:0.7558\n",
      "Epoch [2/10], Step [109/600], Loss: 0.7179, Train Acc:0.7559\n",
      "Epoch [2/10], Step [110/600], Loss: 0.7957, Train Acc:0.7556\n",
      "Epoch [2/10], Step [111/600], Loss: 0.8361, Train Acc:0.7554\n",
      "Epoch [2/10], Step [112/600], Loss: 0.7400, Train Acc:0.7553\n",
      "Epoch [2/10], Step [113/600], Loss: 0.7553, Train Acc:0.7553\n",
      "Epoch [2/10], Step [114/600], Loss: 0.7368, Train Acc:0.7559\n",
      "Epoch [2/10], Step [115/600], Loss: 0.7450, Train Acc:0.7558\n",
      "Epoch [2/10], Step [116/600], Loss: 0.7207, Train Acc:0.7562\n",
      "Epoch [2/10], Step [117/600], Loss: 0.6473, Train Acc:0.7568\n",
      "Epoch [2/10], Step [118/600], Loss: 0.7911, Train Acc:0.7569\n",
      "Epoch [2/10], Step [119/600], Loss: 0.7821, Train Acc:0.7570\n",
      "Epoch [2/10], Step [120/600], Loss: 0.6852, Train Acc:0.7575\n",
      "Epoch [2/10], Step [121/600], Loss: 0.7101, Train Acc:0.7580\n",
      "Epoch [2/10], Step [122/600], Loss: 0.7437, Train Acc:0.7580\n",
      "Epoch [2/10], Step [123/600], Loss: 0.7967, Train Acc:0.7578\n",
      "Epoch [2/10], Step [124/600], Loss: 0.7948, Train Acc:0.7577\n",
      "Epoch [2/10], Step [125/600], Loss: 0.6282, Train Acc:0.7581\n",
      "Epoch [2/10], Step [126/600], Loss: 0.7628, Train Acc:0.7579\n",
      "Epoch [2/10], Step [127/600], Loss: 0.8234, Train Acc:0.7579\n",
      "Epoch [2/10], Step [128/600], Loss: 0.7765, Train Acc:0.7577\n",
      "Epoch [2/10], Step [129/600], Loss: 0.7402, Train Acc:0.7581\n",
      "Epoch [2/10], Step [130/600], Loss: 0.9270, Train Acc:0.7573\n",
      "Epoch [2/10], Step [131/600], Loss: 0.7899, Train Acc:0.7572\n",
      "Epoch [2/10], Step [132/600], Loss: 0.7426, Train Acc:0.7572\n",
      "Epoch [2/10], Step [133/600], Loss: 0.7389, Train Acc:0.7570\n",
      "Epoch [2/10], Step [134/600], Loss: 0.6774, Train Acc:0.7572\n",
      "Epoch [2/10], Step [135/600], Loss: 0.7141, Train Acc:0.7573\n",
      "Epoch [2/10], Step [136/600], Loss: 0.7317, Train Acc:0.7575\n",
      "Epoch [2/10], Step [137/600], Loss: 0.6769, Train Acc:0.7579\n",
      "Epoch [2/10], Step [138/600], Loss: 0.7542, Train Acc:0.7584\n",
      "Epoch [2/10], Step [139/600], Loss: 0.8105, Train Acc:0.7586\n",
      "Epoch [2/10], Step [140/600], Loss: 0.7768, Train Acc:0.7583\n",
      "Epoch [2/10], Step [141/600], Loss: 0.6494, Train Acc:0.7587\n",
      "Epoch [2/10], Step [142/600], Loss: 0.6291, Train Acc:0.7594\n",
      "Epoch [2/10], Step [143/600], Loss: 0.7685, Train Acc:0.7594\n",
      "Epoch [2/10], Step [144/600], Loss: 0.7275, Train Acc:0.7593\n",
      "Epoch [2/10], Step [145/600], Loss: 0.9174, Train Acc:0.7590\n",
      "Epoch [2/10], Step [146/600], Loss: 0.7947, Train Acc:0.7590\n",
      "Epoch [2/10], Step [147/600], Loss: 0.7097, Train Acc:0.7590\n",
      "Epoch [2/10], Step [148/600], Loss: 0.8880, Train Acc:0.7588\n",
      "Epoch [2/10], Step [149/600], Loss: 0.7761, Train Acc:0.7587\n",
      "Epoch [2/10], Step [150/600], Loss: 0.7040, Train Acc:0.7587\n",
      "Epoch [2/10], Step [151/600], Loss: 0.7096, Train Acc:0.7591\n",
      "Epoch [2/10], Step [152/600], Loss: 0.7324, Train Acc:0.7590\n",
      "Epoch [2/10], Step [153/600], Loss: 0.7252, Train Acc:0.7591\n",
      "Epoch [2/10], Step [154/600], Loss: 0.8406, Train Acc:0.7587\n",
      "Epoch [2/10], Step [155/600], Loss: 0.6551, Train Acc:0.7588\n",
      "Epoch [2/10], Step [156/600], Loss: 0.8657, Train Acc:0.7583\n",
      "Epoch [2/10], Step [157/600], Loss: 0.8242, Train Acc:0.7582\n",
      "Epoch [2/10], Step [158/600], Loss: 0.7450, Train Acc:0.7584\n",
      "Epoch [2/10], Step [159/600], Loss: 0.7299, Train Acc:0.7585\n",
      "Epoch [2/10], Step [160/600], Loss: 0.7196, Train Acc:0.7587\n",
      "Epoch [2/10], Step [161/600], Loss: 0.7349, Train Acc:0.7588\n",
      "Epoch [2/10], Step [162/600], Loss: 0.7463, Train Acc:0.7589\n",
      "Epoch [2/10], Step [163/600], Loss: 0.7259, Train Acc:0.7590\n",
      "Epoch [2/10], Step [164/600], Loss: 0.8166, Train Acc:0.7585\n",
      "Epoch [2/10], Step [165/600], Loss: 0.7256, Train Acc:0.7585\n",
      "Epoch [2/10], Step [166/600], Loss: 0.7506, Train Acc:0.7587\n",
      "Epoch [2/10], Step [167/600], Loss: 0.6542, Train Acc:0.7587\n",
      "Epoch [2/10], Step [168/600], Loss: 0.6626, Train Acc:0.7592\n",
      "Epoch [2/10], Step [169/600], Loss: 0.6998, Train Acc:0.7594\n",
      "Epoch [2/10], Step [170/600], Loss: 0.6953, Train Acc:0.7596\n",
      "Epoch [2/10], Step [171/600], Loss: 0.7893, Train Acc:0.7599\n",
      "Epoch [2/10], Step [172/600], Loss: 0.8224, Train Acc:0.7598\n",
      "Epoch [2/10], Step [173/600], Loss: 0.7139, Train Acc:0.7600\n",
      "Epoch [2/10], Step [174/600], Loss: 0.8117, Train Acc:0.7595\n",
      "Epoch [2/10], Step [175/600], Loss: 0.6989, Train Acc:0.7595\n",
      "Epoch [2/10], Step [176/600], Loss: 0.6964, Train Acc:0.7597\n",
      "Epoch [2/10], Step [177/600], Loss: 0.6420, Train Acc:0.7598\n",
      "Epoch [2/10], Step [178/600], Loss: 0.7223, Train Acc:0.7599\n",
      "Epoch [2/10], Step [179/600], Loss: 0.6725, Train Acc:0.7599\n",
      "Epoch [2/10], Step [180/600], Loss: 0.7527, Train Acc:0.7598\n",
      "Epoch [2/10], Step [181/600], Loss: 0.6262, Train Acc:0.7602\n",
      "Epoch [2/10], Step [182/600], Loss: 0.7406, Train Acc:0.7600\n",
      "Epoch [2/10], Step [183/600], Loss: 0.7894, Train Acc:0.7603\n",
      "Epoch [2/10], Step [184/600], Loss: 0.6493, Train Acc:0.7605\n",
      "Epoch [2/10], Step [185/600], Loss: 0.6421, Train Acc:0.7608\n",
      "Epoch [2/10], Step [186/600], Loss: 0.8234, Train Acc:0.7605\n",
      "Epoch [2/10], Step [187/600], Loss: 0.7512, Train Acc:0.7607\n",
      "Epoch [2/10], Step [188/600], Loss: 0.7777, Train Acc:0.7610\n",
      "Epoch [2/10], Step [189/600], Loss: 0.7499, Train Acc:0.7611\n",
      "Epoch [2/10], Step [190/600], Loss: 0.7384, Train Acc:0.7611\n",
      "Epoch [2/10], Step [191/600], Loss: 0.7336, Train Acc:0.7612\n",
      "Epoch [2/10], Step [192/600], Loss: 0.7637, Train Acc:0.7611\n",
      "Epoch [2/10], Step [193/600], Loss: 0.6892, Train Acc:0.7613\n",
      "Epoch [2/10], Step [194/600], Loss: 0.7284, Train Acc:0.7614\n",
      "Epoch [2/10], Step [195/600], Loss: 0.7123, Train Acc:0.7616\n",
      "Epoch [2/10], Step [196/600], Loss: 0.7875, Train Acc:0.7613\n",
      "Epoch [2/10], Step [197/600], Loss: 0.9009, Train Acc:0.7608\n",
      "Epoch [2/10], Step [198/600], Loss: 0.7539, Train Acc:0.7608\n",
      "Epoch [2/10], Step [199/600], Loss: 0.7417, Train Acc:0.7608\n",
      "Epoch [2/10], Step [200/600], Loss: 0.6571, Train Acc:0.7611\n",
      "Epoch [2/10], Step [201/600], Loss: 0.7209, Train Acc:0.7611\n",
      "Epoch [2/10], Step [202/600], Loss: 0.7593, Train Acc:0.7613\n",
      "Epoch [2/10], Step [203/600], Loss: 0.7144, Train Acc:0.7615\n",
      "Epoch [2/10], Step [204/600], Loss: 0.7152, Train Acc:0.7615\n",
      "Epoch [2/10], Step [205/600], Loss: 0.9234, Train Acc:0.7613\n",
      "Epoch [2/10], Step [206/600], Loss: 0.6604, Train Acc:0.7616\n",
      "Epoch [2/10], Step [207/600], Loss: 0.7085, Train Acc:0.7617\n",
      "Epoch [2/10], Step [208/600], Loss: 0.7728, Train Acc:0.7620\n",
      "Epoch [2/10], Step [209/600], Loss: 0.7773, Train Acc:0.7620\n",
      "Epoch [2/10], Step [210/600], Loss: 0.8725, Train Acc:0.7617\n",
      "Epoch [2/10], Step [211/600], Loss: 0.7204, Train Acc:0.7617\n",
      "Epoch [2/10], Step [212/600], Loss: 0.7710, Train Acc:0.7615\n",
      "Epoch [2/10], Step [213/600], Loss: 0.7493, Train Acc:0.7615\n",
      "Epoch [2/10], Step [214/600], Loss: 0.7411, Train Acc:0.7615\n",
      "Epoch [2/10], Step [215/600], Loss: 0.7269, Train Acc:0.7616\n",
      "Epoch [2/10], Step [216/600], Loss: 0.7492, Train Acc:0.7618\n",
      "Epoch [2/10], Step [217/600], Loss: 0.7418, Train Acc:0.7617\n",
      "Epoch [2/10], Step [218/600], Loss: 0.8240, Train Acc:0.7615\n",
      "Epoch [2/10], Step [219/600], Loss: 0.7244, Train Acc:0.7614\n",
      "Epoch [2/10], Step [220/600], Loss: 0.7464, Train Acc:0.7617\n",
      "Epoch [2/10], Step [221/600], Loss: 0.8674, Train Acc:0.7616\n",
      "Epoch [2/10], Step [222/600], Loss: 0.6938, Train Acc:0.7617\n",
      "Epoch [2/10], Step [223/600], Loss: 0.7374, Train Acc:0.7618\n",
      "Epoch [2/10], Step [224/600], Loss: 0.7071, Train Acc:0.7620\n",
      "Epoch [2/10], Step [225/600], Loss: 0.5871, Train Acc:0.7623\n",
      "Epoch [2/10], Step [226/600], Loss: 0.8351, Train Acc:0.7620\n",
      "Epoch [2/10], Step [227/600], Loss: 0.7081, Train Acc:0.7620\n",
      "Epoch [2/10], Step [228/600], Loss: 0.7522, Train Acc:0.7619\n",
      "Epoch [2/10], Step [229/600], Loss: 0.7100, Train Acc:0.7621\n",
      "Epoch [2/10], Step [230/600], Loss: 0.8653, Train Acc:0.7620\n",
      "Epoch [2/10], Step [231/600], Loss: 0.8331, Train Acc:0.7619\n",
      "Epoch [2/10], Step [232/600], Loss: 0.6682, Train Acc:0.7621\n",
      "Epoch [2/10], Step [233/600], Loss: 0.8797, Train Acc:0.7619\n",
      "Epoch [2/10], Step [234/600], Loss: 0.7662, Train Acc:0.7618\n",
      "Epoch [2/10], Step [235/600], Loss: 0.7073, Train Acc:0.7618\n",
      "Epoch [2/10], Step [236/600], Loss: 0.7937, Train Acc:0.7617\n",
      "Epoch [2/10], Step [237/600], Loss: 0.7300, Train Acc:0.7616\n",
      "Epoch [2/10], Step [238/600], Loss: 0.7156, Train Acc:0.7618\n",
      "Epoch [2/10], Step [239/600], Loss: 0.6776, Train Acc:0.7618\n",
      "Epoch [2/10], Step [240/600], Loss: 0.7476, Train Acc:0.7620\n",
      "Epoch [2/10], Step [241/600], Loss: 0.5823, Train Acc:0.7622\n",
      "Epoch [2/10], Step [242/600], Loss: 0.7979, Train Acc:0.7620\n",
      "Epoch [2/10], Step [243/600], Loss: 0.7180, Train Acc:0.7619\n",
      "Epoch [2/10], Step [244/600], Loss: 0.7338, Train Acc:0.7618\n",
      "Epoch [2/10], Step [245/600], Loss: 0.7397, Train Acc:0.7617\n",
      "Epoch [2/10], Step [246/600], Loss: 0.7023, Train Acc:0.7619\n",
      "Epoch [2/10], Step [247/600], Loss: 0.8372, Train Acc:0.7619\n",
      "Epoch [2/10], Step [248/600], Loss: 0.6794, Train Acc:0.7620\n",
      "Epoch [2/10], Step [249/600], Loss: 0.6522, Train Acc:0.7623\n",
      "Epoch [2/10], Step [250/600], Loss: 0.7409, Train Acc:0.7622\n",
      "Epoch [2/10], Step [251/600], Loss: 0.8330, Train Acc:0.7622\n",
      "Epoch [2/10], Step [252/600], Loss: 0.7767, Train Acc:0.7622\n",
      "Epoch [2/10], Step [253/600], Loss: 0.7296, Train Acc:0.7621\n",
      "Epoch [2/10], Step [254/600], Loss: 0.7850, Train Acc:0.7621\n",
      "Epoch [2/10], Step [255/600], Loss: 0.5875, Train Acc:0.7624\n",
      "Epoch [2/10], Step [256/600], Loss: 0.6501, Train Acc:0.7627\n",
      "Epoch [2/10], Step [257/600], Loss: 0.7296, Train Acc:0.7627\n",
      "Epoch [2/10], Step [258/600], Loss: 0.6600, Train Acc:0.7629\n",
      "Epoch [2/10], Step [259/600], Loss: 0.8040, Train Acc:0.7627\n",
      "Epoch [2/10], Step [260/600], Loss: 0.7988, Train Acc:0.7628\n",
      "Epoch [2/10], Step [261/600], Loss: 0.6018, Train Acc:0.7629\n",
      "Epoch [2/10], Step [262/600], Loss: 0.7882, Train Acc:0.7627\n",
      "Epoch [2/10], Step [263/600], Loss: 0.8704, Train Acc:0.7627\n",
      "Epoch [2/10], Step [264/600], Loss: 0.7778, Train Acc:0.7625\n",
      "Epoch [2/10], Step [265/600], Loss: 0.7527, Train Acc:0.7625\n",
      "Epoch [2/10], Step [266/600], Loss: 0.7264, Train Acc:0.7629\n",
      "Epoch [2/10], Step [267/600], Loss: 0.7088, Train Acc:0.7631\n",
      "Epoch [2/10], Step [268/600], Loss: 0.7410, Train Acc:0.7632\n",
      "Epoch [2/10], Step [269/600], Loss: 0.6012, Train Acc:0.7633\n",
      "Epoch [2/10], Step [270/600], Loss: 0.6717, Train Acc:0.7634\n",
      "Epoch [2/10], Step [271/600], Loss: 0.7641, Train Acc:0.7637\n",
      "Epoch [2/10], Step [272/600], Loss: 0.8005, Train Acc:0.7634\n",
      "Epoch [2/10], Step [273/600], Loss: 0.7526, Train Acc:0.7634\n",
      "Epoch [2/10], Step [274/600], Loss: 0.8672, Train Acc:0.7633\n",
      "Epoch [2/10], Step [275/600], Loss: 0.7577, Train Acc:0.7633\n",
      "Epoch [2/10], Step [276/600], Loss: 0.6726, Train Acc:0.7634\n",
      "Epoch [2/10], Step [277/600], Loss: 0.6053, Train Acc:0.7636\n",
      "Epoch [2/10], Step [278/600], Loss: 0.6700, Train Acc:0.7638\n",
      "Epoch [2/10], Step [279/600], Loss: 0.7451, Train Acc:0.7639\n",
      "Epoch [2/10], Step [280/600], Loss: 0.7331, Train Acc:0.7637\n",
      "Epoch [2/10], Step [281/600], Loss: 0.7602, Train Acc:0.7637\n",
      "Epoch [2/10], Step [282/600], Loss: 0.7880, Train Acc:0.7636\n",
      "Epoch [2/10], Step [283/600], Loss: 0.7793, Train Acc:0.7635\n",
      "Epoch [2/10], Step [284/600], Loss: 0.6799, Train Acc:0.7634\n",
      "Epoch [2/10], Step [285/600], Loss: 0.7533, Train Acc:0.7633\n",
      "Epoch [2/10], Step [286/600], Loss: 0.6963, Train Acc:0.7633\n",
      "Epoch [2/10], Step [287/600], Loss: 0.7044, Train Acc:0.7634\n",
      "Epoch [2/10], Step [288/600], Loss: 0.7724, Train Acc:0.7633\n",
      "Epoch [2/10], Step [289/600], Loss: 0.6816, Train Acc:0.7632\n",
      "Epoch [2/10], Step [290/600], Loss: 0.7850, Train Acc:0.7632\n",
      "Epoch [2/10], Step [291/600], Loss: 0.7358, Train Acc:0.7632\n",
      "Epoch [2/10], Step [292/600], Loss: 0.8272, Train Acc:0.7630\n",
      "Epoch [2/10], Step [293/600], Loss: 0.8458, Train Acc:0.7629\n",
      "Epoch [2/10], Step [294/600], Loss: 0.8041, Train Acc:0.7628\n",
      "Epoch [2/10], Step [295/600], Loss: 0.9861, Train Acc:0.7625\n",
      "Epoch [2/10], Step [296/600], Loss: 0.6408, Train Acc:0.7627\n",
      "Epoch [2/10], Step [297/600], Loss: 0.7205, Train Acc:0.7628\n",
      "Epoch [2/10], Step [298/600], Loss: 0.7222, Train Acc:0.7628\n",
      "Epoch [2/10], Step [299/600], Loss: 0.8573, Train Acc:0.7625\n",
      "Epoch [2/10], Step [300/600], Loss: 0.7641, Train Acc:0.7625\n",
      "Epoch [2/10], Step [301/600], Loss: 0.9369, Train Acc:0.7622\n",
      "Epoch [2/10], Step [302/600], Loss: 0.6529, Train Acc:0.7624\n",
      "Epoch [2/10], Step [303/600], Loss: 0.7119, Train Acc:0.7625\n",
      "Epoch [2/10], Step [304/600], Loss: 0.6185, Train Acc:0.7627\n",
      "Epoch [2/10], Step [305/600], Loss: 0.7321, Train Acc:0.7628\n",
      "Epoch [2/10], Step [306/600], Loss: 0.7450, Train Acc:0.7627\n",
      "Epoch [2/10], Step [307/600], Loss: 0.7619, Train Acc:0.7627\n",
      "Epoch [2/10], Step [308/600], Loss: 0.7703, Train Acc:0.7627\n",
      "Epoch [2/10], Step [309/600], Loss: 0.6966, Train Acc:0.7626\n",
      "Epoch [2/10], Step [310/600], Loss: 0.7852, Train Acc:0.7625\n",
      "Epoch [2/10], Step [311/600], Loss: 0.7643, Train Acc:0.7624\n",
      "Epoch [2/10], Step [312/600], Loss: 0.7938, Train Acc:0.7625\n",
      "Epoch [2/10], Step [313/600], Loss: 0.8184, Train Acc:0.7626\n",
      "Epoch [2/10], Step [314/600], Loss: 0.7462, Train Acc:0.7625\n",
      "Epoch [2/10], Step [315/600], Loss: 0.7405, Train Acc:0.7624\n",
      "Epoch [2/10], Step [316/600], Loss: 0.5907, Train Acc:0.7626\n",
      "Epoch [2/10], Step [317/600], Loss: 0.7207, Train Acc:0.7626\n",
      "Epoch [2/10], Step [318/600], Loss: 0.7066, Train Acc:0.7625\n",
      "Epoch [2/10], Step [319/600], Loss: 0.6378, Train Acc:0.7627\n",
      "Epoch [2/10], Step [320/600], Loss: 0.5098, Train Acc:0.7630\n",
      "Epoch [2/10], Step [321/600], Loss: 0.6491, Train Acc:0.7630\n",
      "Epoch [2/10], Step [322/600], Loss: 0.7415, Train Acc:0.7629\n",
      "Epoch [2/10], Step [323/600], Loss: 0.6956, Train Acc:0.7629\n",
      "Epoch [2/10], Step [324/600], Loss: 0.7614, Train Acc:0.7630\n",
      "Epoch [2/10], Step [325/600], Loss: 0.7142, Train Acc:0.7630\n",
      "Epoch [2/10], Step [326/600], Loss: 0.7664, Train Acc:0.7632\n",
      "Epoch [2/10], Step [327/600], Loss: 0.6791, Train Acc:0.7633\n",
      "Epoch [2/10], Step [328/600], Loss: 0.7262, Train Acc:0.7634\n",
      "Epoch [2/10], Step [329/600], Loss: 0.7609, Train Acc:0.7634\n",
      "Epoch [2/10], Step [330/600], Loss: 0.6854, Train Acc:0.7636\n",
      "Epoch [2/10], Step [331/600], Loss: 0.6788, Train Acc:0.7636\n",
      "Epoch [2/10], Step [332/600], Loss: 0.7007, Train Acc:0.7636\n",
      "Epoch [2/10], Step [333/600], Loss: 0.6862, Train Acc:0.7637\n",
      "Epoch [2/10], Step [334/600], Loss: 0.7839, Train Acc:0.7635\n",
      "Epoch [2/10], Step [335/600], Loss: 0.7076, Train Acc:0.7637\n",
      "Epoch [2/10], Step [336/600], Loss: 0.6817, Train Acc:0.7639\n",
      "Epoch [2/10], Step [337/600], Loss: 0.7458, Train Acc:0.7640\n",
      "Epoch [2/10], Step [338/600], Loss: 0.9105, Train Acc:0.7639\n",
      "Epoch [2/10], Step [339/600], Loss: 0.6755, Train Acc:0.7640\n",
      "Epoch [2/10], Step [340/600], Loss: 0.7230, Train Acc:0.7640\n",
      "Epoch [2/10], Step [341/600], Loss: 0.8090, Train Acc:0.7640\n",
      "Epoch [2/10], Step [342/600], Loss: 0.6904, Train Acc:0.7640\n",
      "Epoch [2/10], Step [343/600], Loss: 0.6134, Train Acc:0.7642\n",
      "Epoch [2/10], Step [344/600], Loss: 0.7515, Train Acc:0.7640\n",
      "Epoch [2/10], Step [345/600], Loss: 0.7244, Train Acc:0.7640\n",
      "Epoch [2/10], Step [346/600], Loss: 0.6728, Train Acc:0.7641\n",
      "Epoch [2/10], Step [347/600], Loss: 0.7895, Train Acc:0.7641\n",
      "Epoch [2/10], Step [348/600], Loss: 0.8015, Train Acc:0.7640\n",
      "Epoch [2/10], Step [349/600], Loss: 0.5990, Train Acc:0.7643\n",
      "Epoch [2/10], Step [350/600], Loss: 0.7721, Train Acc:0.7643\n",
      "Epoch [2/10], Step [351/600], Loss: 0.6620, Train Acc:0.7644\n",
      "Epoch [2/10], Step [352/600], Loss: 0.6902, Train Acc:0.7645\n",
      "Epoch [2/10], Step [353/600], Loss: 0.8335, Train Acc:0.7645\n",
      "Epoch [2/10], Step [354/600], Loss: 0.6805, Train Acc:0.7645\n",
      "Epoch [2/10], Step [355/600], Loss: 0.6817, Train Acc:0.7646\n",
      "Epoch [2/10], Step [356/600], Loss: 0.7602, Train Acc:0.7646\n",
      "Epoch [2/10], Step [357/600], Loss: 0.5983, Train Acc:0.7647\n",
      "Epoch [2/10], Step [358/600], Loss: 0.6620, Train Acc:0.7647\n",
      "Epoch [2/10], Step [359/600], Loss: 0.8170, Train Acc:0.7648\n",
      "Epoch [2/10], Step [360/600], Loss: 0.6916, Train Acc:0.7648\n",
      "Epoch [2/10], Step [361/600], Loss: 0.6993, Train Acc:0.7648\n",
      "Epoch [2/10], Step [362/600], Loss: 0.7803, Train Acc:0.7647\n",
      "Epoch [2/10], Step [363/600], Loss: 0.6800, Train Acc:0.7647\n",
      "Epoch [2/10], Step [364/600], Loss: 0.8436, Train Acc:0.7645\n",
      "Epoch [2/10], Step [365/600], Loss: 0.7505, Train Acc:0.7644\n",
      "Epoch [2/10], Step [366/600], Loss: 0.7283, Train Acc:0.7645\n",
      "Epoch [2/10], Step [367/600], Loss: 0.7528, Train Acc:0.7645\n",
      "Epoch [2/10], Step [368/600], Loss: 0.6850, Train Acc:0.7645\n",
      "Epoch [2/10], Step [369/600], Loss: 0.7146, Train Acc:0.7644\n",
      "Epoch [2/10], Step [370/600], Loss: 0.6841, Train Acc:0.7646\n",
      "Epoch [2/10], Step [371/600], Loss: 0.7573, Train Acc:0.7646\n",
      "Epoch [2/10], Step [372/600], Loss: 0.7207, Train Acc:0.7648\n",
      "Epoch [2/10], Step [373/600], Loss: 0.6539, Train Acc:0.7648\n",
      "Epoch [2/10], Step [374/600], Loss: 0.7272, Train Acc:0.7648\n",
      "Epoch [2/10], Step [375/600], Loss: 0.7689, Train Acc:0.7649\n",
      "Epoch [2/10], Step [376/600], Loss: 0.8626, Train Acc:0.7648\n",
      "Epoch [2/10], Step [377/600], Loss: 0.6079, Train Acc:0.7650\n",
      "Epoch [2/10], Step [378/600], Loss: 0.6365, Train Acc:0.7652\n",
      "Epoch [2/10], Step [379/600], Loss: 0.7227, Train Acc:0.7651\n",
      "Epoch [2/10], Step [380/600], Loss: 0.7397, Train Acc:0.7650\n",
      "Epoch [2/10], Step [381/600], Loss: 0.5506, Train Acc:0.7652\n",
      "Epoch [2/10], Step [382/600], Loss: 0.6628, Train Acc:0.7653\n",
      "Epoch [2/10], Step [383/600], Loss: 0.7969, Train Acc:0.7653\n",
      "Epoch [2/10], Step [384/600], Loss: 0.6027, Train Acc:0.7654\n",
      "Epoch [2/10], Step [385/600], Loss: 0.6554, Train Acc:0.7656\n",
      "Epoch [2/10], Step [386/600], Loss: 0.8136, Train Acc:0.7655\n",
      "Epoch [2/10], Step [387/600], Loss: 0.6865, Train Acc:0.7655\n",
      "Epoch [2/10], Step [388/600], Loss: 0.6993, Train Acc:0.7655\n",
      "Epoch [2/10], Step [389/600], Loss: 0.7013, Train Acc:0.7655\n",
      "Epoch [2/10], Step [390/600], Loss: 0.6516, Train Acc:0.7656\n",
      "Epoch [2/10], Step [391/600], Loss: 0.6642, Train Acc:0.7657\n",
      "Epoch [2/10], Step [392/600], Loss: 0.6554, Train Acc:0.7658\n",
      "Epoch [2/10], Step [393/600], Loss: 0.7127, Train Acc:0.7659\n",
      "Epoch [2/10], Step [394/600], Loss: 0.6608, Train Acc:0.7659\n",
      "Epoch [2/10], Step [395/600], Loss: 0.6955, Train Acc:0.7658\n",
      "Epoch [2/10], Step [396/600], Loss: 0.5968, Train Acc:0.7660\n",
      "Epoch [2/10], Step [397/600], Loss: 0.7438, Train Acc:0.7661\n",
      "Epoch [2/10], Step [398/600], Loss: 0.8529, Train Acc:0.7659\n",
      "Epoch [2/10], Step [399/600], Loss: 0.8178, Train Acc:0.7658\n",
      "Epoch [2/10], Step [400/600], Loss: 0.6476, Train Acc:0.7659\n",
      "Epoch [2/10], Step [401/600], Loss: 0.5931, Train Acc:0.7661\n",
      "Epoch [2/10], Step [402/600], Loss: 0.6582, Train Acc:0.7663\n",
      "Epoch [2/10], Step [403/600], Loss: 0.7193, Train Acc:0.7662\n",
      "Epoch [2/10], Step [404/600], Loss: 0.7513, Train Acc:0.7662\n",
      "Epoch [2/10], Step [405/600], Loss: 0.6990, Train Acc:0.7663\n",
      "Epoch [2/10], Step [406/600], Loss: 0.7570, Train Acc:0.7663\n",
      "Epoch [2/10], Step [407/600], Loss: 0.7392, Train Acc:0.7662\n",
      "Epoch [2/10], Step [408/600], Loss: 0.6660, Train Acc:0.7664\n",
      "Epoch [2/10], Step [409/600], Loss: 0.7047, Train Acc:0.7664\n",
      "Epoch [2/10], Step [410/600], Loss: 0.7551, Train Acc:0.7663\n",
      "Epoch [2/10], Step [411/600], Loss: 0.7129, Train Acc:0.7664\n",
      "Epoch [2/10], Step [412/600], Loss: 0.7822, Train Acc:0.7662\n",
      "Epoch [2/10], Step [413/600], Loss: 0.7966, Train Acc:0.7661\n",
      "Epoch [2/10], Step [414/600], Loss: 0.6234, Train Acc:0.7662\n",
      "Epoch [2/10], Step [415/600], Loss: 0.7169, Train Acc:0.7661\n",
      "Epoch [2/10], Step [416/600], Loss: 0.7801, Train Acc:0.7661\n",
      "Epoch [2/10], Step [417/600], Loss: 0.7685, Train Acc:0.7659\n",
      "Epoch [2/10], Step [418/600], Loss: 0.8814, Train Acc:0.7657\n",
      "Epoch [2/10], Step [419/600], Loss: 0.6871, Train Acc:0.7658\n",
      "Epoch [2/10], Step [420/600], Loss: 0.7018, Train Acc:0.7659\n",
      "Epoch [2/10], Step [421/600], Loss: 0.6247, Train Acc:0.7660\n",
      "Epoch [2/10], Step [422/600], Loss: 0.5864, Train Acc:0.7663\n",
      "Epoch [2/10], Step [423/600], Loss: 0.6304, Train Acc:0.7663\n",
      "Epoch [2/10], Step [424/600], Loss: 0.7138, Train Acc:0.7662\n",
      "Epoch [2/10], Step [425/600], Loss: 0.6504, Train Acc:0.7664\n",
      "Epoch [2/10], Step [426/600], Loss: 0.7254, Train Acc:0.7665\n",
      "Epoch [2/10], Step [427/600], Loss: 0.6236, Train Acc:0.7665\n",
      "Epoch [2/10], Step [428/600], Loss: 0.7684, Train Acc:0.7665\n",
      "Epoch [2/10], Step [429/600], Loss: 0.6841, Train Acc:0.7667\n",
      "Epoch [2/10], Step [430/600], Loss: 0.6848, Train Acc:0.7668\n",
      "Epoch [2/10], Step [431/600], Loss: 0.5948, Train Acc:0.7670\n",
      "Epoch [2/10], Step [432/600], Loss: 0.6845, Train Acc:0.7672\n",
      "Epoch [2/10], Step [433/600], Loss: 0.6921, Train Acc:0.7673\n",
      "Epoch [2/10], Step [434/600], Loss: 0.6696, Train Acc:0.7675\n",
      "Epoch [2/10], Step [435/600], Loss: 0.8138, Train Acc:0.7674\n",
      "Epoch [2/10], Step [436/600], Loss: 0.7319, Train Acc:0.7673\n",
      "Epoch [2/10], Step [437/600], Loss: 0.6683, Train Acc:0.7675\n",
      "Epoch [2/10], Step [438/600], Loss: 0.6806, Train Acc:0.7676\n",
      "Epoch [2/10], Step [439/600], Loss: 0.6754, Train Acc:0.7676\n",
      "Epoch [2/10], Step [440/600], Loss: 0.7072, Train Acc:0.7676\n",
      "Epoch [2/10], Step [441/600], Loss: 0.6320, Train Acc:0.7678\n",
      "Epoch [2/10], Step [442/600], Loss: 0.8237, Train Acc:0.7675\n",
      "Epoch [2/10], Step [443/600], Loss: 0.7294, Train Acc:0.7675\n",
      "Epoch [2/10], Step [444/600], Loss: 0.6486, Train Acc:0.7675\n",
      "Epoch [2/10], Step [445/600], Loss: 0.8886, Train Acc:0.7673\n",
      "Epoch [2/10], Step [446/600], Loss: 0.7557, Train Acc:0.7673\n",
      "Epoch [2/10], Step [447/600], Loss: 0.8114, Train Acc:0.7673\n",
      "Epoch [2/10], Step [448/600], Loss: 0.6458, Train Acc:0.7673\n",
      "Epoch [2/10], Step [449/600], Loss: 0.7735, Train Acc:0.7673\n",
      "Epoch [2/10], Step [450/600], Loss: 0.6827, Train Acc:0.7674\n",
      "Epoch [2/10], Step [451/600], Loss: 0.6720, Train Acc:0.7675\n",
      "Epoch [2/10], Step [452/600], Loss: 0.5805, Train Acc:0.7677\n",
      "Epoch [2/10], Step [453/600], Loss: 0.6585, Train Acc:0.7678\n",
      "Epoch [2/10], Step [454/600], Loss: 0.6113, Train Acc:0.7679\n",
      "Epoch [2/10], Step [455/600], Loss: 0.7933, Train Acc:0.7677\n",
      "Epoch [2/10], Step [456/600], Loss: 0.7058, Train Acc:0.7677\n",
      "Epoch [2/10], Step [457/600], Loss: 0.6721, Train Acc:0.7678\n",
      "Epoch [2/10], Step [458/600], Loss: 0.7292, Train Acc:0.7678\n",
      "Epoch [2/10], Step [459/600], Loss: 0.6795, Train Acc:0.7678\n",
      "Epoch [2/10], Step [460/600], Loss: 0.8011, Train Acc:0.7677\n",
      "Epoch [2/10], Step [461/600], Loss: 0.7330, Train Acc:0.7678\n",
      "Epoch [2/10], Step [462/600], Loss: 0.6804, Train Acc:0.7678\n",
      "Epoch [2/10], Step [463/600], Loss: 0.6984, Train Acc:0.7678\n",
      "Epoch [2/10], Step [464/600], Loss: 0.6630, Train Acc:0.7678\n",
      "Epoch [2/10], Step [465/600], Loss: 0.6578, Train Acc:0.7679\n",
      "Epoch [2/10], Step [466/600], Loss: 0.7777, Train Acc:0.7678\n",
      "Epoch [2/10], Step [467/600], Loss: 0.5813, Train Acc:0.7680\n",
      "Epoch [2/10], Step [468/600], Loss: 0.7423, Train Acc:0.7679\n",
      "Epoch [2/10], Step [469/600], Loss: 0.8534, Train Acc:0.7678\n",
      "Epoch [2/10], Step [470/600], Loss: 0.7016, Train Acc:0.7677\n",
      "Epoch [2/10], Step [471/600], Loss: 0.6395, Train Acc:0.7677\n",
      "Epoch [2/10], Step [472/600], Loss: 0.7970, Train Acc:0.7677\n",
      "Epoch [2/10], Step [473/600], Loss: 0.8009, Train Acc:0.7676\n",
      "Epoch [2/10], Step [474/600], Loss: 0.7674, Train Acc:0.7676\n",
      "Epoch [2/10], Step [475/600], Loss: 0.6352, Train Acc:0.7678\n",
      "Epoch [2/10], Step [476/600], Loss: 0.6902, Train Acc:0.7678\n",
      "Epoch [2/10], Step [477/600], Loss: 0.6486, Train Acc:0.7679\n",
      "Epoch [2/10], Step [478/600], Loss: 0.6165, Train Acc:0.7680\n",
      "Epoch [2/10], Step [479/600], Loss: 0.7285, Train Acc:0.7680\n",
      "Epoch [2/10], Step [480/600], Loss: 0.6063, Train Acc:0.7682\n",
      "Epoch [2/10], Step [481/600], Loss: 0.8061, Train Acc:0.7682\n",
      "Epoch [2/10], Step [482/600], Loss: 0.6900, Train Acc:0.7682\n",
      "Epoch [2/10], Step [483/600], Loss: 0.7354, Train Acc:0.7681\n",
      "Epoch [2/10], Step [484/600], Loss: 0.6910, Train Acc:0.7682\n",
      "Epoch [2/10], Step [485/600], Loss: 0.7333, Train Acc:0.7682\n",
      "Epoch [2/10], Step [486/600], Loss: 0.7132, Train Acc:0.7683\n",
      "Epoch [2/10], Step [487/600], Loss: 0.7084, Train Acc:0.7683\n",
      "Epoch [2/10], Step [488/600], Loss: 0.6982, Train Acc:0.7683\n",
      "Epoch [2/10], Step [489/600], Loss: 0.6613, Train Acc:0.7683\n",
      "Epoch [2/10], Step [490/600], Loss: 0.8140, Train Acc:0.7682\n",
      "Epoch [2/10], Step [491/600], Loss: 0.6480, Train Acc:0.7683\n",
      "Epoch [2/10], Step [492/600], Loss: 0.5922, Train Acc:0.7684\n",
      "Epoch [2/10], Step [493/600], Loss: 0.7087, Train Acc:0.7684\n",
      "Epoch [2/10], Step [494/600], Loss: 0.5183, Train Acc:0.7686\n",
      "Epoch [2/10], Step [495/600], Loss: 0.7154, Train Acc:0.7686\n",
      "Epoch [2/10], Step [496/600], Loss: 0.7436, Train Acc:0.7686\n",
      "Epoch [2/10], Step [497/600], Loss: 0.6437, Train Acc:0.7687\n",
      "Epoch [2/10], Step [498/600], Loss: 0.6848, Train Acc:0.7688\n",
      "Epoch [2/10], Step [499/600], Loss: 0.6759, Train Acc:0.7688\n",
      "Epoch [2/10], Step [500/600], Loss: 0.6463, Train Acc:0.7689\n",
      "Epoch [2/10], Step [501/600], Loss: 0.5424, Train Acc:0.7690\n",
      "Epoch [2/10], Step [502/600], Loss: 0.6812, Train Acc:0.7690\n",
      "Epoch [2/10], Step [503/600], Loss: 0.6575, Train Acc:0.7691\n",
      "Epoch [2/10], Step [504/600], Loss: 0.6976, Train Acc:0.7692\n",
      "Epoch [2/10], Step [505/600], Loss: 0.8452, Train Acc:0.7694\n",
      "Epoch [2/10], Step [506/600], Loss: 0.7255, Train Acc:0.7693\n",
      "Epoch [2/10], Step [507/600], Loss: 0.6140, Train Acc:0.7693\n",
      "Epoch [2/10], Step [508/600], Loss: 0.7790, Train Acc:0.7692\n",
      "Epoch [2/10], Step [509/600], Loss: 0.7692, Train Acc:0.7692\n",
      "Epoch [2/10], Step [510/600], Loss: 0.6971, Train Acc:0.7692\n",
      "Epoch [2/10], Step [511/600], Loss: 0.6698, Train Acc:0.7692\n",
      "Epoch [2/10], Step [512/600], Loss: 0.8429, Train Acc:0.7690\n",
      "Epoch [2/10], Step [513/600], Loss: 0.6741, Train Acc:0.7692\n",
      "Epoch [2/10], Step [514/600], Loss: 0.7255, Train Acc:0.7691\n",
      "Epoch [2/10], Step [515/600], Loss: 0.7334, Train Acc:0.7691\n",
      "Epoch [2/10], Step [516/600], Loss: 0.7868, Train Acc:0.7690\n",
      "Epoch [2/10], Step [517/600], Loss: 0.6279, Train Acc:0.7691\n",
      "Epoch [2/10], Step [518/600], Loss: 0.7195, Train Acc:0.7691\n",
      "Epoch [2/10], Step [519/600], Loss: 0.6747, Train Acc:0.7692\n",
      "Epoch [2/10], Step [520/600], Loss: 0.6414, Train Acc:0.7691\n",
      "Epoch [2/10], Step [521/600], Loss: 0.7279, Train Acc:0.7690\n",
      "Epoch [2/10], Step [522/600], Loss: 0.7698, Train Acc:0.7690\n",
      "Epoch [2/10], Step [523/600], Loss: 0.6959, Train Acc:0.7691\n",
      "Epoch [2/10], Step [524/600], Loss: 0.8664, Train Acc:0.7689\n",
      "Epoch [2/10], Step [525/600], Loss: 0.6358, Train Acc:0.7690\n",
      "Epoch [2/10], Step [526/600], Loss: 0.7150, Train Acc:0.7689\n",
      "Epoch [2/10], Step [527/600], Loss: 0.7467, Train Acc:0.7689\n",
      "Epoch [2/10], Step [528/600], Loss: 0.7240, Train Acc:0.7689\n",
      "Epoch [2/10], Step [529/600], Loss: 0.7501, Train Acc:0.7689\n",
      "Epoch [2/10], Step [530/600], Loss: 0.8063, Train Acc:0.7689\n",
      "Epoch [2/10], Step [531/600], Loss: 0.5949, Train Acc:0.7689\n",
      "Epoch [2/10], Step [532/600], Loss: 0.7278, Train Acc:0.7690\n",
      "Epoch [2/10], Step [533/600], Loss: 0.7766, Train Acc:0.7690\n",
      "Epoch [2/10], Step [534/600], Loss: 0.6518, Train Acc:0.7690\n",
      "Epoch [2/10], Step [535/600], Loss: 0.6418, Train Acc:0.7691\n",
      "Epoch [2/10], Step [536/600], Loss: 0.7355, Train Acc:0.7692\n",
      "Epoch [2/10], Step [537/600], Loss: 0.7623, Train Acc:0.7691\n",
      "Epoch [2/10], Step [538/600], Loss: 0.6644, Train Acc:0.7692\n",
      "Epoch [2/10], Step [539/600], Loss: 0.8171, Train Acc:0.7690\n",
      "Epoch [2/10], Step [540/600], Loss: 0.6386, Train Acc:0.7690\n",
      "Epoch [2/10], Step [541/600], Loss: 0.7436, Train Acc:0.7689\n",
      "Epoch [2/10], Step [542/600], Loss: 0.7348, Train Acc:0.7689\n",
      "Epoch [2/10], Step [543/600], Loss: 0.6964, Train Acc:0.7690\n",
      "Epoch [2/10], Step [544/600], Loss: 0.6882, Train Acc:0.7691\n",
      "Epoch [2/10], Step [545/600], Loss: 0.6183, Train Acc:0.7693\n",
      "Epoch [2/10], Step [546/600], Loss: 0.6531, Train Acc:0.7694\n",
      "Epoch [2/10], Step [547/600], Loss: 0.8574, Train Acc:0.7693\n",
      "Epoch [2/10], Step [548/600], Loss: 0.6179, Train Acc:0.7693\n",
      "Epoch [2/10], Step [549/600], Loss: 0.7625, Train Acc:0.7693\n",
      "Epoch [2/10], Step [550/600], Loss: 0.7402, Train Acc:0.7692\n",
      "Epoch [2/10], Step [551/600], Loss: 0.7716, Train Acc:0.7692\n",
      "Epoch [2/10], Step [552/600], Loss: 0.6107, Train Acc:0.7693\n",
      "Epoch [2/10], Step [553/600], Loss: 0.7375, Train Acc:0.7692\n",
      "Epoch [2/10], Step [554/600], Loss: 0.6619, Train Acc:0.7694\n",
      "Epoch [2/10], Step [555/600], Loss: 0.7836, Train Acc:0.7694\n",
      "Epoch [2/10], Step [556/600], Loss: 0.7931, Train Acc:0.7693\n",
      "Epoch [2/10], Step [557/600], Loss: 0.6919, Train Acc:0.7692\n",
      "Epoch [2/10], Step [558/600], Loss: 0.7729, Train Acc:0.7693\n",
      "Epoch [2/10], Step [559/600], Loss: 0.6429, Train Acc:0.7694\n",
      "Epoch [2/10], Step [560/600], Loss: 0.7238, Train Acc:0.7693\n",
      "Epoch [2/10], Step [561/600], Loss: 0.7503, Train Acc:0.7693\n",
      "Epoch [2/10], Step [562/600], Loss: 0.7201, Train Acc:0.7693\n",
      "Epoch [2/10], Step [563/600], Loss: 0.6643, Train Acc:0.7694\n",
      "Epoch [2/10], Step [564/600], Loss: 0.6155, Train Acc:0.7695\n",
      "Epoch [2/10], Step [565/600], Loss: 0.6940, Train Acc:0.7694\n",
      "Epoch [2/10], Step [566/600], Loss: 0.7245, Train Acc:0.7694\n",
      "Epoch [2/10], Step [567/600], Loss: 0.6828, Train Acc:0.7693\n",
      "Epoch [2/10], Step [568/600], Loss: 0.7797, Train Acc:0.7693\n",
      "Epoch [2/10], Step [569/600], Loss: 0.6816, Train Acc:0.7693\n",
      "Epoch [2/10], Step [570/600], Loss: 0.5450, Train Acc:0.7695\n",
      "Epoch [2/10], Step [571/600], Loss: 0.7584, Train Acc:0.7695\n",
      "Epoch [2/10], Step [572/600], Loss: 0.6252, Train Acc:0.7695\n",
      "Epoch [2/10], Step [573/600], Loss: 0.6001, Train Acc:0.7697\n",
      "Epoch [2/10], Step [574/600], Loss: 0.7122, Train Acc:0.7697\n",
      "Epoch [2/10], Step [575/600], Loss: 0.7487, Train Acc:0.7697\n",
      "Epoch [2/10], Step [576/600], Loss: 0.7851, Train Acc:0.7697\n",
      "Epoch [2/10], Step [577/600], Loss: 0.6578, Train Acc:0.7698\n",
      "Epoch [2/10], Step [578/600], Loss: 0.6773, Train Acc:0.7697\n",
      "Epoch [2/10], Step [579/600], Loss: 0.7823, Train Acc:0.7697\n",
      "Epoch [2/10], Step [580/600], Loss: 0.5643, Train Acc:0.7698\n",
      "Epoch [2/10], Step [581/600], Loss: 0.6936, Train Acc:0.7699\n",
      "Epoch [2/10], Step [582/600], Loss: 0.6673, Train Acc:0.7699\n",
      "Epoch [2/10], Step [583/600], Loss: 0.6304, Train Acc:0.7700\n",
      "Epoch [2/10], Step [584/600], Loss: 0.5995, Train Acc:0.7700\n",
      "Epoch [2/10], Step [585/600], Loss: 0.6990, Train Acc:0.7701\n",
      "Epoch [2/10], Step [586/600], Loss: 0.6322, Train Acc:0.7702\n",
      "Epoch [2/10], Step [587/600], Loss: 0.6787, Train Acc:0.7702\n",
      "Epoch [2/10], Step [588/600], Loss: 0.7203, Train Acc:0.7702\n",
      "Epoch [2/10], Step [589/600], Loss: 0.6901, Train Acc:0.7702\n",
      "Epoch [2/10], Step [590/600], Loss: 0.5407, Train Acc:0.7703\n",
      "Epoch [2/10], Step [591/600], Loss: 0.6488, Train Acc:0.7704\n",
      "Epoch [2/10], Step [592/600], Loss: 0.6829, Train Acc:0.7704\n",
      "Epoch [2/10], Step [593/600], Loss: 0.7529, Train Acc:0.7704\n",
      "Epoch [2/10], Step [594/600], Loss: 0.7812, Train Acc:0.7704\n",
      "Epoch [2/10], Step [595/600], Loss: 0.6427, Train Acc:0.7704\n",
      "Epoch [2/10], Step [596/600], Loss: 0.8188, Train Acc:0.7704\n",
      "Epoch [2/10], Step [597/600], Loss: 0.6633, Train Acc:0.7704\n",
      "Epoch [2/10], Step [598/600], Loss: 0.7723, Train Acc:0.7703\n",
      "Epoch [2/10], Step [599/600], Loss: 0.6818, Train Acc:0.7703\n",
      "Epoch [2/10], Step [600/600], Loss: 0.7188, Train Acc:0.7703\n",
      "Epoch [3/10], Step [1/600], Loss: 0.7009, Train Acc:0.7900\n",
      "Epoch [3/10], Step [2/600], Loss: 0.6563, Train Acc:0.8000\n",
      "Epoch [3/10], Step [3/600], Loss: 0.6019, Train Acc:0.7967\n",
      "Epoch [3/10], Step [4/600], Loss: 0.6212, Train Acc:0.8050\n",
      "Epoch [3/10], Step [5/600], Loss: 0.7760, Train Acc:0.7920\n",
      "Epoch [3/10], Step [6/600], Loss: 0.6470, Train Acc:0.8000\n",
      "Epoch [3/10], Step [7/600], Loss: 0.6995, Train Acc:0.8000\n",
      "Epoch [3/10], Step [8/600], Loss: 0.6986, Train Acc:0.7987\n",
      "Epoch [3/10], Step [9/600], Loss: 0.6359, Train Acc:0.8022\n",
      "Epoch [3/10], Step [10/600], Loss: 0.8040, Train Acc:0.8020\n",
      "Epoch [3/10], Step [11/600], Loss: 0.5998, Train Acc:0.8055\n",
      "Epoch [3/10], Step [12/600], Loss: 0.7592, Train Acc:0.8008\n",
      "Epoch [3/10], Step [13/600], Loss: 0.6879, Train Acc:0.7962\n",
      "Epoch [3/10], Step [14/600], Loss: 0.6018, Train Acc:0.7971\n",
      "Epoch [3/10], Step [15/600], Loss: 0.5979, Train Acc:0.7993\n",
      "Epoch [3/10], Step [16/600], Loss: 0.6340, Train Acc:0.7981\n",
      "Epoch [3/10], Step [17/600], Loss: 0.6275, Train Acc:0.7988\n",
      "Epoch [3/10], Step [18/600], Loss: 0.8524, Train Acc:0.7933\n",
      "Epoch [3/10], Step [19/600], Loss: 0.5664, Train Acc:0.7953\n",
      "Epoch [3/10], Step [20/600], Loss: 0.7025, Train Acc:0.7955\n",
      "Epoch [3/10], Step [21/600], Loss: 0.6538, Train Acc:0.7967\n",
      "Epoch [3/10], Step [22/600], Loss: 0.6902, Train Acc:0.7959\n",
      "Epoch [3/10], Step [23/600], Loss: 0.6328, Train Acc:0.7974\n",
      "Epoch [3/10], Step [24/600], Loss: 0.5739, Train Acc:0.7971\n",
      "Epoch [3/10], Step [25/600], Loss: 0.7039, Train Acc:0.7964\n",
      "Epoch [3/10], Step [26/600], Loss: 0.7904, Train Acc:0.7927\n",
      "Epoch [3/10], Step [27/600], Loss: 0.7360, Train Acc:0.7911\n",
      "Epoch [3/10], Step [28/600], Loss: 0.5592, Train Acc:0.7911\n",
      "Epoch [3/10], Step [29/600], Loss: 0.7027, Train Acc:0.7917\n",
      "Epoch [3/10], Step [30/600], Loss: 0.7368, Train Acc:0.7900\n",
      "Epoch [3/10], Step [31/600], Loss: 0.5945, Train Acc:0.7916\n",
      "Epoch [3/10], Step [32/600], Loss: 0.6779, Train Acc:0.7916\n",
      "Epoch [3/10], Step [33/600], Loss: 0.7472, Train Acc:0.7882\n",
      "Epoch [3/10], Step [34/600], Loss: 0.6363, Train Acc:0.7876\n",
      "Epoch [3/10], Step [35/600], Loss: 0.6902, Train Acc:0.7874\n",
      "Epoch [3/10], Step [36/600], Loss: 0.7004, Train Acc:0.7864\n",
      "Epoch [3/10], Step [37/600], Loss: 0.6939, Train Acc:0.7857\n",
      "Epoch [3/10], Step [38/600], Loss: 0.7231, Train Acc:0.7847\n",
      "Epoch [3/10], Step [39/600], Loss: 0.7924, Train Acc:0.7823\n",
      "Epoch [3/10], Step [40/600], Loss: 0.4972, Train Acc:0.7850\n",
      "Epoch [3/10], Step [41/600], Loss: 0.6677, Train Acc:0.7859\n",
      "Epoch [3/10], Step [42/600], Loss: 0.6303, Train Acc:0.7869\n",
      "Epoch [3/10], Step [43/600], Loss: 0.7782, Train Acc:0.7851\n",
      "Epoch [3/10], Step [44/600], Loss: 0.7266, Train Acc:0.7852\n",
      "Epoch [3/10], Step [45/600], Loss: 0.6331, Train Acc:0.7856\n",
      "Epoch [3/10], Step [46/600], Loss: 0.5454, Train Acc:0.7870\n",
      "Epoch [3/10], Step [47/600], Loss: 0.8248, Train Acc:0.7857\n",
      "Epoch [3/10], Step [48/600], Loss: 0.6397, Train Acc:0.7852\n",
      "Epoch [3/10], Step [49/600], Loss: 0.7267, Train Acc:0.7845\n",
      "Epoch [3/10], Step [50/600], Loss: 0.7032, Train Acc:0.7850\n",
      "Epoch [3/10], Step [51/600], Loss: 0.7731, Train Acc:0.7837\n",
      "Epoch [3/10], Step [52/600], Loss: 0.7571, Train Acc:0.7833\n",
      "Epoch [3/10], Step [53/600], Loss: 0.6284, Train Acc:0.7845\n",
      "Epoch [3/10], Step [54/600], Loss: 0.8329, Train Acc:0.7844\n",
      "Epoch [3/10], Step [55/600], Loss: 0.7782, Train Acc:0.7827\n",
      "Epoch [3/10], Step [56/600], Loss: 0.6752, Train Acc:0.7832\n",
      "Epoch [3/10], Step [57/600], Loss: 0.7338, Train Acc:0.7830\n",
      "Epoch [3/10], Step [58/600], Loss: 0.6981, Train Acc:0.7829\n",
      "Epoch [3/10], Step [59/600], Loss: 0.6838, Train Acc:0.7829\n",
      "Epoch [3/10], Step [60/600], Loss: 0.5557, Train Acc:0.7845\n",
      "Epoch [3/10], Step [61/600], Loss: 0.6682, Train Acc:0.7844\n",
      "Epoch [3/10], Step [62/600], Loss: 0.7274, Train Acc:0.7844\n",
      "Epoch [3/10], Step [63/600], Loss: 0.5833, Train Acc:0.7851\n",
      "Epoch [3/10], Step [64/600], Loss: 0.5398, Train Acc:0.7864\n",
      "Epoch [3/10], Step [65/600], Loss: 0.5446, Train Acc:0.7878\n",
      "Epoch [3/10], Step [66/600], Loss: 0.5725, Train Acc:0.7880\n",
      "Epoch [3/10], Step [67/600], Loss: 0.7676, Train Acc:0.7876\n",
      "Epoch [3/10], Step [68/600], Loss: 0.7352, Train Acc:0.7879\n",
      "Epoch [3/10], Step [69/600], Loss: 0.6706, Train Acc:0.7877\n",
      "Epoch [3/10], Step [70/600], Loss: 0.5564, Train Acc:0.7884\n",
      "Epoch [3/10], Step [71/600], Loss: 0.5798, Train Acc:0.7885\n",
      "Epoch [3/10], Step [72/600], Loss: 0.6417, Train Acc:0.7893\n",
      "Epoch [3/10], Step [73/600], Loss: 0.7183, Train Acc:0.7890\n",
      "Epoch [3/10], Step [74/600], Loss: 0.7368, Train Acc:0.7885\n",
      "Epoch [3/10], Step [75/600], Loss: 0.7099, Train Acc:0.7885\n",
      "Epoch [3/10], Step [76/600], Loss: 0.6407, Train Acc:0.7886\n",
      "Epoch [3/10], Step [77/600], Loss: 0.6627, Train Acc:0.7887\n",
      "Epoch [3/10], Step [78/600], Loss: 0.6243, Train Acc:0.7886\n",
      "Epoch [3/10], Step [79/600], Loss: 0.7356, Train Acc:0.7882\n",
      "Epoch [3/10], Step [80/600], Loss: 0.7375, Train Acc:0.7881\n",
      "Epoch [3/10], Step [81/600], Loss: 0.6762, Train Acc:0.7883\n",
      "Epoch [3/10], Step [82/600], Loss: 0.7328, Train Acc:0.7882\n",
      "Epoch [3/10], Step [83/600], Loss: 0.7711, Train Acc:0.7871\n",
      "Epoch [3/10], Step [84/600], Loss: 0.6799, Train Acc:0.7870\n",
      "Epoch [3/10], Step [85/600], Loss: 0.6729, Train Acc:0.7875\n",
      "Epoch [3/10], Step [86/600], Loss: 0.6614, Train Acc:0.7877\n",
      "Epoch [3/10], Step [87/600], Loss: 0.6216, Train Acc:0.7876\n",
      "Epoch [3/10], Step [88/600], Loss: 0.6506, Train Acc:0.7877\n",
      "Epoch [3/10], Step [89/600], Loss: 0.6146, Train Acc:0.7881\n",
      "Epoch [3/10], Step [90/600], Loss: 0.5772, Train Acc:0.7882\n",
      "Epoch [3/10], Step [91/600], Loss: 0.5612, Train Acc:0.7886\n",
      "Epoch [3/10], Step [92/600], Loss: 0.8284, Train Acc:0.7886\n",
      "Epoch [3/10], Step [93/600], Loss: 0.9007, Train Acc:0.7875\n",
      "Epoch [3/10], Step [94/600], Loss: 0.6392, Train Acc:0.7874\n",
      "Epoch [3/10], Step [95/600], Loss: 0.6116, Train Acc:0.7878\n",
      "Epoch [3/10], Step [96/600], Loss: 0.6962, Train Acc:0.7879\n",
      "Epoch [3/10], Step [97/600], Loss: 0.7034, Train Acc:0.7882\n",
      "Epoch [3/10], Step [98/600], Loss: 0.6975, Train Acc:0.7884\n",
      "Epoch [3/10], Step [99/600], Loss: 0.6702, Train Acc:0.7885\n",
      "Epoch [3/10], Step [100/600], Loss: 0.6566, Train Acc:0.7885\n",
      "Epoch [3/10], Step [101/600], Loss: 0.5790, Train Acc:0.7885\n",
      "Epoch [3/10], Step [102/600], Loss: 0.6133, Train Acc:0.7888\n",
      "Epoch [3/10], Step [103/600], Loss: 0.8137, Train Acc:0.7883\n",
      "Epoch [3/10], Step [104/600], Loss: 0.6190, Train Acc:0.7883\n",
      "Epoch [3/10], Step [105/600], Loss: 0.6612, Train Acc:0.7880\n",
      "Epoch [3/10], Step [106/600], Loss: 0.6352, Train Acc:0.7884\n",
      "Epoch [3/10], Step [107/600], Loss: 0.7491, Train Acc:0.7882\n",
      "Epoch [3/10], Step [108/600], Loss: 0.6129, Train Acc:0.7882\n",
      "Epoch [3/10], Step [109/600], Loss: 0.5368, Train Acc:0.7890\n",
      "Epoch [3/10], Step [110/600], Loss: 0.6467, Train Acc:0.7894\n",
      "Epoch [3/10], Step [111/600], Loss: 0.7076, Train Acc:0.7892\n",
      "Epoch [3/10], Step [112/600], Loss: 0.7019, Train Acc:0.7892\n",
      "Epoch [3/10], Step [113/600], Loss: 0.6638, Train Acc:0.7889\n",
      "Epoch [3/10], Step [114/600], Loss: 0.6803, Train Acc:0.7893\n",
      "Epoch [3/10], Step [115/600], Loss: 0.6578, Train Acc:0.7895\n",
      "Epoch [3/10], Step [116/600], Loss: 0.6405, Train Acc:0.7895\n",
      "Epoch [3/10], Step [117/600], Loss: 0.7249, Train Acc:0.7892\n",
      "Epoch [3/10], Step [118/600], Loss: 0.6955, Train Acc:0.7892\n",
      "Epoch [3/10], Step [119/600], Loss: 0.7164, Train Acc:0.7892\n",
      "Epoch [3/10], Step [120/600], Loss: 0.5448, Train Acc:0.7900\n",
      "Epoch [3/10], Step [121/600], Loss: 0.5695, Train Acc:0.7906\n",
      "Epoch [3/10], Step [122/600], Loss: 0.6456, Train Acc:0.7908\n",
      "Epoch [3/10], Step [123/600], Loss: 0.6468, Train Acc:0.7904\n",
      "Epoch [3/10], Step [124/600], Loss: 0.6100, Train Acc:0.7906\n",
      "Epoch [3/10], Step [125/600], Loss: 0.7707, Train Acc:0.7906\n",
      "Epoch [3/10], Step [126/600], Loss: 0.5926, Train Acc:0.7910\n",
      "Epoch [3/10], Step [127/600], Loss: 0.6149, Train Acc:0.7913\n",
      "Epoch [3/10], Step [128/600], Loss: 0.7253, Train Acc:0.7909\n",
      "Epoch [3/10], Step [129/600], Loss: 0.5893, Train Acc:0.7913\n",
      "Epoch [3/10], Step [130/600], Loss: 0.6437, Train Acc:0.7910\n",
      "Epoch [3/10], Step [131/600], Loss: 0.6539, Train Acc:0.7909\n",
      "Epoch [3/10], Step [132/600], Loss: 0.7416, Train Acc:0.7908\n",
      "Epoch [3/10], Step [133/600], Loss: 0.6389, Train Acc:0.7907\n",
      "Epoch [3/10], Step [134/600], Loss: 0.7606, Train Acc:0.7904\n",
      "Epoch [3/10], Step [135/600], Loss: 0.5755, Train Acc:0.7906\n",
      "Epoch [3/10], Step [136/600], Loss: 0.6788, Train Acc:0.7904\n",
      "Epoch [3/10], Step [137/600], Loss: 0.6047, Train Acc:0.7907\n",
      "Epoch [3/10], Step [138/600], Loss: 0.5686, Train Acc:0.7913\n",
      "Epoch [3/10], Step [139/600], Loss: 0.5936, Train Acc:0.7914\n",
      "Epoch [3/10], Step [140/600], Loss: 0.6676, Train Acc:0.7913\n",
      "Epoch [3/10], Step [141/600], Loss: 0.5678, Train Acc:0.7919\n",
      "Epoch [3/10], Step [142/600], Loss: 0.6376, Train Acc:0.7920\n",
      "Epoch [3/10], Step [143/600], Loss: 0.6999, Train Acc:0.7922\n",
      "Epoch [3/10], Step [144/600], Loss: 0.5327, Train Acc:0.7926\n",
      "Epoch [3/10], Step [145/600], Loss: 0.6597, Train Acc:0.7931\n",
      "Epoch [3/10], Step [146/600], Loss: 0.6118, Train Acc:0.7932\n",
      "Epoch [3/10], Step [147/600], Loss: 0.6991, Train Acc:0.7931\n",
      "Epoch [3/10], Step [148/600], Loss: 0.6975, Train Acc:0.7928\n",
      "Epoch [3/10], Step [149/600], Loss: 0.7264, Train Acc:0.7926\n",
      "Epoch [3/10], Step [150/600], Loss: 0.5911, Train Acc:0.7931\n",
      "Epoch [3/10], Step [151/600], Loss: 0.5840, Train Acc:0.7931\n",
      "Epoch [3/10], Step [152/600], Loss: 0.7615, Train Acc:0.7929\n",
      "Epoch [3/10], Step [153/600], Loss: 0.5660, Train Acc:0.7932\n",
      "Epoch [3/10], Step [154/600], Loss: 0.6329, Train Acc:0.7934\n",
      "Epoch [3/10], Step [155/600], Loss: 0.6793, Train Acc:0.7936\n",
      "Epoch [3/10], Step [156/600], Loss: 0.6994, Train Acc:0.7933\n",
      "Epoch [3/10], Step [157/600], Loss: 0.6040, Train Acc:0.7934\n",
      "Epoch [3/10], Step [158/600], Loss: 0.7170, Train Acc:0.7935\n",
      "Epoch [3/10], Step [159/600], Loss: 0.7080, Train Acc:0.7932\n",
      "Epoch [3/10], Step [160/600], Loss: 0.5708, Train Acc:0.7935\n",
      "Epoch [3/10], Step [161/600], Loss: 0.6566, Train Acc:0.7938\n",
      "Epoch [3/10], Step [162/600], Loss: 0.6562, Train Acc:0.7938\n",
      "Epoch [3/10], Step [163/600], Loss: 0.6196, Train Acc:0.7939\n",
      "Epoch [3/10], Step [164/600], Loss: 0.7201, Train Acc:0.7935\n",
      "Epoch [3/10], Step [165/600], Loss: 0.6543, Train Acc:0.7933\n",
      "Epoch [3/10], Step [166/600], Loss: 0.6007, Train Acc:0.7936\n",
      "Epoch [3/10], Step [167/600], Loss: 0.7722, Train Acc:0.7935\n",
      "Epoch [3/10], Step [168/600], Loss: 0.6979, Train Acc:0.7934\n",
      "Epoch [3/10], Step [169/600], Loss: 0.6144, Train Acc:0.7936\n",
      "Epoch [3/10], Step [170/600], Loss: 0.6024, Train Acc:0.7939\n",
      "Epoch [3/10], Step [171/600], Loss: 0.6235, Train Acc:0.7940\n",
      "Epoch [3/10], Step [172/600], Loss: 0.6867, Train Acc:0.7941\n",
      "Epoch [3/10], Step [173/600], Loss: 0.6202, Train Acc:0.7943\n",
      "Epoch [3/10], Step [174/600], Loss: 0.8125, Train Acc:0.7939\n",
      "Epoch [3/10], Step [175/600], Loss: 0.6881, Train Acc:0.7938\n",
      "Epoch [3/10], Step [176/600], Loss: 0.5897, Train Acc:0.7939\n",
      "Epoch [3/10], Step [177/600], Loss: 0.7262, Train Acc:0.7939\n",
      "Epoch [3/10], Step [178/600], Loss: 0.6961, Train Acc:0.7938\n",
      "Epoch [3/10], Step [179/600], Loss: 0.5716, Train Acc:0.7940\n",
      "Epoch [3/10], Step [180/600], Loss: 0.6442, Train Acc:0.7941\n",
      "Epoch [3/10], Step [181/600], Loss: 0.5389, Train Acc:0.7946\n",
      "Epoch [3/10], Step [182/600], Loss: 0.8072, Train Acc:0.7942\n",
      "Epoch [3/10], Step [183/600], Loss: 0.7200, Train Acc:0.7940\n",
      "Epoch [3/10], Step [184/600], Loss: 0.6410, Train Acc:0.7939\n",
      "Epoch [3/10], Step [185/600], Loss: 0.6641, Train Acc:0.7938\n",
      "Epoch [3/10], Step [186/600], Loss: 0.6820, Train Acc:0.7940\n",
      "Epoch [3/10], Step [187/600], Loss: 0.6757, Train Acc:0.7940\n",
      "Epoch [3/10], Step [188/600], Loss: 0.6514, Train Acc:0.7940\n",
      "Epoch [3/10], Step [189/600], Loss: 0.6741, Train Acc:0.7941\n",
      "Epoch [3/10], Step [190/600], Loss: 0.9084, Train Acc:0.7934\n",
      "Epoch [3/10], Step [191/600], Loss: 0.6006, Train Acc:0.7934\n",
      "Epoch [3/10], Step [192/600], Loss: 0.6063, Train Acc:0.7934\n",
      "Epoch [3/10], Step [193/600], Loss: 0.7659, Train Acc:0.7931\n",
      "Epoch [3/10], Step [194/600], Loss: 0.6599, Train Acc:0.7931\n",
      "Epoch [3/10], Step [195/600], Loss: 0.6599, Train Acc:0.7929\n",
      "Epoch [3/10], Step [196/600], Loss: 0.7062, Train Acc:0.7927\n",
      "Epoch [3/10], Step [197/600], Loss: 0.7259, Train Acc:0.7927\n",
      "Epoch [3/10], Step [198/600], Loss: 0.6883, Train Acc:0.7928\n",
      "Epoch [3/10], Step [199/600], Loss: 0.7161, Train Acc:0.7927\n",
      "Epoch [3/10], Step [200/600], Loss: 0.8355, Train Acc:0.7926\n",
      "Epoch [3/10], Step [201/600], Loss: 0.6865, Train Acc:0.7926\n",
      "Epoch [3/10], Step [202/600], Loss: 0.6150, Train Acc:0.7928\n",
      "Epoch [3/10], Step [203/600], Loss: 0.6595, Train Acc:0.7927\n",
      "Epoch [3/10], Step [204/600], Loss: 0.6754, Train Acc:0.7927\n",
      "Epoch [3/10], Step [205/600], Loss: 0.7229, Train Acc:0.7924\n",
      "Epoch [3/10], Step [206/600], Loss: 0.6087, Train Acc:0.7925\n",
      "Epoch [3/10], Step [207/600], Loss: 0.7806, Train Acc:0.7925\n",
      "Epoch [3/10], Step [208/600], Loss: 0.7258, Train Acc:0.7925\n",
      "Epoch [3/10], Step [209/600], Loss: 0.6593, Train Acc:0.7925\n",
      "Epoch [3/10], Step [210/600], Loss: 0.6675, Train Acc:0.7924\n",
      "Epoch [3/10], Step [211/600], Loss: 0.6706, Train Acc:0.7926\n",
      "Epoch [3/10], Step [212/600], Loss: 0.8501, Train Acc:0.7920\n",
      "Epoch [3/10], Step [213/600], Loss: 0.6973, Train Acc:0.7919\n",
      "Epoch [3/10], Step [214/600], Loss: 0.6785, Train Acc:0.7921\n",
      "Epoch [3/10], Step [215/600], Loss: 0.6074, Train Acc:0.7922\n",
      "Epoch [3/10], Step [216/600], Loss: 0.6431, Train Acc:0.7921\n",
      "Epoch [3/10], Step [217/600], Loss: 0.6783, Train Acc:0.7921\n",
      "Epoch [3/10], Step [218/600], Loss: 0.7238, Train Acc:0.7920\n",
      "Epoch [3/10], Step [219/600], Loss: 0.6446, Train Acc:0.7921\n",
      "Epoch [3/10], Step [220/600], Loss: 0.7942, Train Acc:0.7920\n",
      "Epoch [3/10], Step [221/600], Loss: 0.7006, Train Acc:0.7919\n",
      "Epoch [3/10], Step [222/600], Loss: 0.8975, Train Acc:0.7915\n",
      "Epoch [3/10], Step [223/600], Loss: 0.5382, Train Acc:0.7917\n",
      "Epoch [3/10], Step [224/600], Loss: 0.6279, Train Acc:0.7917\n",
      "Epoch [3/10], Step [225/600], Loss: 0.6939, Train Acc:0.7917\n",
      "Epoch [3/10], Step [226/600], Loss: 0.7478, Train Acc:0.7914\n",
      "Epoch [3/10], Step [227/600], Loss: 0.6610, Train Acc:0.7912\n",
      "Epoch [3/10], Step [228/600], Loss: 0.5972, Train Acc:0.7915\n",
      "Epoch [3/10], Step [229/600], Loss: 0.6573, Train Acc:0.7915\n",
      "Epoch [3/10], Step [230/600], Loss: 0.6158, Train Acc:0.7914\n",
      "Epoch [3/10], Step [231/600], Loss: 0.6162, Train Acc:0.7916\n",
      "Epoch [3/10], Step [232/600], Loss: 0.6450, Train Acc:0.7915\n",
      "Epoch [3/10], Step [233/600], Loss: 0.5678, Train Acc:0.7917\n",
      "Epoch [3/10], Step [234/600], Loss: 0.6251, Train Acc:0.7915\n",
      "Epoch [3/10], Step [235/600], Loss: 0.6936, Train Acc:0.7915\n",
      "Epoch [3/10], Step [236/600], Loss: 0.7849, Train Acc:0.7913\n",
      "Epoch [3/10], Step [237/600], Loss: 0.7209, Train Acc:0.7911\n",
      "Epoch [3/10], Step [238/600], Loss: 0.5962, Train Acc:0.7912\n",
      "Epoch [3/10], Step [239/600], Loss: 0.7170, Train Acc:0.7910\n",
      "Epoch [3/10], Step [240/600], Loss: 0.6017, Train Acc:0.7912\n",
      "Epoch [3/10], Step [241/600], Loss: 0.6434, Train Acc:0.7912\n",
      "Epoch [3/10], Step [242/600], Loss: 0.6390, Train Acc:0.7911\n",
      "Epoch [3/10], Step [243/600], Loss: 0.7690, Train Acc:0.7911\n",
      "Epoch [3/10], Step [244/600], Loss: 0.5641, Train Acc:0.7912\n",
      "Epoch [3/10], Step [245/600], Loss: 0.6417, Train Acc:0.7910\n",
      "Epoch [3/10], Step [246/600], Loss: 0.7043, Train Acc:0.7911\n",
      "Epoch [3/10], Step [247/600], Loss: 0.6298, Train Acc:0.7911\n",
      "Epoch [3/10], Step [248/600], Loss: 0.5885, Train Acc:0.7913\n",
      "Epoch [3/10], Step [249/600], Loss: 0.8271, Train Acc:0.7909\n",
      "Epoch [3/10], Step [250/600], Loss: 0.6257, Train Acc:0.7910\n",
      "Epoch [3/10], Step [251/600], Loss: 0.6330, Train Acc:0.7911\n",
      "Epoch [3/10], Step [252/600], Loss: 0.7316, Train Acc:0.7909\n",
      "Epoch [3/10], Step [253/600], Loss: 0.7717, Train Acc:0.7906\n",
      "Epoch [3/10], Step [254/600], Loss: 0.5565, Train Acc:0.7907\n",
      "Epoch [3/10], Step [255/600], Loss: 0.6397, Train Acc:0.7909\n",
      "Epoch [3/10], Step [256/600], Loss: 0.7094, Train Acc:0.7908\n",
      "Epoch [3/10], Step [257/600], Loss: 0.6520, Train Acc:0.7908\n",
      "Epoch [3/10], Step [258/600], Loss: 0.6054, Train Acc:0.7909\n",
      "Epoch [3/10], Step [259/600], Loss: 0.6611, Train Acc:0.7910\n",
      "Epoch [3/10], Step [260/600], Loss: 0.5412, Train Acc:0.7912\n",
      "Epoch [3/10], Step [261/600], Loss: 0.6380, Train Acc:0.7910\n",
      "Epoch [3/10], Step [262/600], Loss: 0.8192, Train Acc:0.7910\n",
      "Epoch [3/10], Step [263/600], Loss: 0.6656, Train Acc:0.7910\n",
      "Epoch [3/10], Step [264/600], Loss: 0.6120, Train Acc:0.7911\n",
      "Epoch [3/10], Step [265/600], Loss: 0.7038, Train Acc:0.7909\n",
      "Epoch [3/10], Step [266/600], Loss: 0.7334, Train Acc:0.7906\n",
      "Epoch [3/10], Step [267/600], Loss: 0.6682, Train Acc:0.7906\n",
      "Epoch [3/10], Step [268/600], Loss: 0.4925, Train Acc:0.7908\n",
      "Epoch [3/10], Step [269/600], Loss: 0.7016, Train Acc:0.7907\n",
      "Epoch [3/10], Step [270/600], Loss: 0.6864, Train Acc:0.7906\n",
      "Epoch [3/10], Step [271/600], Loss: 0.6076, Train Acc:0.7908\n",
      "Epoch [3/10], Step [272/600], Loss: 0.5713, Train Acc:0.7908\n",
      "Epoch [3/10], Step [273/600], Loss: 0.7019, Train Acc:0.7906\n",
      "Epoch [3/10], Step [274/600], Loss: 0.6064, Train Acc:0.7907\n",
      "Epoch [3/10], Step [275/600], Loss: 0.6527, Train Acc:0.7908\n",
      "Epoch [3/10], Step [276/600], Loss: 0.6078, Train Acc:0.7909\n",
      "Epoch [3/10], Step [277/600], Loss: 0.6491, Train Acc:0.7908\n",
      "Epoch [3/10], Step [278/600], Loss: 0.7008, Train Acc:0.7907\n",
      "Epoch [3/10], Step [279/600], Loss: 0.6709, Train Acc:0.7907\n",
      "Epoch [3/10], Step [280/600], Loss: 0.6549, Train Acc:0.7906\n",
      "Epoch [3/10], Step [281/600], Loss: 0.7427, Train Acc:0.7903\n",
      "Epoch [3/10], Step [282/600], Loss: 0.7765, Train Acc:0.7900\n",
      "Epoch [3/10], Step [283/600], Loss: 0.7924, Train Acc:0.7898\n",
      "Epoch [3/10], Step [284/600], Loss: 0.5334, Train Acc:0.7901\n",
      "Epoch [3/10], Step [285/600], Loss: 0.8651, Train Acc:0.7899\n",
      "Epoch [3/10], Step [286/600], Loss: 0.6221, Train Acc:0.7899\n",
      "Epoch [3/10], Step [287/600], Loss: 0.5047, Train Acc:0.7900\n",
      "Epoch [3/10], Step [288/600], Loss: 0.6060, Train Acc:0.7901\n",
      "Epoch [3/10], Step [289/600], Loss: 0.6995, Train Acc:0.7901\n",
      "Epoch [3/10], Step [290/600], Loss: 0.6514, Train Acc:0.7902\n",
      "Epoch [3/10], Step [291/600], Loss: 0.6542, Train Acc:0.7902\n",
      "Epoch [3/10], Step [292/600], Loss: 0.5554, Train Acc:0.7904\n",
      "Epoch [3/10], Step [293/600], Loss: 0.7369, Train Acc:0.7904\n",
      "Epoch [3/10], Step [294/600], Loss: 0.5882, Train Acc:0.7903\n",
      "Epoch [3/10], Step [295/600], Loss: 0.7509, Train Acc:0.7902\n",
      "Epoch [3/10], Step [296/600], Loss: 0.7311, Train Acc:0.7901\n",
      "Epoch [3/10], Step [297/600], Loss: 0.6521, Train Acc:0.7902\n",
      "Epoch [3/10], Step [298/600], Loss: 0.7529, Train Acc:0.7901\n",
      "Epoch [3/10], Step [299/600], Loss: 0.7193, Train Acc:0.7900\n",
      "Epoch [3/10], Step [300/600], Loss: 0.6667, Train Acc:0.7900\n",
      "Epoch [3/10], Step [301/600], Loss: 0.7441, Train Acc:0.7898\n",
      "Epoch [3/10], Step [302/600], Loss: 0.6829, Train Acc:0.7897\n",
      "Epoch [3/10], Step [303/600], Loss: 0.6623, Train Acc:0.7897\n",
      "Epoch [3/10], Step [304/600], Loss: 0.8041, Train Acc:0.7895\n",
      "Epoch [3/10], Step [305/600], Loss: 0.6111, Train Acc:0.7894\n",
      "Epoch [3/10], Step [306/600], Loss: 0.6673, Train Acc:0.7893\n",
      "Epoch [3/10], Step [307/600], Loss: 0.7710, Train Acc:0.7891\n",
      "Epoch [3/10], Step [308/600], Loss: 0.6927, Train Acc:0.7890\n",
      "Epoch [3/10], Step [309/600], Loss: 0.6500, Train Acc:0.7889\n",
      "Epoch [3/10], Step [310/600], Loss: 0.5678, Train Acc:0.7891\n",
      "Epoch [3/10], Step [311/600], Loss: 0.6241, Train Acc:0.7892\n",
      "Epoch [3/10], Step [312/600], Loss: 0.6550, Train Acc:0.7893\n",
      "Epoch [3/10], Step [313/600], Loss: 0.5939, Train Acc:0.7894\n",
      "Epoch [3/10], Step [314/600], Loss: 0.5044, Train Acc:0.7896\n",
      "Epoch [3/10], Step [315/600], Loss: 0.6825, Train Acc:0.7895\n",
      "Epoch [3/10], Step [316/600], Loss: 0.6080, Train Acc:0.7896\n",
      "Epoch [3/10], Step [317/600], Loss: 0.6675, Train Acc:0.7896\n",
      "Epoch [3/10], Step [318/600], Loss: 0.5633, Train Acc:0.7898\n",
      "Epoch [3/10], Step [319/600], Loss: 0.6355, Train Acc:0.7897\n",
      "Epoch [3/10], Step [320/600], Loss: 0.6401, Train Acc:0.7898\n",
      "Epoch [3/10], Step [321/600], Loss: 0.6742, Train Acc:0.7897\n",
      "Epoch [3/10], Step [322/600], Loss: 0.5849, Train Acc:0.7897\n",
      "Epoch [3/10], Step [323/600], Loss: 0.6376, Train Acc:0.7896\n",
      "Epoch [3/10], Step [324/600], Loss: 0.7458, Train Acc:0.7895\n",
      "Epoch [3/10], Step [325/600], Loss: 0.6037, Train Acc:0.7896\n",
      "Epoch [3/10], Step [326/600], Loss: 0.6272, Train Acc:0.7897\n",
      "Epoch [3/10], Step [327/600], Loss: 0.5729, Train Acc:0.7898\n",
      "Epoch [3/10], Step [328/600], Loss: 0.6105, Train Acc:0.7900\n",
      "Epoch [3/10], Step [329/600], Loss: 0.6726, Train Acc:0.7899\n",
      "Epoch [3/10], Step [330/600], Loss: 0.6663, Train Acc:0.7898\n",
      "Epoch [3/10], Step [331/600], Loss: 0.5782, Train Acc:0.7900\n",
      "Epoch [3/10], Step [332/600], Loss: 0.5683, Train Acc:0.7901\n",
      "Epoch [3/10], Step [333/600], Loss: 0.6206, Train Acc:0.7901\n",
      "Epoch [3/10], Step [334/600], Loss: 0.7192, Train Acc:0.7900\n",
      "Epoch [3/10], Step [335/600], Loss: 0.6143, Train Acc:0.7900\n",
      "Epoch [3/10], Step [336/600], Loss: 0.7332, Train Acc:0.7900\n",
      "Epoch [3/10], Step [337/600], Loss: 0.6553, Train Acc:0.7899\n",
      "Epoch [3/10], Step [338/600], Loss: 0.5650, Train Acc:0.7899\n",
      "Epoch [3/10], Step [339/600], Loss: 0.8061, Train Acc:0.7897\n",
      "Epoch [3/10], Step [340/600], Loss: 0.6341, Train Acc:0.7896\n",
      "Epoch [3/10], Step [341/600], Loss: 0.6468, Train Acc:0.7897\n",
      "Epoch [3/10], Step [342/600], Loss: 0.5747, Train Acc:0.7897\n",
      "Epoch [3/10], Step [343/600], Loss: 0.5796, Train Acc:0.7899\n",
      "Epoch [3/10], Step [344/600], Loss: 0.5538, Train Acc:0.7900\n",
      "Epoch [3/10], Step [345/600], Loss: 0.6888, Train Acc:0.7899\n",
      "Epoch [3/10], Step [346/600], Loss: 0.6966, Train Acc:0.7899\n",
      "Epoch [3/10], Step [347/600], Loss: 0.4888, Train Acc:0.7901\n",
      "Epoch [3/10], Step [348/600], Loss: 0.7189, Train Acc:0.7901\n",
      "Epoch [3/10], Step [349/600], Loss: 0.6650, Train Acc:0.7902\n",
      "Epoch [3/10], Step [350/600], Loss: 0.5586, Train Acc:0.7903\n",
      "Epoch [3/10], Step [351/600], Loss: 0.6181, Train Acc:0.7904\n",
      "Epoch [3/10], Step [352/600], Loss: 0.6495, Train Acc:0.7905\n",
      "Epoch [3/10], Step [353/600], Loss: 0.5619, Train Acc:0.7907\n",
      "Epoch [3/10], Step [354/600], Loss: 0.6936, Train Acc:0.7906\n",
      "Epoch [3/10], Step [355/600], Loss: 0.7652, Train Acc:0.7905\n",
      "Epoch [3/10], Step [356/600], Loss: 0.6462, Train Acc:0.7905\n",
      "Epoch [3/10], Step [357/600], Loss: 0.6458, Train Acc:0.7905\n",
      "Epoch [3/10], Step [358/600], Loss: 0.6755, Train Acc:0.7904\n",
      "Epoch [3/10], Step [359/600], Loss: 0.6674, Train Acc:0.7905\n",
      "Epoch [3/10], Step [360/600], Loss: 0.6449, Train Acc:0.7905\n",
      "Epoch [3/10], Step [361/600], Loss: 0.6172, Train Acc:0.7905\n",
      "Epoch [3/10], Step [362/600], Loss: 0.6364, Train Acc:0.7904\n",
      "Epoch [3/10], Step [363/600], Loss: 0.7055, Train Acc:0.7904\n",
      "Epoch [3/10], Step [364/600], Loss: 0.6527, Train Acc:0.7904\n",
      "Epoch [3/10], Step [365/600], Loss: 0.5989, Train Acc:0.7903\n",
      "Epoch [3/10], Step [366/600], Loss: 0.6779, Train Acc:0.7903\n",
      "Epoch [3/10], Step [367/600], Loss: 0.6617, Train Acc:0.7904\n",
      "Epoch [3/10], Step [368/600], Loss: 0.5278, Train Acc:0.7906\n",
      "Epoch [3/10], Step [369/600], Loss: 0.5377, Train Acc:0.7908\n",
      "Epoch [3/10], Step [370/600], Loss: 0.5166, Train Acc:0.7910\n",
      "Epoch [3/10], Step [371/600], Loss: 0.4796, Train Acc:0.7912\n",
      "Epoch [3/10], Step [372/600], Loss: 0.6409, Train Acc:0.7911\n",
      "Epoch [3/10], Step [373/600], Loss: 0.5866, Train Acc:0.7913\n",
      "Epoch [3/10], Step [374/600], Loss: 0.8190, Train Acc:0.7910\n",
      "Epoch [3/10], Step [375/600], Loss: 0.6025, Train Acc:0.7911\n",
      "Epoch [3/10], Step [376/600], Loss: 0.4721, Train Acc:0.7913\n",
      "Epoch [3/10], Step [377/600], Loss: 0.6846, Train Acc:0.7914\n",
      "Epoch [3/10], Step [378/600], Loss: 0.6285, Train Acc:0.7916\n",
      "Epoch [3/10], Step [379/600], Loss: 0.7151, Train Acc:0.7914\n",
      "Epoch [3/10], Step [380/600], Loss: 0.6103, Train Acc:0.7913\n",
      "Epoch [3/10], Step [381/600], Loss: 0.6024, Train Acc:0.7914\n",
      "Epoch [3/10], Step [382/600], Loss: 0.7844, Train Acc:0.7914\n",
      "Epoch [3/10], Step [383/600], Loss: 0.6680, Train Acc:0.7915\n",
      "Epoch [3/10], Step [384/600], Loss: 0.5565, Train Acc:0.7914\n",
      "Epoch [3/10], Step [385/600], Loss: 0.7331, Train Acc:0.7912\n",
      "Epoch [3/10], Step [386/600], Loss: 0.8145, Train Acc:0.7911\n",
      "Epoch [3/10], Step [387/600], Loss: 0.5823, Train Acc:0.7913\n",
      "Epoch [3/10], Step [388/600], Loss: 0.6809, Train Acc:0.7913\n",
      "Epoch [3/10], Step [389/600], Loss: 0.6628, Train Acc:0.7913\n",
      "Epoch [3/10], Step [390/600], Loss: 0.6087, Train Acc:0.7915\n",
      "Epoch [3/10], Step [391/600], Loss: 0.6955, Train Acc:0.7915\n",
      "Epoch [3/10], Step [392/600], Loss: 0.6599, Train Acc:0.7916\n",
      "Epoch [3/10], Step [393/600], Loss: 0.5815, Train Acc:0.7915\n",
      "Epoch [3/10], Step [394/600], Loss: 0.7244, Train Acc:0.7914\n",
      "Epoch [3/10], Step [395/600], Loss: 0.6707, Train Acc:0.7914\n",
      "Epoch [3/10], Step [396/600], Loss: 0.6805, Train Acc:0.7914\n",
      "Epoch [3/10], Step [397/600], Loss: 0.5902, Train Acc:0.7915\n",
      "Epoch [3/10], Step [398/600], Loss: 0.5787, Train Acc:0.7915\n",
      "Epoch [3/10], Step [399/600], Loss: 0.5644, Train Acc:0.7916\n",
      "Epoch [3/10], Step [400/600], Loss: 0.6845, Train Acc:0.7915\n",
      "Epoch [3/10], Step [401/600], Loss: 0.5929, Train Acc:0.7916\n",
      "Epoch [3/10], Step [402/600], Loss: 0.5517, Train Acc:0.7917\n",
      "Epoch [3/10], Step [403/600], Loss: 0.7497, Train Acc:0.7916\n",
      "Epoch [3/10], Step [404/600], Loss: 0.6327, Train Acc:0.7917\n",
      "Epoch [3/10], Step [405/600], Loss: 0.7284, Train Acc:0.7916\n",
      "Epoch [3/10], Step [406/600], Loss: 0.6085, Train Acc:0.7915\n",
      "Epoch [3/10], Step [407/600], Loss: 0.5687, Train Acc:0.7916\n",
      "Epoch [3/10], Step [408/600], Loss: 0.7442, Train Acc:0.7915\n",
      "Epoch [3/10], Step [409/600], Loss: 0.6119, Train Acc:0.7916\n",
      "Epoch [3/10], Step [410/600], Loss: 0.7693, Train Acc:0.7915\n",
      "Epoch [3/10], Step [411/600], Loss: 0.5301, Train Acc:0.7916\n",
      "Epoch [3/10], Step [412/600], Loss: 0.6812, Train Acc:0.7916\n",
      "Epoch [3/10], Step [413/600], Loss: 0.5392, Train Acc:0.7916\n",
      "Epoch [3/10], Step [414/600], Loss: 0.6841, Train Acc:0.7915\n",
      "Epoch [3/10], Step [415/600], Loss: 0.6256, Train Acc:0.7916\n",
      "Epoch [3/10], Step [416/600], Loss: 0.6565, Train Acc:0.7917\n",
      "Epoch [3/10], Step [417/600], Loss: 0.5813, Train Acc:0.7918\n",
      "Epoch [3/10], Step [418/600], Loss: 0.7084, Train Acc:0.7917\n",
      "Epoch [3/10], Step [419/600], Loss: 0.6633, Train Acc:0.7916\n",
      "Epoch [3/10], Step [420/600], Loss: 0.5721, Train Acc:0.7917\n",
      "Epoch [3/10], Step [421/600], Loss: 0.7751, Train Acc:0.7915\n",
      "Epoch [3/10], Step [422/600], Loss: 0.5834, Train Acc:0.7915\n",
      "Epoch [3/10], Step [423/600], Loss: 0.5956, Train Acc:0.7916\n",
      "Epoch [3/10], Step [424/600], Loss: 0.6954, Train Acc:0.7915\n",
      "Epoch [3/10], Step [425/600], Loss: 0.7444, Train Acc:0.7913\n",
      "Epoch [3/10], Step [426/600], Loss: 0.6130, Train Acc:0.7913\n",
      "Epoch [3/10], Step [427/600], Loss: 0.6806, Train Acc:0.7913\n",
      "Epoch [3/10], Step [428/600], Loss: 0.6726, Train Acc:0.7913\n",
      "Epoch [3/10], Step [429/600], Loss: 0.7144, Train Acc:0.7913\n",
      "Epoch [3/10], Step [430/600], Loss: 0.6930, Train Acc:0.7913\n",
      "Epoch [3/10], Step [431/600], Loss: 0.5477, Train Acc:0.7915\n",
      "Epoch [3/10], Step [432/600], Loss: 0.5890, Train Acc:0.7915\n",
      "Epoch [3/10], Step [433/600], Loss: 0.7089, Train Acc:0.7914\n",
      "Epoch [3/10], Step [434/600], Loss: 0.6828, Train Acc:0.7914\n",
      "Epoch [3/10], Step [435/600], Loss: 0.7243, Train Acc:0.7912\n",
      "Epoch [3/10], Step [436/600], Loss: 0.7477, Train Acc:0.7911\n",
      "Epoch [3/10], Step [437/600], Loss: 0.6624, Train Acc:0.7910\n",
      "Epoch [3/10], Step [438/600], Loss: 0.7528, Train Acc:0.7909\n",
      "Epoch [3/10], Step [439/600], Loss: 0.6647, Train Acc:0.7909\n",
      "Epoch [3/10], Step [440/600], Loss: 0.6731, Train Acc:0.7909\n",
      "Epoch [3/10], Step [441/600], Loss: 0.5949, Train Acc:0.7910\n",
      "Epoch [3/10], Step [442/600], Loss: 0.6171, Train Acc:0.7910\n",
      "Epoch [3/10], Step [443/600], Loss: 0.7778, Train Acc:0.7910\n",
      "Epoch [3/10], Step [444/600], Loss: 0.6244, Train Acc:0.7910\n",
      "Epoch [3/10], Step [445/600], Loss: 0.5480, Train Acc:0.7911\n",
      "Epoch [3/10], Step [446/600], Loss: 0.6284, Train Acc:0.7911\n",
      "Epoch [3/10], Step [447/600], Loss: 0.6145, Train Acc:0.7909\n",
      "Epoch [3/10], Step [448/600], Loss: 0.5592, Train Acc:0.7911\n",
      "Epoch [3/10], Step [449/600], Loss: 0.6709, Train Acc:0.7910\n",
      "Epoch [3/10], Step [450/600], Loss: 0.5938, Train Acc:0.7910\n",
      "Epoch [3/10], Step [451/600], Loss: 0.6495, Train Acc:0.7911\n",
      "Epoch [3/10], Step [452/600], Loss: 0.6121, Train Acc:0.7912\n",
      "Epoch [3/10], Step [453/600], Loss: 0.6805, Train Acc:0.7912\n",
      "Epoch [3/10], Step [454/600], Loss: 0.6001, Train Acc:0.7913\n",
      "Epoch [3/10], Step [455/600], Loss: 0.6273, Train Acc:0.7914\n",
      "Epoch [3/10], Step [456/600], Loss: 0.6917, Train Acc:0.7914\n",
      "Epoch [3/10], Step [457/600], Loss: 0.6019, Train Acc:0.7914\n",
      "Epoch [3/10], Step [458/600], Loss: 0.6518, Train Acc:0.7913\n",
      "Epoch [3/10], Step [459/600], Loss: 0.7662, Train Acc:0.7912\n",
      "Epoch [3/10], Step [460/600], Loss: 0.5875, Train Acc:0.7913\n",
      "Epoch [3/10], Step [461/600], Loss: 0.5663, Train Acc:0.7914\n",
      "Epoch [3/10], Step [462/600], Loss: 0.6113, Train Acc:0.7914\n",
      "Epoch [3/10], Step [463/600], Loss: 0.7297, Train Acc:0.7913\n",
      "Epoch [3/10], Step [464/600], Loss: 0.6486, Train Acc:0.7914\n",
      "Epoch [3/10], Step [465/600], Loss: 0.7635, Train Acc:0.7911\n",
      "Epoch [3/10], Step [466/600], Loss: 0.6519, Train Acc:0.7911\n",
      "Epoch [3/10], Step [467/600], Loss: 0.6182, Train Acc:0.7912\n",
      "Epoch [3/10], Step [468/600], Loss: 0.6926, Train Acc:0.7912\n",
      "Epoch [3/10], Step [469/600], Loss: 0.6126, Train Acc:0.7913\n",
      "Epoch [3/10], Step [470/600], Loss: 0.6604, Train Acc:0.7913\n",
      "Epoch [3/10], Step [471/600], Loss: 0.6718, Train Acc:0.7913\n",
      "Epoch [3/10], Step [472/600], Loss: 0.6361, Train Acc:0.7913\n",
      "Epoch [3/10], Step [473/600], Loss: 0.6249, Train Acc:0.7914\n",
      "Epoch [3/10], Step [474/600], Loss: 0.5881, Train Acc:0.7915\n",
      "Epoch [3/10], Step [475/600], Loss: 0.5628, Train Acc:0.7917\n",
      "Epoch [3/10], Step [476/600], Loss: 0.5851, Train Acc:0.7917\n",
      "Epoch [3/10], Step [477/600], Loss: 0.5802, Train Acc:0.7918\n",
      "Epoch [3/10], Step [478/600], Loss: 0.5559, Train Acc:0.7919\n",
      "Epoch [3/10], Step [479/600], Loss: 0.6431, Train Acc:0.7919\n",
      "Epoch [3/10], Step [480/600], Loss: 0.5999, Train Acc:0.7919\n",
      "Epoch [3/10], Step [481/600], Loss: 0.6997, Train Acc:0.7919\n",
      "Epoch [3/10], Step [482/600], Loss: 0.7359, Train Acc:0.7919\n",
      "Epoch [3/10], Step [483/600], Loss: 0.6204, Train Acc:0.7919\n",
      "Epoch [3/10], Step [484/600], Loss: 0.6729, Train Acc:0.7918\n",
      "Epoch [3/10], Step [485/600], Loss: 0.6421, Train Acc:0.7918\n",
      "Epoch [3/10], Step [486/600], Loss: 0.5905, Train Acc:0.7920\n",
      "Epoch [3/10], Step [487/600], Loss: 0.6506, Train Acc:0.7919\n",
      "Epoch [3/10], Step [488/600], Loss: 0.7917, Train Acc:0.7919\n",
      "Epoch [3/10], Step [489/600], Loss: 0.6889, Train Acc:0.7919\n",
      "Epoch [3/10], Step [490/600], Loss: 0.6847, Train Acc:0.7919\n",
      "Epoch [3/10], Step [491/600], Loss: 0.5475, Train Acc:0.7919\n",
      "Epoch [3/10], Step [492/600], Loss: 0.7244, Train Acc:0.7918\n",
      "Epoch [3/10], Step [493/600], Loss: 0.8269, Train Acc:0.7917\n",
      "Epoch [3/10], Step [494/600], Loss: 0.7177, Train Acc:0.7916\n",
      "Epoch [3/10], Step [495/600], Loss: 0.6000, Train Acc:0.7916\n",
      "Epoch [3/10], Step [496/600], Loss: 0.6226, Train Acc:0.7917\n",
      "Epoch [3/10], Step [497/600], Loss: 0.7358, Train Acc:0.7916\n",
      "Epoch [3/10], Step [498/600], Loss: 0.5454, Train Acc:0.7917\n",
      "Epoch [3/10], Step [499/600], Loss: 0.6725, Train Acc:0.7917\n",
      "Epoch [3/10], Step [500/600], Loss: 0.7207, Train Acc:0.7916\n",
      "Epoch [3/10], Step [501/600], Loss: 0.4237, Train Acc:0.7919\n",
      "Epoch [3/10], Step [502/600], Loss: 0.6860, Train Acc:0.7918\n",
      "Epoch [3/10], Step [503/600], Loss: 0.6718, Train Acc:0.7917\n",
      "Epoch [3/10], Step [504/600], Loss: 0.6126, Train Acc:0.7918\n",
      "Epoch [3/10], Step [505/600], Loss: 0.5840, Train Acc:0.7919\n",
      "Epoch [3/10], Step [506/600], Loss: 0.6669, Train Acc:0.7918\n",
      "Epoch [3/10], Step [507/600], Loss: 0.6680, Train Acc:0.7917\n",
      "Epoch [3/10], Step [508/600], Loss: 0.5799, Train Acc:0.7918\n",
      "Epoch [3/10], Step [509/600], Loss: 0.6645, Train Acc:0.7918\n",
      "Epoch [3/10], Step [510/600], Loss: 0.7405, Train Acc:0.7918\n",
      "Epoch [3/10], Step [511/600], Loss: 0.6015, Train Acc:0.7917\n",
      "Epoch [3/10], Step [512/600], Loss: 0.6300, Train Acc:0.7917\n",
      "Epoch [3/10], Step [513/600], Loss: 0.8072, Train Acc:0.7916\n",
      "Epoch [3/10], Step [514/600], Loss: 0.5335, Train Acc:0.7917\n",
      "Epoch [3/10], Step [515/600], Loss: 0.7029, Train Acc:0.7917\n",
      "Epoch [3/10], Step [516/600], Loss: 0.6748, Train Acc:0.7916\n",
      "Epoch [3/10], Step [517/600], Loss: 0.6745, Train Acc:0.7916\n",
      "Epoch [3/10], Step [518/600], Loss: 0.5593, Train Acc:0.7916\n",
      "Epoch [3/10], Step [519/600], Loss: 0.6555, Train Acc:0.7917\n",
      "Epoch [3/10], Step [520/600], Loss: 0.5638, Train Acc:0.7918\n",
      "Epoch [3/10], Step [521/600], Loss: 0.5047, Train Acc:0.7918\n",
      "Epoch [3/10], Step [522/600], Loss: 0.5938, Train Acc:0.7919\n",
      "Epoch [3/10], Step [523/600], Loss: 0.5462, Train Acc:0.7919\n",
      "Epoch [3/10], Step [524/600], Loss: 0.7357, Train Acc:0.7918\n",
      "Epoch [3/10], Step [525/600], Loss: 0.6203, Train Acc:0.7918\n",
      "Epoch [3/10], Step [526/600], Loss: 0.5612, Train Acc:0.7918\n",
      "Epoch [3/10], Step [527/600], Loss: 0.6965, Train Acc:0.7919\n",
      "Epoch [3/10], Step [528/600], Loss: 0.7080, Train Acc:0.7919\n",
      "Epoch [3/10], Step [529/600], Loss: 0.5393, Train Acc:0.7921\n",
      "Epoch [3/10], Step [530/600], Loss: 0.7466, Train Acc:0.7920\n",
      "Epoch [3/10], Step [531/600], Loss: 0.6442, Train Acc:0.7920\n",
      "Epoch [3/10], Step [532/600], Loss: 0.5512, Train Acc:0.7922\n",
      "Epoch [3/10], Step [533/600], Loss: 0.5491, Train Acc:0.7923\n",
      "Epoch [3/10], Step [534/600], Loss: 0.6590, Train Acc:0.7923\n",
      "Epoch [3/10], Step [535/600], Loss: 0.5791, Train Acc:0.7924\n",
      "Epoch [3/10], Step [536/600], Loss: 0.6424, Train Acc:0.7924\n",
      "Epoch [3/10], Step [537/600], Loss: 0.6234, Train Acc:0.7923\n",
      "Epoch [3/10], Step [538/600], Loss: 0.5669, Train Acc:0.7924\n",
      "Epoch [3/10], Step [539/600], Loss: 0.6795, Train Acc:0.7923\n",
      "Epoch [3/10], Step [540/600], Loss: 0.6526, Train Acc:0.7924\n",
      "Epoch [3/10], Step [541/600], Loss: 0.6181, Train Acc:0.7924\n",
      "Epoch [3/10], Step [542/600], Loss: 0.5631, Train Acc:0.7925\n",
      "Epoch [3/10], Step [543/600], Loss: 0.6464, Train Acc:0.7924\n",
      "Epoch [3/10], Step [544/600], Loss: 0.6266, Train Acc:0.7924\n",
      "Epoch [3/10], Step [545/600], Loss: 0.7411, Train Acc:0.7924\n",
      "Epoch [3/10], Step [546/600], Loss: 0.6587, Train Acc:0.7924\n",
      "Epoch [3/10], Step [547/600], Loss: 0.7479, Train Acc:0.7923\n",
      "Epoch [3/10], Step [548/600], Loss: 0.5619, Train Acc:0.7924\n",
      "Epoch [3/10], Step [549/600], Loss: 0.6107, Train Acc:0.7923\n",
      "Epoch [3/10], Step [550/600], Loss: 0.5816, Train Acc:0.7924\n",
      "Epoch [3/10], Step [551/600], Loss: 0.6205, Train Acc:0.7924\n",
      "Epoch [3/10], Step [552/600], Loss: 0.7632, Train Acc:0.7923\n",
      "Epoch [3/10], Step [553/600], Loss: 0.6483, Train Acc:0.7923\n",
      "Epoch [3/10], Step [554/600], Loss: 0.6439, Train Acc:0.7923\n",
      "Epoch [3/10], Step [555/600], Loss: 0.5426, Train Acc:0.7923\n",
      "Epoch [3/10], Step [556/600], Loss: 0.4695, Train Acc:0.7925\n",
      "Epoch [3/10], Step [557/600], Loss: 0.5337, Train Acc:0.7925\n",
      "Epoch [3/10], Step [558/600], Loss: 0.9071, Train Acc:0.7924\n",
      "Epoch [3/10], Step [559/600], Loss: 0.5333, Train Acc:0.7926\n",
      "Epoch [3/10], Step [560/600], Loss: 0.7397, Train Acc:0.7924\n",
      "Epoch [3/10], Step [561/600], Loss: 0.6535, Train Acc:0.7924\n",
      "Epoch [3/10], Step [562/600], Loss: 0.6906, Train Acc:0.7923\n",
      "Epoch [3/10], Step [563/600], Loss: 0.5912, Train Acc:0.7924\n",
      "Epoch [3/10], Step [564/600], Loss: 0.6217, Train Acc:0.7924\n",
      "Epoch [3/10], Step [565/600], Loss: 0.6171, Train Acc:0.7925\n",
      "Epoch [3/10], Step [566/600], Loss: 0.6505, Train Acc:0.7925\n",
      "Epoch [3/10], Step [567/600], Loss: 0.5669, Train Acc:0.7926\n",
      "Epoch [3/10], Step [568/600], Loss: 0.7384, Train Acc:0.7925\n",
      "Epoch [3/10], Step [569/600], Loss: 0.4632, Train Acc:0.7926\n",
      "Epoch [3/10], Step [570/600], Loss: 0.6179, Train Acc:0.7927\n",
      "Epoch [3/10], Step [571/600], Loss: 0.5830, Train Acc:0.7927\n",
      "Epoch [3/10], Step [572/600], Loss: 0.7153, Train Acc:0.7926\n",
      "Epoch [3/10], Step [573/600], Loss: 0.7265, Train Acc:0.7926\n",
      "Epoch [3/10], Step [574/600], Loss: 0.6550, Train Acc:0.7927\n",
      "Epoch [3/10], Step [575/600], Loss: 0.7748, Train Acc:0.7926\n",
      "Epoch [3/10], Step [576/600], Loss: 0.5789, Train Acc:0.7927\n",
      "Epoch [3/10], Step [577/600], Loss: 0.5898, Train Acc:0.7927\n",
      "Epoch [3/10], Step [578/600], Loss: 0.6910, Train Acc:0.7926\n",
      "Epoch [3/10], Step [579/600], Loss: 0.7186, Train Acc:0.7926\n",
      "Epoch [3/10], Step [580/600], Loss: 0.5577, Train Acc:0.7926\n",
      "Epoch [3/10], Step [581/600], Loss: 0.5818, Train Acc:0.7927\n",
      "Epoch [3/10], Step [582/600], Loss: 0.7400, Train Acc:0.7925\n",
      "Epoch [3/10], Step [583/600], Loss: 0.6220, Train Acc:0.7925\n",
      "Epoch [3/10], Step [584/600], Loss: 0.7471, Train Acc:0.7924\n",
      "Epoch [3/10], Step [585/600], Loss: 0.7502, Train Acc:0.7924\n",
      "Epoch [3/10], Step [586/600], Loss: 0.6270, Train Acc:0.7924\n",
      "Epoch [3/10], Step [587/600], Loss: 0.5236, Train Acc:0.7925\n",
      "Epoch [3/10], Step [588/600], Loss: 0.6084, Train Acc:0.7925\n",
      "Epoch [3/10], Step [589/600], Loss: 0.7239, Train Acc:0.7925\n",
      "Epoch [3/10], Step [590/600], Loss: 0.6408, Train Acc:0.7925\n",
      "Epoch [3/10], Step [591/600], Loss: 0.5645, Train Acc:0.7926\n",
      "Epoch [3/10], Step [592/600], Loss: 0.6509, Train Acc:0.7926\n",
      "Epoch [3/10], Step [593/600], Loss: 0.5960, Train Acc:0.7926\n",
      "Epoch [3/10], Step [594/600], Loss: 0.7471, Train Acc:0.7925\n",
      "Epoch [3/10], Step [595/600], Loss: 0.5974, Train Acc:0.7926\n",
      "Epoch [3/10], Step [596/600], Loss: 0.6835, Train Acc:0.7925\n",
      "Epoch [3/10], Step [597/600], Loss: 0.7367, Train Acc:0.7925\n",
      "Epoch [3/10], Step [598/600], Loss: 0.6380, Train Acc:0.7925\n",
      "Epoch [3/10], Step [599/600], Loss: 0.5877, Train Acc:0.7926\n",
      "Epoch [3/10], Step [600/600], Loss: 0.5897, Train Acc:0.7926\n",
      "Epoch [4/10], Step [1/600], Loss: 0.6028, Train Acc:0.8000\n",
      "Epoch [4/10], Step [2/600], Loss: 0.6949, Train Acc:0.7850\n",
      "Epoch [4/10], Step [3/600], Loss: 0.6785, Train Acc:0.7800\n",
      "Epoch [4/10], Step [4/600], Loss: 0.6171, Train Acc:0.7900\n",
      "Epoch [4/10], Step [5/600], Loss: 0.6347, Train Acc:0.7960\n",
      "Epoch [4/10], Step [6/600], Loss: 0.7386, Train Acc:0.7817\n",
      "Epoch [4/10], Step [7/600], Loss: 0.5786, Train Acc:0.7900\n",
      "Epoch [4/10], Step [8/600], Loss: 0.5612, Train Acc:0.7937\n",
      "Epoch [4/10], Step [9/600], Loss: 0.6722, Train Acc:0.7967\n",
      "Epoch [4/10], Step [10/600], Loss: 0.5469, Train Acc:0.8010\n",
      "Epoch [4/10], Step [11/600], Loss: 0.6133, Train Acc:0.8000\n",
      "Epoch [4/10], Step [12/600], Loss: 0.5628, Train Acc:0.8025\n",
      "Epoch [4/10], Step [13/600], Loss: 0.7055, Train Acc:0.7954\n",
      "Epoch [4/10], Step [14/600], Loss: 0.6728, Train Acc:0.7950\n",
      "Epoch [4/10], Step [15/600], Loss: 0.6179, Train Acc:0.7967\n",
      "Epoch [4/10], Step [16/600], Loss: 0.4752, Train Acc:0.8031\n",
      "Epoch [4/10], Step [17/600], Loss: 0.5767, Train Acc:0.8006\n",
      "Epoch [4/10], Step [18/600], Loss: 0.6971, Train Acc:0.8017\n",
      "Epoch [4/10], Step [19/600], Loss: 0.5728, Train Acc:0.8032\n",
      "Epoch [4/10], Step [20/600], Loss: 0.5916, Train Acc:0.8030\n",
      "Epoch [4/10], Step [21/600], Loss: 0.7126, Train Acc:0.8019\n",
      "Epoch [4/10], Step [22/600], Loss: 0.6140, Train Acc:0.8000\n",
      "Epoch [4/10], Step [23/600], Loss: 0.7002, Train Acc:0.8017\n",
      "Epoch [4/10], Step [24/600], Loss: 0.8349, Train Acc:0.7987\n",
      "Epoch [4/10], Step [25/600], Loss: 0.7424, Train Acc:0.7968\n",
      "Epoch [4/10], Step [26/600], Loss: 0.5878, Train Acc:0.7958\n",
      "Epoch [4/10], Step [27/600], Loss: 0.6839, Train Acc:0.7933\n",
      "Epoch [4/10], Step [28/600], Loss: 0.6202, Train Acc:0.7929\n",
      "Epoch [4/10], Step [29/600], Loss: 0.7483, Train Acc:0.7907\n",
      "Epoch [4/10], Step [30/600], Loss: 0.6472, Train Acc:0.7900\n",
      "Epoch [4/10], Step [31/600], Loss: 0.7803, Train Acc:0.7894\n",
      "Epoch [4/10], Step [32/600], Loss: 0.7395, Train Acc:0.7881\n",
      "Epoch [4/10], Step [33/600], Loss: 0.6680, Train Acc:0.7891\n",
      "Epoch [4/10], Step [34/600], Loss: 0.6552, Train Acc:0.7894\n",
      "Epoch [4/10], Step [35/600], Loss: 0.5951, Train Acc:0.7900\n",
      "Epoch [4/10], Step [36/600], Loss: 0.5424, Train Acc:0.7911\n",
      "Epoch [4/10], Step [37/600], Loss: 0.5501, Train Acc:0.7916\n",
      "Epoch [4/10], Step [38/600], Loss: 0.8320, Train Acc:0.7895\n",
      "Epoch [4/10], Step [39/600], Loss: 0.6761, Train Acc:0.7887\n",
      "Epoch [4/10], Step [40/600], Loss: 0.6269, Train Acc:0.7880\n",
      "Epoch [4/10], Step [41/600], Loss: 0.6613, Train Acc:0.7873\n",
      "Epoch [4/10], Step [42/600], Loss: 0.7098, Train Acc:0.7864\n",
      "Epoch [4/10], Step [43/600], Loss: 0.9679, Train Acc:0.7842\n",
      "Epoch [4/10], Step [44/600], Loss: 0.5988, Train Acc:0.7852\n",
      "Epoch [4/10], Step [45/600], Loss: 0.6144, Train Acc:0.7847\n",
      "Epoch [4/10], Step [46/600], Loss: 0.7641, Train Acc:0.7833\n",
      "Epoch [4/10], Step [47/600], Loss: 0.6032, Train Acc:0.7834\n",
      "Epoch [4/10], Step [48/600], Loss: 0.5483, Train Acc:0.7842\n",
      "Epoch [4/10], Step [49/600], Loss: 0.5631, Train Acc:0.7859\n",
      "Epoch [4/10], Step [50/600], Loss: 0.5460, Train Acc:0.7868\n",
      "Epoch [4/10], Step [51/600], Loss: 0.6168, Train Acc:0.7869\n",
      "Epoch [4/10], Step [52/600], Loss: 0.5220, Train Acc:0.7883\n",
      "Epoch [4/10], Step [53/600], Loss: 0.7603, Train Acc:0.7874\n",
      "Epoch [4/10], Step [54/600], Loss: 0.5839, Train Acc:0.7887\n",
      "Epoch [4/10], Step [55/600], Loss: 0.7125, Train Acc:0.7884\n",
      "Epoch [4/10], Step [56/600], Loss: 0.6424, Train Acc:0.7886\n",
      "Epoch [4/10], Step [57/600], Loss: 0.5257, Train Acc:0.7893\n",
      "Epoch [4/10], Step [58/600], Loss: 0.5666, Train Acc:0.7891\n",
      "Epoch [4/10], Step [59/600], Loss: 0.5746, Train Acc:0.7898\n",
      "Epoch [4/10], Step [60/600], Loss: 0.6068, Train Acc:0.7902\n",
      "Epoch [4/10], Step [61/600], Loss: 0.5777, Train Acc:0.7908\n",
      "Epoch [4/10], Step [62/600], Loss: 0.6800, Train Acc:0.7903\n",
      "Epoch [4/10], Step [63/600], Loss: 0.4378, Train Acc:0.7919\n",
      "Epoch [4/10], Step [64/600], Loss: 0.6948, Train Acc:0.7917\n",
      "Epoch [4/10], Step [65/600], Loss: 0.5738, Train Acc:0.7925\n",
      "Epoch [4/10], Step [66/600], Loss: 0.7149, Train Acc:0.7921\n",
      "Epoch [4/10], Step [67/600], Loss: 0.6138, Train Acc:0.7922\n",
      "Epoch [4/10], Step [68/600], Loss: 0.5948, Train Acc:0.7924\n",
      "Epoch [4/10], Step [69/600], Loss: 0.6466, Train Acc:0.7922\n",
      "Epoch [4/10], Step [70/600], Loss: 0.6125, Train Acc:0.7924\n",
      "Epoch [4/10], Step [71/600], Loss: 0.5162, Train Acc:0.7931\n",
      "Epoch [4/10], Step [72/600], Loss: 0.7934, Train Acc:0.7931\n",
      "Epoch [4/10], Step [73/600], Loss: 0.6541, Train Acc:0.7933\n",
      "Epoch [4/10], Step [74/600], Loss: 0.5255, Train Acc:0.7934\n",
      "Epoch [4/10], Step [75/600], Loss: 0.6083, Train Acc:0.7940\n",
      "Epoch [4/10], Step [76/600], Loss: 0.6252, Train Acc:0.7943\n",
      "Epoch [4/10], Step [77/600], Loss: 0.6438, Train Acc:0.7938\n",
      "Epoch [4/10], Step [78/600], Loss: 0.7671, Train Acc:0.7932\n",
      "Epoch [4/10], Step [79/600], Loss: 0.6508, Train Acc:0.7927\n",
      "Epoch [4/10], Step [80/600], Loss: 0.6698, Train Acc:0.7924\n",
      "Epoch [4/10], Step [81/600], Loss: 0.6387, Train Acc:0.7925\n",
      "Epoch [4/10], Step [82/600], Loss: 0.6008, Train Acc:0.7927\n",
      "Epoch [4/10], Step [83/600], Loss: 0.7160, Train Acc:0.7924\n",
      "Epoch [4/10], Step [84/600], Loss: 0.6610, Train Acc:0.7921\n",
      "Epoch [4/10], Step [85/600], Loss: 0.7158, Train Acc:0.7918\n",
      "Epoch [4/10], Step [86/600], Loss: 0.6713, Train Acc:0.7915\n",
      "Epoch [4/10], Step [87/600], Loss: 0.6581, Train Acc:0.7915\n",
      "Epoch [4/10], Step [88/600], Loss: 0.5904, Train Acc:0.7915\n",
      "Epoch [4/10], Step [89/600], Loss: 0.6591, Train Acc:0.7915\n",
      "Epoch [4/10], Step [90/600], Loss: 0.8365, Train Acc:0.7907\n",
      "Epoch [4/10], Step [91/600], Loss: 0.5717, Train Acc:0.7908\n",
      "Epoch [4/10], Step [92/600], Loss: 0.6269, Train Acc:0.7905\n",
      "Epoch [4/10], Step [93/600], Loss: 0.6931, Train Acc:0.7902\n",
      "Epoch [4/10], Step [94/600], Loss: 0.6566, Train Acc:0.7900\n",
      "Epoch [4/10], Step [95/600], Loss: 0.5840, Train Acc:0.7908\n",
      "Epoch [4/10], Step [96/600], Loss: 0.6513, Train Acc:0.7909\n",
      "Epoch [4/10], Step [97/600], Loss: 0.5468, Train Acc:0.7915\n",
      "Epoch [4/10], Step [98/600], Loss: 0.5143, Train Acc:0.7922\n",
      "Epoch [4/10], Step [99/600], Loss: 0.6246, Train Acc:0.7920\n",
      "Epoch [4/10], Step [100/600], Loss: 0.5866, Train Acc:0.7924\n",
      "Epoch [4/10], Step [101/600], Loss: 0.7567, Train Acc:0.7917\n",
      "Epoch [4/10], Step [102/600], Loss: 0.6856, Train Acc:0.7919\n",
      "Epoch [4/10], Step [103/600], Loss: 0.6200, Train Acc:0.7917\n",
      "Epoch [4/10], Step [104/600], Loss: 0.7074, Train Acc:0.7912\n",
      "Epoch [4/10], Step [105/600], Loss: 0.5680, Train Acc:0.7912\n",
      "Epoch [4/10], Step [106/600], Loss: 0.5986, Train Acc:0.7916\n",
      "Epoch [4/10], Step [107/600], Loss: 0.5335, Train Acc:0.7921\n",
      "Epoch [4/10], Step [108/600], Loss: 0.7061, Train Acc:0.7919\n",
      "Epoch [4/10], Step [109/600], Loss: 0.5835, Train Acc:0.7922\n",
      "Epoch [4/10], Step [110/600], Loss: 0.6685, Train Acc:0.7922\n",
      "Epoch [4/10], Step [111/600], Loss: 0.6778, Train Acc:0.7921\n",
      "Epoch [4/10], Step [112/600], Loss: 0.7030, Train Acc:0.7919\n",
      "Epoch [4/10], Step [113/600], Loss: 0.5778, Train Acc:0.7921\n",
      "Epoch [4/10], Step [114/600], Loss: 0.6990, Train Acc:0.7920\n",
      "Epoch [4/10], Step [115/600], Loss: 0.6097, Train Acc:0.7917\n",
      "Epoch [4/10], Step [116/600], Loss: 0.6090, Train Acc:0.7920\n",
      "Epoch [4/10], Step [117/600], Loss: 0.5661, Train Acc:0.7921\n",
      "Epoch [4/10], Step [118/600], Loss: 0.8212, Train Acc:0.7918\n",
      "Epoch [4/10], Step [119/600], Loss: 0.6320, Train Acc:0.7918\n",
      "Epoch [4/10], Step [120/600], Loss: 0.5129, Train Acc:0.7923\n",
      "Epoch [4/10], Step [121/600], Loss: 0.5776, Train Acc:0.7929\n",
      "Epoch [4/10], Step [122/600], Loss: 0.6828, Train Acc:0.7925\n",
      "Epoch [4/10], Step [123/600], Loss: 0.6992, Train Acc:0.7924\n",
      "Epoch [4/10], Step [124/600], Loss: 0.7812, Train Acc:0.7924\n",
      "Epoch [4/10], Step [125/600], Loss: 0.5530, Train Acc:0.7930\n",
      "Epoch [4/10], Step [126/600], Loss: 0.6430, Train Acc:0.7935\n",
      "Epoch [4/10], Step [127/600], Loss: 0.6515, Train Acc:0.7930\n",
      "Epoch [4/10], Step [128/600], Loss: 0.5988, Train Acc:0.7930\n",
      "Epoch [4/10], Step [129/600], Loss: 0.7070, Train Acc:0.7931\n",
      "Epoch [4/10], Step [130/600], Loss: 0.5814, Train Acc:0.7932\n",
      "Epoch [4/10], Step [131/600], Loss: 0.5709, Train Acc:0.7937\n",
      "Epoch [4/10], Step [132/600], Loss: 0.6710, Train Acc:0.7935\n",
      "Epoch [4/10], Step [133/600], Loss: 0.5598, Train Acc:0.7937\n",
      "Epoch [4/10], Step [134/600], Loss: 0.6083, Train Acc:0.7939\n",
      "Epoch [4/10], Step [135/600], Loss: 0.6434, Train Acc:0.7936\n",
      "Epoch [4/10], Step [136/600], Loss: 0.5989, Train Acc:0.7940\n",
      "Epoch [4/10], Step [137/600], Loss: 0.5749, Train Acc:0.7942\n",
      "Epoch [4/10], Step [138/600], Loss: 0.6696, Train Acc:0.7944\n",
      "Epoch [4/10], Step [139/600], Loss: 0.5918, Train Acc:0.7947\n",
      "Epoch [4/10], Step [140/600], Loss: 0.6263, Train Acc:0.7951\n",
      "Epoch [4/10], Step [141/600], Loss: 0.6248, Train Acc:0.7952\n",
      "Epoch [4/10], Step [142/600], Loss: 0.6931, Train Acc:0.7950\n",
      "Epoch [4/10], Step [143/600], Loss: 0.6390, Train Acc:0.7952\n",
      "Epoch [4/10], Step [144/600], Loss: 0.7158, Train Acc:0.7951\n",
      "Epoch [4/10], Step [145/600], Loss: 0.6514, Train Acc:0.7950\n",
      "Epoch [4/10], Step [146/600], Loss: 0.5978, Train Acc:0.7953\n",
      "Epoch [4/10], Step [147/600], Loss: 0.6462, Train Acc:0.7952\n",
      "Epoch [4/10], Step [148/600], Loss: 0.7574, Train Acc:0.7953\n",
      "Epoch [4/10], Step [149/600], Loss: 0.7578, Train Acc:0.7948\n",
      "Epoch [4/10], Step [150/600], Loss: 0.7158, Train Acc:0.7945\n",
      "Epoch [4/10], Step [151/600], Loss: 0.6096, Train Acc:0.7947\n",
      "Epoch [4/10], Step [152/600], Loss: 0.6933, Train Acc:0.7947\n",
      "Epoch [4/10], Step [153/600], Loss: 0.5995, Train Acc:0.7948\n",
      "Epoch [4/10], Step [154/600], Loss: 0.6414, Train Acc:0.7945\n",
      "Epoch [4/10], Step [155/600], Loss: 0.6470, Train Acc:0.7942\n",
      "Epoch [4/10], Step [156/600], Loss: 0.4756, Train Acc:0.7946\n",
      "Epoch [4/10], Step [157/600], Loss: 0.5019, Train Acc:0.7950\n",
      "Epoch [4/10], Step [158/600], Loss: 0.7150, Train Acc:0.7947\n",
      "Epoch [4/10], Step [159/600], Loss: 0.6215, Train Acc:0.7948\n",
      "Epoch [4/10], Step [160/600], Loss: 0.4775, Train Acc:0.7953\n",
      "Epoch [4/10], Step [161/600], Loss: 0.5514, Train Acc:0.7952\n",
      "Epoch [4/10], Step [162/600], Loss: 0.6022, Train Acc:0.7953\n",
      "Epoch [4/10], Step [163/600], Loss: 0.5120, Train Acc:0.7956\n",
      "Epoch [4/10], Step [164/600], Loss: 0.6751, Train Acc:0.7955\n",
      "Epoch [4/10], Step [165/600], Loss: 0.7700, Train Acc:0.7952\n",
      "Epoch [4/10], Step [166/600], Loss: 0.6010, Train Acc:0.7955\n",
      "Epoch [4/10], Step [167/600], Loss: 0.5844, Train Acc:0.7956\n",
      "Epoch [4/10], Step [168/600], Loss: 0.6616, Train Acc:0.7957\n",
      "Epoch [4/10], Step [169/600], Loss: 0.6587, Train Acc:0.7954\n",
      "Epoch [4/10], Step [170/600], Loss: 0.6051, Train Acc:0.7956\n",
      "Epoch [4/10], Step [171/600], Loss: 0.6889, Train Acc:0.7955\n",
      "Epoch [4/10], Step [172/600], Loss: 0.6150, Train Acc:0.7956\n",
      "Epoch [4/10], Step [173/600], Loss: 0.5975, Train Acc:0.7956\n",
      "Epoch [4/10], Step [174/600], Loss: 0.5335, Train Acc:0.7959\n",
      "Epoch [4/10], Step [175/600], Loss: 0.6306, Train Acc:0.7960\n",
      "Epoch [4/10], Step [176/600], Loss: 0.6025, Train Acc:0.7959\n",
      "Epoch [4/10], Step [177/600], Loss: 0.5825, Train Acc:0.7959\n",
      "Epoch [4/10], Step [178/600], Loss: 0.5781, Train Acc:0.7960\n",
      "Epoch [4/10], Step [179/600], Loss: 0.6967, Train Acc:0.7956\n",
      "Epoch [4/10], Step [180/600], Loss: 0.6638, Train Acc:0.7956\n",
      "Epoch [4/10], Step [181/600], Loss: 0.6677, Train Acc:0.7954\n",
      "Epoch [4/10], Step [182/600], Loss: 0.6878, Train Acc:0.7955\n",
      "Epoch [4/10], Step [183/600], Loss: 0.5830, Train Acc:0.7957\n",
      "Epoch [4/10], Step [184/600], Loss: 0.6168, Train Acc:0.7957\n",
      "Epoch [4/10], Step [185/600], Loss: 0.5669, Train Acc:0.7957\n",
      "Epoch [4/10], Step [186/600], Loss: 0.7010, Train Acc:0.7957\n",
      "Epoch [4/10], Step [187/600], Loss: 0.5780, Train Acc:0.7958\n",
      "Epoch [4/10], Step [188/600], Loss: 0.4753, Train Acc:0.7963\n",
      "Epoch [4/10], Step [189/600], Loss: 0.7427, Train Acc:0.7961\n",
      "Epoch [4/10], Step [190/600], Loss: 0.6655, Train Acc:0.7962\n",
      "Epoch [4/10], Step [191/600], Loss: 0.5466, Train Acc:0.7963\n",
      "Epoch [4/10], Step [192/600], Loss: 0.5535, Train Acc:0.7965\n",
      "Epoch [4/10], Step [193/600], Loss: 0.5755, Train Acc:0.7965\n",
      "Epoch [4/10], Step [194/600], Loss: 0.6089, Train Acc:0.7965\n",
      "Epoch [4/10], Step [195/600], Loss: 0.6446, Train Acc:0.7965\n",
      "Epoch [4/10], Step [196/600], Loss: 0.6617, Train Acc:0.7963\n",
      "Epoch [4/10], Step [197/600], Loss: 0.6161, Train Acc:0.7963\n",
      "Epoch [4/10], Step [198/600], Loss: 0.7716, Train Acc:0.7960\n",
      "Epoch [4/10], Step [199/600], Loss: 0.6322, Train Acc:0.7959\n",
      "Epoch [4/10], Step [200/600], Loss: 0.6630, Train Acc:0.7961\n",
      "Epoch [4/10], Step [201/600], Loss: 0.4936, Train Acc:0.7964\n",
      "Epoch [4/10], Step [202/600], Loss: 0.6012, Train Acc:0.7962\n",
      "Epoch [4/10], Step [203/600], Loss: 0.6601, Train Acc:0.7964\n",
      "Epoch [4/10], Step [204/600], Loss: 0.6144, Train Acc:0.7964\n",
      "Epoch [4/10], Step [205/600], Loss: 0.6455, Train Acc:0.7964\n",
      "Epoch [4/10], Step [206/600], Loss: 0.5092, Train Acc:0.7966\n",
      "Epoch [4/10], Step [207/600], Loss: 0.5095, Train Acc:0.7968\n",
      "Epoch [4/10], Step [208/600], Loss: 0.6640, Train Acc:0.7967\n",
      "Epoch [4/10], Step [209/600], Loss: 0.6438, Train Acc:0.7970\n",
      "Epoch [4/10], Step [210/600], Loss: 0.5421, Train Acc:0.7971\n",
      "Epoch [4/10], Step [211/600], Loss: 0.5911, Train Acc:0.7971\n",
      "Epoch [4/10], Step [212/600], Loss: 0.7288, Train Acc:0.7970\n",
      "Epoch [4/10], Step [213/600], Loss: 0.5066, Train Acc:0.7975\n",
      "Epoch [4/10], Step [214/600], Loss: 0.6758, Train Acc:0.7975\n",
      "Epoch [4/10], Step [215/600], Loss: 0.7073, Train Acc:0.7974\n",
      "Epoch [4/10], Step [216/600], Loss: 0.5837, Train Acc:0.7975\n",
      "Epoch [4/10], Step [217/600], Loss: 0.5594, Train Acc:0.7975\n",
      "Epoch [4/10], Step [218/600], Loss: 0.5802, Train Acc:0.7975\n",
      "Epoch [4/10], Step [219/600], Loss: 0.4715, Train Acc:0.7978\n",
      "Epoch [4/10], Step [220/600], Loss: 0.5931, Train Acc:0.7979\n",
      "Epoch [4/10], Step [221/600], Loss: 0.6691, Train Acc:0.7978\n",
      "Epoch [4/10], Step [222/600], Loss: 0.5547, Train Acc:0.7978\n",
      "Epoch [4/10], Step [223/600], Loss: 0.6086, Train Acc:0.7979\n",
      "Epoch [4/10], Step [224/600], Loss: 0.6950, Train Acc:0.7979\n",
      "Epoch [4/10], Step [225/600], Loss: 0.5611, Train Acc:0.7980\n",
      "Epoch [4/10], Step [226/600], Loss: 0.6135, Train Acc:0.7980\n",
      "Epoch [4/10], Step [227/600], Loss: 0.5685, Train Acc:0.7980\n",
      "Epoch [4/10], Step [228/600], Loss: 0.6076, Train Acc:0.7982\n",
      "Epoch [4/10], Step [229/600], Loss: 0.5824, Train Acc:0.7983\n",
      "Epoch [4/10], Step [230/600], Loss: 0.6425, Train Acc:0.7982\n",
      "Epoch [4/10], Step [231/600], Loss: 0.6383, Train Acc:0.7982\n",
      "Epoch [4/10], Step [232/600], Loss: 0.7209, Train Acc:0.7981\n",
      "Epoch [4/10], Step [233/600], Loss: 0.6462, Train Acc:0.7979\n",
      "Epoch [4/10], Step [234/600], Loss: 0.6556, Train Acc:0.7979\n",
      "Epoch [4/10], Step [235/600], Loss: 0.6486, Train Acc:0.7981\n",
      "Epoch [4/10], Step [236/600], Loss: 0.6740, Train Acc:0.7980\n",
      "Epoch [4/10], Step [237/600], Loss: 0.7320, Train Acc:0.7979\n",
      "Epoch [4/10], Step [238/600], Loss: 0.5244, Train Acc:0.7981\n",
      "Epoch [4/10], Step [239/600], Loss: 0.4734, Train Acc:0.7984\n",
      "Epoch [4/10], Step [240/600], Loss: 0.6360, Train Acc:0.7983\n",
      "Epoch [4/10], Step [241/600], Loss: 0.6524, Train Acc:0.7983\n",
      "Epoch [4/10], Step [242/600], Loss: 0.6400, Train Acc:0.7983\n",
      "Epoch [4/10], Step [243/600], Loss: 0.6101, Train Acc:0.7981\n",
      "Epoch [4/10], Step [244/600], Loss: 0.7366, Train Acc:0.7979\n",
      "Epoch [4/10], Step [245/600], Loss: 0.6450, Train Acc:0.7977\n",
      "Epoch [4/10], Step [246/600], Loss: 0.6317, Train Acc:0.7975\n",
      "Epoch [4/10], Step [247/600], Loss: 0.7234, Train Acc:0.7974\n",
      "Epoch [4/10], Step [248/600], Loss: 0.6114, Train Acc:0.7975\n",
      "Epoch [4/10], Step [249/600], Loss: 0.6018, Train Acc:0.7976\n",
      "Epoch [4/10], Step [250/600], Loss: 0.7034, Train Acc:0.7974\n",
      "Epoch [4/10], Step [251/600], Loss: 0.6424, Train Acc:0.7973\n",
      "Epoch [4/10], Step [252/600], Loss: 0.6890, Train Acc:0.7972\n",
      "Epoch [4/10], Step [253/600], Loss: 0.5213, Train Acc:0.7973\n",
      "Epoch [4/10], Step [254/600], Loss: 0.5855, Train Acc:0.7973\n",
      "Epoch [4/10], Step [255/600], Loss: 0.6080, Train Acc:0.7974\n",
      "Epoch [4/10], Step [256/600], Loss: 0.5699, Train Acc:0.7975\n",
      "Epoch [4/10], Step [257/600], Loss: 0.5263, Train Acc:0.7975\n",
      "Epoch [4/10], Step [258/600], Loss: 0.5683, Train Acc:0.7975\n",
      "Epoch [4/10], Step [259/600], Loss: 0.6392, Train Acc:0.7976\n",
      "Epoch [4/10], Step [260/600], Loss: 0.5717, Train Acc:0.7977\n",
      "Epoch [4/10], Step [261/600], Loss: 0.6523, Train Acc:0.7976\n",
      "Epoch [4/10], Step [262/600], Loss: 0.5526, Train Acc:0.7979\n",
      "Epoch [4/10], Step [263/600], Loss: 0.6900, Train Acc:0.7976\n",
      "Epoch [4/10], Step [264/600], Loss: 0.7735, Train Acc:0.7975\n",
      "Epoch [4/10], Step [265/600], Loss: 0.4926, Train Acc:0.7978\n",
      "Epoch [4/10], Step [266/600], Loss: 0.5284, Train Acc:0.7981\n",
      "Epoch [4/10], Step [267/600], Loss: 0.5693, Train Acc:0.7982\n",
      "Epoch [4/10], Step [268/600], Loss: 0.5179, Train Acc:0.7986\n",
      "Epoch [4/10], Step [269/600], Loss: 0.5775, Train Acc:0.7986\n",
      "Epoch [4/10], Step [270/600], Loss: 0.5523, Train Acc:0.7988\n",
      "Epoch [4/10], Step [271/600], Loss: 0.5938, Train Acc:0.7988\n",
      "Epoch [4/10], Step [272/600], Loss: 0.6004, Train Acc:0.7989\n",
      "Epoch [4/10], Step [273/600], Loss: 0.6017, Train Acc:0.7989\n",
      "Epoch [4/10], Step [274/600], Loss: 0.5662, Train Acc:0.7989\n",
      "Epoch [4/10], Step [275/600], Loss: 0.7195, Train Acc:0.7985\n",
      "Epoch [4/10], Step [276/600], Loss: 0.6765, Train Acc:0.7986\n",
      "Epoch [4/10], Step [277/600], Loss: 0.6285, Train Acc:0.7984\n",
      "Epoch [4/10], Step [278/600], Loss: 0.5804, Train Acc:0.7985\n",
      "Epoch [4/10], Step [279/600], Loss: 0.6737, Train Acc:0.7984\n",
      "Epoch [4/10], Step [280/600], Loss: 0.6999, Train Acc:0.7983\n",
      "Epoch [4/10], Step [281/600], Loss: 0.5499, Train Acc:0.7985\n",
      "Epoch [4/10], Step [282/600], Loss: 0.5068, Train Acc:0.7987\n",
      "Epoch [4/10], Step [283/600], Loss: 0.5844, Train Acc:0.7988\n",
      "Epoch [4/10], Step [284/600], Loss: 0.6775, Train Acc:0.7987\n",
      "Epoch [4/10], Step [285/600], Loss: 0.5457, Train Acc:0.7988\n",
      "Epoch [4/10], Step [286/600], Loss: 0.6216, Train Acc:0.7989\n",
      "Epoch [4/10], Step [287/600], Loss: 0.7275, Train Acc:0.7987\n",
      "Epoch [4/10], Step [288/600], Loss: 0.5719, Train Acc:0.7989\n",
      "Epoch [4/10], Step [289/600], Loss: 0.6962, Train Acc:0.7988\n",
      "Epoch [4/10], Step [290/600], Loss: 0.5385, Train Acc:0.7988\n",
      "Epoch [4/10], Step [291/600], Loss: 0.5249, Train Acc:0.7990\n",
      "Epoch [4/10], Step [292/600], Loss: 0.6814, Train Acc:0.7990\n",
      "Epoch [4/10], Step [293/600], Loss: 0.6851, Train Acc:0.7989\n",
      "Epoch [4/10], Step [294/600], Loss: 0.6165, Train Acc:0.7990\n",
      "Epoch [4/10], Step [295/600], Loss: 0.6593, Train Acc:0.7989\n",
      "Epoch [4/10], Step [296/600], Loss: 0.5130, Train Acc:0.7992\n",
      "Epoch [4/10], Step [297/600], Loss: 0.5384, Train Acc:0.7993\n",
      "Epoch [4/10], Step [298/600], Loss: 0.4787, Train Acc:0.7996\n",
      "Epoch [4/10], Step [299/600], Loss: 0.6150, Train Acc:0.7996\n",
      "Epoch [4/10], Step [300/600], Loss: 0.5265, Train Acc:0.7996\n",
      "Epoch [4/10], Step [301/600], Loss: 0.6395, Train Acc:0.7997\n",
      "Epoch [4/10], Step [302/600], Loss: 0.5803, Train Acc:0.7997\n",
      "Epoch [4/10], Step [303/600], Loss: 0.6099, Train Acc:0.7997\n",
      "Epoch [4/10], Step [304/600], Loss: 0.5112, Train Acc:0.7999\n",
      "Epoch [4/10], Step [305/600], Loss: 0.6390, Train Acc:0.8000\n",
      "Epoch [4/10], Step [306/600], Loss: 0.6819, Train Acc:0.7999\n",
      "Epoch [4/10], Step [307/600], Loss: 0.6799, Train Acc:0.7997\n",
      "Epoch [4/10], Step [308/600], Loss: 0.8674, Train Acc:0.7993\n",
      "Epoch [4/10], Step [309/600], Loss: 0.5553, Train Acc:0.7994\n",
      "Epoch [4/10], Step [310/600], Loss: 0.6221, Train Acc:0.7993\n",
      "Epoch [4/10], Step [311/600], Loss: 0.8127, Train Acc:0.7992\n",
      "Epoch [4/10], Step [312/600], Loss: 0.5347, Train Acc:0.7993\n",
      "Epoch [4/10], Step [313/600], Loss: 0.5931, Train Acc:0.7993\n",
      "Epoch [4/10], Step [314/600], Loss: 0.5886, Train Acc:0.7994\n",
      "Epoch [4/10], Step [315/600], Loss: 0.5997, Train Acc:0.7995\n",
      "Epoch [4/10], Step [316/600], Loss: 0.5010, Train Acc:0.7996\n",
      "Epoch [4/10], Step [317/600], Loss: 0.5558, Train Acc:0.7996\n",
      "Epoch [4/10], Step [318/600], Loss: 0.6299, Train Acc:0.7996\n",
      "Epoch [4/10], Step [319/600], Loss: 0.5807, Train Acc:0.7997\n",
      "Epoch [4/10], Step [320/600], Loss: 0.6029, Train Acc:0.7997\n",
      "Epoch [4/10], Step [321/600], Loss: 0.5876, Train Acc:0.7999\n",
      "Epoch [4/10], Step [322/600], Loss: 0.6029, Train Acc:0.7999\n",
      "Epoch [4/10], Step [323/600], Loss: 0.6388, Train Acc:0.8000\n",
      "Epoch [4/10], Step [324/600], Loss: 0.5416, Train Acc:0.8001\n",
      "Epoch [4/10], Step [325/600], Loss: 0.5324, Train Acc:0.8002\n",
      "Epoch [4/10], Step [326/600], Loss: 0.5503, Train Acc:0.8003\n",
      "Epoch [4/10], Step [327/600], Loss: 0.6238, Train Acc:0.8003\n",
      "Epoch [4/10], Step [328/600], Loss: 0.5678, Train Acc:0.8004\n",
      "Epoch [4/10], Step [329/600], Loss: 0.5334, Train Acc:0.8005\n",
      "Epoch [4/10], Step [330/600], Loss: 0.5956, Train Acc:0.8007\n",
      "Epoch [4/10], Step [331/600], Loss: 0.6518, Train Acc:0.8006\n",
      "Epoch [4/10], Step [332/600], Loss: 0.6524, Train Acc:0.8005\n",
      "Epoch [4/10], Step [333/600], Loss: 0.6712, Train Acc:0.8005\n",
      "Epoch [4/10], Step [334/600], Loss: 0.5009, Train Acc:0.8005\n",
      "Epoch [4/10], Step [335/600], Loss: 0.6130, Train Acc:0.8005\n",
      "Epoch [4/10], Step [336/600], Loss: 0.6309, Train Acc:0.8004\n",
      "Epoch [4/10], Step [337/600], Loss: 0.6294, Train Acc:0.8004\n",
      "Epoch [4/10], Step [338/600], Loss: 0.5164, Train Acc:0.8006\n",
      "Epoch [4/10], Step [339/600], Loss: 0.6358, Train Acc:0.8006\n",
      "Epoch [4/10], Step [340/600], Loss: 0.5818, Train Acc:0.8006\n",
      "Epoch [4/10], Step [341/600], Loss: 0.5523, Train Acc:0.8006\n",
      "Epoch [4/10], Step [342/600], Loss: 0.5135, Train Acc:0.8008\n",
      "Epoch [4/10], Step [343/600], Loss: 0.6577, Train Acc:0.8007\n",
      "Epoch [4/10], Step [344/600], Loss: 0.5855, Train Acc:0.8007\n",
      "Epoch [4/10], Step [345/600], Loss: 0.4643, Train Acc:0.8009\n",
      "Epoch [4/10], Step [346/600], Loss: 0.5655, Train Acc:0.8010\n",
      "Epoch [4/10], Step [347/600], Loss: 0.5259, Train Acc:0.8010\n",
      "Epoch [4/10], Step [348/600], Loss: 0.7024, Train Acc:0.8009\n",
      "Epoch [4/10], Step [349/600], Loss: 0.5625, Train Acc:0.8010\n",
      "Epoch [4/10], Step [350/600], Loss: 0.7367, Train Acc:0.8009\n",
      "Epoch [4/10], Step [351/600], Loss: 0.6948, Train Acc:0.8007\n",
      "Epoch [4/10], Step [352/600], Loss: 0.6524, Train Acc:0.8008\n",
      "Epoch [4/10], Step [353/600], Loss: 0.6854, Train Acc:0.8008\n",
      "Epoch [4/10], Step [354/600], Loss: 0.5580, Train Acc:0.8008\n",
      "Epoch [4/10], Step [355/600], Loss: 0.5698, Train Acc:0.8008\n",
      "Epoch [4/10], Step [356/600], Loss: 0.6239, Train Acc:0.8009\n",
      "Epoch [4/10], Step [357/600], Loss: 0.7072, Train Acc:0.8008\n",
      "Epoch [4/10], Step [358/600], Loss: 0.6874, Train Acc:0.8008\n",
      "Epoch [4/10], Step [359/600], Loss: 0.4991, Train Acc:0.8011\n",
      "Epoch [4/10], Step [360/600], Loss: 0.5946, Train Acc:0.8011\n",
      "Epoch [4/10], Step [361/600], Loss: 0.5114, Train Acc:0.8012\n",
      "Epoch [4/10], Step [362/600], Loss: 0.6634, Train Acc:0.8012\n",
      "Epoch [4/10], Step [363/600], Loss: 0.6585, Train Acc:0.8012\n",
      "Epoch [4/10], Step [364/600], Loss: 0.6500, Train Acc:0.8012\n",
      "Epoch [4/10], Step [365/600], Loss: 0.4909, Train Acc:0.8014\n",
      "Epoch [4/10], Step [366/600], Loss: 0.5235, Train Acc:0.8016\n",
      "Epoch [4/10], Step [367/600], Loss: 0.5648, Train Acc:0.8016\n",
      "Epoch [4/10], Step [368/600], Loss: 0.5903, Train Acc:0.8017\n",
      "Epoch [4/10], Step [369/600], Loss: 0.5473, Train Acc:0.8018\n",
      "Epoch [4/10], Step [370/600], Loss: 0.7408, Train Acc:0.8016\n",
      "Epoch [4/10], Step [371/600], Loss: 0.7134, Train Acc:0.8015\n",
      "Epoch [4/10], Step [372/600], Loss: 0.5007, Train Acc:0.8015\n",
      "Epoch [4/10], Step [373/600], Loss: 0.6269, Train Acc:0.8014\n",
      "Epoch [4/10], Step [374/600], Loss: 0.5265, Train Acc:0.8015\n",
      "Epoch [4/10], Step [375/600], Loss: 0.4505, Train Acc:0.8018\n",
      "Epoch [4/10], Step [376/600], Loss: 0.7318, Train Acc:0.8016\n",
      "Epoch [4/10], Step [377/600], Loss: 0.5168, Train Acc:0.8016\n",
      "Epoch [4/10], Step [378/600], Loss: 0.5975, Train Acc:0.8016\n",
      "Epoch [4/10], Step [379/600], Loss: 0.6063, Train Acc:0.8015\n",
      "Epoch [4/10], Step [380/600], Loss: 0.5875, Train Acc:0.8016\n",
      "Epoch [4/10], Step [381/600], Loss: 0.6805, Train Acc:0.8014\n",
      "Epoch [4/10], Step [382/600], Loss: 0.5934, Train Acc:0.8014\n",
      "Epoch [4/10], Step [383/600], Loss: 0.5628, Train Acc:0.8015\n",
      "Epoch [4/10], Step [384/600], Loss: 0.4714, Train Acc:0.8018\n",
      "Epoch [4/10], Step [385/600], Loss: 0.7375, Train Acc:0.8016\n",
      "Epoch [4/10], Step [386/600], Loss: 0.6875, Train Acc:0.8016\n",
      "Epoch [4/10], Step [387/600], Loss: 0.5808, Train Acc:0.8016\n",
      "Epoch [4/10], Step [388/600], Loss: 0.6115, Train Acc:0.8016\n",
      "Epoch [4/10], Step [389/600], Loss: 0.7196, Train Acc:0.8015\n",
      "Epoch [4/10], Step [390/600], Loss: 0.5869, Train Acc:0.8016\n",
      "Epoch [4/10], Step [391/600], Loss: 0.6132, Train Acc:0.8016\n",
      "Epoch [4/10], Step [392/600], Loss: 0.5750, Train Acc:0.8017\n",
      "Epoch [4/10], Step [393/600], Loss: 0.4869, Train Acc:0.8019\n",
      "Epoch [4/10], Step [394/600], Loss: 0.6635, Train Acc:0.8019\n",
      "Epoch [4/10], Step [395/600], Loss: 0.5600, Train Acc:0.8019\n",
      "Epoch [4/10], Step [396/600], Loss: 0.5253, Train Acc:0.8019\n",
      "Epoch [4/10], Step [397/600], Loss: 0.5964, Train Acc:0.8019\n",
      "Epoch [4/10], Step [398/600], Loss: 0.6250, Train Acc:0.8019\n",
      "Epoch [4/10], Step [399/600], Loss: 0.6660, Train Acc:0.8018\n",
      "Epoch [4/10], Step [400/600], Loss: 0.5143, Train Acc:0.8020\n",
      "Epoch [4/10], Step [401/600], Loss: 0.7066, Train Acc:0.8019\n",
      "Epoch [4/10], Step [402/600], Loss: 0.5658, Train Acc:0.8019\n",
      "Epoch [4/10], Step [403/600], Loss: 0.5112, Train Acc:0.8021\n",
      "Epoch [4/10], Step [404/600], Loss: 0.5274, Train Acc:0.8022\n",
      "Epoch [4/10], Step [405/600], Loss: 0.6109, Train Acc:0.8022\n",
      "Epoch [4/10], Step [406/600], Loss: 0.5782, Train Acc:0.8022\n",
      "Epoch [4/10], Step [407/600], Loss: 0.6882, Train Acc:0.8022\n",
      "Epoch [4/10], Step [408/600], Loss: 0.6283, Train Acc:0.8021\n",
      "Epoch [4/10], Step [409/600], Loss: 0.6270, Train Acc:0.8021\n",
      "Epoch [4/10], Step [410/600], Loss: 0.5233, Train Acc:0.8022\n",
      "Epoch [4/10], Step [411/600], Loss: 0.5327, Train Acc:0.8021\n",
      "Epoch [4/10], Step [412/600], Loss: 0.5493, Train Acc:0.8021\n",
      "Epoch [4/10], Step [413/600], Loss: 0.5439, Train Acc:0.8021\n",
      "Epoch [4/10], Step [414/600], Loss: 0.6032, Train Acc:0.8021\n",
      "Epoch [4/10], Step [415/600], Loss: 0.6170, Train Acc:0.8021\n",
      "Epoch [4/10], Step [416/600], Loss: 0.6966, Train Acc:0.8021\n",
      "Epoch [4/10], Step [417/600], Loss: 0.6433, Train Acc:0.8020\n",
      "Epoch [4/10], Step [418/600], Loss: 0.5120, Train Acc:0.8021\n",
      "Epoch [4/10], Step [419/600], Loss: 0.6831, Train Acc:0.8019\n",
      "Epoch [4/10], Step [420/600], Loss: 0.6382, Train Acc:0.8020\n",
      "Epoch [4/10], Step [421/600], Loss: 0.5077, Train Acc:0.8021\n",
      "Epoch [4/10], Step [422/600], Loss: 0.5778, Train Acc:0.8022\n",
      "Epoch [4/10], Step [423/600], Loss: 0.6364, Train Acc:0.8021\n",
      "Epoch [4/10], Step [424/600], Loss: 0.6490, Train Acc:0.8021\n",
      "Epoch [4/10], Step [425/600], Loss: 0.4737, Train Acc:0.8022\n",
      "Epoch [4/10], Step [426/600], Loss: 0.7096, Train Acc:0.8021\n",
      "Epoch [4/10], Step [427/600], Loss: 0.5453, Train Acc:0.8021\n",
      "Epoch [4/10], Step [428/600], Loss: 0.5596, Train Acc:0.8021\n",
      "Epoch [4/10], Step [429/600], Loss: 0.5238, Train Acc:0.8023\n",
      "Epoch [4/10], Step [430/600], Loss: 0.6459, Train Acc:0.8023\n",
      "Epoch [4/10], Step [431/600], Loss: 0.6586, Train Acc:0.8022\n",
      "Epoch [4/10], Step [432/600], Loss: 0.5910, Train Acc:0.8023\n",
      "Epoch [4/10], Step [433/600], Loss: 0.6185, Train Acc:0.8024\n",
      "Epoch [4/10], Step [434/600], Loss: 0.6339, Train Acc:0.8024\n",
      "Epoch [4/10], Step [435/600], Loss: 0.6960, Train Acc:0.8024\n",
      "Epoch [4/10], Step [436/600], Loss: 0.4786, Train Acc:0.8025\n",
      "Epoch [4/10], Step [437/600], Loss: 0.6581, Train Acc:0.8025\n",
      "Epoch [4/10], Step [438/600], Loss: 0.5709, Train Acc:0.8025\n",
      "Epoch [4/10], Step [439/600], Loss: 0.6095, Train Acc:0.8024\n",
      "Epoch [4/10], Step [440/600], Loss: 0.5404, Train Acc:0.8025\n",
      "Epoch [4/10], Step [441/600], Loss: 0.6729, Train Acc:0.8024\n",
      "Epoch [4/10], Step [442/600], Loss: 0.5741, Train Acc:0.8025\n",
      "Epoch [4/10], Step [443/600], Loss: 0.6548, Train Acc:0.8024\n",
      "Epoch [4/10], Step [444/600], Loss: 0.5515, Train Acc:0.8025\n",
      "Epoch [4/10], Step [445/600], Loss: 0.7630, Train Acc:0.8024\n",
      "Epoch [4/10], Step [446/600], Loss: 0.5783, Train Acc:0.8025\n",
      "Epoch [4/10], Step [447/600], Loss: 0.5509, Train Acc:0.8026\n",
      "Epoch [4/10], Step [448/600], Loss: 0.5962, Train Acc:0.8026\n",
      "Epoch [4/10], Step [449/600], Loss: 0.6014, Train Acc:0.8025\n",
      "Epoch [4/10], Step [450/600], Loss: 0.5875, Train Acc:0.8025\n",
      "Epoch [4/10], Step [451/600], Loss: 0.6213, Train Acc:0.8026\n",
      "Epoch [4/10], Step [452/600], Loss: 0.5637, Train Acc:0.8027\n",
      "Epoch [4/10], Step [453/600], Loss: 0.6914, Train Acc:0.8026\n",
      "Epoch [4/10], Step [454/600], Loss: 0.6295, Train Acc:0.8026\n",
      "Epoch [4/10], Step [455/600], Loss: 0.5838, Train Acc:0.8026\n",
      "Epoch [4/10], Step [456/600], Loss: 0.5262, Train Acc:0.8027\n",
      "Epoch [4/10], Step [457/600], Loss: 0.5232, Train Acc:0.8029\n",
      "Epoch [4/10], Step [458/600], Loss: 0.4637, Train Acc:0.8030\n",
      "Epoch [4/10], Step [459/600], Loss: 0.6181, Train Acc:0.8031\n",
      "Epoch [4/10], Step [460/600], Loss: 0.6309, Train Acc:0.8031\n",
      "Epoch [4/10], Step [461/600], Loss: 0.7098, Train Acc:0.8029\n",
      "Epoch [4/10], Step [462/600], Loss: 0.5178, Train Acc:0.8029\n",
      "Epoch [4/10], Step [463/600], Loss: 0.5698, Train Acc:0.8030\n",
      "Epoch [4/10], Step [464/600], Loss: 0.6934, Train Acc:0.8030\n",
      "Epoch [4/10], Step [465/600], Loss: 0.5022, Train Acc:0.8031\n",
      "Epoch [4/10], Step [466/600], Loss: 0.6490, Train Acc:0.8030\n",
      "Epoch [4/10], Step [467/600], Loss: 0.5507, Train Acc:0.8032\n",
      "Epoch [4/10], Step [468/600], Loss: 0.6224, Train Acc:0.8032\n",
      "Epoch [4/10], Step [469/600], Loss: 0.6954, Train Acc:0.8031\n",
      "Epoch [4/10], Step [470/600], Loss: 0.5077, Train Acc:0.8030\n",
      "Epoch [4/10], Step [471/600], Loss: 0.7366, Train Acc:0.8029\n",
      "Epoch [4/10], Step [472/600], Loss: 0.5686, Train Acc:0.8029\n",
      "Epoch [4/10], Step [473/600], Loss: 0.5366, Train Acc:0.8030\n",
      "Epoch [4/10], Step [474/600], Loss: 0.5585, Train Acc:0.8031\n",
      "Epoch [4/10], Step [475/600], Loss: 0.6654, Train Acc:0.8031\n",
      "Epoch [4/10], Step [476/600], Loss: 0.5170, Train Acc:0.8031\n",
      "Epoch [4/10], Step [477/600], Loss: 0.6191, Train Acc:0.8032\n",
      "Epoch [4/10], Step [478/600], Loss: 0.6153, Train Acc:0.8032\n",
      "Epoch [4/10], Step [479/600], Loss: 0.5891, Train Acc:0.8032\n",
      "Epoch [4/10], Step [480/600], Loss: 0.6888, Train Acc:0.8031\n",
      "Epoch [4/10], Step [481/600], Loss: 0.6969, Train Acc:0.8031\n",
      "Epoch [4/10], Step [482/600], Loss: 0.5776, Train Acc:0.8032\n",
      "Epoch [4/10], Step [483/600], Loss: 0.5565, Train Acc:0.8033\n",
      "Epoch [4/10], Step [484/600], Loss: 0.6872, Train Acc:0.8032\n",
      "Epoch [4/10], Step [485/600], Loss: 0.6221, Train Acc:0.8031\n",
      "Epoch [4/10], Step [486/600], Loss: 0.6285, Train Acc:0.8032\n",
      "Epoch [4/10], Step [487/600], Loss: 0.6853, Train Acc:0.8031\n",
      "Epoch [4/10], Step [488/600], Loss: 0.5319, Train Acc:0.8031\n",
      "Epoch [4/10], Step [489/600], Loss: 0.5487, Train Acc:0.8032\n",
      "Epoch [4/10], Step [490/600], Loss: 0.7184, Train Acc:0.8032\n",
      "Epoch [4/10], Step [491/600], Loss: 0.6116, Train Acc:0.8032\n",
      "Epoch [4/10], Step [492/600], Loss: 0.6039, Train Acc:0.8031\n",
      "Epoch [4/10], Step [493/600], Loss: 0.7185, Train Acc:0.8031\n",
      "Epoch [4/10], Step [494/600], Loss: 0.5596, Train Acc:0.8032\n",
      "Epoch [4/10], Step [495/600], Loss: 0.5419, Train Acc:0.8032\n",
      "Epoch [4/10], Step [496/600], Loss: 0.6112, Train Acc:0.8032\n",
      "Epoch [4/10], Step [497/600], Loss: 0.5132, Train Acc:0.8033\n",
      "Epoch [4/10], Step [498/600], Loss: 0.5386, Train Acc:0.8034\n",
      "Epoch [4/10], Step [499/600], Loss: 0.6498, Train Acc:0.8033\n",
      "Epoch [4/10], Step [500/600], Loss: 0.7296, Train Acc:0.8032\n",
      "Epoch [4/10], Step [501/600], Loss: 0.6705, Train Acc:0.8032\n",
      "Epoch [4/10], Step [502/600], Loss: 0.6243, Train Acc:0.8032\n",
      "Epoch [4/10], Step [503/600], Loss: 0.5981, Train Acc:0.8031\n",
      "Epoch [4/10], Step [504/600], Loss: 0.6508, Train Acc:0.8031\n",
      "Epoch [4/10], Step [505/600], Loss: 0.5218, Train Acc:0.8032\n",
      "Epoch [4/10], Step [506/600], Loss: 0.7051, Train Acc:0.8031\n",
      "Epoch [4/10], Step [507/600], Loss: 0.5391, Train Acc:0.8032\n",
      "Epoch [4/10], Step [508/600], Loss: 0.5279, Train Acc:0.8032\n",
      "Epoch [4/10], Step [509/600], Loss: 0.5770, Train Acc:0.8033\n",
      "Epoch [4/10], Step [510/600], Loss: 0.6601, Train Acc:0.8032\n",
      "Epoch [4/10], Step [511/600], Loss: 0.6113, Train Acc:0.8032\n",
      "Epoch [4/10], Step [512/600], Loss: 0.5280, Train Acc:0.8034\n",
      "Epoch [4/10], Step [513/600], Loss: 0.6253, Train Acc:0.8034\n",
      "Epoch [4/10], Step [514/600], Loss: 0.4907, Train Acc:0.8035\n",
      "Epoch [4/10], Step [515/600], Loss: 0.8310, Train Acc:0.8033\n",
      "Epoch [4/10], Step [516/600], Loss: 0.5362, Train Acc:0.8034\n",
      "Epoch [4/10], Step [517/600], Loss: 0.6384, Train Acc:0.8034\n",
      "Epoch [4/10], Step [518/600], Loss: 0.4902, Train Acc:0.8035\n",
      "Epoch [4/10], Step [519/600], Loss: 0.3971, Train Acc:0.8037\n",
      "Epoch [4/10], Step [520/600], Loss: 0.6577, Train Acc:0.8037\n",
      "Epoch [4/10], Step [521/600], Loss: 0.5741, Train Acc:0.8037\n",
      "Epoch [4/10], Step [522/600], Loss: 0.6269, Train Acc:0.8037\n",
      "Epoch [4/10], Step [523/600], Loss: 0.4863, Train Acc:0.8038\n",
      "Epoch [4/10], Step [524/600], Loss: 0.4503, Train Acc:0.8039\n",
      "Epoch [4/10], Step [525/600], Loss: 0.8483, Train Acc:0.8037\n",
      "Epoch [4/10], Step [526/600], Loss: 0.6399, Train Acc:0.8036\n",
      "Epoch [4/10], Step [527/600], Loss: 0.6199, Train Acc:0.8037\n",
      "Epoch [4/10], Step [528/600], Loss: 0.5269, Train Acc:0.8037\n",
      "Epoch [4/10], Step [529/600], Loss: 0.7542, Train Acc:0.8036\n",
      "Epoch [4/10], Step [530/600], Loss: 0.5428, Train Acc:0.8036\n",
      "Epoch [4/10], Step [531/600], Loss: 0.6727, Train Acc:0.8036\n",
      "Epoch [4/10], Step [532/600], Loss: 0.5955, Train Acc:0.8036\n",
      "Epoch [4/10], Step [533/600], Loss: 0.5930, Train Acc:0.8036\n",
      "Epoch [4/10], Step [534/600], Loss: 0.7431, Train Acc:0.8035\n",
      "Epoch [4/10], Step [535/600], Loss: 0.5409, Train Acc:0.8036\n",
      "Epoch [4/10], Step [536/600], Loss: 0.5003, Train Acc:0.8037\n",
      "Epoch [4/10], Step [537/600], Loss: 0.6702, Train Acc:0.8036\n",
      "Epoch [4/10], Step [538/600], Loss: 0.5643, Train Acc:0.8037\n",
      "Epoch [4/10], Step [539/600], Loss: 0.7431, Train Acc:0.8034\n",
      "Epoch [4/10], Step [540/600], Loss: 0.6081, Train Acc:0.8034\n",
      "Epoch [4/10], Step [541/600], Loss: 0.5563, Train Acc:0.8034\n",
      "Epoch [4/10], Step [542/600], Loss: 0.6893, Train Acc:0.8033\n",
      "Epoch [4/10], Step [543/600], Loss: 0.6733, Train Acc:0.8034\n",
      "Epoch [4/10], Step [544/600], Loss: 0.5060, Train Acc:0.8035\n",
      "Epoch [4/10], Step [545/600], Loss: 0.5241, Train Acc:0.8036\n",
      "Epoch [4/10], Step [546/600], Loss: 0.5305, Train Acc:0.8036\n",
      "Epoch [4/10], Step [547/600], Loss: 0.5389, Train Acc:0.8036\n",
      "Epoch [4/10], Step [548/600], Loss: 0.5722, Train Acc:0.8036\n",
      "Epoch [4/10], Step [549/600], Loss: 0.6178, Train Acc:0.8036\n",
      "Epoch [4/10], Step [550/600], Loss: 0.7361, Train Acc:0.8035\n",
      "Epoch [4/10], Step [551/600], Loss: 0.5501, Train Acc:0.8036\n",
      "Epoch [4/10], Step [552/600], Loss: 0.5158, Train Acc:0.8037\n",
      "Epoch [4/10], Step [553/600], Loss: 0.6192, Train Acc:0.8036\n",
      "Epoch [4/10], Step [554/600], Loss: 0.5203, Train Acc:0.8037\n",
      "Epoch [4/10], Step [555/600], Loss: 0.6802, Train Acc:0.8036\n",
      "Epoch [4/10], Step [556/600], Loss: 0.5361, Train Acc:0.8036\n",
      "Epoch [4/10], Step [557/600], Loss: 0.6188, Train Acc:0.8036\n",
      "Epoch [4/10], Step [558/600], Loss: 0.7121, Train Acc:0.8036\n",
      "Epoch [4/10], Step [559/600], Loss: 0.5796, Train Acc:0.8036\n",
      "Epoch [4/10], Step [560/600], Loss: 0.6366, Train Acc:0.8037\n",
      "Epoch [4/10], Step [561/600], Loss: 0.4173, Train Acc:0.8038\n",
      "Epoch [4/10], Step [562/600], Loss: 0.6725, Train Acc:0.8037\n",
      "Epoch [4/10], Step [563/600], Loss: 0.6295, Train Acc:0.8037\n",
      "Epoch [4/10], Step [564/600], Loss: 0.6343, Train Acc:0.8037\n",
      "Epoch [4/10], Step [565/600], Loss: 0.4957, Train Acc:0.8038\n",
      "Epoch [4/10], Step [566/600], Loss: 0.5819, Train Acc:0.8039\n",
      "Epoch [4/10], Step [567/600], Loss: 0.5561, Train Acc:0.8039\n",
      "Epoch [4/10], Step [568/600], Loss: 0.6603, Train Acc:0.8039\n",
      "Epoch [4/10], Step [569/600], Loss: 0.6907, Train Acc:0.8039\n",
      "Epoch [4/10], Step [570/600], Loss: 0.5864, Train Acc:0.8038\n",
      "Epoch [4/10], Step [571/600], Loss: 0.6617, Train Acc:0.8038\n",
      "Epoch [4/10], Step [572/600], Loss: 0.5425, Train Acc:0.8040\n",
      "Epoch [4/10], Step [573/600], Loss: 0.5614, Train Acc:0.8040\n",
      "Epoch [4/10], Step [574/600], Loss: 0.5765, Train Acc:0.8040\n",
      "Epoch [4/10], Step [575/600], Loss: 0.7943, Train Acc:0.8039\n",
      "Epoch [4/10], Step [576/600], Loss: 0.5165, Train Acc:0.8040\n",
      "Epoch [4/10], Step [577/600], Loss: 0.8109, Train Acc:0.8038\n",
      "Epoch [4/10], Step [578/600], Loss: 0.5431, Train Acc:0.8038\n",
      "Epoch [4/10], Step [579/600], Loss: 0.6726, Train Acc:0.8038\n",
      "Epoch [4/10], Step [580/600], Loss: 0.5656, Train Acc:0.8038\n",
      "Epoch [4/10], Step [581/600], Loss: 0.6554, Train Acc:0.8038\n",
      "Epoch [4/10], Step [582/600], Loss: 0.6378, Train Acc:0.8038\n",
      "Epoch [4/10], Step [583/600], Loss: 0.5000, Train Acc:0.8039\n",
      "Epoch [4/10], Step [584/600], Loss: 0.5114, Train Acc:0.8040\n",
      "Epoch [4/10], Step [585/600], Loss: 0.6220, Train Acc:0.8040\n",
      "Epoch [4/10], Step [586/600], Loss: 0.6425, Train Acc:0.8040\n",
      "Epoch [4/10], Step [587/600], Loss: 0.4088, Train Acc:0.8041\n",
      "Epoch [4/10], Step [588/600], Loss: 0.6816, Train Acc:0.8040\n",
      "Epoch [4/10], Step [589/600], Loss: 0.5702, Train Acc:0.8040\n",
      "Epoch [4/10], Step [590/600], Loss: 0.6116, Train Acc:0.8040\n",
      "Epoch [4/10], Step [591/600], Loss: 0.5890, Train Acc:0.8040\n",
      "Epoch [4/10], Step [592/600], Loss: 0.6145, Train Acc:0.8039\n",
      "Epoch [4/10], Step [593/600], Loss: 0.6890, Train Acc:0.8039\n",
      "Epoch [4/10], Step [594/600], Loss: 0.6604, Train Acc:0.8038\n",
      "Epoch [4/10], Step [595/600], Loss: 0.5191, Train Acc:0.8039\n",
      "Epoch [4/10], Step [596/600], Loss: 0.5951, Train Acc:0.8039\n",
      "Epoch [4/10], Step [597/600], Loss: 0.5017, Train Acc:0.8039\n",
      "Epoch [4/10], Step [598/600], Loss: 0.6489, Train Acc:0.8038\n",
      "Epoch [4/10], Step [599/600], Loss: 0.5922, Train Acc:0.8038\n",
      "Epoch [4/10], Step [600/600], Loss: 0.5799, Train Acc:0.8038\n",
      "Epoch [5/10], Step [1/600], Loss: 0.7133, Train Acc:0.7600\n",
      "Epoch [5/10], Step [2/600], Loss: 0.6289, Train Acc:0.7450\n",
      "Epoch [5/10], Step [3/600], Loss: 0.4941, Train Acc:0.7800\n",
      "Epoch [5/10], Step [4/600], Loss: 0.5574, Train Acc:0.7875\n",
      "Epoch [5/10], Step [5/600], Loss: 0.6862, Train Acc:0.7840\n",
      "Epoch [5/10], Step [6/600], Loss: 0.6856, Train Acc:0.7817\n",
      "Epoch [5/10], Step [7/600], Loss: 0.6829, Train Acc:0.7786\n",
      "Epoch [5/10], Step [8/600], Loss: 0.6490, Train Acc:0.7812\n",
      "Epoch [5/10], Step [9/600], Loss: 0.6024, Train Acc:0.7867\n",
      "Epoch [5/10], Step [10/600], Loss: 0.7054, Train Acc:0.7820\n",
      "Epoch [5/10], Step [11/600], Loss: 0.5947, Train Acc:0.7900\n",
      "Epoch [5/10], Step [12/600], Loss: 0.6104, Train Acc:0.7908\n",
      "Epoch [5/10], Step [13/600], Loss: 0.5921, Train Acc:0.7931\n",
      "Epoch [5/10], Step [14/600], Loss: 0.6132, Train Acc:0.7929\n",
      "Epoch [5/10], Step [15/600], Loss: 0.6740, Train Acc:0.7907\n",
      "Epoch [5/10], Step [16/600], Loss: 0.5189, Train Acc:0.7950\n",
      "Epoch [5/10], Step [17/600], Loss: 0.5405, Train Acc:0.7982\n",
      "Epoch [5/10], Step [18/600], Loss: 0.5600, Train Acc:0.7983\n",
      "Epoch [5/10], Step [19/600], Loss: 0.6281, Train Acc:0.7958\n",
      "Epoch [5/10], Step [20/600], Loss: 0.6494, Train Acc:0.7935\n",
      "Epoch [5/10], Step [21/600], Loss: 0.5378, Train Acc:0.7952\n",
      "Epoch [5/10], Step [22/600], Loss: 0.5436, Train Acc:0.7977\n",
      "Epoch [5/10], Step [23/600], Loss: 0.5120, Train Acc:0.7991\n",
      "Epoch [5/10], Step [24/600], Loss: 0.5248, Train Acc:0.8013\n",
      "Epoch [5/10], Step [25/600], Loss: 0.7533, Train Acc:0.7996\n",
      "Epoch [5/10], Step [26/600], Loss: 0.7237, Train Acc:0.7981\n",
      "Epoch [5/10], Step [27/600], Loss: 0.7237, Train Acc:0.7978\n",
      "Epoch [5/10], Step [28/600], Loss: 0.5725, Train Acc:0.7993\n",
      "Epoch [5/10], Step [29/600], Loss: 0.5811, Train Acc:0.8000\n",
      "Epoch [5/10], Step [30/600], Loss: 0.6803, Train Acc:0.7997\n",
      "Epoch [5/10], Step [31/600], Loss: 0.6458, Train Acc:0.7987\n",
      "Epoch [5/10], Step [32/600], Loss: 0.6877, Train Acc:0.7956\n",
      "Epoch [5/10], Step [33/600], Loss: 0.5162, Train Acc:0.7961\n",
      "Epoch [5/10], Step [34/600], Loss: 0.5050, Train Acc:0.7982\n",
      "Epoch [5/10], Step [35/600], Loss: 0.5548, Train Acc:0.7977\n",
      "Epoch [5/10], Step [36/600], Loss: 0.5024, Train Acc:0.7989\n",
      "Epoch [5/10], Step [37/600], Loss: 0.5674, Train Acc:0.8003\n",
      "Epoch [5/10], Step [38/600], Loss: 0.7291, Train Acc:0.7989\n",
      "Epoch [5/10], Step [39/600], Loss: 0.6439, Train Acc:0.7987\n",
      "Epoch [5/10], Step [40/600], Loss: 0.6610, Train Acc:0.7965\n",
      "Epoch [5/10], Step [41/600], Loss: 0.6520, Train Acc:0.7961\n",
      "Epoch [5/10], Step [42/600], Loss: 0.5497, Train Acc:0.7964\n",
      "Epoch [5/10], Step [43/600], Loss: 0.6960, Train Acc:0.7951\n",
      "Epoch [5/10], Step [44/600], Loss: 0.6020, Train Acc:0.7955\n",
      "Epoch [5/10], Step [45/600], Loss: 0.4872, Train Acc:0.7967\n",
      "Epoch [5/10], Step [46/600], Loss: 0.6003, Train Acc:0.7967\n",
      "Epoch [5/10], Step [47/600], Loss: 0.6811, Train Acc:0.7957\n",
      "Epoch [5/10], Step [48/600], Loss: 0.4472, Train Acc:0.7977\n",
      "Epoch [5/10], Step [49/600], Loss: 0.5965, Train Acc:0.7980\n",
      "Epoch [5/10], Step [50/600], Loss: 0.5727, Train Acc:0.7984\n",
      "Epoch [5/10], Step [51/600], Loss: 0.5733, Train Acc:0.7990\n",
      "Epoch [5/10], Step [52/600], Loss: 0.7022, Train Acc:0.7985\n",
      "Epoch [5/10], Step [53/600], Loss: 0.6616, Train Acc:0.7985\n",
      "Epoch [5/10], Step [54/600], Loss: 0.4499, Train Acc:0.7998\n",
      "Epoch [5/10], Step [55/600], Loss: 0.5934, Train Acc:0.8002\n",
      "Epoch [5/10], Step [56/600], Loss: 0.5306, Train Acc:0.7996\n",
      "Epoch [5/10], Step [57/600], Loss: 0.7133, Train Acc:0.7995\n",
      "Epoch [5/10], Step [58/600], Loss: 0.5821, Train Acc:0.7995\n",
      "Epoch [5/10], Step [59/600], Loss: 0.5878, Train Acc:0.7998\n",
      "Epoch [5/10], Step [60/600], Loss: 0.4821, Train Acc:0.8010\n",
      "Epoch [5/10], Step [61/600], Loss: 0.5954, Train Acc:0.8010\n",
      "Epoch [5/10], Step [62/600], Loss: 0.5715, Train Acc:0.8015\n",
      "Epoch [5/10], Step [63/600], Loss: 0.5485, Train Acc:0.8029\n",
      "Epoch [5/10], Step [64/600], Loss: 0.5654, Train Acc:0.8034\n",
      "Epoch [5/10], Step [65/600], Loss: 0.6316, Train Acc:0.8035\n",
      "Epoch [5/10], Step [66/600], Loss: 0.4739, Train Acc:0.8047\n",
      "Epoch [5/10], Step [67/600], Loss: 0.5459, Train Acc:0.8051\n",
      "Epoch [5/10], Step [68/600], Loss: 0.7577, Train Acc:0.8038\n",
      "Epoch [5/10], Step [69/600], Loss: 0.5636, Train Acc:0.8043\n",
      "Epoch [5/10], Step [70/600], Loss: 0.6449, Train Acc:0.8039\n",
      "Epoch [5/10], Step [71/600], Loss: 0.6136, Train Acc:0.8041\n",
      "Epoch [5/10], Step [72/600], Loss: 0.7680, Train Acc:0.8032\n",
      "Epoch [5/10], Step [73/600], Loss: 0.7368, Train Acc:0.8025\n",
      "Epoch [5/10], Step [74/600], Loss: 0.6075, Train Acc:0.8026\n",
      "Epoch [5/10], Step [75/600], Loss: 0.6691, Train Acc:0.8025\n",
      "Epoch [5/10], Step [76/600], Loss: 0.5628, Train Acc:0.8026\n",
      "Epoch [5/10], Step [77/600], Loss: 0.6247, Train Acc:0.8027\n",
      "Epoch [5/10], Step [78/600], Loss: 0.5452, Train Acc:0.8033\n",
      "Epoch [5/10], Step [79/600], Loss: 0.6834, Train Acc:0.8030\n",
      "Epoch [5/10], Step [80/600], Loss: 0.5652, Train Acc:0.8035\n",
      "Epoch [5/10], Step [81/600], Loss: 0.6164, Train Acc:0.8035\n",
      "Epoch [5/10], Step [82/600], Loss: 0.5852, Train Acc:0.8028\n",
      "Epoch [5/10], Step [83/600], Loss: 0.5849, Train Acc:0.8030\n",
      "Epoch [5/10], Step [84/600], Loss: 0.5492, Train Acc:0.8031\n",
      "Epoch [5/10], Step [85/600], Loss: 0.4968, Train Acc:0.8039\n",
      "Epoch [5/10], Step [86/600], Loss: 0.7603, Train Acc:0.8034\n",
      "Epoch [5/10], Step [87/600], Loss: 0.5744, Train Acc:0.8036\n",
      "Epoch [5/10], Step [88/600], Loss: 0.5564, Train Acc:0.8032\n",
      "Epoch [5/10], Step [89/600], Loss: 0.4641, Train Acc:0.8042\n",
      "Epoch [5/10], Step [90/600], Loss: 0.5946, Train Acc:0.8042\n",
      "Epoch [5/10], Step [91/600], Loss: 0.5823, Train Acc:0.8047\n",
      "Epoch [5/10], Step [92/600], Loss: 0.5670, Train Acc:0.8048\n",
      "Epoch [5/10], Step [93/600], Loss: 0.4615, Train Acc:0.8056\n",
      "Epoch [5/10], Step [94/600], Loss: 0.5690, Train Acc:0.8060\n",
      "Epoch [5/10], Step [95/600], Loss: 0.5624, Train Acc:0.8063\n",
      "Epoch [5/10], Step [96/600], Loss: 0.5860, Train Acc:0.8064\n",
      "Epoch [5/10], Step [97/600], Loss: 0.7213, Train Acc:0.8056\n",
      "Epoch [5/10], Step [98/600], Loss: 0.6571, Train Acc:0.8054\n",
      "Epoch [5/10], Step [99/600], Loss: 0.7145, Train Acc:0.8051\n",
      "Epoch [5/10], Step [100/600], Loss: 0.5566, Train Acc:0.8055\n",
      "Epoch [5/10], Step [101/600], Loss: 0.6354, Train Acc:0.8052\n",
      "Epoch [5/10], Step [102/600], Loss: 0.6046, Train Acc:0.8057\n",
      "Epoch [5/10], Step [103/600], Loss: 0.5584, Train Acc:0.8060\n",
      "Epoch [5/10], Step [104/600], Loss: 0.5643, Train Acc:0.8060\n",
      "Epoch [5/10], Step [105/600], Loss: 0.7198, Train Acc:0.8054\n",
      "Epoch [5/10], Step [106/600], Loss: 0.6476, Train Acc:0.8055\n",
      "Epoch [5/10], Step [107/600], Loss: 0.6230, Train Acc:0.8057\n",
      "Epoch [5/10], Step [108/600], Loss: 0.6804, Train Acc:0.8056\n",
      "Epoch [5/10], Step [109/600], Loss: 0.5904, Train Acc:0.8052\n",
      "Epoch [5/10], Step [110/600], Loss: 0.7946, Train Acc:0.8045\n",
      "Epoch [5/10], Step [111/600], Loss: 0.4893, Train Acc:0.8050\n",
      "Epoch [5/10], Step [112/600], Loss: 0.6398, Train Acc:0.8046\n",
      "Epoch [5/10], Step [113/600], Loss: 0.4821, Train Acc:0.8054\n",
      "Epoch [5/10], Step [114/600], Loss: 0.5741, Train Acc:0.8052\n",
      "Epoch [5/10], Step [115/600], Loss: 0.6194, Train Acc:0.8053\n",
      "Epoch [5/10], Step [116/600], Loss: 0.5609, Train Acc:0.8056\n",
      "Epoch [5/10], Step [117/600], Loss: 0.4688, Train Acc:0.8060\n",
      "Epoch [5/10], Step [118/600], Loss: 0.6499, Train Acc:0.8059\n",
      "Epoch [5/10], Step [119/600], Loss: 0.6212, Train Acc:0.8062\n",
      "Epoch [5/10], Step [120/600], Loss: 0.6420, Train Acc:0.8059\n",
      "Epoch [5/10], Step [121/600], Loss: 0.5840, Train Acc:0.8062\n",
      "Epoch [5/10], Step [122/600], Loss: 0.5741, Train Acc:0.8066\n",
      "Epoch [5/10], Step [123/600], Loss: 0.6716, Train Acc:0.8060\n",
      "Epoch [5/10], Step [124/600], Loss: 0.5190, Train Acc:0.8062\n",
      "Epoch [5/10], Step [125/600], Loss: 0.5068, Train Acc:0.8064\n",
      "Epoch [5/10], Step [126/600], Loss: 0.4911, Train Acc:0.8067\n",
      "Epoch [5/10], Step [127/600], Loss: 0.6662, Train Acc:0.8069\n",
      "Epoch [5/10], Step [128/600], Loss: 0.7106, Train Acc:0.8064\n",
      "Epoch [5/10], Step [129/600], Loss: 0.7079, Train Acc:0.8063\n",
      "Epoch [5/10], Step [130/600], Loss: 0.5040, Train Acc:0.8067\n",
      "Epoch [5/10], Step [131/600], Loss: 0.6040, Train Acc:0.8069\n",
      "Epoch [5/10], Step [132/600], Loss: 0.5563, Train Acc:0.8073\n",
      "Epoch [5/10], Step [133/600], Loss: 0.5287, Train Acc:0.8071\n",
      "Epoch [5/10], Step [134/600], Loss: 0.6747, Train Acc:0.8069\n",
      "Epoch [5/10], Step [135/600], Loss: 0.5708, Train Acc:0.8068\n",
      "Epoch [5/10], Step [136/600], Loss: 0.7639, Train Acc:0.8065\n",
      "Epoch [5/10], Step [137/600], Loss: 0.5184, Train Acc:0.8068\n",
      "Epoch [5/10], Step [138/600], Loss: 0.6848, Train Acc:0.8063\n",
      "Epoch [5/10], Step [139/600], Loss: 0.5299, Train Acc:0.8065\n",
      "Epoch [5/10], Step [140/600], Loss: 0.5790, Train Acc:0.8065\n",
      "Epoch [5/10], Step [141/600], Loss: 0.6018, Train Acc:0.8065\n",
      "Epoch [5/10], Step [142/600], Loss: 0.7298, Train Acc:0.8063\n",
      "Epoch [5/10], Step [143/600], Loss: 0.7005, Train Acc:0.8059\n",
      "Epoch [5/10], Step [144/600], Loss: 0.6146, Train Acc:0.8059\n",
      "Epoch [5/10], Step [145/600], Loss: 0.5762, Train Acc:0.8060\n",
      "Epoch [5/10], Step [146/600], Loss: 0.6432, Train Acc:0.8060\n",
      "Epoch [5/10], Step [147/600], Loss: 0.3805, Train Acc:0.8068\n",
      "Epoch [5/10], Step [148/600], Loss: 0.5182, Train Acc:0.8071\n",
      "Epoch [5/10], Step [149/600], Loss: 0.6381, Train Acc:0.8072\n",
      "Epoch [5/10], Step [150/600], Loss: 0.5190, Train Acc:0.8074\n",
      "Epoch [5/10], Step [151/600], Loss: 0.6321, Train Acc:0.8075\n",
      "Epoch [5/10], Step [152/600], Loss: 0.6252, Train Acc:0.8075\n",
      "Epoch [5/10], Step [153/600], Loss: 0.5783, Train Acc:0.8076\n",
      "Epoch [5/10], Step [154/600], Loss: 0.5804, Train Acc:0.8074\n",
      "Epoch [5/10], Step [155/600], Loss: 0.6023, Train Acc:0.8074\n",
      "Epoch [5/10], Step [156/600], Loss: 0.5761, Train Acc:0.8072\n",
      "Epoch [5/10], Step [157/600], Loss: 0.6110, Train Acc:0.8070\n",
      "Epoch [5/10], Step [158/600], Loss: 0.5358, Train Acc:0.8072\n",
      "Epoch [5/10], Step [159/600], Loss: 0.4800, Train Acc:0.8074\n",
      "Epoch [5/10], Step [160/600], Loss: 0.6119, Train Acc:0.8076\n",
      "Epoch [5/10], Step [161/600], Loss: 0.7081, Train Acc:0.8071\n",
      "Epoch [5/10], Step [162/600], Loss: 0.7128, Train Acc:0.8067\n",
      "Epoch [5/10], Step [163/600], Loss: 0.6410, Train Acc:0.8066\n",
      "Epoch [5/10], Step [164/600], Loss: 0.6305, Train Acc:0.8066\n",
      "Epoch [5/10], Step [165/600], Loss: 0.7760, Train Acc:0.8061\n",
      "Epoch [5/10], Step [166/600], Loss: 0.5841, Train Acc:0.8061\n",
      "Epoch [5/10], Step [167/600], Loss: 0.4910, Train Acc:0.8065\n",
      "Epoch [5/10], Step [168/600], Loss: 0.5681, Train Acc:0.8066\n",
      "Epoch [5/10], Step [169/600], Loss: 0.6842, Train Acc:0.8063\n",
      "Epoch [5/10], Step [170/600], Loss: 0.5988, Train Acc:0.8062\n",
      "Epoch [5/10], Step [171/600], Loss: 0.5091, Train Acc:0.8065\n",
      "Epoch [5/10], Step [172/600], Loss: 0.6376, Train Acc:0.8066\n",
      "Epoch [5/10], Step [173/600], Loss: 0.6187, Train Acc:0.8065\n",
      "Epoch [5/10], Step [174/600], Loss: 0.5916, Train Acc:0.8067\n",
      "Epoch [5/10], Step [175/600], Loss: 0.5959, Train Acc:0.8067\n",
      "Epoch [5/10], Step [176/600], Loss: 0.5483, Train Acc:0.8066\n",
      "Epoch [5/10], Step [177/600], Loss: 0.6626, Train Acc:0.8068\n",
      "Epoch [5/10], Step [178/600], Loss: 0.5200, Train Acc:0.8070\n",
      "Epoch [5/10], Step [179/600], Loss: 0.5836, Train Acc:0.8069\n",
      "Epoch [5/10], Step [180/600], Loss: 0.5679, Train Acc:0.8070\n",
      "Epoch [5/10], Step [181/600], Loss: 0.4754, Train Acc:0.8075\n",
      "Epoch [5/10], Step [182/600], Loss: 0.5438, Train Acc:0.8075\n",
      "Epoch [5/10], Step [183/600], Loss: 0.5958, Train Acc:0.8076\n",
      "Epoch [5/10], Step [184/600], Loss: 0.5631, Train Acc:0.8077\n",
      "Epoch [5/10], Step [185/600], Loss: 0.6634, Train Acc:0.8076\n",
      "Epoch [5/10], Step [186/600], Loss: 0.6895, Train Acc:0.8075\n",
      "Epoch [5/10], Step [187/600], Loss: 0.5880, Train Acc:0.8076\n",
      "Epoch [5/10], Step [188/600], Loss: 0.5264, Train Acc:0.8078\n",
      "Epoch [5/10], Step [189/600], Loss: 0.4710, Train Acc:0.8081\n",
      "Epoch [5/10], Step [190/600], Loss: 0.6840, Train Acc:0.8078\n",
      "Epoch [5/10], Step [191/600], Loss: 0.5513, Train Acc:0.8079\n",
      "Epoch [5/10], Step [192/600], Loss: 0.6676, Train Acc:0.8078\n",
      "Epoch [5/10], Step [193/600], Loss: 0.5177, Train Acc:0.8081\n",
      "Epoch [5/10], Step [194/600], Loss: 0.5640, Train Acc:0.8079\n",
      "Epoch [5/10], Step [195/600], Loss: 0.7350, Train Acc:0.8078\n",
      "Epoch [5/10], Step [196/600], Loss: 0.6059, Train Acc:0.8077\n",
      "Epoch [5/10], Step [197/600], Loss: 0.6116, Train Acc:0.8076\n",
      "Epoch [5/10], Step [198/600], Loss: 0.5786, Train Acc:0.8077\n",
      "Epoch [5/10], Step [199/600], Loss: 0.6711, Train Acc:0.8075\n",
      "Epoch [5/10], Step [200/600], Loss: 0.6400, Train Acc:0.8075\n",
      "Epoch [5/10], Step [201/600], Loss: 0.7471, Train Acc:0.8075\n",
      "Epoch [5/10], Step [202/600], Loss: 0.5846, Train Acc:0.8076\n",
      "Epoch [5/10], Step [203/600], Loss: 0.5481, Train Acc:0.8077\n",
      "Epoch [5/10], Step [204/600], Loss: 0.4626, Train Acc:0.8082\n",
      "Epoch [5/10], Step [205/600], Loss: 0.5104, Train Acc:0.8084\n",
      "Epoch [5/10], Step [206/600], Loss: 0.5875, Train Acc:0.8083\n",
      "Epoch [5/10], Step [207/600], Loss: 0.6027, Train Acc:0.8084\n",
      "Epoch [5/10], Step [208/600], Loss: 0.5710, Train Acc:0.8086\n",
      "Epoch [5/10], Step [209/600], Loss: 0.5952, Train Acc:0.8085\n",
      "Epoch [5/10], Step [210/600], Loss: 0.5116, Train Acc:0.8086\n",
      "Epoch [5/10], Step [211/600], Loss: 0.5026, Train Acc:0.8088\n",
      "Epoch [5/10], Step [212/600], Loss: 0.5927, Train Acc:0.8090\n",
      "Epoch [5/10], Step [213/600], Loss: 0.7052, Train Acc:0.8089\n",
      "Epoch [5/10], Step [214/600], Loss: 0.5608, Train Acc:0.8092\n",
      "Epoch [5/10], Step [215/600], Loss: 0.5472, Train Acc:0.8092\n",
      "Epoch [5/10], Step [216/600], Loss: 0.6406, Train Acc:0.8092\n",
      "Epoch [5/10], Step [217/600], Loss: 0.5019, Train Acc:0.8095\n",
      "Epoch [5/10], Step [218/600], Loss: 0.4777, Train Acc:0.8096\n",
      "Epoch [5/10], Step [219/600], Loss: 0.6554, Train Acc:0.8097\n",
      "Epoch [5/10], Step [220/600], Loss: 0.5737, Train Acc:0.8097\n",
      "Epoch [5/10], Step [221/600], Loss: 0.5430, Train Acc:0.8098\n",
      "Epoch [5/10], Step [222/600], Loss: 0.5708, Train Acc:0.8099\n",
      "Epoch [5/10], Step [223/600], Loss: 0.5708, Train Acc:0.8098\n",
      "Epoch [5/10], Step [224/600], Loss: 0.5485, Train Acc:0.8098\n",
      "Epoch [5/10], Step [225/600], Loss: 0.6996, Train Acc:0.8096\n",
      "Epoch [5/10], Step [226/600], Loss: 0.5697, Train Acc:0.8097\n",
      "Epoch [5/10], Step [227/600], Loss: 0.5575, Train Acc:0.8096\n",
      "Epoch [5/10], Step [228/600], Loss: 0.6297, Train Acc:0.8095\n",
      "Epoch [5/10], Step [229/600], Loss: 0.5817, Train Acc:0.8096\n",
      "Epoch [5/10], Step [230/600], Loss: 0.4727, Train Acc:0.8097\n",
      "Epoch [5/10], Step [231/600], Loss: 0.6097, Train Acc:0.8099\n",
      "Epoch [5/10], Step [232/600], Loss: 0.6079, Train Acc:0.8098\n",
      "Epoch [5/10], Step [233/600], Loss: 0.5757, Train Acc:0.8098\n",
      "Epoch [5/10], Step [234/600], Loss: 0.5968, Train Acc:0.8097\n",
      "Epoch [5/10], Step [235/600], Loss: 0.6236, Train Acc:0.8096\n",
      "Epoch [5/10], Step [236/600], Loss: 0.4858, Train Acc:0.8096\n",
      "Epoch [5/10], Step [237/600], Loss: 0.5835, Train Acc:0.8094\n",
      "Epoch [5/10], Step [238/600], Loss: 0.6158, Train Acc:0.8093\n",
      "Epoch [5/10], Step [239/600], Loss: 0.8489, Train Acc:0.8090\n",
      "Epoch [5/10], Step [240/600], Loss: 0.6707, Train Acc:0.8089\n",
      "Epoch [5/10], Step [241/600], Loss: 0.6394, Train Acc:0.8088\n",
      "Epoch [5/10], Step [242/600], Loss: 0.5146, Train Acc:0.8090\n",
      "Epoch [5/10], Step [243/600], Loss: 0.5822, Train Acc:0.8089\n",
      "Epoch [5/10], Step [244/600], Loss: 0.6400, Train Acc:0.8089\n",
      "Epoch [5/10], Step [245/600], Loss: 0.5962, Train Acc:0.8089\n",
      "Epoch [5/10], Step [246/600], Loss: 0.6495, Train Acc:0.8089\n",
      "Epoch [5/10], Step [247/600], Loss: 0.6603, Train Acc:0.8089\n",
      "Epoch [5/10], Step [248/600], Loss: 0.6705, Train Acc:0.8089\n",
      "Epoch [5/10], Step [249/600], Loss: 0.5568, Train Acc:0.8091\n",
      "Epoch [5/10], Step [250/600], Loss: 0.5109, Train Acc:0.8093\n",
      "Epoch [5/10], Step [251/600], Loss: 0.5799, Train Acc:0.8094\n",
      "Epoch [5/10], Step [252/600], Loss: 0.5395, Train Acc:0.8095\n",
      "Epoch [5/10], Step [253/600], Loss: 0.6282, Train Acc:0.8094\n",
      "Epoch [5/10], Step [254/600], Loss: 0.4682, Train Acc:0.8096\n",
      "Epoch [5/10], Step [255/600], Loss: 0.6531, Train Acc:0.8095\n",
      "Epoch [5/10], Step [256/600], Loss: 0.6248, Train Acc:0.8093\n",
      "Epoch [5/10], Step [257/600], Loss: 0.6373, Train Acc:0.8092\n",
      "Epoch [5/10], Step [258/600], Loss: 0.4633, Train Acc:0.8095\n",
      "Epoch [5/10], Step [259/600], Loss: 0.5141, Train Acc:0.8097\n",
      "Epoch [5/10], Step [260/600], Loss: 0.5522, Train Acc:0.8096\n",
      "Epoch [5/10], Step [261/600], Loss: 0.5691, Train Acc:0.8095\n",
      "Epoch [5/10], Step [262/600], Loss: 0.5904, Train Acc:0.8095\n",
      "Epoch [5/10], Step [263/600], Loss: 0.4838, Train Acc:0.8097\n",
      "Epoch [5/10], Step [264/600], Loss: 0.5191, Train Acc:0.8098\n",
      "Epoch [5/10], Step [265/600], Loss: 0.5584, Train Acc:0.8098\n",
      "Epoch [5/10], Step [266/600], Loss: 0.4993, Train Acc:0.8100\n",
      "Epoch [5/10], Step [267/600], Loss: 0.5467, Train Acc:0.8100\n",
      "Epoch [5/10], Step [268/600], Loss: 0.7133, Train Acc:0.8099\n",
      "Epoch [5/10], Step [269/600], Loss: 0.6052, Train Acc:0.8099\n",
      "Epoch [5/10], Step [270/600], Loss: 0.4640, Train Acc:0.8100\n",
      "Epoch [5/10], Step [271/600], Loss: 0.5845, Train Acc:0.8103\n",
      "Epoch [5/10], Step [272/600], Loss: 0.4584, Train Acc:0.8104\n",
      "Epoch [5/10], Step [273/600], Loss: 0.5699, Train Acc:0.8106\n",
      "Epoch [5/10], Step [274/600], Loss: 0.6276, Train Acc:0.8104\n",
      "Epoch [5/10], Step [275/600], Loss: 0.5936, Train Acc:0.8104\n",
      "Epoch [5/10], Step [276/600], Loss: 0.5414, Train Acc:0.8104\n",
      "Epoch [5/10], Step [277/600], Loss: 0.5090, Train Acc:0.8104\n",
      "Epoch [5/10], Step [278/600], Loss: 0.5748, Train Acc:0.8104\n",
      "Epoch [5/10], Step [279/600], Loss: 0.5216, Train Acc:0.8106\n",
      "Epoch [5/10], Step [280/600], Loss: 0.6597, Train Acc:0.8105\n",
      "Epoch [5/10], Step [281/600], Loss: 0.5506, Train Acc:0.8106\n",
      "Epoch [5/10], Step [282/600], Loss: 0.5182, Train Acc:0.8107\n",
      "Epoch [5/10], Step [283/600], Loss: 0.8507, Train Acc:0.8104\n",
      "Epoch [5/10], Step [284/600], Loss: 0.6236, Train Acc:0.8105\n",
      "Epoch [5/10], Step [285/600], Loss: 0.6021, Train Acc:0.8105\n",
      "Epoch [5/10], Step [286/600], Loss: 0.6096, Train Acc:0.8104\n",
      "Epoch [5/10], Step [287/600], Loss: 0.5340, Train Acc:0.8106\n",
      "Epoch [5/10], Step [288/600], Loss: 0.5528, Train Acc:0.8107\n",
      "Epoch [5/10], Step [289/600], Loss: 0.6192, Train Acc:0.8107\n",
      "Epoch [5/10], Step [290/600], Loss: 0.6151, Train Acc:0.8107\n",
      "Epoch [5/10], Step [291/600], Loss: 0.4655, Train Acc:0.8109\n",
      "Epoch [5/10], Step [292/600], Loss: 0.5225, Train Acc:0.8109\n",
      "Epoch [5/10], Step [293/600], Loss: 0.5109, Train Acc:0.8111\n",
      "Epoch [5/10], Step [294/600], Loss: 0.5724, Train Acc:0.8111\n",
      "Epoch [5/10], Step [295/600], Loss: 0.5865, Train Acc:0.8111\n",
      "Epoch [5/10], Step [296/600], Loss: 0.5774, Train Acc:0.8109\n",
      "Epoch [5/10], Step [297/600], Loss: 0.5453, Train Acc:0.8109\n",
      "Epoch [5/10], Step [298/600], Loss: 0.6350, Train Acc:0.8107\n",
      "Epoch [5/10], Step [299/600], Loss: 0.4610, Train Acc:0.8108\n",
      "Epoch [5/10], Step [300/600], Loss: 0.5986, Train Acc:0.8108\n",
      "Epoch [5/10], Step [301/600], Loss: 0.6930, Train Acc:0.8106\n",
      "Epoch [5/10], Step [302/600], Loss: 0.5333, Train Acc:0.8108\n",
      "Epoch [5/10], Step [303/600], Loss: 0.5996, Train Acc:0.8107\n",
      "Epoch [5/10], Step [304/600], Loss: 0.6594, Train Acc:0.8106\n",
      "Epoch [5/10], Step [305/600], Loss: 0.9099, Train Acc:0.8102\n",
      "Epoch [5/10], Step [306/600], Loss: 0.5589, Train Acc:0.8103\n",
      "Epoch [5/10], Step [307/600], Loss: 0.5399, Train Acc:0.8104\n",
      "Epoch [5/10], Step [308/600], Loss: 0.5766, Train Acc:0.8103\n",
      "Epoch [5/10], Step [309/600], Loss: 0.6452, Train Acc:0.8103\n",
      "Epoch [5/10], Step [310/600], Loss: 0.5887, Train Acc:0.8102\n",
      "Epoch [5/10], Step [311/600], Loss: 0.6211, Train Acc:0.8102\n",
      "Epoch [5/10], Step [312/600], Loss: 0.5455, Train Acc:0.8103\n",
      "Epoch [5/10], Step [313/600], Loss: 0.4929, Train Acc:0.8105\n",
      "Epoch [5/10], Step [314/600], Loss: 0.5650, Train Acc:0.8105\n",
      "Epoch [5/10], Step [315/600], Loss: 0.7073, Train Acc:0.8102\n",
      "Epoch [5/10], Step [316/600], Loss: 0.4947, Train Acc:0.8103\n",
      "Epoch [5/10], Step [317/600], Loss: 0.5649, Train Acc:0.8103\n",
      "Epoch [5/10], Step [318/600], Loss: 0.5172, Train Acc:0.8103\n",
      "Epoch [5/10], Step [319/600], Loss: 0.5283, Train Acc:0.8105\n",
      "Epoch [5/10], Step [320/600], Loss: 0.4430, Train Acc:0.8107\n",
      "Epoch [5/10], Step [321/600], Loss: 0.3934, Train Acc:0.8109\n",
      "Epoch [5/10], Step [322/600], Loss: 0.6136, Train Acc:0.8108\n",
      "Epoch [5/10], Step [323/600], Loss: 0.6325, Train Acc:0.8109\n",
      "Epoch [5/10], Step [324/600], Loss: 0.6442, Train Acc:0.8107\n",
      "Epoch [5/10], Step [325/600], Loss: 0.7404, Train Acc:0.8103\n",
      "Epoch [5/10], Step [326/600], Loss: 0.5561, Train Acc:0.8103\n",
      "Epoch [5/10], Step [327/600], Loss: 0.5689, Train Acc:0.8104\n",
      "Epoch [5/10], Step [328/600], Loss: 0.5635, Train Acc:0.8103\n",
      "Epoch [5/10], Step [329/600], Loss: 0.5620, Train Acc:0.8103\n",
      "Epoch [5/10], Step [330/600], Loss: 0.5840, Train Acc:0.8102\n",
      "Epoch [5/10], Step [331/600], Loss: 0.5090, Train Acc:0.8104\n",
      "Epoch [5/10], Step [332/600], Loss: 0.4286, Train Acc:0.8105\n",
      "Epoch [5/10], Step [333/600], Loss: 0.5399, Train Acc:0.8105\n",
      "Epoch [5/10], Step [334/600], Loss: 0.5808, Train Acc:0.8106\n",
      "Epoch [5/10], Step [335/600], Loss: 0.4129, Train Acc:0.8107\n",
      "Epoch [5/10], Step [336/600], Loss: 0.4796, Train Acc:0.8108\n",
      "Epoch [5/10], Step [337/600], Loss: 0.6073, Train Acc:0.8108\n",
      "Epoch [5/10], Step [338/600], Loss: 0.6848, Train Acc:0.8107\n",
      "Epoch [5/10], Step [339/600], Loss: 0.8013, Train Acc:0.8105\n",
      "Epoch [5/10], Step [340/600], Loss: 0.6426, Train Acc:0.8105\n",
      "Epoch [5/10], Step [341/600], Loss: 0.6620, Train Acc:0.8104\n",
      "Epoch [5/10], Step [342/600], Loss: 0.6429, Train Acc:0.8101\n",
      "Epoch [5/10], Step [343/600], Loss: 0.5221, Train Acc:0.8102\n",
      "Epoch [5/10], Step [344/600], Loss: 0.6225, Train Acc:0.8101\n",
      "Epoch [5/10], Step [345/600], Loss: 0.6832, Train Acc:0.8100\n",
      "Epoch [5/10], Step [346/600], Loss: 0.5178, Train Acc:0.8100\n",
      "Epoch [5/10], Step [347/600], Loss: 0.7165, Train Acc:0.8099\n",
      "Epoch [5/10], Step [348/600], Loss: 0.6195, Train Acc:0.8098\n",
      "Epoch [5/10], Step [349/600], Loss: 0.6011, Train Acc:0.8097\n",
      "Epoch [5/10], Step [350/600], Loss: 0.5728, Train Acc:0.8097\n",
      "Epoch [5/10], Step [351/600], Loss: 0.6114, Train Acc:0.8097\n",
      "Epoch [5/10], Step [352/600], Loss: 0.5711, Train Acc:0.8097\n",
      "Epoch [5/10], Step [353/600], Loss: 0.5059, Train Acc:0.8097\n",
      "Epoch [5/10], Step [354/600], Loss: 0.6370, Train Acc:0.8096\n",
      "Epoch [5/10], Step [355/600], Loss: 0.6885, Train Acc:0.8096\n",
      "Epoch [5/10], Step [356/600], Loss: 0.6129, Train Acc:0.8095\n",
      "Epoch [5/10], Step [357/600], Loss: 0.5519, Train Acc:0.8096\n",
      "Epoch [5/10], Step [358/600], Loss: 0.5368, Train Acc:0.8097\n",
      "Epoch [5/10], Step [359/600], Loss: 0.6005, Train Acc:0.8099\n",
      "Epoch [5/10], Step [360/600], Loss: 0.5771, Train Acc:0.8100\n",
      "Epoch [5/10], Step [361/600], Loss: 0.5640, Train Acc:0.8101\n",
      "Epoch [5/10], Step [362/600], Loss: 0.6794, Train Acc:0.8100\n",
      "Epoch [5/10], Step [363/600], Loss: 0.4515, Train Acc:0.8102\n",
      "Epoch [5/10], Step [364/600], Loss: 0.7044, Train Acc:0.8101\n",
      "Epoch [5/10], Step [365/600], Loss: 0.7271, Train Acc:0.8100\n",
      "Epoch [5/10], Step [366/600], Loss: 0.5927, Train Acc:0.8100\n",
      "Epoch [5/10], Step [367/600], Loss: 0.5948, Train Acc:0.8099\n",
      "Epoch [5/10], Step [368/600], Loss: 0.6044, Train Acc:0.8099\n",
      "Epoch [5/10], Step [369/600], Loss: 0.5182, Train Acc:0.8099\n",
      "Epoch [5/10], Step [370/600], Loss: 0.5149, Train Acc:0.8100\n",
      "Epoch [5/10], Step [371/600], Loss: 0.5196, Train Acc:0.8101\n",
      "Epoch [5/10], Step [372/600], Loss: 0.6196, Train Acc:0.8100\n",
      "Epoch [5/10], Step [373/600], Loss: 0.6171, Train Acc:0.8099\n",
      "Epoch [5/10], Step [374/600], Loss: 0.5242, Train Acc:0.8099\n",
      "Epoch [5/10], Step [375/600], Loss: 0.6188, Train Acc:0.8099\n",
      "Epoch [5/10], Step [376/600], Loss: 0.5495, Train Acc:0.8100\n",
      "Epoch [5/10], Step [377/600], Loss: 0.5383, Train Acc:0.8101\n",
      "Epoch [5/10], Step [378/600], Loss: 0.5763, Train Acc:0.8101\n",
      "Epoch [5/10], Step [379/600], Loss: 0.5918, Train Acc:0.8102\n",
      "Epoch [5/10], Step [380/600], Loss: 0.6249, Train Acc:0.8103\n",
      "Epoch [5/10], Step [381/600], Loss: 0.5214, Train Acc:0.8104\n",
      "Epoch [5/10], Step [382/600], Loss: 0.6704, Train Acc:0.8104\n",
      "Epoch [5/10], Step [383/600], Loss: 0.5634, Train Acc:0.8105\n",
      "Epoch [5/10], Step [384/600], Loss: 0.6000, Train Acc:0.8104\n",
      "Epoch [5/10], Step [385/600], Loss: 0.6176, Train Acc:0.8104\n",
      "Epoch [5/10], Step [386/600], Loss: 0.6341, Train Acc:0.8103\n",
      "Epoch [5/10], Step [387/600], Loss: 0.5691, Train Acc:0.8102\n",
      "Epoch [5/10], Step [388/600], Loss: 0.5624, Train Acc:0.8103\n",
      "Epoch [5/10], Step [389/600], Loss: 0.6088, Train Acc:0.8103\n",
      "Epoch [5/10], Step [390/600], Loss: 0.6412, Train Acc:0.8103\n",
      "Epoch [5/10], Step [391/600], Loss: 0.5694, Train Acc:0.8102\n",
      "Epoch [5/10], Step [392/600], Loss: 0.5813, Train Acc:0.8101\n",
      "Epoch [5/10], Step [393/600], Loss: 0.5775, Train Acc:0.8101\n",
      "Epoch [5/10], Step [394/600], Loss: 0.5192, Train Acc:0.8101\n",
      "Epoch [5/10], Step [395/600], Loss: 0.4460, Train Acc:0.8102\n",
      "Epoch [5/10], Step [396/600], Loss: 0.6702, Train Acc:0.8100\n",
      "Epoch [5/10], Step [397/600], Loss: 0.5019, Train Acc:0.8102\n",
      "Epoch [5/10], Step [398/600], Loss: 0.5368, Train Acc:0.8103\n",
      "Epoch [5/10], Step [399/600], Loss: 0.6654, Train Acc:0.8101\n",
      "Epoch [5/10], Step [400/600], Loss: 0.5924, Train Acc:0.8101\n",
      "Epoch [5/10], Step [401/600], Loss: 0.7273, Train Acc:0.8100\n",
      "Epoch [5/10], Step [402/600], Loss: 0.6837, Train Acc:0.8098\n",
      "Epoch [5/10], Step [403/600], Loss: 0.6609, Train Acc:0.8098\n",
      "Epoch [5/10], Step [404/600], Loss: 0.5702, Train Acc:0.8099\n",
      "Epoch [5/10], Step [405/600], Loss: 0.4900, Train Acc:0.8100\n",
      "Epoch [5/10], Step [406/600], Loss: 0.7131, Train Acc:0.8100\n",
      "Epoch [5/10], Step [407/600], Loss: 0.5754, Train Acc:0.8100\n",
      "Epoch [5/10], Step [408/600], Loss: 0.5959, Train Acc:0.8098\n",
      "Epoch [5/10], Step [409/600], Loss: 0.7497, Train Acc:0.8097\n",
      "Epoch [5/10], Step [410/600], Loss: 0.5572, Train Acc:0.8096\n",
      "Epoch [5/10], Step [411/600], Loss: 0.6318, Train Acc:0.8096\n",
      "Epoch [5/10], Step [412/600], Loss: 0.6478, Train Acc:0.8096\n",
      "Epoch [5/10], Step [413/600], Loss: 0.6622, Train Acc:0.8095\n",
      "Epoch [5/10], Step [414/600], Loss: 0.6493, Train Acc:0.8093\n",
      "Epoch [5/10], Step [415/600], Loss: 0.5520, Train Acc:0.8093\n",
      "Epoch [5/10], Step [416/600], Loss: 0.4380, Train Acc:0.8095\n",
      "Epoch [5/10], Step [417/600], Loss: 0.5411, Train Acc:0.8095\n",
      "Epoch [5/10], Step [418/600], Loss: 0.4850, Train Acc:0.8096\n",
      "Epoch [5/10], Step [419/600], Loss: 0.5455, Train Acc:0.8096\n",
      "Epoch [5/10], Step [420/600], Loss: 0.5421, Train Acc:0.8097\n",
      "Epoch [5/10], Step [421/600], Loss: 0.6676, Train Acc:0.8097\n",
      "Epoch [5/10], Step [422/600], Loss: 0.6446, Train Acc:0.8097\n",
      "Epoch [5/10], Step [423/600], Loss: 0.6194, Train Acc:0.8097\n",
      "Epoch [5/10], Step [424/600], Loss: 0.6639, Train Acc:0.8097\n",
      "Epoch [5/10], Step [425/600], Loss: 0.5825, Train Acc:0.8098\n",
      "Epoch [5/10], Step [426/600], Loss: 0.6326, Train Acc:0.8097\n",
      "Epoch [5/10], Step [427/600], Loss: 0.5469, Train Acc:0.8098\n",
      "Epoch [5/10], Step [428/600], Loss: 0.5770, Train Acc:0.8099\n",
      "Epoch [5/10], Step [429/600], Loss: 0.5648, Train Acc:0.8098\n",
      "Epoch [5/10], Step [430/600], Loss: 0.6266, Train Acc:0.8097\n",
      "Epoch [5/10], Step [431/600], Loss: 0.6401, Train Acc:0.8097\n",
      "Epoch [5/10], Step [432/600], Loss: 0.6380, Train Acc:0.8097\n",
      "Epoch [5/10], Step [433/600], Loss: 0.6334, Train Acc:0.8096\n",
      "Epoch [5/10], Step [434/600], Loss: 0.6886, Train Acc:0.8094\n",
      "Epoch [5/10], Step [435/600], Loss: 0.4487, Train Acc:0.8096\n",
      "Epoch [5/10], Step [436/600], Loss: 0.5162, Train Acc:0.8096\n",
      "Epoch [5/10], Step [437/600], Loss: 0.4642, Train Acc:0.8097\n",
      "Epoch [5/10], Step [438/600], Loss: 0.5975, Train Acc:0.8097\n",
      "Epoch [5/10], Step [439/600], Loss: 0.5825, Train Acc:0.8098\n",
      "Epoch [5/10], Step [440/600], Loss: 0.6357, Train Acc:0.8098\n",
      "Epoch [5/10], Step [441/600], Loss: 0.6832, Train Acc:0.8096\n",
      "Epoch [5/10], Step [442/600], Loss: 0.7182, Train Acc:0.8096\n",
      "Epoch [5/10], Step [443/600], Loss: 0.6161, Train Acc:0.8096\n",
      "Epoch [5/10], Step [444/600], Loss: 0.6001, Train Acc:0.8096\n",
      "Epoch [5/10], Step [445/600], Loss: 0.6340, Train Acc:0.8096\n",
      "Epoch [5/10], Step [446/600], Loss: 0.6206, Train Acc:0.8095\n",
      "Epoch [5/10], Step [447/600], Loss: 0.5249, Train Acc:0.8095\n",
      "Epoch [5/10], Step [448/600], Loss: 0.4892, Train Acc:0.8096\n",
      "Epoch [5/10], Step [449/600], Loss: 0.4697, Train Acc:0.8097\n",
      "Epoch [5/10], Step [450/600], Loss: 0.7266, Train Acc:0.8096\n",
      "Epoch [5/10], Step [451/600], Loss: 0.6035, Train Acc:0.8096\n",
      "Epoch [5/10], Step [452/600], Loss: 0.4894, Train Acc:0.8097\n",
      "Epoch [5/10], Step [453/600], Loss: 0.5473, Train Acc:0.8098\n",
      "Epoch [5/10], Step [454/600], Loss: 0.5488, Train Acc:0.8099\n",
      "Epoch [5/10], Step [455/600], Loss: 0.6077, Train Acc:0.8099\n",
      "Epoch [5/10], Step [456/600], Loss: 0.5540, Train Acc:0.8099\n",
      "Epoch [5/10], Step [457/600], Loss: 0.5528, Train Acc:0.8099\n",
      "Epoch [5/10], Step [458/600], Loss: 0.6158, Train Acc:0.8100\n",
      "Epoch [5/10], Step [459/600], Loss: 0.4379, Train Acc:0.8102\n",
      "Epoch [5/10], Step [460/600], Loss: 0.4897, Train Acc:0.8102\n",
      "Epoch [5/10], Step [461/600], Loss: 0.4830, Train Acc:0.8103\n",
      "Epoch [5/10], Step [462/600], Loss: 0.6596, Train Acc:0.8102\n",
      "Epoch [5/10], Step [463/600], Loss: 0.7168, Train Acc:0.8101\n",
      "Epoch [5/10], Step [464/600], Loss: 0.6898, Train Acc:0.8100\n",
      "Epoch [5/10], Step [465/600], Loss: 0.4807, Train Acc:0.8102\n",
      "Epoch [5/10], Step [466/600], Loss: 0.4953, Train Acc:0.8102\n",
      "Epoch [5/10], Step [467/600], Loss: 0.4712, Train Acc:0.8103\n",
      "Epoch [5/10], Step [468/600], Loss: 0.7647, Train Acc:0.8100\n",
      "Epoch [5/10], Step [469/600], Loss: 0.5537, Train Acc:0.8101\n",
      "Epoch [5/10], Step [470/600], Loss: 0.5707, Train Acc:0.8101\n",
      "Epoch [5/10], Step [471/600], Loss: 0.5328, Train Acc:0.8102\n",
      "Epoch [5/10], Step [472/600], Loss: 0.6330, Train Acc:0.8101\n",
      "Epoch [5/10], Step [473/600], Loss: 0.5977, Train Acc:0.8101\n",
      "Epoch [5/10], Step [474/600], Loss: 0.4840, Train Acc:0.8103\n",
      "Epoch [5/10], Step [475/600], Loss: 0.5281, Train Acc:0.8103\n",
      "Epoch [5/10], Step [476/600], Loss: 0.5319, Train Acc:0.8103\n",
      "Epoch [5/10], Step [477/600], Loss: 0.6312, Train Acc:0.8103\n",
      "Epoch [5/10], Step [478/600], Loss: 0.5177, Train Acc:0.8104\n",
      "Epoch [5/10], Step [479/600], Loss: 0.5557, Train Acc:0.8104\n",
      "Epoch [5/10], Step [480/600], Loss: 0.5914, Train Acc:0.8105\n",
      "Epoch [5/10], Step [481/600], Loss: 0.6515, Train Acc:0.8104\n",
      "Epoch [5/10], Step [482/600], Loss: 0.5918, Train Acc:0.8104\n",
      "Epoch [5/10], Step [483/600], Loss: 0.5700, Train Acc:0.8104\n",
      "Epoch [5/10], Step [484/600], Loss: 0.6527, Train Acc:0.8103\n",
      "Epoch [5/10], Step [485/600], Loss: 0.6356, Train Acc:0.8103\n",
      "Epoch [5/10], Step [486/600], Loss: 0.5266, Train Acc:0.8104\n",
      "Epoch [5/10], Step [487/600], Loss: 0.5541, Train Acc:0.8104\n",
      "Epoch [5/10], Step [488/600], Loss: 0.6085, Train Acc:0.8104\n",
      "Epoch [5/10], Step [489/600], Loss: 0.5340, Train Acc:0.8104\n",
      "Epoch [5/10], Step [490/600], Loss: 0.5935, Train Acc:0.8104\n",
      "Epoch [5/10], Step [491/600], Loss: 0.6229, Train Acc:0.8104\n",
      "Epoch [5/10], Step [492/600], Loss: 0.5127, Train Acc:0.8105\n",
      "Epoch [5/10], Step [493/600], Loss: 0.4959, Train Acc:0.8105\n",
      "Epoch [5/10], Step [494/600], Loss: 0.5166, Train Acc:0.8105\n",
      "Epoch [5/10], Step [495/600], Loss: 0.5671, Train Acc:0.8105\n",
      "Epoch [5/10], Step [496/600], Loss: 0.5859, Train Acc:0.8106\n",
      "Epoch [5/10], Step [497/600], Loss: 0.5608, Train Acc:0.8106\n",
      "Epoch [5/10], Step [498/600], Loss: 0.6182, Train Acc:0.8105\n",
      "Epoch [5/10], Step [499/600], Loss: 0.5936, Train Acc:0.8105\n",
      "Epoch [5/10], Step [500/600], Loss: 0.4443, Train Acc:0.8107\n",
      "Epoch [5/10], Step [501/600], Loss: 0.5651, Train Acc:0.8108\n",
      "Epoch [5/10], Step [502/600], Loss: 0.5478, Train Acc:0.8108\n",
      "Epoch [5/10], Step [503/600], Loss: 0.5654, Train Acc:0.8109\n",
      "Epoch [5/10], Step [504/600], Loss: 0.5856, Train Acc:0.8109\n",
      "Epoch [5/10], Step [505/600], Loss: 0.5003, Train Acc:0.8109\n",
      "Epoch [5/10], Step [506/600], Loss: 0.7251, Train Acc:0.8107\n",
      "Epoch [5/10], Step [507/600], Loss: 0.4387, Train Acc:0.8108\n",
      "Epoch [5/10], Step [508/600], Loss: 0.6123, Train Acc:0.8108\n",
      "Epoch [5/10], Step [509/600], Loss: 0.4855, Train Acc:0.8110\n",
      "Epoch [5/10], Step [510/600], Loss: 0.5336, Train Acc:0.8110\n",
      "Epoch [5/10], Step [511/600], Loss: 0.6231, Train Acc:0.8108\n",
      "Epoch [5/10], Step [512/600], Loss: 0.4546, Train Acc:0.8109\n",
      "Epoch [5/10], Step [513/600], Loss: 0.5364, Train Acc:0.8110\n",
      "Epoch [5/10], Step [514/600], Loss: 0.4704, Train Acc:0.8111\n",
      "Epoch [5/10], Step [515/600], Loss: 0.6447, Train Acc:0.8110\n",
      "Epoch [5/10], Step [516/600], Loss: 0.5166, Train Acc:0.8112\n",
      "Epoch [5/10], Step [517/600], Loss: 0.5447, Train Acc:0.8112\n",
      "Epoch [5/10], Step [518/600], Loss: 0.5959, Train Acc:0.8110\n",
      "Epoch [5/10], Step [519/600], Loss: 0.5627, Train Acc:0.8110\n",
      "Epoch [5/10], Step [520/600], Loss: 0.5218, Train Acc:0.8111\n",
      "Epoch [5/10], Step [521/600], Loss: 0.7085, Train Acc:0.8110\n",
      "Epoch [5/10], Step [522/600], Loss: 0.7213, Train Acc:0.8109\n",
      "Epoch [5/10], Step [523/600], Loss: 0.5076, Train Acc:0.8109\n",
      "Epoch [5/10], Step [524/600], Loss: 0.5558, Train Acc:0.8109\n",
      "Epoch [5/10], Step [525/600], Loss: 0.4823, Train Acc:0.8110\n",
      "Epoch [5/10], Step [526/600], Loss: 0.5523, Train Acc:0.8111\n",
      "Epoch [5/10], Step [527/600], Loss: 0.4887, Train Acc:0.8111\n",
      "Epoch [5/10], Step [528/600], Loss: 0.5345, Train Acc:0.8112\n",
      "Epoch [5/10], Step [529/600], Loss: 0.5605, Train Acc:0.8112\n",
      "Epoch [5/10], Step [530/600], Loss: 0.8481, Train Acc:0.8111\n",
      "Epoch [5/10], Step [531/600], Loss: 0.5838, Train Acc:0.8111\n",
      "Epoch [5/10], Step [532/600], Loss: 0.5387, Train Acc:0.8110\n",
      "Epoch [5/10], Step [533/600], Loss: 0.5977, Train Acc:0.8111\n",
      "Epoch [5/10], Step [534/600], Loss: 0.5475, Train Acc:0.8111\n",
      "Epoch [5/10], Step [535/600], Loss: 0.5423, Train Acc:0.8112\n",
      "Epoch [5/10], Step [536/600], Loss: 0.5663, Train Acc:0.8112\n",
      "Epoch [5/10], Step [537/600], Loss: 0.5998, Train Acc:0.8111\n",
      "Epoch [5/10], Step [538/600], Loss: 0.5784, Train Acc:0.8110\n",
      "Epoch [5/10], Step [539/600], Loss: 0.6737, Train Acc:0.8109\n",
      "Epoch [5/10], Step [540/600], Loss: 0.4863, Train Acc:0.8109\n",
      "Epoch [5/10], Step [541/600], Loss: 0.5511, Train Acc:0.8110\n",
      "Epoch [5/10], Step [542/600], Loss: 0.5669, Train Acc:0.8110\n",
      "Epoch [5/10], Step [543/600], Loss: 0.4583, Train Acc:0.8111\n",
      "Epoch [5/10], Step [544/600], Loss: 0.5449, Train Acc:0.8112\n",
      "Epoch [5/10], Step [545/600], Loss: 0.5744, Train Acc:0.8112\n",
      "Epoch [5/10], Step [546/600], Loss: 0.5410, Train Acc:0.8113\n",
      "Epoch [5/10], Step [547/600], Loss: 0.5229, Train Acc:0.8113\n",
      "Epoch [5/10], Step [548/600], Loss: 0.6420, Train Acc:0.8112\n",
      "Epoch [5/10], Step [549/600], Loss: 0.3890, Train Acc:0.8115\n",
      "Epoch [5/10], Step [550/600], Loss: 0.5256, Train Acc:0.8115\n",
      "Epoch [5/10], Step [551/600], Loss: 0.6785, Train Acc:0.8115\n",
      "Epoch [5/10], Step [552/600], Loss: 0.6275, Train Acc:0.8114\n",
      "Epoch [5/10], Step [553/600], Loss: 0.6532, Train Acc:0.8113\n",
      "Epoch [5/10], Step [554/600], Loss: 0.5382, Train Acc:0.8113\n",
      "Epoch [5/10], Step [555/600], Loss: 0.6072, Train Acc:0.8113\n",
      "Epoch [5/10], Step [556/600], Loss: 0.6503, Train Acc:0.8113\n",
      "Epoch [5/10], Step [557/600], Loss: 0.5646, Train Acc:0.8112\n",
      "Epoch [5/10], Step [558/600], Loss: 0.5043, Train Acc:0.8113\n",
      "Epoch [5/10], Step [559/600], Loss: 0.6087, Train Acc:0.8113\n",
      "Epoch [5/10], Step [560/600], Loss: 0.5054, Train Acc:0.8113\n",
      "Epoch [5/10], Step [561/600], Loss: 0.5560, Train Acc:0.8113\n",
      "Epoch [5/10], Step [562/600], Loss: 0.5830, Train Acc:0.8112\n",
      "Epoch [5/10], Step [563/600], Loss: 0.5331, Train Acc:0.8113\n",
      "Epoch [5/10], Step [564/600], Loss: 0.8734, Train Acc:0.8111\n",
      "Epoch [5/10], Step [565/600], Loss: 0.6166, Train Acc:0.8110\n",
      "Epoch [5/10], Step [566/600], Loss: 0.6010, Train Acc:0.8110\n",
      "Epoch [5/10], Step [567/600], Loss: 0.6249, Train Acc:0.8109\n",
      "Epoch [5/10], Step [568/600], Loss: 0.6773, Train Acc:0.8108\n",
      "Epoch [5/10], Step [569/600], Loss: 0.4358, Train Acc:0.8109\n",
      "Epoch [5/10], Step [570/600], Loss: 0.5473, Train Acc:0.8110\n",
      "Epoch [5/10], Step [571/600], Loss: 0.4112, Train Acc:0.8110\n",
      "Epoch [5/10], Step [572/600], Loss: 0.6618, Train Acc:0.8110\n",
      "Epoch [5/10], Step [573/600], Loss: 0.4526, Train Acc:0.8111\n",
      "Epoch [5/10], Step [574/600], Loss: 0.5606, Train Acc:0.8111\n",
      "Epoch [5/10], Step [575/600], Loss: 0.5557, Train Acc:0.8111\n",
      "Epoch [5/10], Step [576/600], Loss: 0.4519, Train Acc:0.8112\n",
      "Epoch [5/10], Step [577/600], Loss: 0.5702, Train Acc:0.8112\n",
      "Epoch [5/10], Step [578/600], Loss: 0.5558, Train Acc:0.8113\n",
      "Epoch [5/10], Step [579/600], Loss: 0.5626, Train Acc:0.8113\n",
      "Epoch [5/10], Step [580/600], Loss: 0.6570, Train Acc:0.8113\n",
      "Epoch [5/10], Step [581/600], Loss: 0.4955, Train Acc:0.8113\n",
      "Epoch [5/10], Step [582/600], Loss: 0.6096, Train Acc:0.8113\n",
      "Epoch [5/10], Step [583/600], Loss: 0.7459, Train Acc:0.8111\n",
      "Epoch [5/10], Step [584/600], Loss: 0.5417, Train Acc:0.8112\n",
      "Epoch [5/10], Step [585/600], Loss: 0.5688, Train Acc:0.8112\n",
      "Epoch [5/10], Step [586/600], Loss: 0.6200, Train Acc:0.8112\n",
      "Epoch [5/10], Step [587/600], Loss: 0.6437, Train Acc:0.8112\n",
      "Epoch [5/10], Step [588/600], Loss: 0.6603, Train Acc:0.8111\n",
      "Epoch [5/10], Step [589/600], Loss: 0.5600, Train Acc:0.8111\n",
      "Epoch [5/10], Step [590/600], Loss: 0.4836, Train Acc:0.8112\n",
      "Epoch [5/10], Step [591/600], Loss: 0.5701, Train Acc:0.8112\n",
      "Epoch [5/10], Step [592/600], Loss: 0.5550, Train Acc:0.8112\n",
      "Epoch [5/10], Step [593/600], Loss: 0.4878, Train Acc:0.8112\n",
      "Epoch [5/10], Step [594/600], Loss: 0.4992, Train Acc:0.8112\n",
      "Epoch [5/10], Step [595/600], Loss: 0.6103, Train Acc:0.8113\n",
      "Epoch [5/10], Step [596/600], Loss: 0.6907, Train Acc:0.8112\n",
      "Epoch [5/10], Step [597/600], Loss: 0.7323, Train Acc:0.8111\n",
      "Epoch [5/10], Step [598/600], Loss: 0.4686, Train Acc:0.8112\n",
      "Epoch [5/10], Step [599/600], Loss: 0.4357, Train Acc:0.8113\n",
      "Epoch [5/10], Step [600/600], Loss: 0.5577, Train Acc:0.8113\n",
      "Epoch [6/10], Step [1/600], Loss: 0.6088, Train Acc:0.7900\n",
      "Epoch [6/10], Step [2/600], Loss: 0.5380, Train Acc:0.8050\n",
      "Epoch [6/10], Step [3/600], Loss: 0.7506, Train Acc:0.8033\n",
      "Epoch [6/10], Step [4/600], Loss: 0.6014, Train Acc:0.8000\n",
      "Epoch [6/10], Step [5/600], Loss: 0.5660, Train Acc:0.8060\n",
      "Epoch [6/10], Step [6/600], Loss: 0.6781, Train Acc:0.8050\n",
      "Epoch [6/10], Step [7/600], Loss: 0.5799, Train Acc:0.8100\n",
      "Epoch [6/10], Step [8/600], Loss: 0.5156, Train Acc:0.8163\n",
      "Epoch [6/10], Step [9/600], Loss: 0.5473, Train Acc:0.8189\n",
      "Epoch [6/10], Step [10/600], Loss: 0.5445, Train Acc:0.8160\n",
      "Epoch [6/10], Step [11/600], Loss: 0.6768, Train Acc:0.8118\n",
      "Epoch [6/10], Step [12/600], Loss: 0.6010, Train Acc:0.8133\n",
      "Epoch [6/10], Step [13/600], Loss: 0.4007, Train Acc:0.8223\n",
      "Epoch [6/10], Step [14/600], Loss: 0.5386, Train Acc:0.8214\n",
      "Epoch [6/10], Step [15/600], Loss: 0.6684, Train Acc:0.8160\n",
      "Epoch [6/10], Step [16/600], Loss: 0.6583, Train Acc:0.8169\n",
      "Epoch [6/10], Step [17/600], Loss: 0.6355, Train Acc:0.8153\n",
      "Epoch [6/10], Step [18/600], Loss: 0.7012, Train Acc:0.8128\n",
      "Epoch [6/10], Step [19/600], Loss: 0.5812, Train Acc:0.8111\n",
      "Epoch [6/10], Step [20/600], Loss: 0.5319, Train Acc:0.8125\n",
      "Epoch [6/10], Step [21/600], Loss: 0.5572, Train Acc:0.8124\n",
      "Epoch [6/10], Step [22/600], Loss: 0.6192, Train Acc:0.8123\n",
      "Epoch [6/10], Step [23/600], Loss: 0.5546, Train Acc:0.8104\n",
      "Epoch [6/10], Step [24/600], Loss: 0.4455, Train Acc:0.8137\n",
      "Epoch [6/10], Step [25/600], Loss: 0.4846, Train Acc:0.8140\n",
      "Epoch [6/10], Step [26/600], Loss: 0.4322, Train Acc:0.8162\n",
      "Epoch [6/10], Step [27/600], Loss: 0.5553, Train Acc:0.8178\n",
      "Epoch [6/10], Step [28/600], Loss: 0.5491, Train Acc:0.8186\n",
      "Epoch [6/10], Step [29/600], Loss: 0.6998, Train Acc:0.8183\n",
      "Epoch [6/10], Step [30/600], Loss: 0.5518, Train Acc:0.8170\n",
      "Epoch [6/10], Step [31/600], Loss: 0.5615, Train Acc:0.8168\n",
      "Epoch [6/10], Step [32/600], Loss: 0.5816, Train Acc:0.8169\n",
      "Epoch [6/10], Step [33/600], Loss: 0.5557, Train Acc:0.8167\n",
      "Epoch [6/10], Step [34/600], Loss: 0.5409, Train Acc:0.8176\n",
      "Epoch [6/10], Step [35/600], Loss: 0.6792, Train Acc:0.8157\n",
      "Epoch [6/10], Step [36/600], Loss: 0.5482, Train Acc:0.8156\n",
      "Epoch [6/10], Step [37/600], Loss: 0.5823, Train Acc:0.8154\n",
      "Epoch [6/10], Step [38/600], Loss: 0.6182, Train Acc:0.8153\n",
      "Epoch [6/10], Step [39/600], Loss: 0.7207, Train Acc:0.8138\n",
      "Epoch [6/10], Step [40/600], Loss: 0.5269, Train Acc:0.8133\n",
      "Epoch [6/10], Step [41/600], Loss: 0.6058, Train Acc:0.8132\n",
      "Epoch [6/10], Step [42/600], Loss: 0.6279, Train Acc:0.8121\n",
      "Epoch [6/10], Step [43/600], Loss: 0.5341, Train Acc:0.8121\n",
      "Epoch [6/10], Step [44/600], Loss: 0.6290, Train Acc:0.8120\n",
      "Epoch [6/10], Step [45/600], Loss: 0.5102, Train Acc:0.8120\n",
      "Epoch [6/10], Step [46/600], Loss: 0.5711, Train Acc:0.8117\n",
      "Epoch [6/10], Step [47/600], Loss: 0.5613, Train Acc:0.8123\n",
      "Epoch [6/10], Step [48/600], Loss: 0.5686, Train Acc:0.8127\n",
      "Epoch [6/10], Step [49/600], Loss: 0.4979, Train Acc:0.8133\n",
      "Epoch [6/10], Step [50/600], Loss: 0.5392, Train Acc:0.8140\n",
      "Epoch [6/10], Step [51/600], Loss: 0.5907, Train Acc:0.8139\n",
      "Epoch [6/10], Step [52/600], Loss: 0.7503, Train Acc:0.8127\n",
      "Epoch [6/10], Step [53/600], Loss: 0.6013, Train Acc:0.8125\n",
      "Epoch [6/10], Step [54/600], Loss: 0.6701, Train Acc:0.8122\n",
      "Epoch [6/10], Step [55/600], Loss: 0.6982, Train Acc:0.8113\n",
      "Epoch [6/10], Step [56/600], Loss: 0.6731, Train Acc:0.8104\n",
      "Epoch [6/10], Step [57/600], Loss: 0.5562, Train Acc:0.8102\n",
      "Epoch [6/10], Step [58/600], Loss: 0.6511, Train Acc:0.8098\n",
      "Epoch [6/10], Step [59/600], Loss: 0.6600, Train Acc:0.8097\n",
      "Epoch [6/10], Step [60/600], Loss: 0.5911, Train Acc:0.8093\n",
      "Epoch [6/10], Step [61/600], Loss: 0.7123, Train Acc:0.8084\n",
      "Epoch [6/10], Step [62/600], Loss: 0.4496, Train Acc:0.8089\n",
      "Epoch [6/10], Step [63/600], Loss: 0.6048, Train Acc:0.8092\n",
      "Epoch [6/10], Step [64/600], Loss: 0.7140, Train Acc:0.8084\n",
      "Epoch [6/10], Step [65/600], Loss: 0.6795, Train Acc:0.8078\n",
      "Epoch [6/10], Step [66/600], Loss: 0.5075, Train Acc:0.8086\n",
      "Epoch [6/10], Step [67/600], Loss: 0.6298, Train Acc:0.8087\n",
      "Epoch [6/10], Step [68/600], Loss: 0.4405, Train Acc:0.8093\n",
      "Epoch [6/10], Step [69/600], Loss: 0.5102, Train Acc:0.8096\n",
      "Epoch [6/10], Step [70/600], Loss: 0.5460, Train Acc:0.8100\n",
      "Epoch [6/10], Step [71/600], Loss: 0.4913, Train Acc:0.8103\n",
      "Epoch [6/10], Step [72/600], Loss: 0.4788, Train Acc:0.8113\n",
      "Epoch [6/10], Step [73/600], Loss: 0.5221, Train Acc:0.8115\n",
      "Epoch [6/10], Step [74/600], Loss: 0.6164, Train Acc:0.8118\n",
      "Epoch [6/10], Step [75/600], Loss: 0.6461, Train Acc:0.8115\n",
      "Epoch [6/10], Step [76/600], Loss: 0.6507, Train Acc:0.8107\n",
      "Epoch [6/10], Step [77/600], Loss: 0.5487, Train Acc:0.8106\n",
      "Epoch [6/10], Step [78/600], Loss: 0.6904, Train Acc:0.8099\n",
      "Epoch [6/10], Step [79/600], Loss: 0.5700, Train Acc:0.8097\n",
      "Epoch [6/10], Step [80/600], Loss: 0.6630, Train Acc:0.8089\n",
      "Epoch [6/10], Step [81/600], Loss: 0.5966, Train Acc:0.8085\n",
      "Epoch [6/10], Step [82/600], Loss: 0.6382, Train Acc:0.8083\n",
      "Epoch [6/10], Step [83/600], Loss: 0.6784, Train Acc:0.8081\n",
      "Epoch [6/10], Step [84/600], Loss: 0.5114, Train Acc:0.8082\n",
      "Epoch [6/10], Step [85/600], Loss: 0.5364, Train Acc:0.8084\n",
      "Epoch [6/10], Step [86/600], Loss: 0.4278, Train Acc:0.8091\n",
      "Epoch [6/10], Step [87/600], Loss: 0.4861, Train Acc:0.8097\n",
      "Epoch [6/10], Step [88/600], Loss: 0.5299, Train Acc:0.8098\n",
      "Epoch [6/10], Step [89/600], Loss: 0.5035, Train Acc:0.8101\n",
      "Epoch [6/10], Step [90/600], Loss: 0.4285, Train Acc:0.8111\n",
      "Epoch [6/10], Step [91/600], Loss: 0.4614, Train Acc:0.8116\n",
      "Epoch [6/10], Step [92/600], Loss: 0.5322, Train Acc:0.8117\n",
      "Epoch [6/10], Step [93/600], Loss: 0.5029, Train Acc:0.8120\n",
      "Epoch [6/10], Step [94/600], Loss: 0.5038, Train Acc:0.8126\n",
      "Epoch [6/10], Step [95/600], Loss: 0.6425, Train Acc:0.8117\n",
      "Epoch [6/10], Step [96/600], Loss: 0.6199, Train Acc:0.8116\n",
      "Epoch [6/10], Step [97/600], Loss: 0.5863, Train Acc:0.8112\n",
      "Epoch [6/10], Step [98/600], Loss: 0.6609, Train Acc:0.8107\n",
      "Epoch [6/10], Step [99/600], Loss: 0.5803, Train Acc:0.8103\n",
      "Epoch [6/10], Step [100/600], Loss: 0.6347, Train Acc:0.8102\n",
      "Epoch [6/10], Step [101/600], Loss: 0.4707, Train Acc:0.8107\n",
      "Epoch [6/10], Step [102/600], Loss: 0.6553, Train Acc:0.8107\n",
      "Epoch [6/10], Step [103/600], Loss: 0.5038, Train Acc:0.8112\n",
      "Epoch [6/10], Step [104/600], Loss: 0.5614, Train Acc:0.8111\n",
      "Epoch [6/10], Step [105/600], Loss: 0.5926, Train Acc:0.8110\n",
      "Epoch [6/10], Step [106/600], Loss: 0.5766, Train Acc:0.8113\n",
      "Epoch [6/10], Step [107/600], Loss: 0.5677, Train Acc:0.8115\n",
      "Epoch [6/10], Step [108/600], Loss: 0.4866, Train Acc:0.8120\n",
      "Epoch [6/10], Step [109/600], Loss: 0.5859, Train Acc:0.8121\n",
      "Epoch [6/10], Step [110/600], Loss: 0.5872, Train Acc:0.8121\n",
      "Epoch [6/10], Step [111/600], Loss: 0.6065, Train Acc:0.8122\n",
      "Epoch [6/10], Step [112/600], Loss: 0.4661, Train Acc:0.8130\n",
      "Epoch [6/10], Step [113/600], Loss: 0.5845, Train Acc:0.8132\n",
      "Epoch [6/10], Step [114/600], Loss: 0.5226, Train Acc:0.8132\n",
      "Epoch [6/10], Step [115/600], Loss: 0.5281, Train Acc:0.8137\n",
      "Epoch [6/10], Step [116/600], Loss: 0.6870, Train Acc:0.8134\n",
      "Epoch [6/10], Step [117/600], Loss: 0.6363, Train Acc:0.8130\n",
      "Epoch [6/10], Step [118/600], Loss: 0.6014, Train Acc:0.8125\n",
      "Epoch [6/10], Step [119/600], Loss: 0.5635, Train Acc:0.8125\n",
      "Epoch [6/10], Step [120/600], Loss: 0.5844, Train Acc:0.8126\n",
      "Epoch [6/10], Step [121/600], Loss: 0.5293, Train Acc:0.8126\n",
      "Epoch [6/10], Step [122/600], Loss: 0.6129, Train Acc:0.8126\n",
      "Epoch [6/10], Step [123/600], Loss: 0.5887, Train Acc:0.8127\n",
      "Epoch [6/10], Step [124/600], Loss: 0.4970, Train Acc:0.8129\n",
      "Epoch [6/10], Step [125/600], Loss: 0.6120, Train Acc:0.8126\n",
      "Epoch [6/10], Step [126/600], Loss: 0.7124, Train Acc:0.8123\n",
      "Epoch [6/10], Step [127/600], Loss: 0.6927, Train Acc:0.8122\n",
      "Epoch [6/10], Step [128/600], Loss: 0.6078, Train Acc:0.8122\n",
      "Epoch [6/10], Step [129/600], Loss: 0.7025, Train Acc:0.8116\n",
      "Epoch [6/10], Step [130/600], Loss: 0.5195, Train Acc:0.8120\n",
      "Epoch [6/10], Step [131/600], Loss: 0.6808, Train Acc:0.8119\n",
      "Epoch [6/10], Step [132/600], Loss: 0.5411, Train Acc:0.8119\n",
      "Epoch [6/10], Step [133/600], Loss: 0.4849, Train Acc:0.8120\n",
      "Epoch [6/10], Step [134/600], Loss: 0.6503, Train Acc:0.8115\n",
      "Epoch [6/10], Step [135/600], Loss: 0.5550, Train Acc:0.8116\n",
      "Epoch [6/10], Step [136/600], Loss: 0.7147, Train Acc:0.8114\n",
      "Epoch [6/10], Step [137/600], Loss: 0.4857, Train Acc:0.8118\n",
      "Epoch [6/10], Step [138/600], Loss: 0.4697, Train Acc:0.8123\n",
      "Epoch [6/10], Step [139/600], Loss: 0.6040, Train Acc:0.8123\n",
      "Epoch [6/10], Step [140/600], Loss: 0.5081, Train Acc:0.8125\n",
      "Epoch [6/10], Step [141/600], Loss: 0.6281, Train Acc:0.8124\n",
      "Epoch [6/10], Step [142/600], Loss: 0.6391, Train Acc:0.8121\n",
      "Epoch [6/10], Step [143/600], Loss: 0.6690, Train Acc:0.8119\n",
      "Epoch [6/10], Step [144/600], Loss: 0.5404, Train Acc:0.8120\n",
      "Epoch [6/10], Step [145/600], Loss: 0.5927, Train Acc:0.8124\n",
      "Epoch [6/10], Step [146/600], Loss: 0.4888, Train Acc:0.8127\n",
      "Epoch [6/10], Step [147/600], Loss: 0.6357, Train Acc:0.8124\n",
      "Epoch [6/10], Step [148/600], Loss: 0.5090, Train Acc:0.8126\n",
      "Epoch [6/10], Step [149/600], Loss: 0.4849, Train Acc:0.8132\n",
      "Epoch [6/10], Step [150/600], Loss: 0.6390, Train Acc:0.8130\n",
      "Epoch [6/10], Step [151/600], Loss: 0.4603, Train Acc:0.8133\n",
      "Epoch [6/10], Step [152/600], Loss: 0.4630, Train Acc:0.8135\n",
      "Epoch [6/10], Step [153/600], Loss: 0.5500, Train Acc:0.8135\n",
      "Epoch [6/10], Step [154/600], Loss: 0.5772, Train Acc:0.8135\n",
      "Epoch [6/10], Step [155/600], Loss: 0.4804, Train Acc:0.8138\n",
      "Epoch [6/10], Step [156/600], Loss: 0.6026, Train Acc:0.8138\n",
      "Epoch [6/10], Step [157/600], Loss: 0.5522, Train Acc:0.8136\n",
      "Epoch [6/10], Step [158/600], Loss: 0.6600, Train Acc:0.8134\n",
      "Epoch [6/10], Step [159/600], Loss: 0.4815, Train Acc:0.8136\n",
      "Epoch [6/10], Step [160/600], Loss: 0.4758, Train Acc:0.8137\n",
      "Epoch [6/10], Step [161/600], Loss: 0.7005, Train Acc:0.8135\n",
      "Epoch [6/10], Step [162/600], Loss: 0.5007, Train Acc:0.8138\n",
      "Epoch [6/10], Step [163/600], Loss: 0.4991, Train Acc:0.8139\n",
      "Epoch [6/10], Step [164/600], Loss: 0.5454, Train Acc:0.8138\n",
      "Epoch [6/10], Step [165/600], Loss: 0.5916, Train Acc:0.8136\n",
      "Epoch [6/10], Step [166/600], Loss: 0.5948, Train Acc:0.8137\n",
      "Epoch [6/10], Step [167/600], Loss: 0.5553, Train Acc:0.8138\n",
      "Epoch [6/10], Step [168/600], Loss: 0.5493, Train Acc:0.8139\n",
      "Epoch [6/10], Step [169/600], Loss: 0.6004, Train Acc:0.8140\n",
      "Epoch [6/10], Step [170/600], Loss: 0.5039, Train Acc:0.8142\n",
      "Epoch [6/10], Step [171/600], Loss: 0.6164, Train Acc:0.8139\n",
      "Epoch [6/10], Step [172/600], Loss: 0.6791, Train Acc:0.8135\n",
      "Epoch [6/10], Step [173/600], Loss: 0.7473, Train Acc:0.8134\n",
      "Epoch [6/10], Step [174/600], Loss: 0.5688, Train Acc:0.8135\n",
      "Epoch [6/10], Step [175/600], Loss: 0.5772, Train Acc:0.8134\n",
      "Epoch [6/10], Step [176/600], Loss: 0.5328, Train Acc:0.8134\n",
      "Epoch [6/10], Step [177/600], Loss: 0.6011, Train Acc:0.8133\n",
      "Epoch [6/10], Step [178/600], Loss: 0.5302, Train Acc:0.8131\n",
      "Epoch [6/10], Step [179/600], Loss: 0.5407, Train Acc:0.8133\n",
      "Epoch [6/10], Step [180/600], Loss: 0.5724, Train Acc:0.8136\n",
      "Epoch [6/10], Step [181/600], Loss: 0.5804, Train Acc:0.8136\n",
      "Epoch [6/10], Step [182/600], Loss: 0.6499, Train Acc:0.8136\n",
      "Epoch [6/10], Step [183/600], Loss: 0.6194, Train Acc:0.8136\n",
      "Epoch [6/10], Step [184/600], Loss: 0.5775, Train Acc:0.8133\n",
      "Epoch [6/10], Step [185/600], Loss: 0.4683, Train Acc:0.8132\n",
      "Epoch [6/10], Step [186/600], Loss: 0.4819, Train Acc:0.8134\n",
      "Epoch [6/10], Step [187/600], Loss: 0.7288, Train Acc:0.8133\n",
      "Epoch [6/10], Step [188/600], Loss: 0.5708, Train Acc:0.8132\n",
      "Epoch [6/10], Step [189/600], Loss: 0.5308, Train Acc:0.8132\n",
      "Epoch [6/10], Step [190/600], Loss: 0.4695, Train Acc:0.8134\n",
      "Epoch [6/10], Step [191/600], Loss: 0.6523, Train Acc:0.8132\n",
      "Epoch [6/10], Step [192/600], Loss: 0.4166, Train Acc:0.8137\n",
      "Epoch [6/10], Step [193/600], Loss: 0.5385, Train Acc:0.8139\n",
      "Epoch [6/10], Step [194/600], Loss: 0.6388, Train Acc:0.8139\n",
      "Epoch [6/10], Step [195/600], Loss: 0.7528, Train Acc:0.8135\n",
      "Epoch [6/10], Step [196/600], Loss: 0.5440, Train Acc:0.8136\n",
      "Epoch [6/10], Step [197/600], Loss: 0.5192, Train Acc:0.8138\n",
      "Epoch [6/10], Step [198/600], Loss: 0.5568, Train Acc:0.8139\n",
      "Epoch [6/10], Step [199/600], Loss: 0.4934, Train Acc:0.8141\n",
      "Epoch [6/10], Step [200/600], Loss: 0.7954, Train Acc:0.8139\n",
      "Epoch [6/10], Step [201/600], Loss: 0.6894, Train Acc:0.8136\n",
      "Epoch [6/10], Step [202/600], Loss: 0.5664, Train Acc:0.8137\n",
      "Epoch [6/10], Step [203/600], Loss: 0.6566, Train Acc:0.8136\n",
      "Epoch [6/10], Step [204/600], Loss: 0.4513, Train Acc:0.8138\n",
      "Epoch [6/10], Step [205/600], Loss: 0.5503, Train Acc:0.8138\n",
      "Epoch [6/10], Step [206/600], Loss: 0.5714, Train Acc:0.8136\n",
      "Epoch [6/10], Step [207/600], Loss: 0.4854, Train Acc:0.8137\n",
      "Epoch [6/10], Step [208/600], Loss: 0.5560, Train Acc:0.8137\n",
      "Epoch [6/10], Step [209/600], Loss: 0.6362, Train Acc:0.8136\n",
      "Epoch [6/10], Step [210/600], Loss: 0.6205, Train Acc:0.8137\n",
      "Epoch [6/10], Step [211/600], Loss: 0.5794, Train Acc:0.8136\n",
      "Epoch [6/10], Step [212/600], Loss: 0.7037, Train Acc:0.8136\n",
      "Epoch [6/10], Step [213/600], Loss: 0.4826, Train Acc:0.8138\n",
      "Epoch [6/10], Step [214/600], Loss: 0.4384, Train Acc:0.8140\n",
      "Epoch [6/10], Step [215/600], Loss: 0.4074, Train Acc:0.8143\n",
      "Epoch [6/10], Step [216/600], Loss: 0.6077, Train Acc:0.8142\n",
      "Epoch [6/10], Step [217/600], Loss: 0.5392, Train Acc:0.8141\n",
      "Epoch [6/10], Step [218/600], Loss: 0.5151, Train Acc:0.8142\n",
      "Epoch [6/10], Step [219/600], Loss: 0.5570, Train Acc:0.8142\n",
      "Epoch [6/10], Step [220/600], Loss: 0.5509, Train Acc:0.8144\n",
      "Epoch [6/10], Step [221/600], Loss: 0.5149, Train Acc:0.8145\n",
      "Epoch [6/10], Step [222/600], Loss: 0.6148, Train Acc:0.8145\n",
      "Epoch [6/10], Step [223/600], Loss: 0.5912, Train Acc:0.8145\n",
      "Epoch [6/10], Step [224/600], Loss: 0.5604, Train Acc:0.8146\n",
      "Epoch [6/10], Step [225/600], Loss: 0.7526, Train Acc:0.8143\n",
      "Epoch [6/10], Step [226/600], Loss: 0.5318, Train Acc:0.8142\n",
      "Epoch [6/10], Step [227/600], Loss: 0.4502, Train Acc:0.8145\n",
      "Epoch [6/10], Step [228/600], Loss: 0.4759, Train Acc:0.8148\n",
      "Epoch [6/10], Step [229/600], Loss: 0.5708, Train Acc:0.8149\n",
      "Epoch [6/10], Step [230/600], Loss: 0.5844, Train Acc:0.8148\n",
      "Epoch [6/10], Step [231/600], Loss: 0.5208, Train Acc:0.8150\n",
      "Epoch [6/10], Step [232/600], Loss: 0.5234, Train Acc:0.8149\n",
      "Epoch [6/10], Step [233/600], Loss: 0.5180, Train Acc:0.8151\n",
      "Epoch [6/10], Step [234/600], Loss: 0.5523, Train Acc:0.8152\n",
      "Epoch [6/10], Step [235/600], Loss: 0.4392, Train Acc:0.8154\n",
      "Epoch [6/10], Step [236/600], Loss: 0.6324, Train Acc:0.8153\n",
      "Epoch [6/10], Step [237/600], Loss: 0.4965, Train Acc:0.8155\n",
      "Epoch [6/10], Step [238/600], Loss: 0.5183, Train Acc:0.8157\n",
      "Epoch [6/10], Step [239/600], Loss: 0.6145, Train Acc:0.8156\n",
      "Epoch [6/10], Step [240/600], Loss: 0.6455, Train Acc:0.8157\n",
      "Epoch [6/10], Step [241/600], Loss: 0.5269, Train Acc:0.8156\n",
      "Epoch [6/10], Step [242/600], Loss: 0.5465, Train Acc:0.8155\n",
      "Epoch [6/10], Step [243/600], Loss: 0.5207, Train Acc:0.8157\n",
      "Epoch [6/10], Step [244/600], Loss: 0.5203, Train Acc:0.8157\n",
      "Epoch [6/10], Step [245/600], Loss: 0.5089, Train Acc:0.8159\n",
      "Epoch [6/10], Step [246/600], Loss: 0.5470, Train Acc:0.8158\n",
      "Epoch [6/10], Step [247/600], Loss: 0.5241, Train Acc:0.8158\n",
      "Epoch [6/10], Step [248/600], Loss: 0.5014, Train Acc:0.8159\n",
      "Epoch [6/10], Step [249/600], Loss: 0.4968, Train Acc:0.8159\n",
      "Epoch [6/10], Step [250/600], Loss: 0.4476, Train Acc:0.8160\n",
      "Epoch [6/10], Step [251/600], Loss: 0.5336, Train Acc:0.8161\n",
      "Epoch [6/10], Step [252/600], Loss: 0.6089, Train Acc:0.8160\n",
      "Epoch [6/10], Step [253/600], Loss: 0.6262, Train Acc:0.8157\n",
      "Epoch [6/10], Step [254/600], Loss: 0.5932, Train Acc:0.8156\n",
      "Epoch [6/10], Step [255/600], Loss: 0.5280, Train Acc:0.8156\n",
      "Epoch [6/10], Step [256/600], Loss: 0.5448, Train Acc:0.8157\n",
      "Epoch [6/10], Step [257/600], Loss: 0.6996, Train Acc:0.8156\n",
      "Epoch [6/10], Step [258/600], Loss: 0.4578, Train Acc:0.8159\n",
      "Epoch [6/10], Step [259/600], Loss: 0.4936, Train Acc:0.8160\n",
      "Epoch [6/10], Step [260/600], Loss: 0.4559, Train Acc:0.8162\n",
      "Epoch [6/10], Step [261/600], Loss: 0.7655, Train Acc:0.8159\n",
      "Epoch [6/10], Step [262/600], Loss: 0.5998, Train Acc:0.8160\n",
      "Epoch [6/10], Step [263/600], Loss: 0.5227, Train Acc:0.8160\n",
      "Epoch [6/10], Step [264/600], Loss: 0.5764, Train Acc:0.8160\n",
      "Epoch [6/10], Step [265/600], Loss: 0.5514, Train Acc:0.8160\n",
      "Epoch [6/10], Step [266/600], Loss: 0.5808, Train Acc:0.8161\n",
      "Epoch [6/10], Step [267/600], Loss: 0.6256, Train Acc:0.8160\n",
      "Epoch [6/10], Step [268/600], Loss: 0.5463, Train Acc:0.8158\n",
      "Epoch [6/10], Step [269/600], Loss: 0.6452, Train Acc:0.8159\n",
      "Epoch [6/10], Step [270/600], Loss: 0.5487, Train Acc:0.8159\n",
      "Epoch [6/10], Step [271/600], Loss: 0.4945, Train Acc:0.8161\n",
      "Epoch [6/10], Step [272/600], Loss: 0.5314, Train Acc:0.8161\n",
      "Epoch [6/10], Step [273/600], Loss: 0.6027, Train Acc:0.8162\n",
      "Epoch [6/10], Step [274/600], Loss: 0.6333, Train Acc:0.8160\n",
      "Epoch [6/10], Step [275/600], Loss: 0.5101, Train Acc:0.8160\n",
      "Epoch [6/10], Step [276/600], Loss: 0.5705, Train Acc:0.8161\n",
      "Epoch [6/10], Step [277/600], Loss: 0.6504, Train Acc:0.8160\n",
      "Epoch [6/10], Step [278/600], Loss: 0.4862, Train Acc:0.8162\n",
      "Epoch [6/10], Step [279/600], Loss: 0.6120, Train Acc:0.8159\n",
      "Epoch [6/10], Step [280/600], Loss: 0.5996, Train Acc:0.8160\n",
      "Epoch [6/10], Step [281/600], Loss: 0.5566, Train Acc:0.8159\n",
      "Epoch [6/10], Step [282/600], Loss: 0.5498, Train Acc:0.8161\n",
      "Epoch [6/10], Step [283/600], Loss: 0.6454, Train Acc:0.8159\n",
      "Epoch [6/10], Step [284/600], Loss: 0.5805, Train Acc:0.8159\n",
      "Epoch [6/10], Step [285/600], Loss: 0.5569, Train Acc:0.8159\n",
      "Epoch [6/10], Step [286/600], Loss: 0.5493, Train Acc:0.8160\n",
      "Epoch [6/10], Step [287/600], Loss: 0.7279, Train Acc:0.8159\n",
      "Epoch [6/10], Step [288/600], Loss: 0.5370, Train Acc:0.8160\n",
      "Epoch [6/10], Step [289/600], Loss: 0.5813, Train Acc:0.8159\n",
      "Epoch [6/10], Step [290/600], Loss: 0.5041, Train Acc:0.8160\n",
      "Epoch [6/10], Step [291/600], Loss: 0.6408, Train Acc:0.8159\n",
      "Epoch [6/10], Step [292/600], Loss: 0.5175, Train Acc:0.8158\n",
      "Epoch [6/10], Step [293/600], Loss: 0.6237, Train Acc:0.8156\n",
      "Epoch [6/10], Step [294/600], Loss: 0.5017, Train Acc:0.8156\n",
      "Epoch [6/10], Step [295/600], Loss: 0.5211, Train Acc:0.8156\n",
      "Epoch [6/10], Step [296/600], Loss: 0.6412, Train Acc:0.8156\n",
      "Epoch [6/10], Step [297/600], Loss: 0.5891, Train Acc:0.8156\n",
      "Epoch [6/10], Step [298/600], Loss: 0.6010, Train Acc:0.8154\n",
      "Epoch [6/10], Step [299/600], Loss: 0.4950, Train Acc:0.8154\n",
      "Epoch [6/10], Step [300/600], Loss: 0.5025, Train Acc:0.8155\n",
      "Epoch [6/10], Step [301/600], Loss: 0.6064, Train Acc:0.8155\n",
      "Epoch [6/10], Step [302/600], Loss: 0.6198, Train Acc:0.8153\n",
      "Epoch [6/10], Step [303/600], Loss: 0.4422, Train Acc:0.8156\n",
      "Epoch [6/10], Step [304/600], Loss: 0.5418, Train Acc:0.8155\n",
      "Epoch [6/10], Step [305/600], Loss: 0.6009, Train Acc:0.8156\n",
      "Epoch [6/10], Step [306/600], Loss: 0.4764, Train Acc:0.8157\n",
      "Epoch [6/10], Step [307/600], Loss: 0.5024, Train Acc:0.8159\n",
      "Epoch [6/10], Step [308/600], Loss: 0.5227, Train Acc:0.8161\n",
      "Epoch [6/10], Step [309/600], Loss: 0.7191, Train Acc:0.8158\n",
      "Epoch [6/10], Step [310/600], Loss: 0.5434, Train Acc:0.8158\n",
      "Epoch [6/10], Step [311/600], Loss: 0.3993, Train Acc:0.8160\n",
      "Epoch [6/10], Step [312/600], Loss: 0.6236, Train Acc:0.8159\n",
      "Epoch [6/10], Step [313/600], Loss: 0.4484, Train Acc:0.8160\n",
      "Epoch [6/10], Step [314/600], Loss: 0.4497, Train Acc:0.8163\n",
      "Epoch [6/10], Step [315/600], Loss: 0.5362, Train Acc:0.8163\n",
      "Epoch [6/10], Step [316/600], Loss: 0.6531, Train Acc:0.8161\n",
      "Epoch [6/10], Step [317/600], Loss: 0.4552, Train Acc:0.8162\n",
      "Epoch [6/10], Step [318/600], Loss: 0.5467, Train Acc:0.8163\n",
      "Epoch [6/10], Step [319/600], Loss: 0.4658, Train Acc:0.8165\n",
      "Epoch [6/10], Step [320/600], Loss: 0.4688, Train Acc:0.8166\n",
      "Epoch [6/10], Step [321/600], Loss: 0.4947, Train Acc:0.8165\n",
      "Epoch [6/10], Step [322/600], Loss: 0.6432, Train Acc:0.8163\n",
      "Epoch [6/10], Step [323/600], Loss: 0.5092, Train Acc:0.8165\n",
      "Epoch [6/10], Step [324/600], Loss: 0.6781, Train Acc:0.8165\n",
      "Epoch [6/10], Step [325/600], Loss: 0.7166, Train Acc:0.8163\n",
      "Epoch [6/10], Step [326/600], Loss: 0.6143, Train Acc:0.8162\n",
      "Epoch [6/10], Step [327/600], Loss: 0.5445, Train Acc:0.8163\n",
      "Epoch [6/10], Step [328/600], Loss: 0.6125, Train Acc:0.8162\n",
      "Epoch [6/10], Step [329/600], Loss: 0.5046, Train Acc:0.8163\n",
      "Epoch [6/10], Step [330/600], Loss: 0.6435, Train Acc:0.8162\n",
      "Epoch [6/10], Step [331/600], Loss: 0.5440, Train Acc:0.8163\n",
      "Epoch [6/10], Step [332/600], Loss: 0.5297, Train Acc:0.8164\n",
      "Epoch [6/10], Step [333/600], Loss: 0.5702, Train Acc:0.8163\n",
      "Epoch [6/10], Step [334/600], Loss: 0.5740, Train Acc:0.8163\n",
      "Epoch [6/10], Step [335/600], Loss: 0.4502, Train Acc:0.8164\n",
      "Epoch [6/10], Step [336/600], Loss: 0.5061, Train Acc:0.8165\n",
      "Epoch [6/10], Step [337/600], Loss: 0.3777, Train Acc:0.8168\n",
      "Epoch [6/10], Step [338/600], Loss: 0.4165, Train Acc:0.8170\n",
      "Epoch [6/10], Step [339/600], Loss: 0.6127, Train Acc:0.8169\n",
      "Epoch [6/10], Step [340/600], Loss: 0.5086, Train Acc:0.8171\n",
      "Epoch [6/10], Step [341/600], Loss: 0.4114, Train Acc:0.8173\n",
      "Epoch [6/10], Step [342/600], Loss: 0.4436, Train Acc:0.8174\n",
      "Epoch [6/10], Step [343/600], Loss: 0.4359, Train Acc:0.8175\n",
      "Epoch [6/10], Step [344/600], Loss: 0.6717, Train Acc:0.8175\n",
      "Epoch [6/10], Step [345/600], Loss: 0.4751, Train Acc:0.8176\n",
      "Epoch [6/10], Step [346/600], Loss: 0.5559, Train Acc:0.8177\n",
      "Epoch [6/10], Step [347/600], Loss: 0.5102, Train Acc:0.8178\n",
      "Epoch [6/10], Step [348/600], Loss: 0.4946, Train Acc:0.8179\n",
      "Epoch [6/10], Step [349/600], Loss: 0.6276, Train Acc:0.8178\n",
      "Epoch [6/10], Step [350/600], Loss: 0.4471, Train Acc:0.8179\n",
      "Epoch [6/10], Step [351/600], Loss: 0.7729, Train Acc:0.8177\n",
      "Epoch [6/10], Step [352/600], Loss: 0.5768, Train Acc:0.8177\n",
      "Epoch [6/10], Step [353/600], Loss: 0.4436, Train Acc:0.8178\n",
      "Epoch [6/10], Step [354/600], Loss: 0.6716, Train Acc:0.8177\n",
      "Epoch [6/10], Step [355/600], Loss: 0.5790, Train Acc:0.8176\n",
      "Epoch [6/10], Step [356/600], Loss: 0.6170, Train Acc:0.8176\n",
      "Epoch [6/10], Step [357/600], Loss: 0.4926, Train Acc:0.8177\n",
      "Epoch [6/10], Step [358/600], Loss: 0.7969, Train Acc:0.8175\n",
      "Epoch [6/10], Step [359/600], Loss: 0.3913, Train Acc:0.8177\n",
      "Epoch [6/10], Step [360/600], Loss: 0.4822, Train Acc:0.8178\n",
      "Epoch [6/10], Step [361/600], Loss: 0.5130, Train Acc:0.8179\n",
      "Epoch [6/10], Step [362/600], Loss: 0.5789, Train Acc:0.8179\n",
      "Epoch [6/10], Step [363/600], Loss: 0.5452, Train Acc:0.8179\n",
      "Epoch [6/10], Step [364/600], Loss: 0.5525, Train Acc:0.8179\n",
      "Epoch [6/10], Step [365/600], Loss: 0.5843, Train Acc:0.8179\n",
      "Epoch [6/10], Step [366/600], Loss: 0.4783, Train Acc:0.8180\n",
      "Epoch [6/10], Step [367/600], Loss: 0.6345, Train Acc:0.8180\n",
      "Epoch [6/10], Step [368/600], Loss: 0.5477, Train Acc:0.8179\n",
      "Epoch [6/10], Step [369/600], Loss: 0.4576, Train Acc:0.8180\n",
      "Epoch [6/10], Step [370/600], Loss: 0.5144, Train Acc:0.8181\n",
      "Epoch [6/10], Step [371/600], Loss: 0.4785, Train Acc:0.8182\n",
      "Epoch [6/10], Step [372/600], Loss: 0.5410, Train Acc:0.8183\n",
      "Epoch [6/10], Step [373/600], Loss: 0.5690, Train Acc:0.8183\n",
      "Epoch [6/10], Step [374/600], Loss: 0.6047, Train Acc:0.8184\n",
      "Epoch [6/10], Step [375/600], Loss: 0.5297, Train Acc:0.8185\n",
      "Epoch [6/10], Step [376/600], Loss: 0.4993, Train Acc:0.8186\n",
      "Epoch [6/10], Step [377/600], Loss: 0.6350, Train Acc:0.8184\n",
      "Epoch [6/10], Step [378/600], Loss: 0.5066, Train Acc:0.8185\n",
      "Epoch [6/10], Step [379/600], Loss: 0.4642, Train Acc:0.8186\n",
      "Epoch [6/10], Step [380/600], Loss: 0.4790, Train Acc:0.8187\n",
      "Epoch [6/10], Step [381/600], Loss: 0.4992, Train Acc:0.8188\n",
      "Epoch [6/10], Step [382/600], Loss: 0.7027, Train Acc:0.8186\n",
      "Epoch [6/10], Step [383/600], Loss: 0.5561, Train Acc:0.8187\n",
      "Epoch [6/10], Step [384/600], Loss: 0.6150, Train Acc:0.8187\n",
      "Epoch [6/10], Step [385/600], Loss: 0.7200, Train Acc:0.8186\n",
      "Epoch [6/10], Step [386/600], Loss: 0.6844, Train Acc:0.8186\n",
      "Epoch [6/10], Step [387/600], Loss: 0.5442, Train Acc:0.8187\n",
      "Epoch [6/10], Step [388/600], Loss: 0.5572, Train Acc:0.8188\n",
      "Epoch [6/10], Step [389/600], Loss: 0.4657, Train Acc:0.8189\n",
      "Epoch [6/10], Step [390/600], Loss: 0.6115, Train Acc:0.8187\n",
      "Epoch [6/10], Step [391/600], Loss: 0.5866, Train Acc:0.8188\n",
      "Epoch [6/10], Step [392/600], Loss: 0.5889, Train Acc:0.8189\n",
      "Epoch [6/10], Step [393/600], Loss: 0.5437, Train Acc:0.8189\n",
      "Epoch [6/10], Step [394/600], Loss: 0.5659, Train Acc:0.8188\n",
      "Epoch [6/10], Step [395/600], Loss: 0.6053, Train Acc:0.8188\n",
      "Epoch [6/10], Step [396/600], Loss: 0.5356, Train Acc:0.8189\n",
      "Epoch [6/10], Step [397/600], Loss: 0.5711, Train Acc:0.8187\n",
      "Epoch [6/10], Step [398/600], Loss: 0.5450, Train Acc:0.8188\n",
      "Epoch [6/10], Step [399/600], Loss: 0.4967, Train Acc:0.8189\n",
      "Epoch [6/10], Step [400/600], Loss: 0.5138, Train Acc:0.8190\n",
      "Epoch [6/10], Step [401/600], Loss: 0.5661, Train Acc:0.8190\n",
      "Epoch [6/10], Step [402/600], Loss: 0.5795, Train Acc:0.8189\n",
      "Epoch [6/10], Step [403/600], Loss: 0.6725, Train Acc:0.8188\n",
      "Epoch [6/10], Step [404/600], Loss: 0.4546, Train Acc:0.8189\n",
      "Epoch [6/10], Step [405/600], Loss: 0.6302, Train Acc:0.8188\n",
      "Epoch [6/10], Step [406/600], Loss: 0.5620, Train Acc:0.8187\n",
      "Epoch [6/10], Step [407/600], Loss: 0.4321, Train Acc:0.8188\n",
      "Epoch [6/10], Step [408/600], Loss: 0.5276, Train Acc:0.8189\n",
      "Epoch [6/10], Step [409/600], Loss: 0.6569, Train Acc:0.8187\n",
      "Epoch [6/10], Step [410/600], Loss: 0.5993, Train Acc:0.8188\n",
      "Epoch [6/10], Step [411/600], Loss: 0.6710, Train Acc:0.8186\n",
      "Epoch [6/10], Step [412/600], Loss: 0.4768, Train Acc:0.8188\n",
      "Epoch [6/10], Step [413/600], Loss: 0.5264, Train Acc:0.8188\n",
      "Epoch [6/10], Step [414/600], Loss: 0.5987, Train Acc:0.8188\n",
      "Epoch [6/10], Step [415/600], Loss: 0.5869, Train Acc:0.8187\n",
      "Epoch [6/10], Step [416/600], Loss: 0.6220, Train Acc:0.8187\n",
      "Epoch [6/10], Step [417/600], Loss: 0.5892, Train Acc:0.8187\n",
      "Epoch [6/10], Step [418/600], Loss: 0.6277, Train Acc:0.8186\n",
      "Epoch [6/10], Step [419/600], Loss: 0.6206, Train Acc:0.8185\n",
      "Epoch [6/10], Step [420/600], Loss: 0.5385, Train Acc:0.8185\n",
      "Epoch [6/10], Step [421/600], Loss: 0.5402, Train Acc:0.8184\n",
      "Epoch [6/10], Step [422/600], Loss: 0.5058, Train Acc:0.8184\n",
      "Epoch [6/10], Step [423/600], Loss: 0.4969, Train Acc:0.8185\n",
      "Epoch [6/10], Step [424/600], Loss: 0.7368, Train Acc:0.8183\n",
      "Epoch [6/10], Step [425/600], Loss: 0.6370, Train Acc:0.8182\n",
      "Epoch [6/10], Step [426/600], Loss: 0.5682, Train Acc:0.8183\n",
      "Epoch [6/10], Step [427/600], Loss: 0.4796, Train Acc:0.8183\n",
      "Epoch [6/10], Step [428/600], Loss: 0.5288, Train Acc:0.8184\n",
      "Epoch [6/10], Step [429/600], Loss: 0.4898, Train Acc:0.8184\n",
      "Epoch [6/10], Step [430/600], Loss: 0.7639, Train Acc:0.8182\n",
      "Epoch [6/10], Step [431/600], Loss: 0.6886, Train Acc:0.8182\n",
      "Epoch [6/10], Step [432/600], Loss: 0.6021, Train Acc:0.8181\n",
      "Epoch [6/10], Step [433/600], Loss: 0.5878, Train Acc:0.8182\n",
      "Epoch [6/10], Step [434/600], Loss: 0.5648, Train Acc:0.8181\n",
      "Epoch [6/10], Step [435/600], Loss: 0.5068, Train Acc:0.8182\n",
      "Epoch [6/10], Step [436/600], Loss: 0.6028, Train Acc:0.8181\n",
      "Epoch [6/10], Step [437/600], Loss: 0.5755, Train Acc:0.8182\n",
      "Epoch [6/10], Step [438/600], Loss: 0.5637, Train Acc:0.8182\n",
      "Epoch [6/10], Step [439/600], Loss: 0.5511, Train Acc:0.8182\n",
      "Epoch [6/10], Step [440/600], Loss: 0.7018, Train Acc:0.8180\n",
      "Epoch [6/10], Step [441/600], Loss: 0.5853, Train Acc:0.8179\n",
      "Epoch [6/10], Step [442/600], Loss: 0.4867, Train Acc:0.8180\n",
      "Epoch [6/10], Step [443/600], Loss: 0.5034, Train Acc:0.8181\n",
      "Epoch [6/10], Step [444/600], Loss: 0.6150, Train Acc:0.8180\n",
      "Epoch [6/10], Step [445/600], Loss: 0.6485, Train Acc:0.8180\n",
      "Epoch [6/10], Step [446/600], Loss: 0.5924, Train Acc:0.8178\n",
      "Epoch [6/10], Step [447/600], Loss: 0.3935, Train Acc:0.8181\n",
      "Epoch [6/10], Step [448/600], Loss: 0.6051, Train Acc:0.8180\n",
      "Epoch [6/10], Step [449/600], Loss: 0.5613, Train Acc:0.8181\n",
      "Epoch [6/10], Step [450/600], Loss: 0.6025, Train Acc:0.8180\n",
      "Epoch [6/10], Step [451/600], Loss: 0.5876, Train Acc:0.8180\n",
      "Epoch [6/10], Step [452/600], Loss: 0.4789, Train Acc:0.8181\n",
      "Epoch [6/10], Step [453/600], Loss: 0.4613, Train Acc:0.8182\n",
      "Epoch [6/10], Step [454/600], Loss: 0.4441, Train Acc:0.8183\n",
      "Epoch [6/10], Step [455/600], Loss: 0.6359, Train Acc:0.8182\n",
      "Epoch [6/10], Step [456/600], Loss: 0.5710, Train Acc:0.8182\n",
      "Epoch [6/10], Step [457/600], Loss: 0.5503, Train Acc:0.8182\n",
      "Epoch [6/10], Step [458/600], Loss: 0.5316, Train Acc:0.8182\n",
      "Epoch [6/10], Step [459/600], Loss: 0.5444, Train Acc:0.8183\n",
      "Epoch [6/10], Step [460/600], Loss: 0.5446, Train Acc:0.8183\n",
      "Epoch [6/10], Step [461/600], Loss: 0.5676, Train Acc:0.8183\n",
      "Epoch [6/10], Step [462/600], Loss: 0.5784, Train Acc:0.8182\n",
      "Epoch [6/10], Step [463/600], Loss: 0.4058, Train Acc:0.8183\n",
      "Epoch [6/10], Step [464/600], Loss: 0.5754, Train Acc:0.8184\n",
      "Epoch [6/10], Step [465/600], Loss: 0.5544, Train Acc:0.8184\n",
      "Epoch [6/10], Step [466/600], Loss: 0.5677, Train Acc:0.8184\n",
      "Epoch [6/10], Step [467/600], Loss: 0.6453, Train Acc:0.8185\n",
      "Epoch [6/10], Step [468/600], Loss: 0.5809, Train Acc:0.8184\n",
      "Epoch [6/10], Step [469/600], Loss: 0.6793, Train Acc:0.8183\n",
      "Epoch [6/10], Step [470/600], Loss: 0.6652, Train Acc:0.8182\n",
      "Epoch [6/10], Step [471/600], Loss: 0.5652, Train Acc:0.8181\n",
      "Epoch [6/10], Step [472/600], Loss: 0.5953, Train Acc:0.8181\n",
      "Epoch [6/10], Step [473/600], Loss: 0.6211, Train Acc:0.8179\n",
      "Epoch [6/10], Step [474/600], Loss: 0.5880, Train Acc:0.8178\n",
      "Epoch [6/10], Step [475/600], Loss: 0.6133, Train Acc:0.8177\n",
      "Epoch [6/10], Step [476/600], Loss: 0.6691, Train Acc:0.8178\n",
      "Epoch [6/10], Step [477/600], Loss: 0.6278, Train Acc:0.8178\n",
      "Epoch [6/10], Step [478/600], Loss: 0.5521, Train Acc:0.8178\n",
      "Epoch [6/10], Step [479/600], Loss: 0.6123, Train Acc:0.8177\n",
      "Epoch [6/10], Step [480/600], Loss: 0.5171, Train Acc:0.8177\n",
      "Epoch [6/10], Step [481/600], Loss: 0.5190, Train Acc:0.8178\n",
      "Epoch [6/10], Step [482/600], Loss: 0.7087, Train Acc:0.8178\n",
      "Epoch [6/10], Step [483/600], Loss: 0.5856, Train Acc:0.8179\n",
      "Epoch [6/10], Step [484/600], Loss: 0.6971, Train Acc:0.8178\n",
      "Epoch [6/10], Step [485/600], Loss: 0.5661, Train Acc:0.8178\n",
      "Epoch [6/10], Step [486/600], Loss: 0.3994, Train Acc:0.8179\n",
      "Epoch [6/10], Step [487/600], Loss: 0.4953, Train Acc:0.8180\n",
      "Epoch [6/10], Step [488/600], Loss: 0.7768, Train Acc:0.8177\n",
      "Epoch [6/10], Step [489/600], Loss: 0.6092, Train Acc:0.8177\n",
      "Epoch [6/10], Step [490/600], Loss: 0.6565, Train Acc:0.8177\n",
      "Epoch [6/10], Step [491/600], Loss: 0.4039, Train Acc:0.8178\n",
      "Epoch [6/10], Step [492/600], Loss: 0.5197, Train Acc:0.8178\n",
      "Epoch [6/10], Step [493/600], Loss: 0.6098, Train Acc:0.8178\n",
      "Epoch [6/10], Step [494/600], Loss: 0.4160, Train Acc:0.8180\n",
      "Epoch [6/10], Step [495/600], Loss: 0.5950, Train Acc:0.8178\n",
      "Epoch [6/10], Step [496/600], Loss: 0.4438, Train Acc:0.8178\n",
      "Epoch [6/10], Step [497/600], Loss: 0.7189, Train Acc:0.8177\n",
      "Epoch [6/10], Step [498/600], Loss: 0.5486, Train Acc:0.8177\n",
      "Epoch [6/10], Step [499/600], Loss: 0.6509, Train Acc:0.8177\n",
      "Epoch [6/10], Step [500/600], Loss: 0.5915, Train Acc:0.8177\n",
      "Epoch [6/10], Step [501/600], Loss: 0.5149, Train Acc:0.8178\n",
      "Epoch [6/10], Step [502/600], Loss: 0.4791, Train Acc:0.8179\n",
      "Epoch [6/10], Step [503/600], Loss: 0.5506, Train Acc:0.8178\n",
      "Epoch [6/10], Step [504/600], Loss: 0.5663, Train Acc:0.8178\n",
      "Epoch [6/10], Step [505/600], Loss: 0.5703, Train Acc:0.8179\n",
      "Epoch [6/10], Step [506/600], Loss: 0.4919, Train Acc:0.8179\n",
      "Epoch [6/10], Step [507/600], Loss: 0.6027, Train Acc:0.8179\n",
      "Epoch [6/10], Step [508/600], Loss: 0.5408, Train Acc:0.8179\n",
      "Epoch [6/10], Step [509/600], Loss: 0.5791, Train Acc:0.8178\n",
      "Epoch [6/10], Step [510/600], Loss: 0.4601, Train Acc:0.8178\n",
      "Epoch [6/10], Step [511/600], Loss: 0.5666, Train Acc:0.8178\n",
      "Epoch [6/10], Step [512/600], Loss: 0.5020, Train Acc:0.8178\n",
      "Epoch [6/10], Step [513/600], Loss: 0.6098, Train Acc:0.8178\n",
      "Epoch [6/10], Step [514/600], Loss: 0.6527, Train Acc:0.8178\n",
      "Epoch [6/10], Step [515/600], Loss: 0.4936, Train Acc:0.8179\n",
      "Epoch [6/10], Step [516/600], Loss: 0.5024, Train Acc:0.8179\n",
      "Epoch [6/10], Step [517/600], Loss: 0.5798, Train Acc:0.8179\n",
      "Epoch [6/10], Step [518/600], Loss: 0.5030, Train Acc:0.8179\n",
      "Epoch [6/10], Step [519/600], Loss: 0.4836, Train Acc:0.8180\n",
      "Epoch [6/10], Step [520/600], Loss: 0.6019, Train Acc:0.8180\n",
      "Epoch [6/10], Step [521/600], Loss: 0.6913, Train Acc:0.8179\n",
      "Epoch [6/10], Step [522/600], Loss: 0.5353, Train Acc:0.8179\n",
      "Epoch [6/10], Step [523/600], Loss: 0.6192, Train Acc:0.8179\n",
      "Epoch [6/10], Step [524/600], Loss: 0.6833, Train Acc:0.8178\n",
      "Epoch [6/10], Step [525/600], Loss: 0.4352, Train Acc:0.8178\n",
      "Epoch [6/10], Step [526/600], Loss: 0.6643, Train Acc:0.8177\n",
      "Epoch [6/10], Step [527/600], Loss: 0.5905, Train Acc:0.8177\n",
      "Epoch [6/10], Step [528/600], Loss: 0.6257, Train Acc:0.8176\n",
      "Epoch [6/10], Step [529/600], Loss: 0.5827, Train Acc:0.8175\n",
      "Epoch [6/10], Step [530/600], Loss: 0.5889, Train Acc:0.8175\n",
      "Epoch [6/10], Step [531/600], Loss: 0.6051, Train Acc:0.8175\n",
      "Epoch [6/10], Step [532/600], Loss: 0.6586, Train Acc:0.8175\n",
      "Epoch [6/10], Step [533/600], Loss: 0.4529, Train Acc:0.8176\n",
      "Epoch [6/10], Step [534/600], Loss: 0.5932, Train Acc:0.8175\n",
      "Epoch [6/10], Step [535/600], Loss: 0.5614, Train Acc:0.8175\n",
      "Epoch [6/10], Step [536/600], Loss: 0.4923, Train Acc:0.8176\n",
      "Epoch [6/10], Step [537/600], Loss: 0.5278, Train Acc:0.8176\n",
      "Epoch [6/10], Step [538/600], Loss: 0.4723, Train Acc:0.8176\n",
      "Epoch [6/10], Step [539/600], Loss: 0.6593, Train Acc:0.8176\n",
      "Epoch [6/10], Step [540/600], Loss: 0.4970, Train Acc:0.8176\n",
      "Epoch [6/10], Step [541/600], Loss: 0.5893, Train Acc:0.8175\n",
      "Epoch [6/10], Step [542/600], Loss: 0.4836, Train Acc:0.8175\n",
      "Epoch [6/10], Step [543/600], Loss: 0.5275, Train Acc:0.8176\n",
      "Epoch [6/10], Step [544/600], Loss: 0.5377, Train Acc:0.8176\n",
      "Epoch [6/10], Step [545/600], Loss: 0.4712, Train Acc:0.8177\n",
      "Epoch [6/10], Step [546/600], Loss: 0.4736, Train Acc:0.8179\n",
      "Epoch [6/10], Step [547/600], Loss: 0.5025, Train Acc:0.8179\n",
      "Epoch [6/10], Step [548/600], Loss: 0.6148, Train Acc:0.8179\n",
      "Epoch [6/10], Step [549/600], Loss: 0.5526, Train Acc:0.8179\n",
      "Epoch [6/10], Step [550/600], Loss: 0.4954, Train Acc:0.8180\n",
      "Epoch [6/10], Step [551/600], Loss: 0.5872, Train Acc:0.8181\n",
      "Epoch [6/10], Step [552/600], Loss: 0.6301, Train Acc:0.8180\n",
      "Epoch [6/10], Step [553/600], Loss: 0.5614, Train Acc:0.8180\n",
      "Epoch [6/10], Step [554/600], Loss: 0.5482, Train Acc:0.8181\n",
      "Epoch [6/10], Step [555/600], Loss: 0.5993, Train Acc:0.8181\n",
      "Epoch [6/10], Step [556/600], Loss: 0.5561, Train Acc:0.8181\n",
      "Epoch [6/10], Step [557/600], Loss: 0.7271, Train Acc:0.8180\n",
      "Epoch [6/10], Step [558/600], Loss: 0.5260, Train Acc:0.8180\n",
      "Epoch [6/10], Step [559/600], Loss: 0.6314, Train Acc:0.8179\n",
      "Epoch [6/10], Step [560/600], Loss: 0.7136, Train Acc:0.8177\n",
      "Epoch [6/10], Step [561/600], Loss: 0.5556, Train Acc:0.8176\n",
      "Epoch [6/10], Step [562/600], Loss: 0.5600, Train Acc:0.8176\n",
      "Epoch [6/10], Step [563/600], Loss: 0.4713, Train Acc:0.8176\n",
      "Epoch [6/10], Step [564/600], Loss: 0.4466, Train Acc:0.8176\n",
      "Epoch [6/10], Step [565/600], Loss: 0.6302, Train Acc:0.8176\n",
      "Epoch [6/10], Step [566/600], Loss: 0.4975, Train Acc:0.8176\n",
      "Epoch [6/10], Step [567/600], Loss: 0.6805, Train Acc:0.8175\n",
      "Epoch [6/10], Step [568/600], Loss: 0.5563, Train Acc:0.8176\n",
      "Epoch [6/10], Step [569/600], Loss: 0.5553, Train Acc:0.8175\n",
      "Epoch [6/10], Step [570/600], Loss: 0.7696, Train Acc:0.8174\n",
      "Epoch [6/10], Step [571/600], Loss: 0.5262, Train Acc:0.8174\n",
      "Epoch [6/10], Step [572/600], Loss: 0.4503, Train Acc:0.8175\n",
      "Epoch [6/10], Step [573/600], Loss: 0.6113, Train Acc:0.8175\n",
      "Epoch [6/10], Step [574/600], Loss: 0.5880, Train Acc:0.8175\n",
      "Epoch [6/10], Step [575/600], Loss: 0.5375, Train Acc:0.8175\n",
      "Epoch [6/10], Step [576/600], Loss: 0.6230, Train Acc:0.8176\n",
      "Epoch [6/10], Step [577/600], Loss: 0.6223, Train Acc:0.8175\n",
      "Epoch [6/10], Step [578/600], Loss: 0.5580, Train Acc:0.8176\n",
      "Epoch [6/10], Step [579/600], Loss: 0.5160, Train Acc:0.8176\n",
      "Epoch [6/10], Step [580/600], Loss: 0.6308, Train Acc:0.8175\n",
      "Epoch [6/10], Step [581/600], Loss: 0.7072, Train Acc:0.8175\n",
      "Epoch [6/10], Step [582/600], Loss: 0.5371, Train Acc:0.8175\n",
      "Epoch [6/10], Step [583/600], Loss: 0.6550, Train Acc:0.8174\n",
      "Epoch [6/10], Step [584/600], Loss: 0.6247, Train Acc:0.8174\n",
      "Epoch [6/10], Step [585/600], Loss: 0.5009, Train Acc:0.8175\n",
      "Epoch [6/10], Step [586/600], Loss: 0.5824, Train Acc:0.8174\n",
      "Epoch [6/10], Step [587/600], Loss: 0.5457, Train Acc:0.8174\n",
      "Epoch [6/10], Step [588/600], Loss: 0.4536, Train Acc:0.8174\n",
      "Epoch [6/10], Step [589/600], Loss: 0.4821, Train Acc:0.8175\n",
      "Epoch [6/10], Step [590/600], Loss: 0.5996, Train Acc:0.8174\n",
      "Epoch [6/10], Step [591/600], Loss: 0.7731, Train Acc:0.8173\n",
      "Epoch [6/10], Step [592/600], Loss: 0.5628, Train Acc:0.8173\n",
      "Epoch [6/10], Step [593/600], Loss: 0.5068, Train Acc:0.8174\n",
      "Epoch [6/10], Step [594/600], Loss: 0.5670, Train Acc:0.8173\n",
      "Epoch [6/10], Step [595/600], Loss: 0.5891, Train Acc:0.8173\n",
      "Epoch [6/10], Step [596/600], Loss: 0.5777, Train Acc:0.8174\n",
      "Epoch [6/10], Step [597/600], Loss: 0.5150, Train Acc:0.8174\n",
      "Epoch [6/10], Step [598/600], Loss: 0.6404, Train Acc:0.8174\n",
      "Epoch [6/10], Step [599/600], Loss: 0.6118, Train Acc:0.8173\n",
      "Epoch [6/10], Step [600/600], Loss: 0.5883, Train Acc:0.8173\n",
      "Epoch [7/10], Step [1/600], Loss: 0.6853, Train Acc:0.7800\n",
      "Epoch [7/10], Step [2/600], Loss: 0.4456, Train Acc:0.8200\n",
      "Epoch [7/10], Step [3/600], Loss: 0.4504, Train Acc:0.8333\n",
      "Epoch [7/10], Step [4/600], Loss: 0.4830, Train Acc:0.8475\n",
      "Epoch [7/10], Step [5/600], Loss: 0.6419, Train Acc:0.8300\n",
      "Epoch [7/10], Step [6/600], Loss: 0.6201, Train Acc:0.8217\n",
      "Epoch [7/10], Step [7/600], Loss: 0.4553, Train Acc:0.8271\n",
      "Epoch [7/10], Step [8/600], Loss: 0.5350, Train Acc:0.8337\n",
      "Epoch [7/10], Step [9/600], Loss: 0.5067, Train Acc:0.8333\n",
      "Epoch [7/10], Step [10/600], Loss: 0.6214, Train Acc:0.8270\n",
      "Epoch [7/10], Step [11/600], Loss: 0.4313, Train Acc:0.8282\n",
      "Epoch [7/10], Step [12/600], Loss: 0.5403, Train Acc:0.8283\n",
      "Epoch [7/10], Step [13/600], Loss: 0.4723, Train Acc:0.8285\n",
      "Epoch [7/10], Step [14/600], Loss: 0.5209, Train Acc:0.8279\n",
      "Epoch [7/10], Step [15/600], Loss: 0.5919, Train Acc:0.8260\n",
      "Epoch [7/10], Step [16/600], Loss: 0.5084, Train Acc:0.8269\n",
      "Epoch [7/10], Step [17/600], Loss: 0.5278, Train Acc:0.8259\n",
      "Epoch [7/10], Step [18/600], Loss: 0.5491, Train Acc:0.8244\n",
      "Epoch [7/10], Step [19/600], Loss: 0.6782, Train Acc:0.8232\n",
      "Epoch [7/10], Step [20/600], Loss: 0.5375, Train Acc:0.8220\n",
      "Epoch [7/10], Step [21/600], Loss: 0.5420, Train Acc:0.8238\n",
      "Epoch [7/10], Step [22/600], Loss: 0.5424, Train Acc:0.8264\n",
      "Epoch [7/10], Step [23/600], Loss: 0.5858, Train Acc:0.8239\n",
      "Epoch [7/10], Step [24/600], Loss: 0.6365, Train Acc:0.8221\n",
      "Epoch [7/10], Step [25/600], Loss: 0.4893, Train Acc:0.8224\n",
      "Epoch [7/10], Step [26/600], Loss: 0.4908, Train Acc:0.8227\n",
      "Epoch [7/10], Step [27/600], Loss: 0.5677, Train Acc:0.8226\n",
      "Epoch [7/10], Step [28/600], Loss: 0.5524, Train Acc:0.8221\n",
      "Epoch [7/10], Step [29/600], Loss: 0.5942, Train Acc:0.8210\n",
      "Epoch [7/10], Step [30/600], Loss: 0.4353, Train Acc:0.8227\n",
      "Epoch [7/10], Step [31/600], Loss: 0.6328, Train Acc:0.8213\n",
      "Epoch [7/10], Step [32/600], Loss: 0.5846, Train Acc:0.8216\n",
      "Epoch [7/10], Step [33/600], Loss: 0.6748, Train Acc:0.8194\n",
      "Epoch [7/10], Step [34/600], Loss: 0.6484, Train Acc:0.8185\n",
      "Epoch [7/10], Step [35/600], Loss: 0.5296, Train Acc:0.8183\n",
      "Epoch [7/10], Step [36/600], Loss: 0.4766, Train Acc:0.8186\n",
      "Epoch [7/10], Step [37/600], Loss: 0.6254, Train Acc:0.8168\n",
      "Epoch [7/10], Step [38/600], Loss: 0.5340, Train Acc:0.8168\n",
      "Epoch [7/10], Step [39/600], Loss: 0.5243, Train Acc:0.8167\n",
      "Epoch [7/10], Step [40/600], Loss: 0.5776, Train Acc:0.8170\n",
      "Epoch [7/10], Step [41/600], Loss: 0.5845, Train Acc:0.8171\n",
      "Epoch [7/10], Step [42/600], Loss: 0.6717, Train Acc:0.8157\n",
      "Epoch [7/10], Step [43/600], Loss: 0.5851, Train Acc:0.8158\n",
      "Epoch [7/10], Step [44/600], Loss: 0.4892, Train Acc:0.8166\n",
      "Epoch [7/10], Step [45/600], Loss: 0.5756, Train Acc:0.8162\n",
      "Epoch [7/10], Step [46/600], Loss: 0.5747, Train Acc:0.8170\n",
      "Epoch [7/10], Step [47/600], Loss: 0.6056, Train Acc:0.8166\n",
      "Epoch [7/10], Step [48/600], Loss: 0.4748, Train Acc:0.8175\n",
      "Epoch [7/10], Step [49/600], Loss: 0.5257, Train Acc:0.8176\n",
      "Epoch [7/10], Step [50/600], Loss: 0.6156, Train Acc:0.8168\n",
      "Epoch [7/10], Step [51/600], Loss: 0.4866, Train Acc:0.8178\n",
      "Epoch [7/10], Step [52/600], Loss: 0.5029, Train Acc:0.8185\n",
      "Epoch [7/10], Step [53/600], Loss: 0.5127, Train Acc:0.8187\n",
      "Epoch [7/10], Step [54/600], Loss: 0.4471, Train Acc:0.8193\n",
      "Epoch [7/10], Step [55/600], Loss: 0.6234, Train Acc:0.8185\n",
      "Epoch [7/10], Step [56/600], Loss: 0.4317, Train Acc:0.8193\n",
      "Epoch [7/10], Step [57/600], Loss: 0.4671, Train Acc:0.8200\n",
      "Epoch [7/10], Step [58/600], Loss: 0.5975, Train Acc:0.8202\n",
      "Epoch [7/10], Step [59/600], Loss: 0.6795, Train Acc:0.8198\n",
      "Epoch [7/10], Step [60/600], Loss: 0.6502, Train Acc:0.8203\n",
      "Epoch [7/10], Step [61/600], Loss: 0.4729, Train Acc:0.8208\n",
      "Epoch [7/10], Step [62/600], Loss: 0.6006, Train Acc:0.8203\n",
      "Epoch [7/10], Step [63/600], Loss: 0.6527, Train Acc:0.8187\n",
      "Epoch [7/10], Step [64/600], Loss: 0.4879, Train Acc:0.8192\n",
      "Epoch [7/10], Step [65/600], Loss: 0.4926, Train Acc:0.8191\n",
      "Epoch [7/10], Step [66/600], Loss: 0.3755, Train Acc:0.8209\n",
      "Epoch [7/10], Step [67/600], Loss: 0.5869, Train Acc:0.8204\n",
      "Epoch [7/10], Step [68/600], Loss: 0.4934, Train Acc:0.8203\n",
      "Epoch [7/10], Step [69/600], Loss: 0.4942, Train Acc:0.8203\n",
      "Epoch [7/10], Step [70/600], Loss: 0.5363, Train Acc:0.8200\n",
      "Epoch [7/10], Step [71/600], Loss: 0.6070, Train Acc:0.8193\n",
      "Epoch [7/10], Step [72/600], Loss: 0.5600, Train Acc:0.8194\n",
      "Epoch [7/10], Step [73/600], Loss: 0.5547, Train Acc:0.8196\n",
      "Epoch [7/10], Step [74/600], Loss: 0.4452, Train Acc:0.8201\n",
      "Epoch [7/10], Step [75/600], Loss: 0.7320, Train Acc:0.8192\n",
      "Epoch [7/10], Step [76/600], Loss: 0.5714, Train Acc:0.8192\n",
      "Epoch [7/10], Step [77/600], Loss: 0.5981, Train Acc:0.8190\n",
      "Epoch [7/10], Step [78/600], Loss: 0.5767, Train Acc:0.8187\n",
      "Epoch [7/10], Step [79/600], Loss: 0.6970, Train Acc:0.8180\n",
      "Epoch [7/10], Step [80/600], Loss: 0.5386, Train Acc:0.8177\n",
      "Epoch [7/10], Step [81/600], Loss: 0.5960, Train Acc:0.8175\n",
      "Epoch [7/10], Step [82/600], Loss: 0.6126, Train Acc:0.8178\n",
      "Epoch [7/10], Step [83/600], Loss: 0.4862, Train Acc:0.8184\n",
      "Epoch [7/10], Step [84/600], Loss: 0.5854, Train Acc:0.8183\n",
      "Epoch [7/10], Step [85/600], Loss: 0.4522, Train Acc:0.8191\n",
      "Epoch [7/10], Step [86/600], Loss: 0.4620, Train Acc:0.8199\n",
      "Epoch [7/10], Step [87/600], Loss: 0.7350, Train Acc:0.8189\n",
      "Epoch [7/10], Step [88/600], Loss: 0.4949, Train Acc:0.8193\n",
      "Epoch [7/10], Step [89/600], Loss: 0.6049, Train Acc:0.8194\n",
      "Epoch [7/10], Step [90/600], Loss: 0.5118, Train Acc:0.8191\n",
      "Epoch [7/10], Step [91/600], Loss: 0.5790, Train Acc:0.8189\n",
      "Epoch [7/10], Step [92/600], Loss: 0.5484, Train Acc:0.8189\n",
      "Epoch [7/10], Step [93/600], Loss: 0.5152, Train Acc:0.8190\n",
      "Epoch [7/10], Step [94/600], Loss: 0.5730, Train Acc:0.8188\n",
      "Epoch [7/10], Step [95/600], Loss: 0.5719, Train Acc:0.8184\n",
      "Epoch [7/10], Step [96/600], Loss: 0.4849, Train Acc:0.8191\n",
      "Epoch [7/10], Step [97/600], Loss: 0.4740, Train Acc:0.8194\n",
      "Epoch [7/10], Step [98/600], Loss: 0.5334, Train Acc:0.8201\n",
      "Epoch [7/10], Step [99/600], Loss: 0.5667, Train Acc:0.8202\n",
      "Epoch [7/10], Step [100/600], Loss: 0.4819, Train Acc:0.8208\n",
      "Epoch [7/10], Step [101/600], Loss: 0.6309, Train Acc:0.8202\n",
      "Epoch [7/10], Step [102/600], Loss: 0.5319, Train Acc:0.8199\n",
      "Epoch [7/10], Step [103/600], Loss: 0.4743, Train Acc:0.8202\n",
      "Epoch [7/10], Step [104/600], Loss: 0.5764, Train Acc:0.8199\n",
      "Epoch [7/10], Step [105/600], Loss: 0.5347, Train Acc:0.8198\n",
      "Epoch [7/10], Step [106/600], Loss: 0.6600, Train Acc:0.8197\n",
      "Epoch [7/10], Step [107/600], Loss: 0.5659, Train Acc:0.8197\n",
      "Epoch [7/10], Step [108/600], Loss: 0.5213, Train Acc:0.8198\n",
      "Epoch [7/10], Step [109/600], Loss: 0.7269, Train Acc:0.8189\n",
      "Epoch [7/10], Step [110/600], Loss: 0.6140, Train Acc:0.8188\n",
      "Epoch [7/10], Step [111/600], Loss: 0.5619, Train Acc:0.8190\n",
      "Epoch [7/10], Step [112/600], Loss: 0.5528, Train Acc:0.8191\n",
      "Epoch [7/10], Step [113/600], Loss: 0.6354, Train Acc:0.8184\n",
      "Epoch [7/10], Step [114/600], Loss: 0.6597, Train Acc:0.8185\n",
      "Epoch [7/10], Step [115/600], Loss: 0.5369, Train Acc:0.8184\n",
      "Epoch [7/10], Step [116/600], Loss: 0.5023, Train Acc:0.8186\n",
      "Epoch [7/10], Step [117/600], Loss: 0.5574, Train Acc:0.8185\n",
      "Epoch [7/10], Step [118/600], Loss: 0.4980, Train Acc:0.8185\n",
      "Epoch [7/10], Step [119/600], Loss: 0.6968, Train Acc:0.8179\n",
      "Epoch [7/10], Step [120/600], Loss: 0.5466, Train Acc:0.8183\n",
      "Epoch [7/10], Step [121/600], Loss: 0.5874, Train Acc:0.8179\n",
      "Epoch [7/10], Step [122/600], Loss: 0.5112, Train Acc:0.8182\n",
      "Epoch [7/10], Step [123/600], Loss: 0.6066, Train Acc:0.8177\n",
      "Epoch [7/10], Step [124/600], Loss: 0.5579, Train Acc:0.8178\n",
      "Epoch [7/10], Step [125/600], Loss: 0.5911, Train Acc:0.8177\n",
      "Epoch [7/10], Step [126/600], Loss: 0.5089, Train Acc:0.8177\n",
      "Epoch [7/10], Step [127/600], Loss: 0.6603, Train Acc:0.8170\n",
      "Epoch [7/10], Step [128/600], Loss: 0.4691, Train Acc:0.8172\n",
      "Epoch [7/10], Step [129/600], Loss: 0.5060, Train Acc:0.8172\n",
      "Epoch [7/10], Step [130/600], Loss: 0.5423, Train Acc:0.8174\n",
      "Epoch [7/10], Step [131/600], Loss: 0.6447, Train Acc:0.8173\n",
      "Epoch [7/10], Step [132/600], Loss: 0.7344, Train Acc:0.8170\n",
      "Epoch [7/10], Step [133/600], Loss: 0.6840, Train Acc:0.8168\n",
      "Epoch [7/10], Step [134/600], Loss: 0.5639, Train Acc:0.8168\n",
      "Epoch [7/10], Step [135/600], Loss: 0.5234, Train Acc:0.8167\n",
      "Epoch [7/10], Step [136/600], Loss: 0.5052, Train Acc:0.8170\n",
      "Epoch [7/10], Step [137/600], Loss: 0.4838, Train Acc:0.8171\n",
      "Epoch [7/10], Step [138/600], Loss: 0.5228, Train Acc:0.8172\n",
      "Epoch [7/10], Step [139/600], Loss: 0.4854, Train Acc:0.8175\n",
      "Epoch [7/10], Step [140/600], Loss: 0.4741, Train Acc:0.8177\n",
      "Epoch [7/10], Step [141/600], Loss: 0.5548, Train Acc:0.8182\n",
      "Epoch [7/10], Step [142/600], Loss: 0.4483, Train Acc:0.8187\n",
      "Epoch [7/10], Step [143/600], Loss: 0.5655, Train Acc:0.8185\n",
      "Epoch [7/10], Step [144/600], Loss: 0.5592, Train Acc:0.8185\n",
      "Epoch [7/10], Step [145/600], Loss: 0.5567, Train Acc:0.8188\n",
      "Epoch [7/10], Step [146/600], Loss: 0.5599, Train Acc:0.8187\n",
      "Epoch [7/10], Step [147/600], Loss: 0.5921, Train Acc:0.8186\n",
      "Epoch [7/10], Step [148/600], Loss: 0.5882, Train Acc:0.8183\n",
      "Epoch [7/10], Step [149/600], Loss: 0.5544, Train Acc:0.8181\n",
      "Epoch [7/10], Step [150/600], Loss: 0.5916, Train Acc:0.8179\n",
      "Epoch [7/10], Step [151/600], Loss: 0.5136, Train Acc:0.8182\n",
      "Epoch [7/10], Step [152/600], Loss: 0.5671, Train Acc:0.8180\n",
      "Epoch [7/10], Step [153/600], Loss: 0.5661, Train Acc:0.8180\n",
      "Epoch [7/10], Step [154/600], Loss: 0.4113, Train Acc:0.8184\n",
      "Epoch [7/10], Step [155/600], Loss: 0.5753, Train Acc:0.8185\n",
      "Epoch [7/10], Step [156/600], Loss: 0.5839, Train Acc:0.8184\n",
      "Epoch [7/10], Step [157/600], Loss: 0.6069, Train Acc:0.8179\n",
      "Epoch [7/10], Step [158/600], Loss: 0.6375, Train Acc:0.8178\n",
      "Epoch [7/10], Step [159/600], Loss: 0.6320, Train Acc:0.8177\n",
      "Epoch [7/10], Step [160/600], Loss: 0.4487, Train Acc:0.8181\n",
      "Epoch [7/10], Step [161/600], Loss: 0.4873, Train Acc:0.8186\n",
      "Epoch [7/10], Step [162/600], Loss: 0.5208, Train Acc:0.8188\n",
      "Epoch [7/10], Step [163/600], Loss: 0.5455, Train Acc:0.8187\n",
      "Epoch [7/10], Step [164/600], Loss: 0.5949, Train Acc:0.8186\n",
      "Epoch [7/10], Step [165/600], Loss: 0.5469, Train Acc:0.8186\n",
      "Epoch [7/10], Step [166/600], Loss: 0.5293, Train Acc:0.8187\n",
      "Epoch [7/10], Step [167/600], Loss: 0.5484, Train Acc:0.8185\n",
      "Epoch [7/10], Step [168/600], Loss: 0.6065, Train Acc:0.8185\n",
      "Epoch [7/10], Step [169/600], Loss: 0.5753, Train Acc:0.8186\n",
      "Epoch [7/10], Step [170/600], Loss: 0.4774, Train Acc:0.8189\n",
      "Epoch [7/10], Step [171/600], Loss: 0.5326, Train Acc:0.8192\n",
      "Epoch [7/10], Step [172/600], Loss: 0.4831, Train Acc:0.8193\n",
      "Epoch [7/10], Step [173/600], Loss: 0.5808, Train Acc:0.8192\n",
      "Epoch [7/10], Step [174/600], Loss: 0.4382, Train Acc:0.8195\n",
      "Epoch [7/10], Step [175/600], Loss: 0.4737, Train Acc:0.8197\n",
      "Epoch [7/10], Step [176/600], Loss: 0.4011, Train Acc:0.8198\n",
      "Epoch [7/10], Step [177/600], Loss: 0.5050, Train Acc:0.8200\n",
      "Epoch [7/10], Step [178/600], Loss: 0.7051, Train Acc:0.8201\n",
      "Epoch [7/10], Step [179/600], Loss: 0.5481, Train Acc:0.8201\n",
      "Epoch [7/10], Step [180/600], Loss: 0.6110, Train Acc:0.8199\n",
      "Epoch [7/10], Step [181/600], Loss: 0.4678, Train Acc:0.8200\n",
      "Epoch [7/10], Step [182/600], Loss: 0.6048, Train Acc:0.8197\n",
      "Epoch [7/10], Step [183/600], Loss: 0.5510, Train Acc:0.8196\n",
      "Epoch [7/10], Step [184/600], Loss: 0.5743, Train Acc:0.8197\n",
      "Epoch [7/10], Step [185/600], Loss: 0.6730, Train Acc:0.8192\n",
      "Epoch [7/10], Step [186/600], Loss: 0.4573, Train Acc:0.8194\n",
      "Epoch [7/10], Step [187/600], Loss: 0.4973, Train Acc:0.8195\n",
      "Epoch [7/10], Step [188/600], Loss: 0.5470, Train Acc:0.8195\n",
      "Epoch [7/10], Step [189/600], Loss: 0.5362, Train Acc:0.8197\n",
      "Epoch [7/10], Step [190/600], Loss: 0.5352, Train Acc:0.8197\n",
      "Epoch [7/10], Step [191/600], Loss: 0.5633, Train Acc:0.8196\n",
      "Epoch [7/10], Step [192/600], Loss: 0.4645, Train Acc:0.8199\n",
      "Epoch [7/10], Step [193/600], Loss: 0.5553, Train Acc:0.8198\n",
      "Epoch [7/10], Step [194/600], Loss: 0.4520, Train Acc:0.8199\n",
      "Epoch [7/10], Step [195/600], Loss: 0.6544, Train Acc:0.8196\n",
      "Epoch [7/10], Step [196/600], Loss: 0.5879, Train Acc:0.8196\n",
      "Epoch [7/10], Step [197/600], Loss: 0.3916, Train Acc:0.8201\n",
      "Epoch [7/10], Step [198/600], Loss: 0.4837, Train Acc:0.8203\n",
      "Epoch [7/10], Step [199/600], Loss: 0.5864, Train Acc:0.8203\n",
      "Epoch [7/10], Step [200/600], Loss: 0.5881, Train Acc:0.8202\n",
      "Epoch [7/10], Step [201/600], Loss: 0.5342, Train Acc:0.8201\n",
      "Epoch [7/10], Step [202/600], Loss: 0.5684, Train Acc:0.8200\n",
      "Epoch [7/10], Step [203/600], Loss: 0.5367, Train Acc:0.8200\n",
      "Epoch [7/10], Step [204/600], Loss: 0.4793, Train Acc:0.8202\n",
      "Epoch [7/10], Step [205/600], Loss: 0.6011, Train Acc:0.8199\n",
      "Epoch [7/10], Step [206/600], Loss: 0.5398, Train Acc:0.8196\n",
      "Epoch [7/10], Step [207/600], Loss: 0.6912, Train Acc:0.8195\n",
      "Epoch [7/10], Step [208/600], Loss: 0.5020, Train Acc:0.8197\n",
      "Epoch [7/10], Step [209/600], Loss: 0.4606, Train Acc:0.8200\n",
      "Epoch [7/10], Step [210/600], Loss: 0.6143, Train Acc:0.8199\n",
      "Epoch [7/10], Step [211/600], Loss: 0.4358, Train Acc:0.8199\n",
      "Epoch [7/10], Step [212/600], Loss: 0.4792, Train Acc:0.8202\n",
      "Epoch [7/10], Step [213/600], Loss: 0.4818, Train Acc:0.8204\n",
      "Epoch [7/10], Step [214/600], Loss: 0.6723, Train Acc:0.8201\n",
      "Epoch [7/10], Step [215/600], Loss: 0.6358, Train Acc:0.8200\n",
      "Epoch [7/10], Step [216/600], Loss: 0.3795, Train Acc:0.8204\n",
      "Epoch [7/10], Step [217/600], Loss: 0.5351, Train Acc:0.8202\n",
      "Epoch [7/10], Step [218/600], Loss: 0.5399, Train Acc:0.8203\n",
      "Epoch [7/10], Step [219/600], Loss: 0.4818, Train Acc:0.8203\n",
      "Epoch [7/10], Step [220/600], Loss: 0.6443, Train Acc:0.8203\n",
      "Epoch [7/10], Step [221/600], Loss: 0.5246, Train Acc:0.8203\n",
      "Epoch [7/10], Step [222/600], Loss: 0.5532, Train Acc:0.8204\n",
      "Epoch [7/10], Step [223/600], Loss: 0.4969, Train Acc:0.8205\n",
      "Epoch [7/10], Step [224/600], Loss: 0.5804, Train Acc:0.8204\n",
      "Epoch [7/10], Step [225/600], Loss: 0.4741, Train Acc:0.8206\n",
      "Epoch [7/10], Step [226/600], Loss: 0.4620, Train Acc:0.8207\n",
      "Epoch [7/10], Step [227/600], Loss: 0.5373, Train Acc:0.8207\n",
      "Epoch [7/10], Step [228/600], Loss: 0.4931, Train Acc:0.8205\n",
      "Epoch [7/10], Step [229/600], Loss: 0.5322, Train Acc:0.8205\n",
      "Epoch [7/10], Step [230/600], Loss: 0.6579, Train Acc:0.8203\n",
      "Epoch [7/10], Step [231/600], Loss: 0.5691, Train Acc:0.8204\n",
      "Epoch [7/10], Step [232/600], Loss: 0.4884, Train Acc:0.8204\n",
      "Epoch [7/10], Step [233/600], Loss: 0.4575, Train Acc:0.8206\n",
      "Epoch [7/10], Step [234/600], Loss: 0.4914, Train Acc:0.8205\n",
      "Epoch [7/10], Step [235/600], Loss: 0.5739, Train Acc:0.8205\n",
      "Epoch [7/10], Step [236/600], Loss: 0.7574, Train Acc:0.8201\n",
      "Epoch [7/10], Step [237/600], Loss: 0.4425, Train Acc:0.8203\n",
      "Epoch [7/10], Step [238/600], Loss: 0.5434, Train Acc:0.8203\n",
      "Epoch [7/10], Step [239/600], Loss: 0.6249, Train Acc:0.8203\n",
      "Epoch [7/10], Step [240/600], Loss: 0.4335, Train Acc:0.8205\n",
      "Epoch [7/10], Step [241/600], Loss: 0.5430, Train Acc:0.8204\n",
      "Epoch [7/10], Step [242/600], Loss: 0.5151, Train Acc:0.8205\n",
      "Epoch [7/10], Step [243/600], Loss: 0.6396, Train Acc:0.8202\n",
      "Epoch [7/10], Step [244/600], Loss: 0.4997, Train Acc:0.8204\n",
      "Epoch [7/10], Step [245/600], Loss: 0.5725, Train Acc:0.8203\n",
      "Epoch [7/10], Step [246/600], Loss: 0.6480, Train Acc:0.8203\n",
      "Epoch [7/10], Step [247/600], Loss: 0.6400, Train Acc:0.8200\n",
      "Epoch [7/10], Step [248/600], Loss: 0.5640, Train Acc:0.8200\n",
      "Epoch [7/10], Step [249/600], Loss: 0.6522, Train Acc:0.8199\n",
      "Epoch [7/10], Step [250/600], Loss: 0.4840, Train Acc:0.8200\n",
      "Epoch [7/10], Step [251/600], Loss: 0.4954, Train Acc:0.8202\n",
      "Epoch [7/10], Step [252/600], Loss: 0.6258, Train Acc:0.8204\n",
      "Epoch [7/10], Step [253/600], Loss: 0.4650, Train Acc:0.8206\n",
      "Epoch [7/10], Step [254/600], Loss: 0.5507, Train Acc:0.8206\n",
      "Epoch [7/10], Step [255/600], Loss: 0.5224, Train Acc:0.8207\n",
      "Epoch [7/10], Step [256/600], Loss: 0.5089, Train Acc:0.8207\n",
      "Epoch [7/10], Step [257/600], Loss: 0.5441, Train Acc:0.8205\n",
      "Epoch [7/10], Step [258/600], Loss: 0.4576, Train Acc:0.8207\n",
      "Epoch [7/10], Step [259/600], Loss: 0.6002, Train Acc:0.8208\n",
      "Epoch [7/10], Step [260/600], Loss: 0.4650, Train Acc:0.8208\n",
      "Epoch [7/10], Step [261/600], Loss: 0.5924, Train Acc:0.8208\n",
      "Epoch [7/10], Step [262/600], Loss: 0.5847, Train Acc:0.8209\n",
      "Epoch [7/10], Step [263/600], Loss: 0.5961, Train Acc:0.8210\n",
      "Epoch [7/10], Step [264/600], Loss: 0.4933, Train Acc:0.8210\n",
      "Epoch [7/10], Step [265/600], Loss: 0.7407, Train Acc:0.8208\n",
      "Epoch [7/10], Step [266/600], Loss: 0.4475, Train Acc:0.8210\n",
      "Epoch [7/10], Step [267/600], Loss: 0.4719, Train Acc:0.8210\n",
      "Epoch [7/10], Step [268/600], Loss: 0.5390, Train Acc:0.8210\n",
      "Epoch [7/10], Step [269/600], Loss: 0.6764, Train Acc:0.8207\n",
      "Epoch [7/10], Step [270/600], Loss: 0.5401, Train Acc:0.8207\n",
      "Epoch [7/10], Step [271/600], Loss: 0.6097, Train Acc:0.8208\n",
      "Epoch [7/10], Step [272/600], Loss: 0.5485, Train Acc:0.8208\n",
      "Epoch [7/10], Step [273/600], Loss: 0.5574, Train Acc:0.8208\n",
      "Epoch [7/10], Step [274/600], Loss: 0.6772, Train Acc:0.8207\n",
      "Epoch [7/10], Step [275/600], Loss: 0.5950, Train Acc:0.8205\n",
      "Epoch [7/10], Step [276/600], Loss: 0.5762, Train Acc:0.8205\n",
      "Epoch [7/10], Step [277/600], Loss: 0.5156, Train Acc:0.8205\n",
      "Epoch [7/10], Step [278/600], Loss: 0.6619, Train Acc:0.8206\n",
      "Epoch [7/10], Step [279/600], Loss: 0.6501, Train Acc:0.8205\n",
      "Epoch [7/10], Step [280/600], Loss: 0.6306, Train Acc:0.8204\n",
      "Epoch [7/10], Step [281/600], Loss: 0.5009, Train Acc:0.8205\n",
      "Epoch [7/10], Step [282/600], Loss: 0.4936, Train Acc:0.8206\n",
      "Epoch [7/10], Step [283/600], Loss: 0.6081, Train Acc:0.8206\n",
      "Epoch [7/10], Step [284/600], Loss: 0.5239, Train Acc:0.8208\n",
      "Epoch [7/10], Step [285/600], Loss: 0.5082, Train Acc:0.8209\n",
      "Epoch [7/10], Step [286/600], Loss: 0.4603, Train Acc:0.8210\n",
      "Epoch [7/10], Step [287/600], Loss: 0.5772, Train Acc:0.8211\n",
      "Epoch [7/10], Step [288/600], Loss: 0.6074, Train Acc:0.8211\n",
      "Epoch [7/10], Step [289/600], Loss: 0.5509, Train Acc:0.8211\n",
      "Epoch [7/10], Step [290/600], Loss: 0.4423, Train Acc:0.8212\n",
      "Epoch [7/10], Step [291/600], Loss: 0.6659, Train Acc:0.8211\n",
      "Epoch [7/10], Step [292/600], Loss: 0.5459, Train Acc:0.8210\n",
      "Epoch [7/10], Step [293/600], Loss: 0.5831, Train Acc:0.8209\n",
      "Epoch [7/10], Step [294/600], Loss: 0.4989, Train Acc:0.8209\n",
      "Epoch [7/10], Step [295/600], Loss: 0.5302, Train Acc:0.8209\n",
      "Epoch [7/10], Step [296/600], Loss: 0.5275, Train Acc:0.8209\n",
      "Epoch [7/10], Step [297/600], Loss: 0.4396, Train Acc:0.8211\n",
      "Epoch [7/10], Step [298/600], Loss: 0.4475, Train Acc:0.8214\n",
      "Epoch [7/10], Step [299/600], Loss: 0.4824, Train Acc:0.8215\n",
      "Epoch [7/10], Step [300/600], Loss: 0.4407, Train Acc:0.8217\n",
      "Epoch [7/10], Step [301/600], Loss: 0.5861, Train Acc:0.8216\n",
      "Epoch [7/10], Step [302/600], Loss: 0.5546, Train Acc:0.8215\n",
      "Epoch [7/10], Step [303/600], Loss: 0.5738, Train Acc:0.8213\n",
      "Epoch [7/10], Step [304/600], Loss: 0.6019, Train Acc:0.8213\n",
      "Epoch [7/10], Step [305/600], Loss: 0.4769, Train Acc:0.8214\n",
      "Epoch [7/10], Step [306/600], Loss: 0.4845, Train Acc:0.8214\n",
      "Epoch [7/10], Step [307/600], Loss: 0.4688, Train Acc:0.8215\n",
      "Epoch [7/10], Step [308/600], Loss: 0.5427, Train Acc:0.8216\n",
      "Epoch [7/10], Step [309/600], Loss: 0.5627, Train Acc:0.8216\n",
      "Epoch [7/10], Step [310/600], Loss: 0.4828, Train Acc:0.8217\n",
      "Epoch [7/10], Step [311/600], Loss: 0.5037, Train Acc:0.8218\n",
      "Epoch [7/10], Step [312/600], Loss: 0.5819, Train Acc:0.8218\n",
      "Epoch [7/10], Step [313/600], Loss: 0.4811, Train Acc:0.8219\n",
      "Epoch [7/10], Step [314/600], Loss: 0.6069, Train Acc:0.8217\n",
      "Epoch [7/10], Step [315/600], Loss: 0.6785, Train Acc:0.8214\n",
      "Epoch [7/10], Step [316/600], Loss: 0.7315, Train Acc:0.8213\n",
      "Epoch [7/10], Step [317/600], Loss: 0.6419, Train Acc:0.8212\n",
      "Epoch [7/10], Step [318/600], Loss: 0.5971, Train Acc:0.8212\n",
      "Epoch [7/10], Step [319/600], Loss: 0.5134, Train Acc:0.8212\n",
      "Epoch [7/10], Step [320/600], Loss: 0.6204, Train Acc:0.8211\n",
      "Epoch [7/10], Step [321/600], Loss: 0.6059, Train Acc:0.8211\n",
      "Epoch [7/10], Step [322/600], Loss: 0.4976, Train Acc:0.8211\n",
      "Epoch [7/10], Step [323/600], Loss: 0.5275, Train Acc:0.8211\n",
      "Epoch [7/10], Step [324/600], Loss: 0.4600, Train Acc:0.8212\n",
      "Epoch [7/10], Step [325/600], Loss: 0.6262, Train Acc:0.8210\n",
      "Epoch [7/10], Step [326/600], Loss: 0.4184, Train Acc:0.8212\n",
      "Epoch [7/10], Step [327/600], Loss: 0.6365, Train Acc:0.8210\n",
      "Epoch [7/10], Step [328/600], Loss: 0.4826, Train Acc:0.8212\n",
      "Epoch [7/10], Step [329/600], Loss: 0.4919, Train Acc:0.8213\n",
      "Epoch [7/10], Step [330/600], Loss: 0.5330, Train Acc:0.8212\n",
      "Epoch [7/10], Step [331/600], Loss: 0.6282, Train Acc:0.8211\n",
      "Epoch [7/10], Step [332/600], Loss: 0.6683, Train Acc:0.8210\n",
      "Epoch [7/10], Step [333/600], Loss: 0.5739, Train Acc:0.8209\n",
      "Epoch [7/10], Step [334/600], Loss: 0.5770, Train Acc:0.8209\n",
      "Epoch [7/10], Step [335/600], Loss: 0.5615, Train Acc:0.8210\n",
      "Epoch [7/10], Step [336/600], Loss: 0.6002, Train Acc:0.8209\n",
      "Epoch [7/10], Step [337/600], Loss: 0.4099, Train Acc:0.8211\n",
      "Epoch [7/10], Step [338/600], Loss: 0.5314, Train Acc:0.8211\n",
      "Epoch [7/10], Step [339/600], Loss: 0.5670, Train Acc:0.8212\n",
      "Epoch [7/10], Step [340/600], Loss: 0.6689, Train Acc:0.8209\n",
      "Epoch [7/10], Step [341/600], Loss: 0.4976, Train Acc:0.8210\n",
      "Epoch [7/10], Step [342/600], Loss: 0.5165, Train Acc:0.8211\n",
      "Epoch [7/10], Step [343/600], Loss: 0.5525, Train Acc:0.8211\n",
      "Epoch [7/10], Step [344/600], Loss: 0.4004, Train Acc:0.8213\n",
      "Epoch [7/10], Step [345/600], Loss: 0.5069, Train Acc:0.8213\n",
      "Epoch [7/10], Step [346/600], Loss: 0.5633, Train Acc:0.8213\n",
      "Epoch [7/10], Step [347/600], Loss: 0.4049, Train Acc:0.8215\n",
      "Epoch [7/10], Step [348/600], Loss: 0.5867, Train Acc:0.8215\n",
      "Epoch [7/10], Step [349/600], Loss: 0.5869, Train Acc:0.8216\n",
      "Epoch [7/10], Step [350/600], Loss: 0.6076, Train Acc:0.8214\n",
      "Epoch [7/10], Step [351/600], Loss: 0.5801, Train Acc:0.8214\n",
      "Epoch [7/10], Step [352/600], Loss: 0.5124, Train Acc:0.8214\n",
      "Epoch [7/10], Step [353/600], Loss: 0.7502, Train Acc:0.8213\n",
      "Epoch [7/10], Step [354/600], Loss: 0.5761, Train Acc:0.8213\n",
      "Epoch [7/10], Step [355/600], Loss: 0.5532, Train Acc:0.8214\n",
      "Epoch [7/10], Step [356/600], Loss: 0.5365, Train Acc:0.8214\n",
      "Epoch [7/10], Step [357/600], Loss: 0.6591, Train Acc:0.8213\n",
      "Epoch [7/10], Step [358/600], Loss: 0.6658, Train Acc:0.8211\n",
      "Epoch [7/10], Step [359/600], Loss: 0.4605, Train Acc:0.8213\n",
      "Epoch [7/10], Step [360/600], Loss: 0.5453, Train Acc:0.8213\n",
      "Epoch [7/10], Step [361/600], Loss: 0.5094, Train Acc:0.8214\n",
      "Epoch [7/10], Step [362/600], Loss: 0.4894, Train Acc:0.8215\n",
      "Epoch [7/10], Step [363/600], Loss: 0.4795, Train Acc:0.8216\n",
      "Epoch [7/10], Step [364/600], Loss: 0.4952, Train Acc:0.8216\n",
      "Epoch [7/10], Step [365/600], Loss: 0.5088, Train Acc:0.8216\n",
      "Epoch [7/10], Step [366/600], Loss: 0.4260, Train Acc:0.8217\n",
      "Epoch [7/10], Step [367/600], Loss: 0.5184, Train Acc:0.8217\n",
      "Epoch [7/10], Step [368/600], Loss: 0.4419, Train Acc:0.8218\n",
      "Epoch [7/10], Step [369/600], Loss: 0.5664, Train Acc:0.8217\n",
      "Epoch [7/10], Step [370/600], Loss: 0.4881, Train Acc:0.8217\n",
      "Epoch [7/10], Step [371/600], Loss: 0.6012, Train Acc:0.8216\n",
      "Epoch [7/10], Step [372/600], Loss: 0.4553, Train Acc:0.8217\n",
      "Epoch [7/10], Step [373/600], Loss: 0.4555, Train Acc:0.8217\n",
      "Epoch [7/10], Step [374/600], Loss: 0.4476, Train Acc:0.8219\n",
      "Epoch [7/10], Step [375/600], Loss: 0.5092, Train Acc:0.8219\n",
      "Epoch [7/10], Step [376/600], Loss: 0.5511, Train Acc:0.8219\n",
      "Epoch [7/10], Step [377/600], Loss: 0.5341, Train Acc:0.8219\n",
      "Epoch [7/10], Step [378/600], Loss: 0.6592, Train Acc:0.8219\n",
      "Epoch [7/10], Step [379/600], Loss: 0.5722, Train Acc:0.8218\n",
      "Epoch [7/10], Step [380/600], Loss: 0.5571, Train Acc:0.8219\n",
      "Epoch [7/10], Step [381/600], Loss: 0.6711, Train Acc:0.8217\n",
      "Epoch [7/10], Step [382/600], Loss: 0.5539, Train Acc:0.8217\n",
      "Epoch [7/10], Step [383/600], Loss: 0.6315, Train Acc:0.8217\n",
      "Epoch [7/10], Step [384/600], Loss: 0.5757, Train Acc:0.8217\n",
      "Epoch [7/10], Step [385/600], Loss: 0.3951, Train Acc:0.8218\n",
      "Epoch [7/10], Step [386/600], Loss: 0.4646, Train Acc:0.8218\n",
      "Epoch [7/10], Step [387/600], Loss: 0.6049, Train Acc:0.8218\n",
      "Epoch [7/10], Step [388/600], Loss: 0.5328, Train Acc:0.8218\n",
      "Epoch [7/10], Step [389/600], Loss: 0.4548, Train Acc:0.8217\n",
      "Epoch [7/10], Step [390/600], Loss: 0.5686, Train Acc:0.8218\n",
      "Epoch [7/10], Step [391/600], Loss: 0.6517, Train Acc:0.8216\n",
      "Epoch [7/10], Step [392/600], Loss: 0.5485, Train Acc:0.8217\n",
      "Epoch [7/10], Step [393/600], Loss: 0.5609, Train Acc:0.8217\n",
      "Epoch [7/10], Step [394/600], Loss: 0.4549, Train Acc:0.8216\n",
      "Epoch [7/10], Step [395/600], Loss: 0.5871, Train Acc:0.8217\n",
      "Epoch [7/10], Step [396/600], Loss: 0.5379, Train Acc:0.8217\n",
      "Epoch [7/10], Step [397/600], Loss: 0.5226, Train Acc:0.8216\n",
      "Epoch [7/10], Step [398/600], Loss: 0.4844, Train Acc:0.8217\n",
      "Epoch [7/10], Step [399/600], Loss: 0.4066, Train Acc:0.8219\n",
      "Epoch [7/10], Step [400/600], Loss: 0.5810, Train Acc:0.8219\n",
      "Epoch [7/10], Step [401/600], Loss: 0.4636, Train Acc:0.8219\n",
      "Epoch [7/10], Step [402/600], Loss: 0.6814, Train Acc:0.8218\n",
      "Epoch [7/10], Step [403/600], Loss: 0.5258, Train Acc:0.8219\n",
      "Epoch [7/10], Step [404/600], Loss: 0.5026, Train Acc:0.8220\n",
      "Epoch [7/10], Step [405/600], Loss: 0.4789, Train Acc:0.8220\n",
      "Epoch [7/10], Step [406/600], Loss: 0.4636, Train Acc:0.8221\n",
      "Epoch [7/10], Step [407/600], Loss: 0.6458, Train Acc:0.8220\n",
      "Epoch [7/10], Step [408/600], Loss: 0.6176, Train Acc:0.8220\n",
      "Epoch [7/10], Step [409/600], Loss: 0.4918, Train Acc:0.8220\n",
      "Epoch [7/10], Step [410/600], Loss: 0.6727, Train Acc:0.8220\n",
      "Epoch [7/10], Step [411/600], Loss: 0.5853, Train Acc:0.8219\n",
      "Epoch [7/10], Step [412/600], Loss: 0.6205, Train Acc:0.8219\n",
      "Epoch [7/10], Step [413/600], Loss: 0.5807, Train Acc:0.8218\n",
      "Epoch [7/10], Step [414/600], Loss: 0.5148, Train Acc:0.8219\n",
      "Epoch [7/10], Step [415/600], Loss: 0.5437, Train Acc:0.8219\n",
      "Epoch [7/10], Step [416/600], Loss: 0.5285, Train Acc:0.8220\n",
      "Epoch [7/10], Step [417/600], Loss: 0.6420, Train Acc:0.8220\n",
      "Epoch [7/10], Step [418/600], Loss: 0.5574, Train Acc:0.8220\n",
      "Epoch [7/10], Step [419/600], Loss: 0.5228, Train Acc:0.8220\n",
      "Epoch [7/10], Step [420/600], Loss: 0.4162, Train Acc:0.8221\n",
      "Epoch [7/10], Step [421/600], Loss: 0.5529, Train Acc:0.8221\n",
      "Epoch [7/10], Step [422/600], Loss: 0.6730, Train Acc:0.8220\n",
      "Epoch [7/10], Step [423/600], Loss: 0.4781, Train Acc:0.8221\n",
      "Epoch [7/10], Step [424/600], Loss: 0.5584, Train Acc:0.8221\n",
      "Epoch [7/10], Step [425/600], Loss: 0.5745, Train Acc:0.8220\n",
      "Epoch [7/10], Step [426/600], Loss: 0.6749, Train Acc:0.8220\n",
      "Epoch [7/10], Step [427/600], Loss: 0.5574, Train Acc:0.8220\n",
      "Epoch [7/10], Step [428/600], Loss: 0.5460, Train Acc:0.8219\n",
      "Epoch [7/10], Step [429/600], Loss: 0.5175, Train Acc:0.8220\n",
      "Epoch [7/10], Step [430/600], Loss: 0.5740, Train Acc:0.8219\n",
      "Epoch [7/10], Step [431/600], Loss: 0.5966, Train Acc:0.8219\n",
      "Epoch [7/10], Step [432/600], Loss: 0.6203, Train Acc:0.8219\n",
      "Epoch [7/10], Step [433/600], Loss: 0.6839, Train Acc:0.8218\n",
      "Epoch [7/10], Step [434/600], Loss: 0.6107, Train Acc:0.8218\n",
      "Epoch [7/10], Step [435/600], Loss: 0.6311, Train Acc:0.8217\n",
      "Epoch [7/10], Step [436/600], Loss: 0.6142, Train Acc:0.8216\n",
      "Epoch [7/10], Step [437/600], Loss: 0.6510, Train Acc:0.8214\n",
      "Epoch [7/10], Step [438/600], Loss: 0.5737, Train Acc:0.8214\n",
      "Epoch [7/10], Step [439/600], Loss: 0.6505, Train Acc:0.8212\n",
      "Epoch [7/10], Step [440/600], Loss: 0.5569, Train Acc:0.8212\n",
      "Epoch [7/10], Step [441/600], Loss: 0.6868, Train Acc:0.8211\n",
      "Epoch [7/10], Step [442/600], Loss: 0.5931, Train Acc:0.8211\n",
      "Epoch [7/10], Step [443/600], Loss: 0.5230, Train Acc:0.8212\n",
      "Epoch [7/10], Step [444/600], Loss: 0.6926, Train Acc:0.8210\n",
      "Epoch [7/10], Step [445/600], Loss: 0.6025, Train Acc:0.8210\n",
      "Epoch [7/10], Step [446/600], Loss: 0.5976, Train Acc:0.8210\n",
      "Epoch [7/10], Step [447/600], Loss: 0.5296, Train Acc:0.8209\n",
      "Epoch [7/10], Step [448/600], Loss: 0.4810, Train Acc:0.8210\n",
      "Epoch [7/10], Step [449/600], Loss: 0.5687, Train Acc:0.8210\n",
      "Epoch [7/10], Step [450/600], Loss: 0.7074, Train Acc:0.8209\n",
      "Epoch [7/10], Step [451/600], Loss: 0.4621, Train Acc:0.8210\n",
      "Epoch [7/10], Step [452/600], Loss: 0.5908, Train Acc:0.8210\n",
      "Epoch [7/10], Step [453/600], Loss: 0.5402, Train Acc:0.8209\n",
      "Epoch [7/10], Step [454/600], Loss: 0.6189, Train Acc:0.8208\n",
      "Epoch [7/10], Step [455/600], Loss: 0.6972, Train Acc:0.8207\n",
      "Epoch [7/10], Step [456/600], Loss: 0.3977, Train Acc:0.8208\n",
      "Epoch [7/10], Step [457/600], Loss: 0.4803, Train Acc:0.8208\n",
      "Epoch [7/10], Step [458/600], Loss: 0.5621, Train Acc:0.8208\n",
      "Epoch [7/10], Step [459/600], Loss: 0.5865, Train Acc:0.8208\n",
      "Epoch [7/10], Step [460/600], Loss: 0.6319, Train Acc:0.8208\n",
      "Epoch [7/10], Step [461/600], Loss: 0.5062, Train Acc:0.8209\n",
      "Epoch [7/10], Step [462/600], Loss: 0.4814, Train Acc:0.8209\n",
      "Epoch [7/10], Step [463/600], Loss: 0.5938, Train Acc:0.8208\n",
      "Epoch [7/10], Step [464/600], Loss: 0.6219, Train Acc:0.8207\n",
      "Epoch [7/10], Step [465/600], Loss: 0.5046, Train Acc:0.8207\n",
      "Epoch [7/10], Step [466/600], Loss: 0.6297, Train Acc:0.8206\n",
      "Epoch [7/10], Step [467/600], Loss: 0.6356, Train Acc:0.8205\n",
      "Epoch [7/10], Step [468/600], Loss: 0.5787, Train Acc:0.8205\n",
      "Epoch [7/10], Step [469/600], Loss: 0.4595, Train Acc:0.8206\n",
      "Epoch [7/10], Step [470/600], Loss: 0.5244, Train Acc:0.8206\n",
      "Epoch [7/10], Step [471/600], Loss: 0.6665, Train Acc:0.8205\n",
      "Epoch [7/10], Step [472/600], Loss: 0.5110, Train Acc:0.8205\n",
      "Epoch [7/10], Step [473/600], Loss: 0.4995, Train Acc:0.8206\n",
      "Epoch [7/10], Step [474/600], Loss: 0.6517, Train Acc:0.8206\n",
      "Epoch [7/10], Step [475/600], Loss: 0.4898, Train Acc:0.8206\n",
      "Epoch [7/10], Step [476/600], Loss: 0.5574, Train Acc:0.8207\n",
      "Epoch [7/10], Step [477/600], Loss: 0.5155, Train Acc:0.8207\n",
      "Epoch [7/10], Step [478/600], Loss: 0.4548, Train Acc:0.8209\n",
      "Epoch [7/10], Step [479/600], Loss: 0.6104, Train Acc:0.8208\n",
      "Epoch [7/10], Step [480/600], Loss: 0.4592, Train Acc:0.8208\n",
      "Epoch [7/10], Step [481/600], Loss: 0.5633, Train Acc:0.8209\n",
      "Epoch [7/10], Step [482/600], Loss: 0.6065, Train Acc:0.8209\n",
      "Epoch [7/10], Step [483/600], Loss: 0.4583, Train Acc:0.8210\n",
      "Epoch [7/10], Step [484/600], Loss: 0.4634, Train Acc:0.8210\n",
      "Epoch [7/10], Step [485/600], Loss: 0.5508, Train Acc:0.8209\n",
      "Epoch [7/10], Step [486/600], Loss: 0.5566, Train Acc:0.8208\n",
      "Epoch [7/10], Step [487/600], Loss: 0.4615, Train Acc:0.8209\n",
      "Epoch [7/10], Step [488/600], Loss: 0.5039, Train Acc:0.8209\n",
      "Epoch [7/10], Step [489/600], Loss: 0.5969, Train Acc:0.8209\n",
      "Epoch [7/10], Step [490/600], Loss: 0.4526, Train Acc:0.8210\n",
      "Epoch [7/10], Step [491/600], Loss: 0.4300, Train Acc:0.8211\n",
      "Epoch [7/10], Step [492/600], Loss: 0.4270, Train Acc:0.8212\n",
      "Epoch [7/10], Step [493/600], Loss: 0.5292, Train Acc:0.8212\n",
      "Epoch [7/10], Step [494/600], Loss: 0.4277, Train Acc:0.8214\n",
      "Epoch [7/10], Step [495/600], Loss: 0.6022, Train Acc:0.8213\n",
      "Epoch [7/10], Step [496/600], Loss: 0.4153, Train Acc:0.8214\n",
      "Epoch [7/10], Step [497/600], Loss: 0.6724, Train Acc:0.8214\n",
      "Epoch [7/10], Step [498/600], Loss: 0.4928, Train Acc:0.8214\n",
      "Epoch [7/10], Step [499/600], Loss: 0.5018, Train Acc:0.8215\n",
      "Epoch [7/10], Step [500/600], Loss: 0.4888, Train Acc:0.8215\n",
      "Epoch [7/10], Step [501/600], Loss: 0.5693, Train Acc:0.8214\n",
      "Epoch [7/10], Step [502/600], Loss: 0.5519, Train Acc:0.8214\n",
      "Epoch [7/10], Step [503/600], Loss: 0.6533, Train Acc:0.8214\n",
      "Epoch [7/10], Step [504/600], Loss: 0.4950, Train Acc:0.8213\n",
      "Epoch [7/10], Step [505/600], Loss: 0.5101, Train Acc:0.8213\n",
      "Epoch [7/10], Step [506/600], Loss: 0.6034, Train Acc:0.8213\n",
      "Epoch [7/10], Step [507/600], Loss: 0.5832, Train Acc:0.8213\n",
      "Epoch [7/10], Step [508/600], Loss: 0.7121, Train Acc:0.8212\n",
      "Epoch [7/10], Step [509/600], Loss: 0.4626, Train Acc:0.8212\n",
      "Epoch [7/10], Step [510/600], Loss: 0.4825, Train Acc:0.8214\n",
      "Epoch [7/10], Step [511/600], Loss: 0.6176, Train Acc:0.8214\n",
      "Epoch [7/10], Step [512/600], Loss: 0.7209, Train Acc:0.8213\n",
      "Epoch [7/10], Step [513/600], Loss: 0.5606, Train Acc:0.8213\n",
      "Epoch [7/10], Step [514/600], Loss: 0.4820, Train Acc:0.8213\n",
      "Epoch [7/10], Step [515/600], Loss: 0.5793, Train Acc:0.8213\n",
      "Epoch [7/10], Step [516/600], Loss: 0.8059, Train Acc:0.8211\n",
      "Epoch [7/10], Step [517/600], Loss: 0.7440, Train Acc:0.8210\n",
      "Epoch [7/10], Step [518/600], Loss: 0.4511, Train Acc:0.8211\n",
      "Epoch [7/10], Step [519/600], Loss: 0.5116, Train Acc:0.8212\n",
      "Epoch [7/10], Step [520/600], Loss: 0.4631, Train Acc:0.8212\n",
      "Epoch [7/10], Step [521/600], Loss: 0.5852, Train Acc:0.8212\n",
      "Epoch [7/10], Step [522/600], Loss: 0.7189, Train Acc:0.8211\n",
      "Epoch [7/10], Step [523/600], Loss: 0.6699, Train Acc:0.8210\n",
      "Epoch [7/10], Step [524/600], Loss: 0.5321, Train Acc:0.8211\n",
      "Epoch [7/10], Step [525/600], Loss: 0.5412, Train Acc:0.8210\n",
      "Epoch [7/10], Step [526/600], Loss: 0.5527, Train Acc:0.8210\n",
      "Epoch [7/10], Step [527/600], Loss: 0.6299, Train Acc:0.8210\n",
      "Epoch [7/10], Step [528/600], Loss: 0.5282, Train Acc:0.8209\n",
      "Epoch [7/10], Step [529/600], Loss: 0.5029, Train Acc:0.8210\n",
      "Epoch [7/10], Step [530/600], Loss: 0.7321, Train Acc:0.8210\n",
      "Epoch [7/10], Step [531/600], Loss: 0.6173, Train Acc:0.8210\n",
      "Epoch [7/10], Step [532/600], Loss: 0.5641, Train Acc:0.8209\n",
      "Epoch [7/10], Step [533/600], Loss: 0.4755, Train Acc:0.8210\n",
      "Epoch [7/10], Step [534/600], Loss: 0.5539, Train Acc:0.8210\n",
      "Epoch [7/10], Step [535/600], Loss: 0.4131, Train Acc:0.8211\n",
      "Epoch [7/10], Step [536/600], Loss: 0.5440, Train Acc:0.8212\n",
      "Epoch [7/10], Step [537/600], Loss: 0.6314, Train Acc:0.8212\n",
      "Epoch [7/10], Step [538/600], Loss: 0.5301, Train Acc:0.8212\n",
      "Epoch [7/10], Step [539/600], Loss: 0.4733, Train Acc:0.8213\n",
      "Epoch [7/10], Step [540/600], Loss: 0.5159, Train Acc:0.8213\n",
      "Epoch [7/10], Step [541/600], Loss: 0.5187, Train Acc:0.8213\n",
      "Epoch [7/10], Step [542/600], Loss: 0.5283, Train Acc:0.8212\n",
      "Epoch [7/10], Step [543/600], Loss: 0.6158, Train Acc:0.8212\n",
      "Epoch [7/10], Step [544/600], Loss: 0.4449, Train Acc:0.8213\n",
      "Epoch [7/10], Step [545/600], Loss: 0.5235, Train Acc:0.8213\n",
      "Epoch [7/10], Step [546/600], Loss: 0.5522, Train Acc:0.8213\n",
      "Epoch [7/10], Step [547/600], Loss: 0.5627, Train Acc:0.8213\n",
      "Epoch [7/10], Step [548/600], Loss: 0.5491, Train Acc:0.8214\n",
      "Epoch [7/10], Step [549/600], Loss: 0.6350, Train Acc:0.8214\n",
      "Epoch [7/10], Step [550/600], Loss: 0.5276, Train Acc:0.8214\n",
      "Epoch [7/10], Step [551/600], Loss: 0.5452, Train Acc:0.8214\n",
      "Epoch [7/10], Step [552/600], Loss: 0.6044, Train Acc:0.8212\n",
      "Epoch [7/10], Step [553/600], Loss: 0.5401, Train Acc:0.8212\n",
      "Epoch [7/10], Step [554/600], Loss: 0.6598, Train Acc:0.8212\n",
      "Epoch [7/10], Step [555/600], Loss: 0.5205, Train Acc:0.8213\n",
      "Epoch [7/10], Step [556/600], Loss: 0.5499, Train Acc:0.8213\n",
      "Epoch [7/10], Step [557/600], Loss: 0.5320, Train Acc:0.8213\n",
      "Epoch [7/10], Step [558/600], Loss: 0.5723, Train Acc:0.8213\n",
      "Epoch [7/10], Step [559/600], Loss: 0.6453, Train Acc:0.8213\n",
      "Epoch [7/10], Step [560/600], Loss: 0.5287, Train Acc:0.8213\n",
      "Epoch [7/10], Step [561/600], Loss: 0.6101, Train Acc:0.8212\n",
      "Epoch [7/10], Step [562/600], Loss: 0.5194, Train Acc:0.8212\n",
      "Epoch [7/10], Step [563/600], Loss: 0.5771, Train Acc:0.8211\n",
      "Epoch [7/10], Step [564/600], Loss: 0.4302, Train Acc:0.8212\n",
      "Epoch [7/10], Step [565/600], Loss: 0.7943, Train Acc:0.8211\n",
      "Epoch [7/10], Step [566/600], Loss: 0.5122, Train Acc:0.8211\n",
      "Epoch [7/10], Step [567/600], Loss: 0.4577, Train Acc:0.8212\n",
      "Epoch [7/10], Step [568/600], Loss: 0.5696, Train Acc:0.8211\n",
      "Epoch [7/10], Step [569/600], Loss: 0.5869, Train Acc:0.8211\n",
      "Epoch [7/10], Step [570/600], Loss: 0.6642, Train Acc:0.8210\n",
      "Epoch [7/10], Step [571/600], Loss: 0.4608, Train Acc:0.8211\n",
      "Epoch [7/10], Step [572/600], Loss: 0.6459, Train Acc:0.8211\n",
      "Epoch [7/10], Step [573/600], Loss: 0.6042, Train Acc:0.8211\n",
      "Epoch [7/10], Step [574/600], Loss: 0.4552, Train Acc:0.8211\n",
      "Epoch [7/10], Step [575/600], Loss: 0.4372, Train Acc:0.8212\n",
      "Epoch [7/10], Step [576/600], Loss: 0.4900, Train Acc:0.8212\n",
      "Epoch [7/10], Step [577/600], Loss: 0.5602, Train Acc:0.8212\n",
      "Epoch [7/10], Step [578/600], Loss: 0.4320, Train Acc:0.8214\n",
      "Epoch [7/10], Step [579/600], Loss: 0.6492, Train Acc:0.8213\n",
      "Epoch [7/10], Step [580/600], Loss: 0.6789, Train Acc:0.8212\n",
      "Epoch [7/10], Step [581/600], Loss: 0.5470, Train Acc:0.8213\n",
      "Epoch [7/10], Step [582/600], Loss: 0.5430, Train Acc:0.8212\n",
      "Epoch [7/10], Step [583/600], Loss: 0.4859, Train Acc:0.8212\n",
      "Epoch [7/10], Step [584/600], Loss: 0.5656, Train Acc:0.8212\n",
      "Epoch [7/10], Step [585/600], Loss: 0.6232, Train Acc:0.8212\n",
      "Epoch [7/10], Step [586/600], Loss: 0.4231, Train Acc:0.8212\n",
      "Epoch [7/10], Step [587/600], Loss: 0.5273, Train Acc:0.8213\n",
      "Epoch [7/10], Step [588/600], Loss: 0.6643, Train Acc:0.8212\n",
      "Epoch [7/10], Step [589/600], Loss: 0.4904, Train Acc:0.8212\n",
      "Epoch [7/10], Step [590/600], Loss: 0.5035, Train Acc:0.8212\n",
      "Epoch [7/10], Step [591/600], Loss: 0.5750, Train Acc:0.8212\n",
      "Epoch [7/10], Step [592/600], Loss: 0.5559, Train Acc:0.8213\n",
      "Epoch [7/10], Step [593/600], Loss: 0.5469, Train Acc:0.8213\n",
      "Epoch [7/10], Step [594/600], Loss: 0.5189, Train Acc:0.8213\n",
      "Epoch [7/10], Step [595/600], Loss: 0.6085, Train Acc:0.8213\n",
      "Epoch [7/10], Step [596/600], Loss: 0.6049, Train Acc:0.8212\n",
      "Epoch [7/10], Step [597/600], Loss: 0.4168, Train Acc:0.8212\n",
      "Epoch [7/10], Step [598/600], Loss: 0.5775, Train Acc:0.8212\n",
      "Epoch [7/10], Step [599/600], Loss: 0.5486, Train Acc:0.8211\n",
      "Epoch [7/10], Step [600/600], Loss: 0.5964, Train Acc:0.8212\n",
      "Epoch [8/10], Step [1/600], Loss: 0.5444, Train Acc:0.8400\n",
      "Epoch [8/10], Step [2/600], Loss: 0.6210, Train Acc:0.8100\n",
      "Epoch [8/10], Step [3/600], Loss: 0.4835, Train Acc:0.8100\n",
      "Epoch [8/10], Step [4/600], Loss: 0.4896, Train Acc:0.8275\n",
      "Epoch [8/10], Step [5/600], Loss: 0.5203, Train Acc:0.8320\n",
      "Epoch [8/10], Step [6/600], Loss: 0.6249, Train Acc:0.8267\n",
      "Epoch [8/10], Step [7/600], Loss: 0.3864, Train Acc:0.8357\n",
      "Epoch [8/10], Step [8/600], Loss: 0.4408, Train Acc:0.8375\n",
      "Epoch [8/10], Step [9/600], Loss: 0.4901, Train Acc:0.8333\n",
      "Epoch [8/10], Step [10/600], Loss: 0.4480, Train Acc:0.8350\n",
      "Epoch [8/10], Step [11/600], Loss: 0.5458, Train Acc:0.8364\n",
      "Epoch [8/10], Step [12/600], Loss: 0.6318, Train Acc:0.8358\n",
      "Epoch [8/10], Step [13/600], Loss: 0.5258, Train Acc:0.8362\n",
      "Epoch [8/10], Step [14/600], Loss: 0.7423, Train Acc:0.8279\n",
      "Epoch [8/10], Step [15/600], Loss: 0.5023, Train Acc:0.8313\n",
      "Epoch [8/10], Step [16/600], Loss: 0.4798, Train Acc:0.8287\n",
      "Epoch [8/10], Step [17/600], Loss: 0.6411, Train Acc:0.8276\n",
      "Epoch [8/10], Step [18/600], Loss: 0.6280, Train Acc:0.8256\n",
      "Epoch [8/10], Step [19/600], Loss: 0.4540, Train Acc:0.8268\n",
      "Epoch [8/10], Step [20/600], Loss: 0.6054, Train Acc:0.8240\n",
      "Epoch [8/10], Step [21/600], Loss: 0.5773, Train Acc:0.8229\n",
      "Epoch [8/10], Step [22/600], Loss: 0.6604, Train Acc:0.8218\n",
      "Epoch [8/10], Step [23/600], Loss: 0.6017, Train Acc:0.8209\n",
      "Epoch [8/10], Step [24/600], Loss: 0.4958, Train Acc:0.8213\n",
      "Epoch [8/10], Step [25/600], Loss: 0.5286, Train Acc:0.8212\n",
      "Epoch [8/10], Step [26/600], Loss: 0.4676, Train Acc:0.8235\n",
      "Epoch [8/10], Step [27/600], Loss: 0.5031, Train Acc:0.8226\n",
      "Epoch [8/10], Step [28/600], Loss: 0.6467, Train Acc:0.8204\n",
      "Epoch [8/10], Step [29/600], Loss: 0.4816, Train Acc:0.8217\n",
      "Epoch [8/10], Step [30/600], Loss: 0.5653, Train Acc:0.8217\n",
      "Epoch [8/10], Step [31/600], Loss: 0.5850, Train Acc:0.8206\n",
      "Epoch [8/10], Step [32/600], Loss: 0.5475, Train Acc:0.8197\n",
      "Epoch [8/10], Step [33/600], Loss: 0.4455, Train Acc:0.8218\n",
      "Epoch [8/10], Step [34/600], Loss: 0.4978, Train Acc:0.8226\n",
      "Epoch [8/10], Step [35/600], Loss: 0.4767, Train Acc:0.8234\n",
      "Epoch [8/10], Step [36/600], Loss: 0.4907, Train Acc:0.8236\n",
      "Epoch [8/10], Step [37/600], Loss: 0.5813, Train Acc:0.8235\n",
      "Epoch [8/10], Step [38/600], Loss: 0.6642, Train Acc:0.8232\n",
      "Epoch [8/10], Step [39/600], Loss: 0.5566, Train Acc:0.8231\n",
      "Epoch [8/10], Step [40/600], Loss: 0.5670, Train Acc:0.8235\n",
      "Epoch [8/10], Step [41/600], Loss: 0.5344, Train Acc:0.8232\n",
      "Epoch [8/10], Step [42/600], Loss: 0.4689, Train Acc:0.8243\n",
      "Epoch [8/10], Step [43/600], Loss: 0.6931, Train Acc:0.8230\n",
      "Epoch [8/10], Step [44/600], Loss: 0.5079, Train Acc:0.8241\n",
      "Epoch [8/10], Step [45/600], Loss: 0.5551, Train Acc:0.8227\n",
      "Epoch [8/10], Step [46/600], Loss: 0.6363, Train Acc:0.8215\n",
      "Epoch [8/10], Step [47/600], Loss: 0.5990, Train Acc:0.8215\n",
      "Epoch [8/10], Step [48/600], Loss: 0.4878, Train Acc:0.8213\n",
      "Epoch [8/10], Step [49/600], Loss: 0.6092, Train Acc:0.8210\n",
      "Epoch [8/10], Step [50/600], Loss: 0.5697, Train Acc:0.8218\n",
      "Epoch [8/10], Step [51/600], Loss: 0.5343, Train Acc:0.8222\n",
      "Epoch [8/10], Step [52/600], Loss: 0.7161, Train Acc:0.8219\n",
      "Epoch [8/10], Step [53/600], Loss: 0.4477, Train Acc:0.8225\n",
      "Epoch [8/10], Step [54/600], Loss: 0.5468, Train Acc:0.8226\n",
      "Epoch [8/10], Step [55/600], Loss: 0.4455, Train Acc:0.8231\n",
      "Epoch [8/10], Step [56/600], Loss: 0.4154, Train Acc:0.8234\n",
      "Epoch [8/10], Step [57/600], Loss: 0.5563, Train Acc:0.8233\n",
      "Epoch [8/10], Step [58/600], Loss: 0.6276, Train Acc:0.8226\n",
      "Epoch [8/10], Step [59/600], Loss: 0.7670, Train Acc:0.8219\n",
      "Epoch [8/10], Step [60/600], Loss: 0.5812, Train Acc:0.8212\n",
      "Epoch [8/10], Step [61/600], Loss: 0.5227, Train Acc:0.8213\n",
      "Epoch [8/10], Step [62/600], Loss: 0.4582, Train Acc:0.8221\n",
      "Epoch [8/10], Step [63/600], Loss: 0.5887, Train Acc:0.8219\n",
      "Epoch [8/10], Step [64/600], Loss: 0.4932, Train Acc:0.8223\n",
      "Epoch [8/10], Step [65/600], Loss: 0.5857, Train Acc:0.8226\n",
      "Epoch [8/10], Step [66/600], Loss: 0.4567, Train Acc:0.8226\n",
      "Epoch [8/10], Step [67/600], Loss: 0.4906, Train Acc:0.8222\n",
      "Epoch [8/10], Step [68/600], Loss: 0.5416, Train Acc:0.8229\n",
      "Epoch [8/10], Step [69/600], Loss: 0.4300, Train Acc:0.8235\n",
      "Epoch [8/10], Step [70/600], Loss: 0.5404, Train Acc:0.8236\n",
      "Epoch [8/10], Step [71/600], Loss: 0.4873, Train Acc:0.8237\n",
      "Epoch [8/10], Step [72/600], Loss: 0.4904, Train Acc:0.8235\n",
      "Epoch [8/10], Step [73/600], Loss: 0.7144, Train Acc:0.8238\n",
      "Epoch [8/10], Step [74/600], Loss: 0.7076, Train Acc:0.8222\n",
      "Epoch [8/10], Step [75/600], Loss: 0.5365, Train Acc:0.8220\n",
      "Epoch [8/10], Step [76/600], Loss: 0.6668, Train Acc:0.8221\n",
      "Epoch [8/10], Step [77/600], Loss: 0.6277, Train Acc:0.8219\n",
      "Epoch [8/10], Step [78/600], Loss: 0.5807, Train Acc:0.8222\n",
      "Epoch [8/10], Step [79/600], Loss: 0.5429, Train Acc:0.8218\n",
      "Epoch [8/10], Step [80/600], Loss: 0.5091, Train Acc:0.8217\n",
      "Epoch [8/10], Step [81/600], Loss: 0.6573, Train Acc:0.8214\n",
      "Epoch [8/10], Step [82/600], Loss: 0.6600, Train Acc:0.8206\n",
      "Epoch [8/10], Step [83/600], Loss: 0.4999, Train Acc:0.8208\n",
      "Epoch [8/10], Step [84/600], Loss: 0.4599, Train Acc:0.8210\n",
      "Epoch [8/10], Step [85/600], Loss: 0.7933, Train Acc:0.8198\n",
      "Epoch [8/10], Step [86/600], Loss: 0.4835, Train Acc:0.8198\n",
      "Epoch [8/10], Step [87/600], Loss: 0.5121, Train Acc:0.8197\n",
      "Epoch [8/10], Step [88/600], Loss: 0.6283, Train Acc:0.8193\n",
      "Epoch [8/10], Step [89/600], Loss: 0.4976, Train Acc:0.8197\n",
      "Epoch [8/10], Step [90/600], Loss: 0.4719, Train Acc:0.8193\n",
      "Epoch [8/10], Step [91/600], Loss: 0.4661, Train Acc:0.8198\n",
      "Epoch [8/10], Step [92/600], Loss: 0.6290, Train Acc:0.8193\n",
      "Epoch [8/10], Step [93/600], Loss: 0.7358, Train Acc:0.8185\n",
      "Epoch [8/10], Step [94/600], Loss: 0.6103, Train Acc:0.8181\n",
      "Epoch [8/10], Step [95/600], Loss: 0.5651, Train Acc:0.8181\n",
      "Epoch [8/10], Step [96/600], Loss: 0.4208, Train Acc:0.8183\n",
      "Epoch [8/10], Step [97/600], Loss: 0.5804, Train Acc:0.8178\n",
      "Epoch [8/10], Step [98/600], Loss: 0.5934, Train Acc:0.8173\n",
      "Epoch [8/10], Step [99/600], Loss: 0.5345, Train Acc:0.8178\n",
      "Epoch [8/10], Step [100/600], Loss: 0.5904, Train Acc:0.8180\n",
      "Epoch [8/10], Step [101/600], Loss: 0.5496, Train Acc:0.8180\n",
      "Epoch [8/10], Step [102/600], Loss: 0.5544, Train Acc:0.8178\n",
      "Epoch [8/10], Step [103/600], Loss: 0.5751, Train Acc:0.8177\n",
      "Epoch [8/10], Step [104/600], Loss: 0.4849, Train Acc:0.8183\n",
      "Epoch [8/10], Step [105/600], Loss: 0.4558, Train Acc:0.8189\n",
      "Epoch [8/10], Step [106/600], Loss: 0.5107, Train Acc:0.8192\n",
      "Epoch [8/10], Step [107/600], Loss: 0.4872, Train Acc:0.8199\n",
      "Epoch [8/10], Step [108/600], Loss: 0.5331, Train Acc:0.8199\n",
      "Epoch [8/10], Step [109/600], Loss: 0.4814, Train Acc:0.8202\n",
      "Epoch [8/10], Step [110/600], Loss: 0.4813, Train Acc:0.8202\n",
      "Epoch [8/10], Step [111/600], Loss: 0.4948, Train Acc:0.8205\n",
      "Epoch [8/10], Step [112/600], Loss: 0.5942, Train Acc:0.8204\n",
      "Epoch [8/10], Step [113/600], Loss: 0.4766, Train Acc:0.8206\n",
      "Epoch [8/10], Step [114/600], Loss: 0.5191, Train Acc:0.8204\n",
      "Epoch [8/10], Step [115/600], Loss: 0.6065, Train Acc:0.8205\n",
      "Epoch [8/10], Step [116/600], Loss: 0.4845, Train Acc:0.8207\n",
      "Epoch [8/10], Step [117/600], Loss: 0.4984, Train Acc:0.8209\n",
      "Epoch [8/10], Step [118/600], Loss: 0.5708, Train Acc:0.8206\n",
      "Epoch [8/10], Step [119/600], Loss: 0.5852, Train Acc:0.8202\n",
      "Epoch [8/10], Step [120/600], Loss: 0.5299, Train Acc:0.8203\n",
      "Epoch [8/10], Step [121/600], Loss: 0.4639, Train Acc:0.8207\n",
      "Epoch [8/10], Step [122/600], Loss: 0.5469, Train Acc:0.8207\n",
      "Epoch [8/10], Step [123/600], Loss: 0.5318, Train Acc:0.8204\n",
      "Epoch [8/10], Step [124/600], Loss: 0.5679, Train Acc:0.8203\n",
      "Epoch [8/10], Step [125/600], Loss: 0.6984, Train Acc:0.8202\n",
      "Epoch [8/10], Step [126/600], Loss: 0.4381, Train Acc:0.8202\n",
      "Epoch [8/10], Step [127/600], Loss: 0.5484, Train Acc:0.8202\n",
      "Epoch [8/10], Step [128/600], Loss: 0.5134, Train Acc:0.8203\n",
      "Epoch [8/10], Step [129/600], Loss: 0.4908, Train Acc:0.8205\n",
      "Epoch [8/10], Step [130/600], Loss: 0.5976, Train Acc:0.8203\n",
      "Epoch [8/10], Step [131/600], Loss: 0.4968, Train Acc:0.8206\n",
      "Epoch [8/10], Step [132/600], Loss: 0.5228, Train Acc:0.8208\n",
      "Epoch [8/10], Step [133/600], Loss: 0.6612, Train Acc:0.8205\n",
      "Epoch [8/10], Step [134/600], Loss: 0.6852, Train Acc:0.8199\n",
      "Epoch [8/10], Step [135/600], Loss: 0.5991, Train Acc:0.8196\n",
      "Epoch [8/10], Step [136/600], Loss: 0.6256, Train Acc:0.8194\n",
      "Epoch [8/10], Step [137/600], Loss: 0.6145, Train Acc:0.8192\n",
      "Epoch [8/10], Step [138/600], Loss: 0.5778, Train Acc:0.8191\n",
      "Epoch [8/10], Step [139/600], Loss: 0.5554, Train Acc:0.8189\n",
      "Epoch [8/10], Step [140/600], Loss: 0.5877, Train Acc:0.8190\n",
      "Epoch [8/10], Step [141/600], Loss: 0.4869, Train Acc:0.8191\n",
      "Epoch [8/10], Step [142/600], Loss: 0.5366, Train Acc:0.8196\n",
      "Epoch [8/10], Step [143/600], Loss: 0.5190, Train Acc:0.8197\n",
      "Epoch [8/10], Step [144/600], Loss: 0.5173, Train Acc:0.8199\n",
      "Epoch [8/10], Step [145/600], Loss: 0.7563, Train Acc:0.8195\n",
      "Epoch [8/10], Step [146/600], Loss: 0.5653, Train Acc:0.8197\n",
      "Epoch [8/10], Step [147/600], Loss: 0.4183, Train Acc:0.8200\n",
      "Epoch [8/10], Step [148/600], Loss: 0.7185, Train Acc:0.8195\n",
      "Epoch [8/10], Step [149/600], Loss: 0.5760, Train Acc:0.8193\n",
      "Epoch [8/10], Step [150/600], Loss: 0.6022, Train Acc:0.8191\n",
      "Epoch [8/10], Step [151/600], Loss: 0.6525, Train Acc:0.8189\n",
      "Epoch [8/10], Step [152/600], Loss: 0.4886, Train Acc:0.8191\n",
      "Epoch [8/10], Step [153/600], Loss: 0.5404, Train Acc:0.8190\n",
      "Epoch [8/10], Step [154/600], Loss: 0.6357, Train Acc:0.8187\n",
      "Epoch [8/10], Step [155/600], Loss: 0.4981, Train Acc:0.8188\n",
      "Epoch [8/10], Step [156/600], Loss: 0.5110, Train Acc:0.8189\n",
      "Epoch [8/10], Step [157/600], Loss: 0.6082, Train Acc:0.8189\n",
      "Epoch [8/10], Step [158/600], Loss: 0.4729, Train Acc:0.8191\n",
      "Epoch [8/10], Step [159/600], Loss: 0.5586, Train Acc:0.8190\n",
      "Epoch [8/10], Step [160/600], Loss: 0.4601, Train Acc:0.8191\n",
      "Epoch [8/10], Step [161/600], Loss: 0.5084, Train Acc:0.8194\n",
      "Epoch [8/10], Step [162/600], Loss: 0.5771, Train Acc:0.8191\n",
      "Epoch [8/10], Step [163/600], Loss: 0.5041, Train Acc:0.8193\n",
      "Epoch [8/10], Step [164/600], Loss: 0.5232, Train Acc:0.8193\n",
      "Epoch [8/10], Step [165/600], Loss: 0.6058, Train Acc:0.8190\n",
      "Epoch [8/10], Step [166/600], Loss: 0.6131, Train Acc:0.8188\n",
      "Epoch [8/10], Step [167/600], Loss: 0.5083, Train Acc:0.8188\n",
      "Epoch [8/10], Step [168/600], Loss: 0.5670, Train Acc:0.8186\n",
      "Epoch [8/10], Step [169/600], Loss: 0.5568, Train Acc:0.8188\n",
      "Epoch [8/10], Step [170/600], Loss: 0.6020, Train Acc:0.8185\n",
      "Epoch [8/10], Step [171/600], Loss: 0.6648, Train Acc:0.8183\n",
      "Epoch [8/10], Step [172/600], Loss: 0.6523, Train Acc:0.8179\n",
      "Epoch [8/10], Step [173/600], Loss: 0.5646, Train Acc:0.8177\n",
      "Epoch [8/10], Step [174/600], Loss: 0.5534, Train Acc:0.8177\n",
      "Epoch [8/10], Step [175/600], Loss: 0.4825, Train Acc:0.8177\n",
      "Epoch [8/10], Step [176/600], Loss: 0.5139, Train Acc:0.8178\n",
      "Epoch [8/10], Step [177/600], Loss: 0.4723, Train Acc:0.8179\n",
      "Epoch [8/10], Step [178/600], Loss: 0.3971, Train Acc:0.8184\n",
      "Epoch [8/10], Step [179/600], Loss: 0.4924, Train Acc:0.8184\n",
      "Epoch [8/10], Step [180/600], Loss: 0.5724, Train Acc:0.8183\n",
      "Epoch [8/10], Step [181/600], Loss: 0.4985, Train Acc:0.8185\n",
      "Epoch [8/10], Step [182/600], Loss: 0.6978, Train Acc:0.8183\n",
      "Epoch [8/10], Step [183/600], Loss: 0.4647, Train Acc:0.8185\n",
      "Epoch [8/10], Step [184/600], Loss: 0.4618, Train Acc:0.8187\n",
      "Epoch [8/10], Step [185/600], Loss: 0.4069, Train Acc:0.8189\n",
      "Epoch [8/10], Step [186/600], Loss: 0.5908, Train Acc:0.8187\n",
      "Epoch [8/10], Step [187/600], Loss: 0.4509, Train Acc:0.8186\n",
      "Epoch [8/10], Step [188/600], Loss: 0.4312, Train Acc:0.8189\n",
      "Epoch [8/10], Step [189/600], Loss: 0.6273, Train Acc:0.8187\n",
      "Epoch [8/10], Step [190/600], Loss: 0.4516, Train Acc:0.8188\n",
      "Epoch [8/10], Step [191/600], Loss: 0.5805, Train Acc:0.8190\n",
      "Epoch [8/10], Step [192/600], Loss: 0.4904, Train Acc:0.8191\n",
      "Epoch [8/10], Step [193/600], Loss: 0.5935, Train Acc:0.8191\n",
      "Epoch [8/10], Step [194/600], Loss: 0.6450, Train Acc:0.8191\n",
      "Epoch [8/10], Step [195/600], Loss: 0.6776, Train Acc:0.8187\n",
      "Epoch [8/10], Step [196/600], Loss: 0.6048, Train Acc:0.8188\n",
      "Epoch [8/10], Step [197/600], Loss: 0.5396, Train Acc:0.8189\n",
      "Epoch [8/10], Step [198/600], Loss: 0.6363, Train Acc:0.8187\n",
      "Epoch [8/10], Step [199/600], Loss: 0.5419, Train Acc:0.8187\n",
      "Epoch [8/10], Step [200/600], Loss: 0.5537, Train Acc:0.8185\n",
      "Epoch [8/10], Step [201/600], Loss: 0.6024, Train Acc:0.8184\n",
      "Epoch [8/10], Step [202/600], Loss: 0.6262, Train Acc:0.8182\n",
      "Epoch [8/10], Step [203/600], Loss: 0.4752, Train Acc:0.8183\n",
      "Epoch [8/10], Step [204/600], Loss: 0.5032, Train Acc:0.8185\n",
      "Epoch [8/10], Step [205/600], Loss: 0.6265, Train Acc:0.8182\n",
      "Epoch [8/10], Step [206/600], Loss: 0.5733, Train Acc:0.8181\n",
      "Epoch [8/10], Step [207/600], Loss: 0.5174, Train Acc:0.8183\n",
      "Epoch [8/10], Step [208/600], Loss: 0.6669, Train Acc:0.8183\n",
      "Epoch [8/10], Step [209/600], Loss: 0.4749, Train Acc:0.8184\n",
      "Epoch [8/10], Step [210/600], Loss: 0.5053, Train Acc:0.8184\n",
      "Epoch [8/10], Step [211/600], Loss: 0.4928, Train Acc:0.8186\n",
      "Epoch [8/10], Step [212/600], Loss: 0.5039, Train Acc:0.8187\n",
      "Epoch [8/10], Step [213/600], Loss: 0.6091, Train Acc:0.8184\n",
      "Epoch [8/10], Step [214/600], Loss: 0.5306, Train Acc:0.8186\n",
      "Epoch [8/10], Step [215/600], Loss: 0.5311, Train Acc:0.8187\n",
      "Epoch [8/10], Step [216/600], Loss: 0.5426, Train Acc:0.8187\n",
      "Epoch [8/10], Step [217/600], Loss: 0.5955, Train Acc:0.8186\n",
      "Epoch [8/10], Step [218/600], Loss: 0.5953, Train Acc:0.8185\n",
      "Epoch [8/10], Step [219/600], Loss: 0.5461, Train Acc:0.8186\n",
      "Epoch [8/10], Step [220/600], Loss: 0.5336, Train Acc:0.8188\n",
      "Epoch [8/10], Step [221/600], Loss: 0.4730, Train Acc:0.8190\n",
      "Epoch [8/10], Step [222/600], Loss: 0.5394, Train Acc:0.8188\n",
      "Epoch [8/10], Step [223/600], Loss: 0.7011, Train Acc:0.8187\n",
      "Epoch [8/10], Step [224/600], Loss: 0.5338, Train Acc:0.8187\n",
      "Epoch [8/10], Step [225/600], Loss: 0.4718, Train Acc:0.8191\n",
      "Epoch [8/10], Step [226/600], Loss: 0.4820, Train Acc:0.8192\n",
      "Epoch [8/10], Step [227/600], Loss: 0.4610, Train Acc:0.8194\n",
      "Epoch [8/10], Step [228/600], Loss: 0.4874, Train Acc:0.8196\n",
      "Epoch [8/10], Step [229/600], Loss: 0.6869, Train Acc:0.8193\n",
      "Epoch [8/10], Step [230/600], Loss: 0.6472, Train Acc:0.8193\n",
      "Epoch [8/10], Step [231/600], Loss: 0.4958, Train Acc:0.8193\n",
      "Epoch [8/10], Step [232/600], Loss: 0.5192, Train Acc:0.8195\n",
      "Epoch [8/10], Step [233/600], Loss: 0.5625, Train Acc:0.8194\n",
      "Epoch [8/10], Step [234/600], Loss: 0.5182, Train Acc:0.8193\n",
      "Epoch [8/10], Step [235/600], Loss: 0.5436, Train Acc:0.8194\n",
      "Epoch [8/10], Step [236/600], Loss: 0.5179, Train Acc:0.8195\n",
      "Epoch [8/10], Step [237/600], Loss: 0.5322, Train Acc:0.8195\n",
      "Epoch [8/10], Step [238/600], Loss: 0.4807, Train Acc:0.8196\n",
      "Epoch [8/10], Step [239/600], Loss: 0.6199, Train Acc:0.8193\n",
      "Epoch [8/10], Step [240/600], Loss: 0.4587, Train Acc:0.8194\n",
      "Epoch [8/10], Step [241/600], Loss: 0.5547, Train Acc:0.8195\n",
      "Epoch [8/10], Step [242/600], Loss: 0.5391, Train Acc:0.8195\n",
      "Epoch [8/10], Step [243/600], Loss: 0.4568, Train Acc:0.8195\n",
      "Epoch [8/10], Step [244/600], Loss: 0.5774, Train Acc:0.8194\n",
      "Epoch [8/10], Step [245/600], Loss: 0.4050, Train Acc:0.8196\n",
      "Epoch [8/10], Step [246/600], Loss: 0.4478, Train Acc:0.8198\n",
      "Epoch [8/10], Step [247/600], Loss: 0.5633, Train Acc:0.8197\n",
      "Epoch [8/10], Step [248/600], Loss: 0.4932, Train Acc:0.8198\n",
      "Epoch [8/10], Step [249/600], Loss: 0.4972, Train Acc:0.8197\n",
      "Epoch [8/10], Step [250/600], Loss: 0.5046, Train Acc:0.8198\n",
      "Epoch [8/10], Step [251/600], Loss: 0.4740, Train Acc:0.8201\n",
      "Epoch [8/10], Step [252/600], Loss: 0.7371, Train Acc:0.8198\n",
      "Epoch [8/10], Step [253/600], Loss: 0.4815, Train Acc:0.8198\n",
      "Epoch [8/10], Step [254/600], Loss: 0.4146, Train Acc:0.8199\n",
      "Epoch [8/10], Step [255/600], Loss: 0.4735, Train Acc:0.8200\n",
      "Epoch [8/10], Step [256/600], Loss: 0.5746, Train Acc:0.8200\n",
      "Epoch [8/10], Step [257/600], Loss: 0.5619, Train Acc:0.8200\n",
      "Epoch [8/10], Step [258/600], Loss: 0.6850, Train Acc:0.8198\n",
      "Epoch [8/10], Step [259/600], Loss: 0.4378, Train Acc:0.8201\n",
      "Epoch [8/10], Step [260/600], Loss: 0.4917, Train Acc:0.8204\n",
      "Epoch [8/10], Step [261/600], Loss: 0.5249, Train Acc:0.8205\n",
      "Epoch [8/10], Step [262/600], Loss: 0.4462, Train Acc:0.8205\n",
      "Epoch [8/10], Step [263/600], Loss: 0.5000, Train Acc:0.8206\n",
      "Epoch [8/10], Step [264/600], Loss: 0.4500, Train Acc:0.8206\n",
      "Epoch [8/10], Step [265/600], Loss: 0.6581, Train Acc:0.8206\n",
      "Epoch [8/10], Step [266/600], Loss: 0.3775, Train Acc:0.8208\n",
      "Epoch [8/10], Step [267/600], Loss: 0.6042, Train Acc:0.8209\n",
      "Epoch [8/10], Step [268/600], Loss: 0.6273, Train Acc:0.8209\n",
      "Epoch [8/10], Step [269/600], Loss: 0.5055, Train Acc:0.8210\n",
      "Epoch [8/10], Step [270/600], Loss: 0.5019, Train Acc:0.8210\n",
      "Epoch [8/10], Step [271/600], Loss: 0.5849, Train Acc:0.8210\n",
      "Epoch [8/10], Step [272/600], Loss: 0.5300, Train Acc:0.8211\n",
      "Epoch [8/10], Step [273/600], Loss: 0.6482, Train Acc:0.8209\n",
      "Epoch [8/10], Step [274/600], Loss: 0.5331, Train Acc:0.8208\n",
      "Epoch [8/10], Step [275/600], Loss: 0.4451, Train Acc:0.8209\n",
      "Epoch [8/10], Step [276/600], Loss: 0.5484, Train Acc:0.8209\n",
      "Epoch [8/10], Step [277/600], Loss: 0.5444, Train Acc:0.8209\n",
      "Epoch [8/10], Step [278/600], Loss: 0.4660, Train Acc:0.8210\n",
      "Epoch [8/10], Step [279/600], Loss: 0.5172, Train Acc:0.8210\n",
      "Epoch [8/10], Step [280/600], Loss: 0.5312, Train Acc:0.8211\n",
      "Epoch [8/10], Step [281/600], Loss: 0.6327, Train Acc:0.8211\n",
      "Epoch [8/10], Step [282/600], Loss: 0.5864, Train Acc:0.8211\n",
      "Epoch [8/10], Step [283/600], Loss: 0.5440, Train Acc:0.8212\n",
      "Epoch [8/10], Step [284/600], Loss: 0.6550, Train Acc:0.8210\n",
      "Epoch [8/10], Step [285/600], Loss: 0.4249, Train Acc:0.8211\n",
      "Epoch [8/10], Step [286/600], Loss: 0.5712, Train Acc:0.8210\n",
      "Epoch [8/10], Step [287/600], Loss: 0.4723, Train Acc:0.8211\n",
      "Epoch [8/10], Step [288/600], Loss: 0.5990, Train Acc:0.8210\n",
      "Epoch [8/10], Step [289/600], Loss: 0.4824, Train Acc:0.8210\n",
      "Epoch [8/10], Step [290/600], Loss: 0.6418, Train Acc:0.8209\n",
      "Epoch [8/10], Step [291/600], Loss: 0.5241, Train Acc:0.8209\n",
      "Epoch [8/10], Step [292/600], Loss: 0.5992, Train Acc:0.8209\n",
      "Epoch [8/10], Step [293/600], Loss: 0.5159, Train Acc:0.8210\n",
      "Epoch [8/10], Step [294/600], Loss: 0.5648, Train Acc:0.8211\n",
      "Epoch [8/10], Step [295/600], Loss: 0.4774, Train Acc:0.8212\n",
      "Epoch [8/10], Step [296/600], Loss: 0.6104, Train Acc:0.8210\n",
      "Epoch [8/10], Step [297/600], Loss: 0.5245, Train Acc:0.8211\n",
      "Epoch [8/10], Step [298/600], Loss: 0.4808, Train Acc:0.8210\n",
      "Epoch [8/10], Step [299/600], Loss: 0.5504, Train Acc:0.8210\n",
      "Epoch [8/10], Step [300/600], Loss: 0.6687, Train Acc:0.8208\n",
      "Epoch [8/10], Step [301/600], Loss: 0.4774, Train Acc:0.8210\n",
      "Epoch [8/10], Step [302/600], Loss: 0.4762, Train Acc:0.8211\n",
      "Epoch [8/10], Step [303/600], Loss: 0.5962, Train Acc:0.8212\n",
      "Epoch [8/10], Step [304/600], Loss: 0.4801, Train Acc:0.8212\n",
      "Epoch [8/10], Step [305/600], Loss: 0.5276, Train Acc:0.8212\n",
      "Epoch [8/10], Step [306/600], Loss: 0.7286, Train Acc:0.8211\n",
      "Epoch [8/10], Step [307/600], Loss: 0.4705, Train Acc:0.8213\n",
      "Epoch [8/10], Step [308/600], Loss: 0.5887, Train Acc:0.8211\n",
      "Epoch [8/10], Step [309/600], Loss: 0.4835, Train Acc:0.8212\n",
      "Epoch [8/10], Step [310/600], Loss: 0.7032, Train Acc:0.8212\n",
      "Epoch [8/10], Step [311/600], Loss: 0.5684, Train Acc:0.8211\n",
      "Epoch [8/10], Step [312/600], Loss: 0.5659, Train Acc:0.8210\n",
      "Epoch [8/10], Step [313/600], Loss: 0.5772, Train Acc:0.8209\n",
      "Epoch [8/10], Step [314/600], Loss: 0.5419, Train Acc:0.8208\n",
      "Epoch [8/10], Step [315/600], Loss: 0.4863, Train Acc:0.8210\n",
      "Epoch [8/10], Step [316/600], Loss: 0.5624, Train Acc:0.8210\n",
      "Epoch [8/10], Step [317/600], Loss: 0.4624, Train Acc:0.8211\n",
      "Epoch [8/10], Step [318/600], Loss: 0.4179, Train Acc:0.8213\n",
      "Epoch [8/10], Step [319/600], Loss: 0.6158, Train Acc:0.8212\n",
      "Epoch [8/10], Step [320/600], Loss: 0.4657, Train Acc:0.8212\n",
      "Epoch [8/10], Step [321/600], Loss: 0.5333, Train Acc:0.8213\n",
      "Epoch [8/10], Step [322/600], Loss: 0.4876, Train Acc:0.8213\n",
      "Epoch [8/10], Step [323/600], Loss: 0.6045, Train Acc:0.8213\n",
      "Epoch [8/10], Step [324/600], Loss: 0.7163, Train Acc:0.8211\n",
      "Epoch [8/10], Step [325/600], Loss: 0.4285, Train Acc:0.8214\n",
      "Epoch [8/10], Step [326/600], Loss: 0.4701, Train Acc:0.8214\n",
      "Epoch [8/10], Step [327/600], Loss: 0.5503, Train Acc:0.8213\n",
      "Epoch [8/10], Step [328/600], Loss: 0.5315, Train Acc:0.8213\n",
      "Epoch [8/10], Step [329/600], Loss: 0.5988, Train Acc:0.8213\n",
      "Epoch [8/10], Step [330/600], Loss: 0.4678, Train Acc:0.8214\n",
      "Epoch [8/10], Step [331/600], Loss: 0.6178, Train Acc:0.8213\n",
      "Epoch [8/10], Step [332/600], Loss: 0.3909, Train Acc:0.8215\n",
      "Epoch [8/10], Step [333/600], Loss: 0.4972, Train Acc:0.8215\n",
      "Epoch [8/10], Step [334/600], Loss: 0.4574, Train Acc:0.8214\n",
      "Epoch [8/10], Step [335/600], Loss: 0.5360, Train Acc:0.8214\n",
      "Epoch [8/10], Step [336/600], Loss: 0.6194, Train Acc:0.8213\n",
      "Epoch [8/10], Step [337/600], Loss: 0.4491, Train Acc:0.8214\n",
      "Epoch [8/10], Step [338/600], Loss: 0.4081, Train Acc:0.8214\n",
      "Epoch [8/10], Step [339/600], Loss: 0.4274, Train Acc:0.8215\n",
      "Epoch [8/10], Step [340/600], Loss: 0.5463, Train Acc:0.8214\n",
      "Epoch [8/10], Step [341/600], Loss: 0.5690, Train Acc:0.8214\n",
      "Epoch [8/10], Step [342/600], Loss: 0.3656, Train Acc:0.8215\n",
      "Epoch [8/10], Step [343/600], Loss: 0.5058, Train Acc:0.8215\n",
      "Epoch [8/10], Step [344/600], Loss: 0.6588, Train Acc:0.8213\n",
      "Epoch [8/10], Step [345/600], Loss: 0.4338, Train Acc:0.8214\n",
      "Epoch [8/10], Step [346/600], Loss: 0.5781, Train Acc:0.8214\n",
      "Epoch [8/10], Step [347/600], Loss: 0.4649, Train Acc:0.8215\n",
      "Epoch [8/10], Step [348/600], Loss: 0.6707, Train Acc:0.8213\n",
      "Epoch [8/10], Step [349/600], Loss: 0.5101, Train Acc:0.8214\n",
      "Epoch [8/10], Step [350/600], Loss: 0.5622, Train Acc:0.8214\n",
      "Epoch [8/10], Step [351/600], Loss: 0.4717, Train Acc:0.8215\n",
      "Epoch [8/10], Step [352/600], Loss: 0.6123, Train Acc:0.8214\n",
      "Epoch [8/10], Step [353/600], Loss: 0.5736, Train Acc:0.8214\n",
      "Epoch [8/10], Step [354/600], Loss: 0.5745, Train Acc:0.8215\n",
      "Epoch [8/10], Step [355/600], Loss: 0.6271, Train Acc:0.8214\n",
      "Epoch [8/10], Step [356/600], Loss: 0.4363, Train Acc:0.8215\n",
      "Epoch [8/10], Step [357/600], Loss: 0.4810, Train Acc:0.8216\n",
      "Epoch [8/10], Step [358/600], Loss: 0.6857, Train Acc:0.8216\n",
      "Epoch [8/10], Step [359/600], Loss: 0.6026, Train Acc:0.8214\n",
      "Epoch [8/10], Step [360/600], Loss: 0.4818, Train Acc:0.8215\n",
      "Epoch [8/10], Step [361/600], Loss: 0.5168, Train Acc:0.8217\n",
      "Epoch [8/10], Step [362/600], Loss: 0.6070, Train Acc:0.8215\n",
      "Epoch [8/10], Step [363/600], Loss: 0.4238, Train Acc:0.8216\n",
      "Epoch [8/10], Step [364/600], Loss: 0.5170, Train Acc:0.8217\n",
      "Epoch [8/10], Step [365/600], Loss: 0.6155, Train Acc:0.8216\n",
      "Epoch [8/10], Step [366/600], Loss: 0.5279, Train Acc:0.8217\n",
      "Epoch [8/10], Step [367/600], Loss: 0.6335, Train Acc:0.8216\n",
      "Epoch [8/10], Step [368/600], Loss: 0.4875, Train Acc:0.8217\n",
      "Epoch [8/10], Step [369/600], Loss: 0.6279, Train Acc:0.8217\n",
      "Epoch [8/10], Step [370/600], Loss: 0.6339, Train Acc:0.8216\n",
      "Epoch [8/10], Step [371/600], Loss: 0.5013, Train Acc:0.8218\n",
      "Epoch [8/10], Step [372/600], Loss: 0.5183, Train Acc:0.8219\n",
      "Epoch [8/10], Step [373/600], Loss: 0.4244, Train Acc:0.8219\n",
      "Epoch [8/10], Step [374/600], Loss: 0.5637, Train Acc:0.8220\n",
      "Epoch [8/10], Step [375/600], Loss: 0.4605, Train Acc:0.8220\n",
      "Epoch [8/10], Step [376/600], Loss: 0.6082, Train Acc:0.8220\n",
      "Epoch [8/10], Step [377/600], Loss: 0.5127, Train Acc:0.8220\n",
      "Epoch [8/10], Step [378/600], Loss: 0.6702, Train Acc:0.8219\n",
      "Epoch [8/10], Step [379/600], Loss: 0.5652, Train Acc:0.8219\n",
      "Epoch [8/10], Step [380/600], Loss: 0.5038, Train Acc:0.8219\n",
      "Epoch [8/10], Step [381/600], Loss: 0.5277, Train Acc:0.8218\n",
      "Epoch [8/10], Step [382/600], Loss: 0.5886, Train Acc:0.8218\n",
      "Epoch [8/10], Step [383/600], Loss: 0.5649, Train Acc:0.8219\n",
      "Epoch [8/10], Step [384/600], Loss: 0.4935, Train Acc:0.8219\n",
      "Epoch [8/10], Step [385/600], Loss: 0.5044, Train Acc:0.8221\n",
      "Epoch [8/10], Step [386/600], Loss: 0.5277, Train Acc:0.8221\n",
      "Epoch [8/10], Step [387/600], Loss: 0.5923, Train Acc:0.8220\n",
      "Epoch [8/10], Step [388/600], Loss: 0.4660, Train Acc:0.8222\n",
      "Epoch [8/10], Step [389/600], Loss: 0.5200, Train Acc:0.8222\n",
      "Epoch [8/10], Step [390/600], Loss: 0.6286, Train Acc:0.8221\n",
      "Epoch [8/10], Step [391/600], Loss: 0.6394, Train Acc:0.8220\n",
      "Epoch [8/10], Step [392/600], Loss: 0.5584, Train Acc:0.8220\n",
      "Epoch [8/10], Step [393/600], Loss: 0.4623, Train Acc:0.8221\n",
      "Epoch [8/10], Step [394/600], Loss: 0.4781, Train Acc:0.8222\n",
      "Epoch [8/10], Step [395/600], Loss: 0.5210, Train Acc:0.8223\n",
      "Epoch [8/10], Step [396/600], Loss: 0.4370, Train Acc:0.8224\n",
      "Epoch [8/10], Step [397/600], Loss: 0.3946, Train Acc:0.8226\n",
      "Epoch [8/10], Step [398/600], Loss: 0.5899, Train Acc:0.8225\n",
      "Epoch [8/10], Step [399/600], Loss: 0.5900, Train Acc:0.8223\n",
      "Epoch [8/10], Step [400/600], Loss: 0.5448, Train Acc:0.8223\n",
      "Epoch [8/10], Step [401/600], Loss: 0.5699, Train Acc:0.8223\n",
      "Epoch [8/10], Step [402/600], Loss: 0.4480, Train Acc:0.8224\n",
      "Epoch [8/10], Step [403/600], Loss: 0.4294, Train Acc:0.8227\n",
      "Epoch [8/10], Step [404/600], Loss: 0.4957, Train Acc:0.8227\n",
      "Epoch [8/10], Step [405/600], Loss: 0.5727, Train Acc:0.8227\n",
      "Epoch [8/10], Step [406/600], Loss: 0.4844, Train Acc:0.8228\n",
      "Epoch [8/10], Step [407/600], Loss: 0.5876, Train Acc:0.8227\n",
      "Epoch [8/10], Step [408/600], Loss: 0.4580, Train Acc:0.8228\n",
      "Epoch [8/10], Step [409/600], Loss: 0.5731, Train Acc:0.8227\n",
      "Epoch [8/10], Step [410/600], Loss: 0.6979, Train Acc:0.8226\n",
      "Epoch [8/10], Step [411/600], Loss: 0.5082, Train Acc:0.8225\n",
      "Epoch [8/10], Step [412/600], Loss: 0.4677, Train Acc:0.8226\n",
      "Epoch [8/10], Step [413/600], Loss: 0.4829, Train Acc:0.8227\n",
      "Epoch [8/10], Step [414/600], Loss: 0.4620, Train Acc:0.8228\n",
      "Epoch [8/10], Step [415/600], Loss: 0.5260, Train Acc:0.8227\n",
      "Epoch [8/10], Step [416/600], Loss: 0.4658, Train Acc:0.8228\n",
      "Epoch [8/10], Step [417/600], Loss: 0.4839, Train Acc:0.8228\n",
      "Epoch [8/10], Step [418/600], Loss: 0.3726, Train Acc:0.8230\n",
      "Epoch [8/10], Step [419/600], Loss: 0.6745, Train Acc:0.8229\n",
      "Epoch [8/10], Step [420/600], Loss: 0.4547, Train Acc:0.8230\n",
      "Epoch [8/10], Step [421/600], Loss: 0.5185, Train Acc:0.8229\n",
      "Epoch [8/10], Step [422/600], Loss: 0.5427, Train Acc:0.8229\n",
      "Epoch [8/10], Step [423/600], Loss: 0.5666, Train Acc:0.8229\n",
      "Epoch [8/10], Step [424/600], Loss: 0.4079, Train Acc:0.8230\n",
      "Epoch [8/10], Step [425/600], Loss: 0.5358, Train Acc:0.8231\n",
      "Epoch [8/10], Step [426/600], Loss: 0.5651, Train Acc:0.8230\n",
      "Epoch [8/10], Step [427/600], Loss: 0.5280, Train Acc:0.8230\n",
      "Epoch [8/10], Step [428/600], Loss: 0.5708, Train Acc:0.8229\n",
      "Epoch [8/10], Step [429/600], Loss: 0.6482, Train Acc:0.8228\n",
      "Epoch [8/10], Step [430/600], Loss: 0.5273, Train Acc:0.8227\n",
      "Epoch [8/10], Step [431/600], Loss: 0.4467, Train Acc:0.8229\n",
      "Epoch [8/10], Step [432/600], Loss: 0.5897, Train Acc:0.8228\n",
      "Epoch [8/10], Step [433/600], Loss: 0.6222, Train Acc:0.8227\n",
      "Epoch [8/10], Step [434/600], Loss: 0.4829, Train Acc:0.8228\n",
      "Epoch [8/10], Step [435/600], Loss: 0.6380, Train Acc:0.8227\n",
      "Epoch [8/10], Step [436/600], Loss: 0.4965, Train Acc:0.8228\n",
      "Epoch [8/10], Step [437/600], Loss: 0.6327, Train Acc:0.8228\n",
      "Epoch [8/10], Step [438/600], Loss: 0.5547, Train Acc:0.8227\n",
      "Epoch [8/10], Step [439/600], Loss: 0.4642, Train Acc:0.8229\n",
      "Epoch [8/10], Step [440/600], Loss: 0.4463, Train Acc:0.8230\n",
      "Epoch [8/10], Step [441/600], Loss: 0.6218, Train Acc:0.8228\n",
      "Epoch [8/10], Step [442/600], Loss: 0.5615, Train Acc:0.8227\n",
      "Epoch [8/10], Step [443/600], Loss: 0.5119, Train Acc:0.8227\n",
      "Epoch [8/10], Step [444/600], Loss: 0.4760, Train Acc:0.8227\n",
      "Epoch [8/10], Step [445/600], Loss: 0.6443, Train Acc:0.8226\n",
      "Epoch [8/10], Step [446/600], Loss: 0.6245, Train Acc:0.8226\n",
      "Epoch [8/10], Step [447/600], Loss: 0.3843, Train Acc:0.8228\n",
      "Epoch [8/10], Step [448/600], Loss: 0.5677, Train Acc:0.8227\n",
      "Epoch [8/10], Step [449/600], Loss: 0.4193, Train Acc:0.8228\n",
      "Epoch [8/10], Step [450/600], Loss: 0.3996, Train Acc:0.8230\n",
      "Epoch [8/10], Step [451/600], Loss: 0.4123, Train Acc:0.8231\n",
      "Epoch [8/10], Step [452/600], Loss: 0.7064, Train Acc:0.8228\n",
      "Epoch [8/10], Step [453/600], Loss: 0.5270, Train Acc:0.8229\n",
      "Epoch [8/10], Step [454/600], Loss: 0.5789, Train Acc:0.8229\n",
      "Epoch [8/10], Step [455/600], Loss: 0.5534, Train Acc:0.8229\n",
      "Epoch [8/10], Step [456/600], Loss: 0.5038, Train Acc:0.8229\n",
      "Epoch [8/10], Step [457/600], Loss: 0.5555, Train Acc:0.8228\n",
      "Epoch [8/10], Step [458/600], Loss: 0.4636, Train Acc:0.8229\n",
      "Epoch [8/10], Step [459/600], Loss: 0.5158, Train Acc:0.8229\n",
      "Epoch [8/10], Step [460/600], Loss: 0.6263, Train Acc:0.8230\n",
      "Epoch [8/10], Step [461/600], Loss: 0.5983, Train Acc:0.8228\n",
      "Epoch [8/10], Step [462/600], Loss: 0.5080, Train Acc:0.8227\n",
      "Epoch [8/10], Step [463/600], Loss: 0.3830, Train Acc:0.8229\n",
      "Epoch [8/10], Step [464/600], Loss: 0.5272, Train Acc:0.8229\n",
      "Epoch [8/10], Step [465/600], Loss: 0.5127, Train Acc:0.8229\n",
      "Epoch [8/10], Step [466/600], Loss: 0.5789, Train Acc:0.8229\n",
      "Epoch [8/10], Step [467/600], Loss: 0.6026, Train Acc:0.8228\n",
      "Epoch [8/10], Step [468/600], Loss: 0.5113, Train Acc:0.8229\n",
      "Epoch [8/10], Step [469/600], Loss: 0.5063, Train Acc:0.8230\n",
      "Epoch [8/10], Step [470/600], Loss: 0.5495, Train Acc:0.8229\n",
      "Epoch [8/10], Step [471/600], Loss: 0.4807, Train Acc:0.8229\n",
      "Epoch [8/10], Step [472/600], Loss: 0.5474, Train Acc:0.8228\n",
      "Epoch [8/10], Step [473/600], Loss: 0.4788, Train Acc:0.8228\n",
      "Epoch [8/10], Step [474/600], Loss: 0.6103, Train Acc:0.8228\n",
      "Epoch [8/10], Step [475/600], Loss: 0.5880, Train Acc:0.8227\n",
      "Epoch [8/10], Step [476/600], Loss: 0.6492, Train Acc:0.8226\n",
      "Epoch [8/10], Step [477/600], Loss: 0.7086, Train Acc:0.8224\n",
      "Epoch [8/10], Step [478/600], Loss: 0.5737, Train Acc:0.8224\n",
      "Epoch [8/10], Step [479/600], Loss: 0.5920, Train Acc:0.8223\n",
      "Epoch [8/10], Step [480/600], Loss: 0.4470, Train Acc:0.8224\n",
      "Epoch [8/10], Step [481/600], Loss: 0.5526, Train Acc:0.8223\n",
      "Epoch [8/10], Step [482/600], Loss: 0.5642, Train Acc:0.8223\n",
      "Epoch [8/10], Step [483/600], Loss: 0.5112, Train Acc:0.8223\n",
      "Epoch [8/10], Step [484/600], Loss: 0.5401, Train Acc:0.8224\n",
      "Epoch [8/10], Step [485/600], Loss: 0.4151, Train Acc:0.8225\n",
      "Epoch [8/10], Step [486/600], Loss: 0.6318, Train Acc:0.8223\n",
      "Epoch [8/10], Step [487/600], Loss: 0.3904, Train Acc:0.8225\n",
      "Epoch [8/10], Step [488/600], Loss: 0.4459, Train Acc:0.8225\n",
      "Epoch [8/10], Step [489/600], Loss: 0.4230, Train Acc:0.8226\n",
      "Epoch [8/10], Step [490/600], Loss: 0.5515, Train Acc:0.8226\n",
      "Epoch [8/10], Step [491/600], Loss: 0.4699, Train Acc:0.8226\n",
      "Epoch [8/10], Step [492/600], Loss: 0.6347, Train Acc:0.8225\n",
      "Epoch [8/10], Step [493/600], Loss: 0.5134, Train Acc:0.8226\n",
      "Epoch [8/10], Step [494/600], Loss: 0.4773, Train Acc:0.8227\n",
      "Epoch [8/10], Step [495/600], Loss: 0.5955, Train Acc:0.8227\n",
      "Epoch [8/10], Step [496/600], Loss: 0.5615, Train Acc:0.8227\n",
      "Epoch [8/10], Step [497/600], Loss: 0.4555, Train Acc:0.8228\n",
      "Epoch [8/10], Step [498/600], Loss: 0.4153, Train Acc:0.8230\n",
      "Epoch [8/10], Step [499/600], Loss: 0.5022, Train Acc:0.8229\n",
      "Epoch [8/10], Step [500/600], Loss: 0.5659, Train Acc:0.8228\n",
      "Epoch [8/10], Step [501/600], Loss: 0.4513, Train Acc:0.8229\n",
      "Epoch [8/10], Step [502/600], Loss: 0.4897, Train Acc:0.8230\n",
      "Epoch [8/10], Step [503/600], Loss: 0.6010, Train Acc:0.8230\n",
      "Epoch [8/10], Step [504/600], Loss: 0.6598, Train Acc:0.8229\n",
      "Epoch [8/10], Step [505/600], Loss: 0.4812, Train Acc:0.8230\n",
      "Epoch [8/10], Step [506/600], Loss: 0.5105, Train Acc:0.8231\n",
      "Epoch [8/10], Step [507/600], Loss: 0.6079, Train Acc:0.8230\n",
      "Epoch [8/10], Step [508/600], Loss: 0.5116, Train Acc:0.8231\n",
      "Epoch [8/10], Step [509/600], Loss: 0.6719, Train Acc:0.8230\n",
      "Epoch [8/10], Step [510/600], Loss: 0.6312, Train Acc:0.8230\n",
      "Epoch [8/10], Step [511/600], Loss: 0.5894, Train Acc:0.8230\n",
      "Epoch [8/10], Step [512/600], Loss: 0.5265, Train Acc:0.8230\n",
      "Epoch [8/10], Step [513/600], Loss: 0.7143, Train Acc:0.8228\n",
      "Epoch [8/10], Step [514/600], Loss: 0.6045, Train Acc:0.8228\n",
      "Epoch [8/10], Step [515/600], Loss: 0.5299, Train Acc:0.8228\n",
      "Epoch [8/10], Step [516/600], Loss: 0.4341, Train Acc:0.8229\n",
      "Epoch [8/10], Step [517/600], Loss: 0.6400, Train Acc:0.8228\n",
      "Epoch [8/10], Step [518/600], Loss: 0.6063, Train Acc:0.8228\n",
      "Epoch [8/10], Step [519/600], Loss: 0.4123, Train Acc:0.8229\n",
      "Epoch [8/10], Step [520/600], Loss: 0.6719, Train Acc:0.8227\n",
      "Epoch [8/10], Step [521/600], Loss: 0.5712, Train Acc:0.8228\n",
      "Epoch [8/10], Step [522/600], Loss: 0.5714, Train Acc:0.8229\n",
      "Epoch [8/10], Step [523/600], Loss: 0.5321, Train Acc:0.8228\n",
      "Epoch [8/10], Step [524/600], Loss: 0.4022, Train Acc:0.8230\n",
      "Epoch [8/10], Step [525/600], Loss: 0.5603, Train Acc:0.8229\n",
      "Epoch [8/10], Step [526/600], Loss: 0.4249, Train Acc:0.8231\n",
      "Epoch [8/10], Step [527/600], Loss: 0.4278, Train Acc:0.8231\n",
      "Epoch [8/10], Step [528/600], Loss: 0.4850, Train Acc:0.8232\n",
      "Epoch [8/10], Step [529/600], Loss: 0.4883, Train Acc:0.8231\n",
      "Epoch [8/10], Step [530/600], Loss: 0.5834, Train Acc:0.8231\n",
      "Epoch [8/10], Step [531/600], Loss: 0.3692, Train Acc:0.8233\n",
      "Epoch [8/10], Step [532/600], Loss: 0.5139, Train Acc:0.8233\n",
      "Epoch [8/10], Step [533/600], Loss: 0.6003, Train Acc:0.8233\n",
      "Epoch [8/10], Step [534/600], Loss: 0.5646, Train Acc:0.8233\n",
      "Epoch [8/10], Step [535/600], Loss: 0.5699, Train Acc:0.8233\n",
      "Epoch [8/10], Step [536/600], Loss: 0.5308, Train Acc:0.8234\n",
      "Epoch [8/10], Step [537/600], Loss: 0.5916, Train Acc:0.8233\n",
      "Epoch [8/10], Step [538/600], Loss: 0.6018, Train Acc:0.8232\n",
      "Epoch [8/10], Step [539/600], Loss: 0.4207, Train Acc:0.8233\n",
      "Epoch [8/10], Step [540/600], Loss: 0.5645, Train Acc:0.8232\n",
      "Epoch [8/10], Step [541/600], Loss: 0.6325, Train Acc:0.8232\n",
      "Epoch [8/10], Step [542/600], Loss: 0.6097, Train Acc:0.8232\n",
      "Epoch [8/10], Step [543/600], Loss: 0.4762, Train Acc:0.8232\n",
      "Epoch [8/10], Step [544/600], Loss: 0.4002, Train Acc:0.8233\n",
      "Epoch [8/10], Step [545/600], Loss: 0.6067, Train Acc:0.8233\n",
      "Epoch [8/10], Step [546/600], Loss: 0.5895, Train Acc:0.8232\n",
      "Epoch [8/10], Step [547/600], Loss: 0.4883, Train Acc:0.8232\n",
      "Epoch [8/10], Step [548/600], Loss: 0.3958, Train Acc:0.8233\n",
      "Epoch [8/10], Step [549/600], Loss: 0.5852, Train Acc:0.8233\n",
      "Epoch [8/10], Step [550/600], Loss: 0.4999, Train Acc:0.8233\n",
      "Epoch [8/10], Step [551/600], Loss: 0.4846, Train Acc:0.8234\n",
      "Epoch [8/10], Step [552/600], Loss: 0.4787, Train Acc:0.8234\n",
      "Epoch [8/10], Step [553/600], Loss: 0.6331, Train Acc:0.8235\n",
      "Epoch [8/10], Step [554/600], Loss: 0.5420, Train Acc:0.8236\n",
      "Epoch [8/10], Step [555/600], Loss: 0.4871, Train Acc:0.8236\n",
      "Epoch [8/10], Step [556/600], Loss: 0.4780, Train Acc:0.8237\n",
      "Epoch [8/10], Step [557/600], Loss: 0.3933, Train Acc:0.8237\n",
      "Epoch [8/10], Step [558/600], Loss: 0.4314, Train Acc:0.8237\n",
      "Epoch [8/10], Step [559/600], Loss: 0.6285, Train Acc:0.8237\n",
      "Epoch [8/10], Step [560/600], Loss: 0.5524, Train Acc:0.8236\n",
      "Epoch [8/10], Step [561/600], Loss: 0.6397, Train Acc:0.8236\n",
      "Epoch [8/10], Step [562/600], Loss: 0.3903, Train Acc:0.8237\n",
      "Epoch [8/10], Step [563/600], Loss: 0.5232, Train Acc:0.8238\n",
      "Epoch [8/10], Step [564/600], Loss: 0.4541, Train Acc:0.8239\n",
      "Epoch [8/10], Step [565/600], Loss: 0.6189, Train Acc:0.8239\n",
      "Epoch [8/10], Step [566/600], Loss: 0.4811, Train Acc:0.8240\n",
      "Epoch [8/10], Step [567/600], Loss: 0.4586, Train Acc:0.8240\n",
      "Epoch [8/10], Step [568/600], Loss: 0.7328, Train Acc:0.8239\n",
      "Epoch [8/10], Step [569/600], Loss: 0.4620, Train Acc:0.8239\n",
      "Epoch [8/10], Step [570/600], Loss: 0.5873, Train Acc:0.8240\n",
      "Epoch [8/10], Step [571/600], Loss: 0.5553, Train Acc:0.8239\n",
      "Epoch [8/10], Step [572/600], Loss: 0.5260, Train Acc:0.8240\n",
      "Epoch [8/10], Step [573/600], Loss: 0.4800, Train Acc:0.8240\n",
      "Epoch [8/10], Step [574/600], Loss: 0.5430, Train Acc:0.8240\n",
      "Epoch [8/10], Step [575/600], Loss: 0.5013, Train Acc:0.8241\n",
      "Epoch [8/10], Step [576/600], Loss: 0.5660, Train Acc:0.8240\n",
      "Epoch [8/10], Step [577/600], Loss: 0.4561, Train Acc:0.8241\n",
      "Epoch [8/10], Step [578/600], Loss: 0.7126, Train Acc:0.8240\n",
      "Epoch [8/10], Step [579/600], Loss: 0.5410, Train Acc:0.8240\n",
      "Epoch [8/10], Step [580/600], Loss: 0.5104, Train Acc:0.8240\n",
      "Epoch [8/10], Step [581/600], Loss: 0.5122, Train Acc:0.8240\n",
      "Epoch [8/10], Step [582/600], Loss: 0.4797, Train Acc:0.8242\n",
      "Epoch [8/10], Step [583/600], Loss: 0.5112, Train Acc:0.8242\n",
      "Epoch [8/10], Step [584/600], Loss: 0.3995, Train Acc:0.8243\n",
      "Epoch [8/10], Step [585/600], Loss: 0.5297, Train Acc:0.8244\n",
      "Epoch [8/10], Step [586/600], Loss: 0.4920, Train Acc:0.8244\n",
      "Epoch [8/10], Step [587/600], Loss: 0.6046, Train Acc:0.8243\n",
      "Epoch [8/10], Step [588/600], Loss: 0.4931, Train Acc:0.8243\n",
      "Epoch [8/10], Step [589/600], Loss: 0.3735, Train Acc:0.8244\n",
      "Epoch [8/10], Step [590/600], Loss: 0.5631, Train Acc:0.8244\n",
      "Epoch [8/10], Step [591/600], Loss: 0.5447, Train Acc:0.8243\n",
      "Epoch [8/10], Step [592/600], Loss: 0.5899, Train Acc:0.8243\n",
      "Epoch [8/10], Step [593/600], Loss: 0.5318, Train Acc:0.8243\n",
      "Epoch [8/10], Step [594/600], Loss: 0.5750, Train Acc:0.8243\n",
      "Epoch [8/10], Step [595/600], Loss: 0.7004, Train Acc:0.8242\n",
      "Epoch [8/10], Step [596/600], Loss: 0.5754, Train Acc:0.8242\n",
      "Epoch [8/10], Step [597/600], Loss: 0.5414, Train Acc:0.8242\n",
      "Epoch [8/10], Step [598/600], Loss: 0.4825, Train Acc:0.8242\n",
      "Epoch [8/10], Step [599/600], Loss: 0.5798, Train Acc:0.8242\n",
      "Epoch [8/10], Step [600/600], Loss: 0.4205, Train Acc:0.8243\n",
      "Epoch [9/10], Step [1/600], Loss: 0.5241, Train Acc:0.7900\n",
      "Epoch [9/10], Step [2/600], Loss: 0.5260, Train Acc:0.8100\n",
      "Epoch [9/10], Step [3/600], Loss: 0.3728, Train Acc:0.8400\n",
      "Epoch [9/10], Step [4/600], Loss: 0.5843, Train Acc:0.8200\n",
      "Epoch [9/10], Step [5/600], Loss: 0.6785, Train Acc:0.8100\n",
      "Epoch [9/10], Step [6/600], Loss: 0.5931, Train Acc:0.8117\n",
      "Epoch [9/10], Step [7/600], Loss: 0.6715, Train Acc:0.8114\n",
      "Epoch [9/10], Step [8/600], Loss: 0.5427, Train Acc:0.8137\n",
      "Epoch [9/10], Step [9/600], Loss: 0.4874, Train Acc:0.8167\n",
      "Epoch [9/10], Step [10/600], Loss: 0.5743, Train Acc:0.8190\n",
      "Epoch [9/10], Step [11/600], Loss: 0.5369, Train Acc:0.8173\n",
      "Epoch [9/10], Step [12/600], Loss: 0.5159, Train Acc:0.8183\n",
      "Epoch [9/10], Step [13/600], Loss: 0.4835, Train Acc:0.8177\n",
      "Epoch [9/10], Step [14/600], Loss: 0.4298, Train Acc:0.8207\n",
      "Epoch [9/10], Step [15/600], Loss: 0.6010, Train Acc:0.8193\n",
      "Epoch [9/10], Step [16/600], Loss: 0.5215, Train Acc:0.8187\n",
      "Epoch [9/10], Step [17/600], Loss: 0.6546, Train Acc:0.8153\n",
      "Epoch [9/10], Step [18/600], Loss: 0.6125, Train Acc:0.8178\n",
      "Epoch [9/10], Step [19/600], Loss: 0.5675, Train Acc:0.8153\n",
      "Epoch [9/10], Step [20/600], Loss: 0.4668, Train Acc:0.8175\n",
      "Epoch [9/10], Step [21/600], Loss: 0.4810, Train Acc:0.8181\n",
      "Epoch [9/10], Step [22/600], Loss: 0.5890, Train Acc:0.8173\n",
      "Epoch [9/10], Step [23/600], Loss: 0.5198, Train Acc:0.8178\n",
      "Epoch [9/10], Step [24/600], Loss: 0.4106, Train Acc:0.8200\n",
      "Epoch [9/10], Step [25/600], Loss: 0.6244, Train Acc:0.8176\n",
      "Epoch [9/10], Step [26/600], Loss: 0.5428, Train Acc:0.8173\n",
      "Epoch [9/10], Step [27/600], Loss: 0.5207, Train Acc:0.8174\n",
      "Epoch [9/10], Step [28/600], Loss: 0.4695, Train Acc:0.8189\n",
      "Epoch [9/10], Step [29/600], Loss: 0.5901, Train Acc:0.8190\n",
      "Epoch [9/10], Step [30/600], Loss: 0.4812, Train Acc:0.8190\n",
      "Epoch [9/10], Step [31/600], Loss: 0.3963, Train Acc:0.8216\n",
      "Epoch [9/10], Step [32/600], Loss: 0.6932, Train Acc:0.8213\n",
      "Epoch [9/10], Step [33/600], Loss: 0.6833, Train Acc:0.8209\n",
      "Epoch [9/10], Step [34/600], Loss: 0.5011, Train Acc:0.8218\n",
      "Epoch [9/10], Step [35/600], Loss: 0.5393, Train Acc:0.8214\n",
      "Epoch [9/10], Step [36/600], Loss: 0.5913, Train Acc:0.8214\n",
      "Epoch [9/10], Step [37/600], Loss: 0.4915, Train Acc:0.8216\n",
      "Epoch [9/10], Step [38/600], Loss: 0.5157, Train Acc:0.8216\n",
      "Epoch [9/10], Step [39/600], Loss: 0.4048, Train Acc:0.8231\n",
      "Epoch [9/10], Step [40/600], Loss: 0.5619, Train Acc:0.8230\n",
      "Epoch [9/10], Step [41/600], Loss: 0.6639, Train Acc:0.8224\n",
      "Epoch [9/10], Step [42/600], Loss: 0.6066, Train Acc:0.8217\n",
      "Epoch [9/10], Step [43/600], Loss: 0.4821, Train Acc:0.8223\n",
      "Epoch [9/10], Step [44/600], Loss: 0.5947, Train Acc:0.8220\n",
      "Epoch [9/10], Step [45/600], Loss: 0.4485, Train Acc:0.8236\n",
      "Epoch [9/10], Step [46/600], Loss: 0.4813, Train Acc:0.8233\n",
      "Epoch [9/10], Step [47/600], Loss: 0.5466, Train Acc:0.8230\n",
      "Epoch [9/10], Step [48/600], Loss: 0.3893, Train Acc:0.8240\n",
      "Epoch [9/10], Step [49/600], Loss: 0.5409, Train Acc:0.8239\n",
      "Epoch [9/10], Step [50/600], Loss: 0.7683, Train Acc:0.8224\n",
      "Epoch [9/10], Step [51/600], Loss: 0.6040, Train Acc:0.8216\n",
      "Epoch [9/10], Step [52/600], Loss: 0.6042, Train Acc:0.8212\n",
      "Epoch [9/10], Step [53/600], Loss: 0.6367, Train Acc:0.8206\n",
      "Epoch [9/10], Step [54/600], Loss: 0.4592, Train Acc:0.8209\n",
      "Epoch [9/10], Step [55/600], Loss: 0.5100, Train Acc:0.8215\n",
      "Epoch [9/10], Step [56/600], Loss: 0.5226, Train Acc:0.8213\n",
      "Epoch [9/10], Step [57/600], Loss: 0.5627, Train Acc:0.8211\n",
      "Epoch [9/10], Step [58/600], Loss: 0.5715, Train Acc:0.8202\n",
      "Epoch [9/10], Step [59/600], Loss: 0.4868, Train Acc:0.8208\n",
      "Epoch [9/10], Step [60/600], Loss: 0.7365, Train Acc:0.8195\n",
      "Epoch [9/10], Step [61/600], Loss: 0.4718, Train Acc:0.8208\n",
      "Epoch [9/10], Step [62/600], Loss: 0.4741, Train Acc:0.8213\n",
      "Epoch [9/10], Step [63/600], Loss: 0.4720, Train Acc:0.8217\n",
      "Epoch [9/10], Step [64/600], Loss: 0.6831, Train Acc:0.8209\n",
      "Epoch [9/10], Step [65/600], Loss: 0.3999, Train Acc:0.8220\n",
      "Epoch [9/10], Step [66/600], Loss: 0.3934, Train Acc:0.8233\n",
      "Epoch [9/10], Step [67/600], Loss: 0.4627, Train Acc:0.8233\n",
      "Epoch [9/10], Step [68/600], Loss: 0.5713, Train Acc:0.8225\n",
      "Epoch [9/10], Step [69/600], Loss: 0.5311, Train Acc:0.8230\n",
      "Epoch [9/10], Step [70/600], Loss: 0.6992, Train Acc:0.8229\n",
      "Epoch [9/10], Step [71/600], Loss: 0.4453, Train Acc:0.8225\n",
      "Epoch [9/10], Step [72/600], Loss: 0.5981, Train Acc:0.8224\n",
      "Epoch [9/10], Step [73/600], Loss: 0.4370, Train Acc:0.8232\n",
      "Epoch [9/10], Step [74/600], Loss: 0.6136, Train Acc:0.8226\n",
      "Epoch [9/10], Step [75/600], Loss: 0.4829, Train Acc:0.8229\n",
      "Epoch [9/10], Step [76/600], Loss: 0.4744, Train Acc:0.8233\n",
      "Epoch [9/10], Step [77/600], Loss: 0.5329, Train Acc:0.8230\n",
      "Epoch [9/10], Step [78/600], Loss: 0.5362, Train Acc:0.8233\n",
      "Epoch [9/10], Step [79/600], Loss: 0.6103, Train Acc:0.8230\n",
      "Epoch [9/10], Step [80/600], Loss: 0.5355, Train Acc:0.8230\n",
      "Epoch [9/10], Step [81/600], Loss: 0.5442, Train Acc:0.8227\n",
      "Epoch [9/10], Step [82/600], Loss: 0.5232, Train Acc:0.8226\n",
      "Epoch [9/10], Step [83/600], Loss: 0.5032, Train Acc:0.8222\n",
      "Epoch [9/10], Step [84/600], Loss: 0.4587, Train Acc:0.8221\n",
      "Epoch [9/10], Step [85/600], Loss: 0.4239, Train Acc:0.8226\n",
      "Epoch [9/10], Step [86/600], Loss: 0.4962, Train Acc:0.8230\n",
      "Epoch [9/10], Step [87/600], Loss: 0.4594, Train Acc:0.8231\n",
      "Epoch [9/10], Step [88/600], Loss: 0.5276, Train Acc:0.8232\n",
      "Epoch [9/10], Step [89/600], Loss: 0.6355, Train Acc:0.8228\n",
      "Epoch [9/10], Step [90/600], Loss: 0.5042, Train Acc:0.8227\n",
      "Epoch [9/10], Step [91/600], Loss: 0.5864, Train Acc:0.8229\n",
      "Epoch [9/10], Step [92/600], Loss: 0.6048, Train Acc:0.8225\n",
      "Epoch [9/10], Step [93/600], Loss: 0.4968, Train Acc:0.8226\n",
      "Epoch [9/10], Step [94/600], Loss: 0.4802, Train Acc:0.8228\n",
      "Epoch [9/10], Step [95/600], Loss: 0.5481, Train Acc:0.8227\n",
      "Epoch [9/10], Step [96/600], Loss: 0.5525, Train Acc:0.8228\n",
      "Epoch [9/10], Step [97/600], Loss: 0.4893, Train Acc:0.8233\n",
      "Epoch [9/10], Step [98/600], Loss: 0.5873, Train Acc:0.8227\n",
      "Epoch [9/10], Step [99/600], Loss: 0.5045, Train Acc:0.8226\n",
      "Epoch [9/10], Step [100/600], Loss: 0.6144, Train Acc:0.8224\n",
      "Epoch [9/10], Step [101/600], Loss: 0.6434, Train Acc:0.8218\n",
      "Epoch [9/10], Step [102/600], Loss: 0.5207, Train Acc:0.8222\n",
      "Epoch [9/10], Step [103/600], Loss: 0.6372, Train Acc:0.8219\n",
      "Epoch [9/10], Step [104/600], Loss: 0.4686, Train Acc:0.8220\n",
      "Epoch [9/10], Step [105/600], Loss: 0.4835, Train Acc:0.8227\n",
      "Epoch [9/10], Step [106/600], Loss: 0.6082, Train Acc:0.8227\n",
      "Epoch [9/10], Step [107/600], Loss: 0.5230, Train Acc:0.8227\n",
      "Epoch [9/10], Step [108/600], Loss: 0.4881, Train Acc:0.8227\n",
      "Epoch [9/10], Step [109/600], Loss: 0.4792, Train Acc:0.8232\n",
      "Epoch [9/10], Step [110/600], Loss: 0.4034, Train Acc:0.8235\n",
      "Epoch [9/10], Step [111/600], Loss: 0.5283, Train Acc:0.8238\n",
      "Epoch [9/10], Step [112/600], Loss: 0.7764, Train Acc:0.8231\n",
      "Epoch [9/10], Step [113/600], Loss: 0.5887, Train Acc:0.8230\n",
      "Epoch [9/10], Step [114/600], Loss: 0.5941, Train Acc:0.8230\n",
      "Epoch [9/10], Step [115/600], Loss: 0.5524, Train Acc:0.8229\n",
      "Epoch [9/10], Step [116/600], Loss: 0.5851, Train Acc:0.8228\n",
      "Epoch [9/10], Step [117/600], Loss: 0.4468, Train Acc:0.8231\n",
      "Epoch [9/10], Step [118/600], Loss: 0.4233, Train Acc:0.8238\n",
      "Epoch [9/10], Step [119/600], Loss: 0.5581, Train Acc:0.8236\n",
      "Epoch [9/10], Step [120/600], Loss: 0.5951, Train Acc:0.8234\n",
      "Epoch [9/10], Step [121/600], Loss: 0.4723, Train Acc:0.8236\n",
      "Epoch [9/10], Step [122/600], Loss: 0.4319, Train Acc:0.8238\n",
      "Epoch [9/10], Step [123/600], Loss: 0.4868, Train Acc:0.8241\n",
      "Epoch [9/10], Step [124/600], Loss: 0.5777, Train Acc:0.8239\n",
      "Epoch [9/10], Step [125/600], Loss: 0.5362, Train Acc:0.8238\n",
      "Epoch [9/10], Step [126/600], Loss: 0.5016, Train Acc:0.8237\n",
      "Epoch [9/10], Step [127/600], Loss: 0.5961, Train Acc:0.8235\n",
      "Epoch [9/10], Step [128/600], Loss: 0.4555, Train Acc:0.8237\n",
      "Epoch [9/10], Step [129/600], Loss: 0.4914, Train Acc:0.8239\n",
      "Epoch [9/10], Step [130/600], Loss: 0.5603, Train Acc:0.8239\n",
      "Epoch [9/10], Step [131/600], Loss: 0.5939, Train Acc:0.8240\n",
      "Epoch [9/10], Step [132/600], Loss: 0.4655, Train Acc:0.8245\n",
      "Epoch [9/10], Step [133/600], Loss: 0.5410, Train Acc:0.8247\n",
      "Epoch [9/10], Step [134/600], Loss: 0.5044, Train Acc:0.8245\n",
      "Epoch [9/10], Step [135/600], Loss: 0.4913, Train Acc:0.8246\n",
      "Epoch [9/10], Step [136/600], Loss: 0.5535, Train Acc:0.8245\n",
      "Epoch [9/10], Step [137/600], Loss: 0.4619, Train Acc:0.8248\n",
      "Epoch [9/10], Step [138/600], Loss: 0.5249, Train Acc:0.8249\n",
      "Epoch [9/10], Step [139/600], Loss: 0.3560, Train Acc:0.8254\n",
      "Epoch [9/10], Step [140/600], Loss: 0.5657, Train Acc:0.8256\n",
      "Epoch [9/10], Step [141/600], Loss: 0.5343, Train Acc:0.8255\n",
      "Epoch [9/10], Step [142/600], Loss: 0.5922, Train Acc:0.8251\n",
      "Epoch [9/10], Step [143/600], Loss: 0.4694, Train Acc:0.8255\n",
      "Epoch [9/10], Step [144/600], Loss: 0.5250, Train Acc:0.8253\n",
      "Epoch [9/10], Step [145/600], Loss: 0.5780, Train Acc:0.8252\n",
      "Epoch [9/10], Step [146/600], Loss: 0.5995, Train Acc:0.8250\n",
      "Epoch [9/10], Step [147/600], Loss: 0.5145, Train Acc:0.8250\n",
      "Epoch [9/10], Step [148/600], Loss: 0.7131, Train Acc:0.8247\n",
      "Epoch [9/10], Step [149/600], Loss: 0.3922, Train Acc:0.8252\n",
      "Epoch [9/10], Step [150/600], Loss: 0.4585, Train Acc:0.8253\n",
      "Epoch [9/10], Step [151/600], Loss: 0.4512, Train Acc:0.8254\n",
      "Epoch [9/10], Step [152/600], Loss: 0.4983, Train Acc:0.8254\n",
      "Epoch [9/10], Step [153/600], Loss: 0.4861, Train Acc:0.8256\n",
      "Epoch [9/10], Step [154/600], Loss: 0.4759, Train Acc:0.8255\n",
      "Epoch [9/10], Step [155/600], Loss: 0.4513, Train Acc:0.8259\n",
      "Epoch [9/10], Step [156/600], Loss: 0.5670, Train Acc:0.8258\n",
      "Epoch [9/10], Step [157/600], Loss: 0.6292, Train Acc:0.8255\n",
      "Epoch [9/10], Step [158/600], Loss: 0.3984, Train Acc:0.8259\n",
      "Epoch [9/10], Step [159/600], Loss: 0.5516, Train Acc:0.8258\n",
      "Epoch [9/10], Step [160/600], Loss: 0.3954, Train Acc:0.8264\n",
      "Epoch [9/10], Step [161/600], Loss: 0.4428, Train Acc:0.8267\n",
      "Epoch [9/10], Step [162/600], Loss: 0.5747, Train Acc:0.8264\n",
      "Epoch [9/10], Step [163/600], Loss: 0.5542, Train Acc:0.8264\n",
      "Epoch [9/10], Step [164/600], Loss: 0.5571, Train Acc:0.8264\n",
      "Epoch [9/10], Step [165/600], Loss: 0.5074, Train Acc:0.8262\n",
      "Epoch [9/10], Step [166/600], Loss: 0.5123, Train Acc:0.8261\n",
      "Epoch [9/10], Step [167/600], Loss: 0.5573, Train Acc:0.8259\n",
      "Epoch [9/10], Step [168/600], Loss: 0.3797, Train Acc:0.8263\n",
      "Epoch [9/10], Step [169/600], Loss: 0.5153, Train Acc:0.8264\n",
      "Epoch [9/10], Step [170/600], Loss: 0.4434, Train Acc:0.8265\n",
      "Epoch [9/10], Step [171/600], Loss: 0.4424, Train Acc:0.8268\n",
      "Epoch [9/10], Step [172/600], Loss: 0.5265, Train Acc:0.8267\n",
      "Epoch [9/10], Step [173/600], Loss: 0.6193, Train Acc:0.8268\n",
      "Epoch [9/10], Step [174/600], Loss: 0.4839, Train Acc:0.8272\n",
      "Epoch [9/10], Step [175/600], Loss: 0.5470, Train Acc:0.8273\n",
      "Epoch [9/10], Step [176/600], Loss: 0.3994, Train Acc:0.8277\n",
      "Epoch [9/10], Step [177/600], Loss: 0.6307, Train Acc:0.8271\n",
      "Epoch [9/10], Step [178/600], Loss: 0.5152, Train Acc:0.8273\n",
      "Epoch [9/10], Step [179/600], Loss: 0.6984, Train Acc:0.8270\n",
      "Epoch [9/10], Step [180/600], Loss: 0.4267, Train Acc:0.8271\n",
      "Epoch [9/10], Step [181/600], Loss: 0.5367, Train Acc:0.8271\n",
      "Epoch [9/10], Step [182/600], Loss: 0.5752, Train Acc:0.8269\n",
      "Epoch [9/10], Step [183/600], Loss: 0.6833, Train Acc:0.8267\n",
      "Epoch [9/10], Step [184/600], Loss: 0.5447, Train Acc:0.8267\n",
      "Epoch [9/10], Step [185/600], Loss: 0.4579, Train Acc:0.8269\n",
      "Epoch [9/10], Step [186/600], Loss: 0.4538, Train Acc:0.8270\n",
      "Epoch [9/10], Step [187/600], Loss: 0.4850, Train Acc:0.8271\n",
      "Epoch [9/10], Step [188/600], Loss: 0.4222, Train Acc:0.8273\n",
      "Epoch [9/10], Step [189/600], Loss: 0.4703, Train Acc:0.8274\n",
      "Epoch [9/10], Step [190/600], Loss: 0.4949, Train Acc:0.8275\n",
      "Epoch [9/10], Step [191/600], Loss: 0.6022, Train Acc:0.8274\n",
      "Epoch [9/10], Step [192/600], Loss: 0.5768, Train Acc:0.8273\n",
      "Epoch [9/10], Step [193/600], Loss: 0.4958, Train Acc:0.8274\n",
      "Epoch [9/10], Step [194/600], Loss: 0.4425, Train Acc:0.8277\n",
      "Epoch [9/10], Step [195/600], Loss: 0.6489, Train Acc:0.8275\n",
      "Epoch [9/10], Step [196/600], Loss: 0.6273, Train Acc:0.8272\n",
      "Epoch [9/10], Step [197/600], Loss: 0.5171, Train Acc:0.8272\n",
      "Epoch [9/10], Step [198/600], Loss: 0.6176, Train Acc:0.8272\n",
      "Epoch [9/10], Step [199/600], Loss: 0.6309, Train Acc:0.8271\n",
      "Epoch [9/10], Step [200/600], Loss: 0.5215, Train Acc:0.8272\n",
      "Epoch [9/10], Step [201/600], Loss: 0.5369, Train Acc:0.8272\n",
      "Epoch [9/10], Step [202/600], Loss: 0.4520, Train Acc:0.8274\n",
      "Epoch [9/10], Step [203/600], Loss: 0.5134, Train Acc:0.8273\n",
      "Epoch [9/10], Step [204/600], Loss: 0.4459, Train Acc:0.8275\n",
      "Epoch [9/10], Step [205/600], Loss: 0.4319, Train Acc:0.8275\n",
      "Epoch [9/10], Step [206/600], Loss: 0.5260, Train Acc:0.8276\n",
      "Epoch [9/10], Step [207/600], Loss: 0.6696, Train Acc:0.8275\n",
      "Epoch [9/10], Step [208/600], Loss: 0.6248, Train Acc:0.8273\n",
      "Epoch [9/10], Step [209/600], Loss: 0.4066, Train Acc:0.8274\n",
      "Epoch [9/10], Step [210/600], Loss: 0.7192, Train Acc:0.8269\n",
      "Epoch [9/10], Step [211/600], Loss: 0.4648, Train Acc:0.8269\n",
      "Epoch [9/10], Step [212/600], Loss: 0.3886, Train Acc:0.8271\n",
      "Epoch [9/10], Step [213/600], Loss: 0.4905, Train Acc:0.8271\n",
      "Epoch [9/10], Step [214/600], Loss: 0.3843, Train Acc:0.8273\n",
      "Epoch [9/10], Step [215/600], Loss: 0.4286, Train Acc:0.8273\n",
      "Epoch [9/10], Step [216/600], Loss: 0.4866, Train Acc:0.8276\n",
      "Epoch [9/10], Step [217/600], Loss: 0.6732, Train Acc:0.8273\n",
      "Epoch [9/10], Step [218/600], Loss: 0.4919, Train Acc:0.8272\n",
      "Epoch [9/10], Step [219/600], Loss: 0.6365, Train Acc:0.8270\n",
      "Epoch [9/10], Step [220/600], Loss: 0.6292, Train Acc:0.8270\n",
      "Epoch [9/10], Step [221/600], Loss: 0.4877, Train Acc:0.8271\n",
      "Epoch [9/10], Step [222/600], Loss: 0.5827, Train Acc:0.8270\n",
      "Epoch [9/10], Step [223/600], Loss: 0.5607, Train Acc:0.8270\n",
      "Epoch [9/10], Step [224/600], Loss: 0.5038, Train Acc:0.8270\n",
      "Epoch [9/10], Step [225/600], Loss: 0.5801, Train Acc:0.8268\n",
      "Epoch [9/10], Step [226/600], Loss: 0.6036, Train Acc:0.8269\n",
      "Epoch [9/10], Step [227/600], Loss: 0.5402, Train Acc:0.8269\n",
      "Epoch [9/10], Step [228/600], Loss: 0.4190, Train Acc:0.8271\n",
      "Epoch [9/10], Step [229/600], Loss: 0.5605, Train Acc:0.8271\n",
      "Epoch [9/10], Step [230/600], Loss: 0.6034, Train Acc:0.8269\n",
      "Epoch [9/10], Step [231/600], Loss: 0.4655, Train Acc:0.8271\n",
      "Epoch [9/10], Step [232/600], Loss: 0.5298, Train Acc:0.8272\n",
      "Epoch [9/10], Step [233/600], Loss: 0.5161, Train Acc:0.8274\n",
      "Epoch [9/10], Step [234/600], Loss: 0.7200, Train Acc:0.8271\n",
      "Epoch [9/10], Step [235/600], Loss: 0.5136, Train Acc:0.8271\n",
      "Epoch [9/10], Step [236/600], Loss: 0.5254, Train Acc:0.8271\n",
      "Epoch [9/10], Step [237/600], Loss: 0.4799, Train Acc:0.8273\n",
      "Epoch [9/10], Step [238/600], Loss: 0.4886, Train Acc:0.8274\n",
      "Epoch [9/10], Step [239/600], Loss: 0.5889, Train Acc:0.8273\n",
      "Epoch [9/10], Step [240/600], Loss: 0.6437, Train Acc:0.8271\n",
      "Epoch [9/10], Step [241/600], Loss: 0.4246, Train Acc:0.8272\n",
      "Epoch [9/10], Step [242/600], Loss: 0.6314, Train Acc:0.8272\n",
      "Epoch [9/10], Step [243/600], Loss: 0.6548, Train Acc:0.8271\n",
      "Epoch [9/10], Step [244/600], Loss: 0.5633, Train Acc:0.8270\n",
      "Epoch [9/10], Step [245/600], Loss: 0.6026, Train Acc:0.8268\n",
      "Epoch [9/10], Step [246/600], Loss: 0.5060, Train Acc:0.8269\n",
      "Epoch [9/10], Step [247/600], Loss: 0.5196, Train Acc:0.8268\n",
      "Epoch [9/10], Step [248/600], Loss: 0.4933, Train Acc:0.8269\n",
      "Epoch [9/10], Step [249/600], Loss: 0.5280, Train Acc:0.8268\n",
      "Epoch [9/10], Step [250/600], Loss: 0.4541, Train Acc:0.8269\n",
      "Epoch [9/10], Step [251/600], Loss: 0.4357, Train Acc:0.8270\n",
      "Epoch [9/10], Step [252/600], Loss: 0.5588, Train Acc:0.8269\n",
      "Epoch [9/10], Step [253/600], Loss: 0.5602, Train Acc:0.8267\n",
      "Epoch [9/10], Step [254/600], Loss: 0.4682, Train Acc:0.8267\n",
      "Epoch [9/10], Step [255/600], Loss: 0.5313, Train Acc:0.8268\n",
      "Epoch [9/10], Step [256/600], Loss: 0.5040, Train Acc:0.8270\n",
      "Epoch [9/10], Step [257/600], Loss: 0.5221, Train Acc:0.8270\n",
      "Epoch [9/10], Step [258/600], Loss: 0.3942, Train Acc:0.8271\n",
      "Epoch [9/10], Step [259/600], Loss: 0.4670, Train Acc:0.8273\n",
      "Epoch [9/10], Step [260/600], Loss: 0.5114, Train Acc:0.8273\n",
      "Epoch [9/10], Step [261/600], Loss: 0.4817, Train Acc:0.8274\n",
      "Epoch [9/10], Step [262/600], Loss: 0.6375, Train Acc:0.8273\n",
      "Epoch [9/10], Step [263/600], Loss: 0.7073, Train Acc:0.8267\n",
      "Epoch [9/10], Step [264/600], Loss: 0.6338, Train Acc:0.8266\n",
      "Epoch [9/10], Step [265/600], Loss: 0.5191, Train Acc:0.8264\n",
      "Epoch [9/10], Step [266/600], Loss: 0.6196, Train Acc:0.8262\n",
      "Epoch [9/10], Step [267/600], Loss: 0.6032, Train Acc:0.8261\n",
      "Epoch [9/10], Step [268/600], Loss: 0.5112, Train Acc:0.8261\n",
      "Epoch [9/10], Step [269/600], Loss: 0.5881, Train Acc:0.8259\n",
      "Epoch [9/10], Step [270/600], Loss: 0.5730, Train Acc:0.8258\n",
      "Epoch [9/10], Step [271/600], Loss: 0.4728, Train Acc:0.8259\n",
      "Epoch [9/10], Step [272/600], Loss: 0.5312, Train Acc:0.8260\n",
      "Epoch [9/10], Step [273/600], Loss: 0.5123, Train Acc:0.8260\n",
      "Epoch [9/10], Step [274/600], Loss: 0.5458, Train Acc:0.8260\n",
      "Epoch [9/10], Step [275/600], Loss: 0.5983, Train Acc:0.8260\n",
      "Epoch [9/10], Step [276/600], Loss: 0.4794, Train Acc:0.8259\n",
      "Epoch [9/10], Step [277/600], Loss: 0.5142, Train Acc:0.8260\n",
      "Epoch [9/10], Step [278/600], Loss: 0.4965, Train Acc:0.8261\n",
      "Epoch [9/10], Step [279/600], Loss: 0.4983, Train Acc:0.8262\n",
      "Epoch [9/10], Step [280/600], Loss: 0.4785, Train Acc:0.8261\n",
      "Epoch [9/10], Step [281/600], Loss: 0.5883, Train Acc:0.8259\n",
      "Epoch [9/10], Step [282/600], Loss: 0.3862, Train Acc:0.8262\n",
      "Epoch [9/10], Step [283/600], Loss: 0.4964, Train Acc:0.8261\n",
      "Epoch [9/10], Step [284/600], Loss: 0.4950, Train Acc:0.8262\n",
      "Epoch [9/10], Step [285/600], Loss: 0.4813, Train Acc:0.8263\n",
      "Epoch [9/10], Step [286/600], Loss: 0.4989, Train Acc:0.8264\n",
      "Epoch [9/10], Step [287/600], Loss: 0.6234, Train Acc:0.8261\n",
      "Epoch [9/10], Step [288/600], Loss: 0.5682, Train Acc:0.8260\n",
      "Epoch [9/10], Step [289/600], Loss: 0.5483, Train Acc:0.8260\n",
      "Epoch [9/10], Step [290/600], Loss: 0.6193, Train Acc:0.8259\n",
      "Epoch [9/10], Step [291/600], Loss: 0.4651, Train Acc:0.8261\n",
      "Epoch [9/10], Step [292/600], Loss: 0.5286, Train Acc:0.8262\n",
      "Epoch [9/10], Step [293/600], Loss: 0.6147, Train Acc:0.8260\n",
      "Epoch [9/10], Step [294/600], Loss: 0.4798, Train Acc:0.8261\n",
      "Epoch [9/10], Step [295/600], Loss: 0.7614, Train Acc:0.8258\n",
      "Epoch [9/10], Step [296/600], Loss: 0.6472, Train Acc:0.8256\n",
      "Epoch [9/10], Step [297/600], Loss: 0.5873, Train Acc:0.8256\n",
      "Epoch [9/10], Step [298/600], Loss: 0.5313, Train Acc:0.8256\n",
      "Epoch [9/10], Step [299/600], Loss: 0.4502, Train Acc:0.8258\n",
      "Epoch [9/10], Step [300/600], Loss: 0.5115, Train Acc:0.8257\n",
      "Epoch [9/10], Step [301/600], Loss: 0.5853, Train Acc:0.8257\n",
      "Epoch [9/10], Step [302/600], Loss: 0.5012, Train Acc:0.8257\n",
      "Epoch [9/10], Step [303/600], Loss: 0.5605, Train Acc:0.8257\n",
      "Epoch [9/10], Step [304/600], Loss: 0.5139, Train Acc:0.8257\n",
      "Epoch [9/10], Step [305/600], Loss: 0.4855, Train Acc:0.8257\n",
      "Epoch [9/10], Step [306/600], Loss: 0.5292, Train Acc:0.8257\n",
      "Epoch [9/10], Step [307/600], Loss: 0.6566, Train Acc:0.8256\n",
      "Epoch [9/10], Step [308/600], Loss: 0.4611, Train Acc:0.8257\n",
      "Epoch [9/10], Step [309/600], Loss: 0.4764, Train Acc:0.8258\n",
      "Epoch [9/10], Step [310/600], Loss: 0.5664, Train Acc:0.8257\n",
      "Epoch [9/10], Step [311/600], Loss: 0.6096, Train Acc:0.8255\n",
      "Epoch [9/10], Step [312/600], Loss: 0.5360, Train Acc:0.8256\n",
      "Epoch [9/10], Step [313/600], Loss: 0.4910, Train Acc:0.8257\n",
      "Epoch [9/10], Step [314/600], Loss: 0.5396, Train Acc:0.8256\n",
      "Epoch [9/10], Step [315/600], Loss: 0.4378, Train Acc:0.8257\n",
      "Epoch [9/10], Step [316/600], Loss: 0.5729, Train Acc:0.8257\n",
      "Epoch [9/10], Step [317/600], Loss: 0.6492, Train Acc:0.8256\n",
      "Epoch [9/10], Step [318/600], Loss: 0.4918, Train Acc:0.8256\n",
      "Epoch [9/10], Step [319/600], Loss: 0.3989, Train Acc:0.8258\n",
      "Epoch [9/10], Step [320/600], Loss: 0.4195, Train Acc:0.8259\n",
      "Epoch [9/10], Step [321/600], Loss: 0.5068, Train Acc:0.8261\n",
      "Epoch [9/10], Step [322/600], Loss: 0.5397, Train Acc:0.8263\n",
      "Epoch [9/10], Step [323/600], Loss: 0.4415, Train Acc:0.8264\n",
      "Epoch [9/10], Step [324/600], Loss: 0.5394, Train Acc:0.8265\n",
      "Epoch [9/10], Step [325/600], Loss: 0.5445, Train Acc:0.8266\n",
      "Epoch [9/10], Step [326/600], Loss: 0.5206, Train Acc:0.8265\n",
      "Epoch [9/10], Step [327/600], Loss: 0.4866, Train Acc:0.8265\n",
      "Epoch [9/10], Step [328/600], Loss: 0.5864, Train Acc:0.8263\n",
      "Epoch [9/10], Step [329/600], Loss: 0.6740, Train Acc:0.8260\n",
      "Epoch [9/10], Step [330/600], Loss: 0.5109, Train Acc:0.8260\n",
      "Epoch [9/10], Step [331/600], Loss: 0.4969, Train Acc:0.8261\n",
      "Epoch [9/10], Step [332/600], Loss: 0.5549, Train Acc:0.8260\n",
      "Epoch [9/10], Step [333/600], Loss: 0.4176, Train Acc:0.8260\n",
      "Epoch [9/10], Step [334/600], Loss: 0.5938, Train Acc:0.8261\n",
      "Epoch [9/10], Step [335/600], Loss: 0.5619, Train Acc:0.8261\n",
      "Epoch [9/10], Step [336/600], Loss: 0.6293, Train Acc:0.8260\n",
      "Epoch [9/10], Step [337/600], Loss: 0.5119, Train Acc:0.8261\n",
      "Epoch [9/10], Step [338/600], Loss: 0.5666, Train Acc:0.8261\n",
      "Epoch [9/10], Step [339/600], Loss: 0.6058, Train Acc:0.8260\n",
      "Epoch [9/10], Step [340/600], Loss: 0.5741, Train Acc:0.8261\n",
      "Epoch [9/10], Step [341/600], Loss: 0.4882, Train Acc:0.8260\n",
      "Epoch [9/10], Step [342/600], Loss: 0.4792, Train Acc:0.8260\n",
      "Epoch [9/10], Step [343/600], Loss: 0.4668, Train Acc:0.8260\n",
      "Epoch [9/10], Step [344/600], Loss: 0.4379, Train Acc:0.8261\n",
      "Epoch [9/10], Step [345/600], Loss: 0.4642, Train Acc:0.8262\n",
      "Epoch [9/10], Step [346/600], Loss: 0.4579, Train Acc:0.8263\n",
      "Epoch [9/10], Step [347/600], Loss: 0.5512, Train Acc:0.8261\n",
      "Epoch [9/10], Step [348/600], Loss: 0.4204, Train Acc:0.8263\n",
      "Epoch [9/10], Step [349/600], Loss: 0.4736, Train Acc:0.8264\n",
      "Epoch [9/10], Step [350/600], Loss: 0.4934, Train Acc:0.8265\n",
      "Epoch [9/10], Step [351/600], Loss: 0.5286, Train Acc:0.8266\n",
      "Epoch [9/10], Step [352/600], Loss: 0.5128, Train Acc:0.8265\n",
      "Epoch [9/10], Step [353/600], Loss: 0.6207, Train Acc:0.8265\n",
      "Epoch [9/10], Step [354/600], Loss: 0.6140, Train Acc:0.8263\n",
      "Epoch [9/10], Step [355/600], Loss: 0.4718, Train Acc:0.8263\n",
      "Epoch [9/10], Step [356/600], Loss: 0.5688, Train Acc:0.8263\n",
      "Epoch [9/10], Step [357/600], Loss: 0.6055, Train Acc:0.8261\n",
      "Epoch [9/10], Step [358/600], Loss: 0.6880, Train Acc:0.8260\n",
      "Epoch [9/10], Step [359/600], Loss: 0.5265, Train Acc:0.8259\n",
      "Epoch [9/10], Step [360/600], Loss: 0.4444, Train Acc:0.8260\n",
      "Epoch [9/10], Step [361/600], Loss: 0.4705, Train Acc:0.8261\n",
      "Epoch [9/10], Step [362/600], Loss: 0.5844, Train Acc:0.8261\n",
      "Epoch [9/10], Step [363/600], Loss: 0.4753, Train Acc:0.8262\n",
      "Epoch [9/10], Step [364/600], Loss: 0.4865, Train Acc:0.8263\n",
      "Epoch [9/10], Step [365/600], Loss: 0.4905, Train Acc:0.8263\n",
      "Epoch [9/10], Step [366/600], Loss: 0.5134, Train Acc:0.8263\n",
      "Epoch [9/10], Step [367/600], Loss: 0.5308, Train Acc:0.8264\n",
      "Epoch [9/10], Step [368/600], Loss: 0.4885, Train Acc:0.8264\n",
      "Epoch [9/10], Step [369/600], Loss: 0.5298, Train Acc:0.8263\n",
      "Epoch [9/10], Step [370/600], Loss: 0.4372, Train Acc:0.8265\n",
      "Epoch [9/10], Step [371/600], Loss: 0.4485, Train Acc:0.8266\n",
      "Epoch [9/10], Step [372/600], Loss: 0.6727, Train Acc:0.8263\n",
      "Epoch [9/10], Step [373/600], Loss: 0.5994, Train Acc:0.8262\n",
      "Epoch [9/10], Step [374/600], Loss: 0.4007, Train Acc:0.8264\n",
      "Epoch [9/10], Step [375/600], Loss: 0.4374, Train Acc:0.8266\n",
      "Epoch [9/10], Step [376/600], Loss: 0.6348, Train Acc:0.8265\n",
      "Epoch [9/10], Step [377/600], Loss: 0.5890, Train Acc:0.8263\n",
      "Epoch [9/10], Step [378/600], Loss: 0.5595, Train Acc:0.8263\n",
      "Epoch [9/10], Step [379/600], Loss: 0.5702, Train Acc:0.8262\n",
      "Epoch [9/10], Step [380/600], Loss: 0.4076, Train Acc:0.8263\n",
      "Epoch [9/10], Step [381/600], Loss: 0.5130, Train Acc:0.8263\n",
      "Epoch [9/10], Step [382/600], Loss: 0.5991, Train Acc:0.8263\n",
      "Epoch [9/10], Step [383/600], Loss: 0.4249, Train Acc:0.8264\n",
      "Epoch [9/10], Step [384/600], Loss: 0.4294, Train Acc:0.8266\n",
      "Epoch [9/10], Step [385/600], Loss: 0.4587, Train Acc:0.8267\n",
      "Epoch [9/10], Step [386/600], Loss: 0.5452, Train Acc:0.8266\n",
      "Epoch [9/10], Step [387/600], Loss: 0.4106, Train Acc:0.8267\n",
      "Epoch [9/10], Step [388/600], Loss: 0.5642, Train Acc:0.8267\n",
      "Epoch [9/10], Step [389/600], Loss: 0.5227, Train Acc:0.8267\n",
      "Epoch [9/10], Step [390/600], Loss: 0.5258, Train Acc:0.8268\n",
      "Epoch [9/10], Step [391/600], Loss: 0.4911, Train Acc:0.8268\n",
      "Epoch [9/10], Step [392/600], Loss: 0.4402, Train Acc:0.8269\n",
      "Epoch [9/10], Step [393/600], Loss: 0.5723, Train Acc:0.8269\n",
      "Epoch [9/10], Step [394/600], Loss: 0.5433, Train Acc:0.8270\n",
      "Epoch [9/10], Step [395/600], Loss: 0.4112, Train Acc:0.8272\n",
      "Epoch [9/10], Step [396/600], Loss: 0.5903, Train Acc:0.8270\n",
      "Epoch [9/10], Step [397/600], Loss: 0.5586, Train Acc:0.8269\n",
      "Epoch [9/10], Step [398/600], Loss: 0.4751, Train Acc:0.8269\n",
      "Epoch [9/10], Step [399/600], Loss: 0.4617, Train Acc:0.8270\n",
      "Epoch [9/10], Step [400/600], Loss: 0.5891, Train Acc:0.8269\n",
      "Epoch [9/10], Step [401/600], Loss: 0.5922, Train Acc:0.8269\n",
      "Epoch [9/10], Step [402/600], Loss: 0.4283, Train Acc:0.8270\n",
      "Epoch [9/10], Step [403/600], Loss: 0.5060, Train Acc:0.8271\n",
      "Epoch [9/10], Step [404/600], Loss: 0.4237, Train Acc:0.8272\n",
      "Epoch [9/10], Step [405/600], Loss: 0.4713, Train Acc:0.8272\n",
      "Epoch [9/10], Step [406/600], Loss: 0.4631, Train Acc:0.8272\n",
      "Epoch [9/10], Step [407/600], Loss: 0.6438, Train Acc:0.8271\n",
      "Epoch [9/10], Step [408/600], Loss: 0.5069, Train Acc:0.8272\n",
      "Epoch [9/10], Step [409/600], Loss: 0.5944, Train Acc:0.8271\n",
      "Epoch [9/10], Step [410/600], Loss: 0.5620, Train Acc:0.8271\n",
      "Epoch [9/10], Step [411/600], Loss: 0.5832, Train Acc:0.8272\n",
      "Epoch [9/10], Step [412/600], Loss: 0.6360, Train Acc:0.8271\n",
      "Epoch [9/10], Step [413/600], Loss: 0.5872, Train Acc:0.8270\n",
      "Epoch [9/10], Step [414/600], Loss: 0.6502, Train Acc:0.8270\n",
      "Epoch [9/10], Step [415/600], Loss: 0.5918, Train Acc:0.8270\n",
      "Epoch [9/10], Step [416/600], Loss: 0.5270, Train Acc:0.8271\n",
      "Epoch [9/10], Step [417/600], Loss: 0.5124, Train Acc:0.8272\n",
      "Epoch [9/10], Step [418/600], Loss: 0.5399, Train Acc:0.8272\n",
      "Epoch [9/10], Step [419/600], Loss: 0.5541, Train Acc:0.8272\n",
      "Epoch [9/10], Step [420/600], Loss: 0.4891, Train Acc:0.8272\n",
      "Epoch [9/10], Step [421/600], Loss: 0.5832, Train Acc:0.8271\n",
      "Epoch [9/10], Step [422/600], Loss: 0.4287, Train Acc:0.8272\n",
      "Epoch [9/10], Step [423/600], Loss: 0.5474, Train Acc:0.8272\n",
      "Epoch [9/10], Step [424/600], Loss: 0.4725, Train Acc:0.8273\n",
      "Epoch [9/10], Step [425/600], Loss: 0.5070, Train Acc:0.8272\n",
      "Epoch [9/10], Step [426/600], Loss: 0.4279, Train Acc:0.8273\n",
      "Epoch [9/10], Step [427/600], Loss: 0.4573, Train Acc:0.8274\n",
      "Epoch [9/10], Step [428/600], Loss: 0.6137, Train Acc:0.8273\n",
      "Epoch [9/10], Step [429/600], Loss: 0.4647, Train Acc:0.8273\n",
      "Epoch [9/10], Step [430/600], Loss: 0.6367, Train Acc:0.8272\n",
      "Epoch [9/10], Step [431/600], Loss: 0.6361, Train Acc:0.8271\n",
      "Epoch [9/10], Step [432/600], Loss: 0.4013, Train Acc:0.8272\n",
      "Epoch [9/10], Step [433/600], Loss: 0.4566, Train Acc:0.8273\n",
      "Epoch [9/10], Step [434/600], Loss: 0.5985, Train Acc:0.8272\n",
      "Epoch [9/10], Step [435/600], Loss: 0.5410, Train Acc:0.8272\n",
      "Epoch [9/10], Step [436/600], Loss: 0.5272, Train Acc:0.8272\n",
      "Epoch [9/10], Step [437/600], Loss: 0.5155, Train Acc:0.8271\n",
      "Epoch [9/10], Step [438/600], Loss: 0.6436, Train Acc:0.8270\n",
      "Epoch [9/10], Step [439/600], Loss: 0.4953, Train Acc:0.8271\n",
      "Epoch [9/10], Step [440/600], Loss: 0.4164, Train Acc:0.8272\n",
      "Epoch [9/10], Step [441/600], Loss: 0.5813, Train Acc:0.8271\n",
      "Epoch [9/10], Step [442/600], Loss: 0.4268, Train Acc:0.8273\n",
      "Epoch [9/10], Step [443/600], Loss: 0.6204, Train Acc:0.8271\n",
      "Epoch [9/10], Step [444/600], Loss: 0.4866, Train Acc:0.8271\n",
      "Epoch [9/10], Step [445/600], Loss: 0.5348, Train Acc:0.8272\n",
      "Epoch [9/10], Step [446/600], Loss: 0.5322, Train Acc:0.8272\n",
      "Epoch [9/10], Step [447/600], Loss: 0.4877, Train Acc:0.8272\n",
      "Epoch [9/10], Step [448/600], Loss: 0.5046, Train Acc:0.8273\n",
      "Epoch [9/10], Step [449/600], Loss: 0.5802, Train Acc:0.8273\n",
      "Epoch [9/10], Step [450/600], Loss: 0.5853, Train Acc:0.8271\n",
      "Epoch [9/10], Step [451/600], Loss: 0.4500, Train Acc:0.8271\n",
      "Epoch [9/10], Step [452/600], Loss: 0.5716, Train Acc:0.8271\n",
      "Epoch [9/10], Step [453/600], Loss: 0.5192, Train Acc:0.8270\n",
      "Epoch [9/10], Step [454/600], Loss: 0.4720, Train Acc:0.8271\n",
      "Epoch [9/10], Step [455/600], Loss: 0.3958, Train Acc:0.8272\n",
      "Epoch [9/10], Step [456/600], Loss: 0.4372, Train Acc:0.8274\n",
      "Epoch [9/10], Step [457/600], Loss: 0.6510, Train Acc:0.8273\n",
      "Epoch [9/10], Step [458/600], Loss: 0.5239, Train Acc:0.8272\n",
      "Epoch [9/10], Step [459/600], Loss: 0.3961, Train Acc:0.8274\n",
      "Epoch [9/10], Step [460/600], Loss: 0.6797, Train Acc:0.8273\n",
      "Epoch [9/10], Step [461/600], Loss: 0.5051, Train Acc:0.8273\n",
      "Epoch [9/10], Step [462/600], Loss: 0.4566, Train Acc:0.8274\n",
      "Epoch [9/10], Step [463/600], Loss: 0.4766, Train Acc:0.8275\n",
      "Epoch [9/10], Step [464/600], Loss: 0.5529, Train Acc:0.8274\n",
      "Epoch [9/10], Step [465/600], Loss: 0.4270, Train Acc:0.8275\n",
      "Epoch [9/10], Step [466/600], Loss: 0.4431, Train Acc:0.8276\n",
      "Epoch [9/10], Step [467/600], Loss: 0.4282, Train Acc:0.8277\n",
      "Epoch [9/10], Step [468/600], Loss: 0.5619, Train Acc:0.8278\n",
      "Epoch [9/10], Step [469/600], Loss: 0.7366, Train Acc:0.8277\n",
      "Epoch [9/10], Step [470/600], Loss: 0.4900, Train Acc:0.8277\n",
      "Epoch [9/10], Step [471/600], Loss: 0.6143, Train Acc:0.8277\n",
      "Epoch [9/10], Step [472/600], Loss: 0.5817, Train Acc:0.8275\n",
      "Epoch [9/10], Step [473/600], Loss: 0.5255, Train Acc:0.8275\n",
      "Epoch [9/10], Step [474/600], Loss: 0.4665, Train Acc:0.8276\n",
      "Epoch [9/10], Step [475/600], Loss: 0.5472, Train Acc:0.8276\n",
      "Epoch [9/10], Step [476/600], Loss: 0.6034, Train Acc:0.8275\n",
      "Epoch [9/10], Step [477/600], Loss: 0.5236, Train Acc:0.8275\n",
      "Epoch [9/10], Step [478/600], Loss: 0.4697, Train Acc:0.8275\n",
      "Epoch [9/10], Step [479/600], Loss: 0.6889, Train Acc:0.8273\n",
      "Epoch [9/10], Step [480/600], Loss: 0.5589, Train Acc:0.8274\n",
      "Epoch [9/10], Step [481/600], Loss: 0.4500, Train Acc:0.8274\n",
      "Epoch [9/10], Step [482/600], Loss: 0.5700, Train Acc:0.8274\n",
      "Epoch [9/10], Step [483/600], Loss: 0.5094, Train Acc:0.8274\n",
      "Epoch [9/10], Step [484/600], Loss: 0.4567, Train Acc:0.8275\n",
      "Epoch [9/10], Step [485/600], Loss: 0.5459, Train Acc:0.8275\n",
      "Epoch [9/10], Step [486/600], Loss: 0.4730, Train Acc:0.8274\n",
      "Epoch [9/10], Step [487/600], Loss: 0.5017, Train Acc:0.8275\n",
      "Epoch [9/10], Step [488/600], Loss: 0.4204, Train Acc:0.8276\n",
      "Epoch [9/10], Step [489/600], Loss: 0.5868, Train Acc:0.8275\n",
      "Epoch [9/10], Step [490/600], Loss: 0.4403, Train Acc:0.8277\n",
      "Epoch [9/10], Step [491/600], Loss: 0.5001, Train Acc:0.8276\n",
      "Epoch [9/10], Step [492/600], Loss: 0.4738, Train Acc:0.8276\n",
      "Epoch [9/10], Step [493/600], Loss: 0.5941, Train Acc:0.8276\n",
      "Epoch [9/10], Step [494/600], Loss: 0.4577, Train Acc:0.8276\n",
      "Epoch [9/10], Step [495/600], Loss: 0.5672, Train Acc:0.8275\n",
      "Epoch [9/10], Step [496/600], Loss: 0.4416, Train Acc:0.8275\n",
      "Epoch [9/10], Step [497/600], Loss: 0.4168, Train Acc:0.8276\n",
      "Epoch [9/10], Step [498/600], Loss: 0.4628, Train Acc:0.8276\n",
      "Epoch [9/10], Step [499/600], Loss: 0.5531, Train Acc:0.8276\n",
      "Epoch [9/10], Step [500/600], Loss: 0.5735, Train Acc:0.8276\n",
      "Epoch [9/10], Step [501/600], Loss: 0.5357, Train Acc:0.8276\n",
      "Epoch [9/10], Step [502/600], Loss: 0.5642, Train Acc:0.8276\n",
      "Epoch [9/10], Step [503/600], Loss: 0.6005, Train Acc:0.8275\n",
      "Epoch [9/10], Step [504/600], Loss: 0.5516, Train Acc:0.8274\n",
      "Epoch [9/10], Step [505/600], Loss: 0.7172, Train Acc:0.8274\n",
      "Epoch [9/10], Step [506/600], Loss: 0.5775, Train Acc:0.8273\n",
      "Epoch [9/10], Step [507/600], Loss: 0.5259, Train Acc:0.8273\n",
      "Epoch [9/10], Step [508/600], Loss: 0.5387, Train Acc:0.8272\n",
      "Epoch [9/10], Step [509/600], Loss: 0.5117, Train Acc:0.8273\n",
      "Epoch [9/10], Step [510/600], Loss: 0.5116, Train Acc:0.8273\n",
      "Epoch [9/10], Step [511/600], Loss: 0.5942, Train Acc:0.8273\n",
      "Epoch [9/10], Step [512/600], Loss: 0.5301, Train Acc:0.8273\n",
      "Epoch [9/10], Step [513/600], Loss: 0.5485, Train Acc:0.8273\n",
      "Epoch [9/10], Step [514/600], Loss: 0.5272, Train Acc:0.8273\n",
      "Epoch [9/10], Step [515/600], Loss: 0.3995, Train Acc:0.8274\n",
      "Epoch [9/10], Step [516/600], Loss: 0.5894, Train Acc:0.8273\n",
      "Epoch [9/10], Step [517/600], Loss: 0.5174, Train Acc:0.8273\n",
      "Epoch [9/10], Step [518/600], Loss: 0.5148, Train Acc:0.8272\n",
      "Epoch [9/10], Step [519/600], Loss: 0.6236, Train Acc:0.8271\n",
      "Epoch [9/10], Step [520/600], Loss: 0.5658, Train Acc:0.8270\n",
      "Epoch [9/10], Step [521/600], Loss: 0.7139, Train Acc:0.8270\n",
      "Epoch [9/10], Step [522/600], Loss: 0.5301, Train Acc:0.8270\n",
      "Epoch [9/10], Step [523/600], Loss: 0.5487, Train Acc:0.8269\n",
      "Epoch [9/10], Step [524/600], Loss: 0.4226, Train Acc:0.8270\n",
      "Epoch [9/10], Step [525/600], Loss: 0.4337, Train Acc:0.8270\n",
      "Epoch [9/10], Step [526/600], Loss: 0.6762, Train Acc:0.8269\n",
      "Epoch [9/10], Step [527/600], Loss: 0.4244, Train Acc:0.8270\n",
      "Epoch [9/10], Step [528/600], Loss: 0.5275, Train Acc:0.8270\n",
      "Epoch [9/10], Step [529/600], Loss: 0.6502, Train Acc:0.8269\n",
      "Epoch [9/10], Step [530/600], Loss: 0.6221, Train Acc:0.8269\n",
      "Epoch [9/10], Step [531/600], Loss: 0.5361, Train Acc:0.8269\n",
      "Epoch [9/10], Step [532/600], Loss: 0.4518, Train Acc:0.8269\n",
      "Epoch [9/10], Step [533/600], Loss: 0.5235, Train Acc:0.8269\n",
      "Epoch [9/10], Step [534/600], Loss: 0.4870, Train Acc:0.8269\n",
      "Epoch [9/10], Step [535/600], Loss: 0.4216, Train Acc:0.8271\n",
      "Epoch [9/10], Step [536/600], Loss: 0.6621, Train Acc:0.8270\n",
      "Epoch [9/10], Step [537/600], Loss: 0.6110, Train Acc:0.8268\n",
      "Epoch [9/10], Step [538/600], Loss: 0.4211, Train Acc:0.8269\n",
      "Epoch [9/10], Step [539/600], Loss: 0.3819, Train Acc:0.8270\n",
      "Epoch [9/10], Step [540/600], Loss: 0.5328, Train Acc:0.8270\n",
      "Epoch [9/10], Step [541/600], Loss: 0.4722, Train Acc:0.8270\n",
      "Epoch [9/10], Step [542/600], Loss: 0.5297, Train Acc:0.8270\n",
      "Epoch [9/10], Step [543/600], Loss: 0.5155, Train Acc:0.8270\n",
      "Epoch [9/10], Step [544/600], Loss: 0.5504, Train Acc:0.8270\n",
      "Epoch [9/10], Step [545/600], Loss: 0.5239, Train Acc:0.8270\n",
      "Epoch [9/10], Step [546/600], Loss: 0.5239, Train Acc:0.8270\n",
      "Epoch [9/10], Step [547/600], Loss: 0.3967, Train Acc:0.8271\n",
      "Epoch [9/10], Step [548/600], Loss: 0.4447, Train Acc:0.8271\n",
      "Epoch [9/10], Step [549/600], Loss: 0.5116, Train Acc:0.8272\n",
      "Epoch [9/10], Step [550/600], Loss: 0.5882, Train Acc:0.8271\n",
      "Epoch [9/10], Step [551/600], Loss: 0.3736, Train Acc:0.8273\n",
      "Epoch [9/10], Step [552/600], Loss: 0.6366, Train Acc:0.8272\n",
      "Epoch [9/10], Step [553/600], Loss: 0.5993, Train Acc:0.8272\n",
      "Epoch [9/10], Step [554/600], Loss: 0.5572, Train Acc:0.8272\n",
      "Epoch [9/10], Step [555/600], Loss: 0.7909, Train Acc:0.8272\n",
      "Epoch [9/10], Step [556/600], Loss: 0.5087, Train Acc:0.8273\n",
      "Epoch [9/10], Step [557/600], Loss: 0.5099, Train Acc:0.8273\n",
      "Epoch [9/10], Step [558/600], Loss: 0.6321, Train Acc:0.8272\n",
      "Epoch [9/10], Step [559/600], Loss: 0.4896, Train Acc:0.8272\n",
      "Epoch [9/10], Step [560/600], Loss: 0.6828, Train Acc:0.8271\n",
      "Epoch [9/10], Step [561/600], Loss: 0.5384, Train Acc:0.8271\n",
      "Epoch [9/10], Step [562/600], Loss: 0.4938, Train Acc:0.8271\n",
      "Epoch [9/10], Step [563/600], Loss: 0.4978, Train Acc:0.8271\n",
      "Epoch [9/10], Step [564/600], Loss: 0.4735, Train Acc:0.8271\n",
      "Epoch [9/10], Step [565/600], Loss: 0.4734, Train Acc:0.8271\n",
      "Epoch [9/10], Step [566/600], Loss: 0.5043, Train Acc:0.8271\n",
      "Epoch [9/10], Step [567/600], Loss: 0.6031, Train Acc:0.8270\n",
      "Epoch [9/10], Step [568/600], Loss: 0.6022, Train Acc:0.8270\n",
      "Epoch [9/10], Step [569/600], Loss: 0.5753, Train Acc:0.8270\n",
      "Epoch [9/10], Step [570/600], Loss: 0.5122, Train Acc:0.8270\n",
      "Epoch [9/10], Step [571/600], Loss: 0.4990, Train Acc:0.8270\n",
      "Epoch [9/10], Step [572/600], Loss: 0.5929, Train Acc:0.8271\n",
      "Epoch [9/10], Step [573/600], Loss: 0.5117, Train Acc:0.8271\n",
      "Epoch [9/10], Step [574/600], Loss: 0.4300, Train Acc:0.8272\n",
      "Epoch [9/10], Step [575/600], Loss: 0.4724, Train Acc:0.8272\n",
      "Epoch [9/10], Step [576/600], Loss: 0.6157, Train Acc:0.8271\n",
      "Epoch [9/10], Step [577/600], Loss: 0.4539, Train Acc:0.8271\n",
      "Epoch [9/10], Step [578/600], Loss: 0.4772, Train Acc:0.8272\n",
      "Epoch [9/10], Step [579/600], Loss: 0.5343, Train Acc:0.8272\n",
      "Epoch [9/10], Step [580/600], Loss: 0.5582, Train Acc:0.8272\n",
      "Epoch [9/10], Step [581/600], Loss: 0.6180, Train Acc:0.8271\n",
      "Epoch [9/10], Step [582/600], Loss: 0.5428, Train Acc:0.8272\n",
      "Epoch [9/10], Step [583/600], Loss: 0.5889, Train Acc:0.8272\n",
      "Epoch [9/10], Step [584/600], Loss: 0.4719, Train Acc:0.8272\n",
      "Epoch [9/10], Step [585/600], Loss: 0.5739, Train Acc:0.8271\n",
      "Epoch [9/10], Step [586/600], Loss: 0.4761, Train Acc:0.8271\n",
      "Epoch [9/10], Step [587/600], Loss: 0.5845, Train Acc:0.8271\n",
      "Epoch [9/10], Step [588/600], Loss: 0.4772, Train Acc:0.8271\n",
      "Epoch [9/10], Step [589/600], Loss: 0.4816, Train Acc:0.8271\n",
      "Epoch [9/10], Step [590/600], Loss: 0.5933, Train Acc:0.8270\n",
      "Epoch [9/10], Step [591/600], Loss: 0.4921, Train Acc:0.8270\n",
      "Epoch [9/10], Step [592/600], Loss: 0.6664, Train Acc:0.8270\n",
      "Epoch [9/10], Step [593/600], Loss: 0.5073, Train Acc:0.8271\n",
      "Epoch [9/10], Step [594/600], Loss: 0.5001, Train Acc:0.8271\n",
      "Epoch [9/10], Step [595/600], Loss: 0.4968, Train Acc:0.8272\n",
      "Epoch [9/10], Step [596/600], Loss: 0.5547, Train Acc:0.8271\n",
      "Epoch [9/10], Step [597/600], Loss: 0.4907, Train Acc:0.8271\n",
      "Epoch [9/10], Step [598/600], Loss: 0.6078, Train Acc:0.8270\n",
      "Epoch [9/10], Step [599/600], Loss: 0.4918, Train Acc:0.8271\n",
      "Epoch [9/10], Step [600/600], Loss: 0.4871, Train Acc:0.8271\n",
      "Epoch [10/10], Step [1/600], Loss: 0.3605, Train Acc:0.9100\n",
      "Epoch [10/10], Step [2/600], Loss: 0.5371, Train Acc:0.8700\n",
      "Epoch [10/10], Step [3/600], Loss: 0.5375, Train Acc:0.8467\n",
      "Epoch [10/10], Step [4/600], Loss: 0.4839, Train Acc:0.8450\n",
      "Epoch [10/10], Step [5/600], Loss: 0.5378, Train Acc:0.8460\n",
      "Epoch [10/10], Step [6/600], Loss: 0.3958, Train Acc:0.8500\n",
      "Epoch [10/10], Step [7/600], Loss: 0.5664, Train Acc:0.8443\n",
      "Epoch [10/10], Step [8/600], Loss: 0.5288, Train Acc:0.8450\n",
      "Epoch [10/10], Step [9/600], Loss: 0.5004, Train Acc:0.8456\n",
      "Epoch [10/10], Step [10/600], Loss: 0.3648, Train Acc:0.8490\n",
      "Epoch [10/10], Step [11/600], Loss: 0.5505, Train Acc:0.8464\n",
      "Epoch [10/10], Step [12/600], Loss: 0.4992, Train Acc:0.8433\n",
      "Epoch [10/10], Step [13/600], Loss: 0.5857, Train Acc:0.8446\n",
      "Epoch [10/10], Step [14/600], Loss: 0.3857, Train Acc:0.8479\n",
      "Epoch [10/10], Step [15/600], Loss: 0.6083, Train Acc:0.8460\n",
      "Epoch [10/10], Step [16/600], Loss: 0.5098, Train Acc:0.8462\n",
      "Epoch [10/10], Step [17/600], Loss: 0.6117, Train Acc:0.8406\n",
      "Epoch [10/10], Step [18/600], Loss: 0.5614, Train Acc:0.8367\n",
      "Epoch [10/10], Step [19/600], Loss: 0.4220, Train Acc:0.8363\n",
      "Epoch [10/10], Step [20/600], Loss: 0.4808, Train Acc:0.8360\n",
      "Epoch [10/10], Step [21/600], Loss: 0.4657, Train Acc:0.8357\n",
      "Epoch [10/10], Step [22/600], Loss: 0.5104, Train Acc:0.8341\n",
      "Epoch [10/10], Step [23/600], Loss: 0.4615, Train Acc:0.8330\n",
      "Epoch [10/10], Step [24/600], Loss: 0.5890, Train Acc:0.8325\n",
      "Epoch [10/10], Step [25/600], Loss: 0.4436, Train Acc:0.8320\n",
      "Epoch [10/10], Step [26/600], Loss: 0.5380, Train Acc:0.8315\n",
      "Epoch [10/10], Step [27/600], Loss: 0.4663, Train Acc:0.8330\n",
      "Epoch [10/10], Step [28/600], Loss: 0.5451, Train Acc:0.8343\n",
      "Epoch [10/10], Step [29/600], Loss: 0.5522, Train Acc:0.8341\n",
      "Epoch [10/10], Step [30/600], Loss: 0.6671, Train Acc:0.8330\n",
      "Epoch [10/10], Step [31/600], Loss: 0.4876, Train Acc:0.8319\n",
      "Epoch [10/10], Step [32/600], Loss: 0.4302, Train Acc:0.8322\n",
      "Epoch [10/10], Step [33/600], Loss: 0.5079, Train Acc:0.8327\n",
      "Epoch [10/10], Step [34/600], Loss: 0.5256, Train Acc:0.8321\n",
      "Epoch [10/10], Step [35/600], Loss: 0.5482, Train Acc:0.8320\n",
      "Epoch [10/10], Step [36/600], Loss: 0.5385, Train Acc:0.8317\n",
      "Epoch [10/10], Step [37/600], Loss: 0.6628, Train Acc:0.8308\n",
      "Epoch [10/10], Step [38/600], Loss: 0.5461, Train Acc:0.8303\n",
      "Epoch [10/10], Step [39/600], Loss: 0.4877, Train Acc:0.8305\n",
      "Epoch [10/10], Step [40/600], Loss: 0.5757, Train Acc:0.8305\n",
      "Epoch [10/10], Step [41/600], Loss: 0.5238, Train Acc:0.8310\n",
      "Epoch [10/10], Step [42/600], Loss: 0.5696, Train Acc:0.8314\n",
      "Epoch [10/10], Step [43/600], Loss: 0.7698, Train Acc:0.8291\n",
      "Epoch [10/10], Step [44/600], Loss: 0.5772, Train Acc:0.8286\n",
      "Epoch [10/10], Step [45/600], Loss: 0.4575, Train Acc:0.8287\n",
      "Epoch [10/10], Step [46/600], Loss: 0.5765, Train Acc:0.8283\n",
      "Epoch [10/10], Step [47/600], Loss: 0.4974, Train Acc:0.8289\n",
      "Epoch [10/10], Step [48/600], Loss: 0.5308, Train Acc:0.8290\n",
      "Epoch [10/10], Step [49/600], Loss: 0.5792, Train Acc:0.8286\n",
      "Epoch [10/10], Step [50/600], Loss: 0.7147, Train Acc:0.8270\n",
      "Epoch [10/10], Step [51/600], Loss: 0.5486, Train Acc:0.8265\n",
      "Epoch [10/10], Step [52/600], Loss: 0.4534, Train Acc:0.8271\n",
      "Epoch [10/10], Step [53/600], Loss: 0.5712, Train Acc:0.8272\n",
      "Epoch [10/10], Step [54/600], Loss: 0.4761, Train Acc:0.8276\n",
      "Epoch [10/10], Step [55/600], Loss: 0.3731, Train Acc:0.8291\n",
      "Epoch [10/10], Step [56/600], Loss: 0.4813, Train Acc:0.8287\n",
      "Epoch [10/10], Step [57/600], Loss: 0.7245, Train Acc:0.8277\n",
      "Epoch [10/10], Step [58/600], Loss: 0.5020, Train Acc:0.8271\n",
      "Epoch [10/10], Step [59/600], Loss: 0.4853, Train Acc:0.8269\n",
      "Epoch [10/10], Step [60/600], Loss: 0.5750, Train Acc:0.8268\n",
      "Epoch [10/10], Step [61/600], Loss: 0.4842, Train Acc:0.8279\n",
      "Epoch [10/10], Step [62/600], Loss: 0.4291, Train Acc:0.8282\n",
      "Epoch [10/10], Step [63/600], Loss: 0.4450, Train Acc:0.8290\n",
      "Epoch [10/10], Step [64/600], Loss: 0.4600, Train Acc:0.8294\n",
      "Epoch [10/10], Step [65/600], Loss: 0.5405, Train Acc:0.8295\n",
      "Epoch [10/10], Step [66/600], Loss: 0.3925, Train Acc:0.8303\n",
      "Epoch [10/10], Step [67/600], Loss: 0.5155, Train Acc:0.8307\n",
      "Epoch [10/10], Step [68/600], Loss: 0.4561, Train Acc:0.8306\n",
      "Epoch [10/10], Step [69/600], Loss: 0.3685, Train Acc:0.8316\n",
      "Epoch [10/10], Step [70/600], Loss: 0.5564, Train Acc:0.8313\n",
      "Epoch [10/10], Step [71/600], Loss: 0.4729, Train Acc:0.8315\n",
      "Epoch [10/10], Step [72/600], Loss: 0.5200, Train Acc:0.8314\n",
      "Epoch [10/10], Step [73/600], Loss: 0.4646, Train Acc:0.8319\n",
      "Epoch [10/10], Step [74/600], Loss: 0.4444, Train Acc:0.8323\n",
      "Epoch [10/10], Step [75/600], Loss: 0.6125, Train Acc:0.8319\n",
      "Epoch [10/10], Step [76/600], Loss: 0.4688, Train Acc:0.8326\n",
      "Epoch [10/10], Step [77/600], Loss: 0.5903, Train Acc:0.8323\n",
      "Epoch [10/10], Step [78/600], Loss: 0.4998, Train Acc:0.8326\n",
      "Epoch [10/10], Step [79/600], Loss: 0.4928, Train Acc:0.8324\n",
      "Epoch [10/10], Step [80/600], Loss: 0.5219, Train Acc:0.8317\n",
      "Epoch [10/10], Step [81/600], Loss: 0.6209, Train Acc:0.8314\n",
      "Epoch [10/10], Step [82/600], Loss: 0.3808, Train Acc:0.8324\n",
      "Epoch [10/10], Step [83/600], Loss: 0.5931, Train Acc:0.8316\n",
      "Epoch [10/10], Step [84/600], Loss: 0.5471, Train Acc:0.8311\n",
      "Epoch [10/10], Step [85/600], Loss: 0.5767, Train Acc:0.8313\n",
      "Epoch [10/10], Step [86/600], Loss: 0.5856, Train Acc:0.8303\n",
      "Epoch [10/10], Step [87/600], Loss: 0.5899, Train Acc:0.8297\n",
      "Epoch [10/10], Step [88/600], Loss: 0.5290, Train Acc:0.8299\n",
      "Epoch [10/10], Step [89/600], Loss: 0.4952, Train Acc:0.8302\n",
      "Epoch [10/10], Step [90/600], Loss: 0.6372, Train Acc:0.8296\n",
      "Epoch [10/10], Step [91/600], Loss: 0.5263, Train Acc:0.8292\n",
      "Epoch [10/10], Step [92/600], Loss: 0.4725, Train Acc:0.8291\n",
      "Epoch [10/10], Step [93/600], Loss: 0.3960, Train Acc:0.8296\n",
      "Epoch [10/10], Step [94/600], Loss: 0.4129, Train Acc:0.8301\n",
      "Epoch [10/10], Step [95/600], Loss: 0.5419, Train Acc:0.8296\n",
      "Epoch [10/10], Step [96/600], Loss: 0.4309, Train Acc:0.8303\n",
      "Epoch [10/10], Step [97/600], Loss: 0.3688, Train Acc:0.8312\n",
      "Epoch [10/10], Step [98/600], Loss: 0.5066, Train Acc:0.8308\n",
      "Epoch [10/10], Step [99/600], Loss: 0.3314, Train Acc:0.8318\n",
      "Epoch [10/10], Step [100/600], Loss: 0.4462, Train Acc:0.8320\n",
      "Epoch [10/10], Step [101/600], Loss: 0.5647, Train Acc:0.8316\n",
      "Epoch [10/10], Step [102/600], Loss: 0.4827, Train Acc:0.8321\n",
      "Epoch [10/10], Step [103/600], Loss: 0.4525, Train Acc:0.8323\n",
      "Epoch [10/10], Step [104/600], Loss: 0.5555, Train Acc:0.8323\n",
      "Epoch [10/10], Step [105/600], Loss: 0.4097, Train Acc:0.8330\n",
      "Epoch [10/10], Step [106/600], Loss: 0.4493, Train Acc:0.8330\n",
      "Epoch [10/10], Step [107/600], Loss: 0.5446, Train Acc:0.8327\n",
      "Epoch [10/10], Step [108/600], Loss: 0.5184, Train Acc:0.8327\n",
      "Epoch [10/10], Step [109/600], Loss: 0.4074, Train Acc:0.8331\n",
      "Epoch [10/10], Step [110/600], Loss: 0.5434, Train Acc:0.8332\n",
      "Epoch [10/10], Step [111/600], Loss: 0.5234, Train Acc:0.8329\n",
      "Epoch [10/10], Step [112/600], Loss: 0.4606, Train Acc:0.8330\n",
      "Epoch [10/10], Step [113/600], Loss: 0.5110, Train Acc:0.8327\n",
      "Epoch [10/10], Step [114/600], Loss: 0.5291, Train Acc:0.8323\n",
      "Epoch [10/10], Step [115/600], Loss: 0.5058, Train Acc:0.8322\n",
      "Epoch [10/10], Step [116/600], Loss: 0.5549, Train Acc:0.8322\n",
      "Epoch [10/10], Step [117/600], Loss: 0.5006, Train Acc:0.8323\n",
      "Epoch [10/10], Step [118/600], Loss: 0.4976, Train Acc:0.8321\n",
      "Epoch [10/10], Step [119/600], Loss: 0.4719, Train Acc:0.8324\n",
      "Epoch [10/10], Step [120/600], Loss: 0.6520, Train Acc:0.8314\n",
      "Epoch [10/10], Step [121/600], Loss: 0.4134, Train Acc:0.8315\n",
      "Epoch [10/10], Step [122/600], Loss: 0.6083, Train Acc:0.8311\n",
      "Epoch [10/10], Step [123/600], Loss: 0.4916, Train Acc:0.8313\n",
      "Epoch [10/10], Step [124/600], Loss: 0.4696, Train Acc:0.8315\n",
      "Epoch [10/10], Step [125/600], Loss: 0.5116, Train Acc:0.8314\n",
      "Epoch [10/10], Step [126/600], Loss: 0.5545, Train Acc:0.8315\n",
      "Epoch [10/10], Step [127/600], Loss: 0.5760, Train Acc:0.8318\n",
      "Epoch [10/10], Step [128/600], Loss: 0.4374, Train Acc:0.8320\n",
      "Epoch [10/10], Step [129/600], Loss: 0.5283, Train Acc:0.8317\n",
      "Epoch [10/10], Step [130/600], Loss: 0.6200, Train Acc:0.8317\n",
      "Epoch [10/10], Step [131/600], Loss: 0.5384, Train Acc:0.8316\n",
      "Epoch [10/10], Step [132/600], Loss: 0.4905, Train Acc:0.8318\n",
      "Epoch [10/10], Step [133/600], Loss: 0.4734, Train Acc:0.8321\n",
      "Epoch [10/10], Step [134/600], Loss: 0.5407, Train Acc:0.8321\n",
      "Epoch [10/10], Step [135/600], Loss: 0.5070, Train Acc:0.8322\n",
      "Epoch [10/10], Step [136/600], Loss: 0.4224, Train Acc:0.8326\n",
      "Epoch [10/10], Step [137/600], Loss: 0.5238, Train Acc:0.8327\n",
      "Epoch [10/10], Step [138/600], Loss: 0.4657, Train Acc:0.8326\n",
      "Epoch [10/10], Step [139/600], Loss: 0.5534, Train Acc:0.8323\n",
      "Epoch [10/10], Step [140/600], Loss: 0.4263, Train Acc:0.8326\n",
      "Epoch [10/10], Step [141/600], Loss: 0.5856, Train Acc:0.8322\n",
      "Epoch [10/10], Step [142/600], Loss: 0.4132, Train Acc:0.8327\n",
      "Epoch [10/10], Step [143/600], Loss: 0.4437, Train Acc:0.8331\n",
      "Epoch [10/10], Step [144/600], Loss: 0.5451, Train Acc:0.8328\n",
      "Epoch [10/10], Step [145/600], Loss: 0.5393, Train Acc:0.8331\n",
      "Epoch [10/10], Step [146/600], Loss: 0.6071, Train Acc:0.8327\n",
      "Epoch [10/10], Step [147/600], Loss: 0.4843, Train Acc:0.8329\n",
      "Epoch [10/10], Step [148/600], Loss: 0.7616, Train Acc:0.8325\n",
      "Epoch [10/10], Step [149/600], Loss: 0.4571, Train Acc:0.8325\n",
      "Epoch [10/10], Step [150/600], Loss: 0.3839, Train Acc:0.8328\n",
      "Epoch [10/10], Step [151/600], Loss: 0.5324, Train Acc:0.8329\n",
      "Epoch [10/10], Step [152/600], Loss: 0.7331, Train Acc:0.8327\n",
      "Epoch [10/10], Step [153/600], Loss: 0.5035, Train Acc:0.8327\n",
      "Epoch [10/10], Step [154/600], Loss: 0.5381, Train Acc:0.8328\n",
      "Epoch [10/10], Step [155/600], Loss: 0.6139, Train Acc:0.8323\n",
      "Epoch [10/10], Step [156/600], Loss: 0.4447, Train Acc:0.8326\n",
      "Epoch [10/10], Step [157/600], Loss: 0.5586, Train Acc:0.8323\n",
      "Epoch [10/10], Step [158/600], Loss: 0.5609, Train Acc:0.8323\n",
      "Epoch [10/10], Step [159/600], Loss: 0.5577, Train Acc:0.8324\n",
      "Epoch [10/10], Step [160/600], Loss: 0.4638, Train Acc:0.8321\n",
      "Epoch [10/10], Step [161/600], Loss: 0.5280, Train Acc:0.8322\n",
      "Epoch [10/10], Step [162/600], Loss: 0.5602, Train Acc:0.8318\n",
      "Epoch [10/10], Step [163/600], Loss: 0.5701, Train Acc:0.8318\n",
      "Epoch [10/10], Step [164/600], Loss: 0.4923, Train Acc:0.8320\n",
      "Epoch [10/10], Step [165/600], Loss: 0.4969, Train Acc:0.8321\n",
      "Epoch [10/10], Step [166/600], Loss: 0.5557, Train Acc:0.8320\n",
      "Epoch [10/10], Step [167/600], Loss: 0.5394, Train Acc:0.8320\n",
      "Epoch [10/10], Step [168/600], Loss: 0.5443, Train Acc:0.8320\n",
      "Epoch [10/10], Step [169/600], Loss: 0.4879, Train Acc:0.8320\n",
      "Epoch [10/10], Step [170/600], Loss: 0.4688, Train Acc:0.8320\n",
      "Epoch [10/10], Step [171/600], Loss: 0.6591, Train Acc:0.8315\n",
      "Epoch [10/10], Step [172/600], Loss: 0.4203, Train Acc:0.8319\n",
      "Epoch [10/10], Step [173/600], Loss: 0.4883, Train Acc:0.8322\n",
      "Epoch [10/10], Step [174/600], Loss: 0.5600, Train Acc:0.8322\n",
      "Epoch [10/10], Step [175/600], Loss: 0.4778, Train Acc:0.8324\n",
      "Epoch [10/10], Step [176/600], Loss: 0.4847, Train Acc:0.8326\n",
      "Epoch [10/10], Step [177/600], Loss: 0.6038, Train Acc:0.8324\n",
      "Epoch [10/10], Step [178/600], Loss: 0.6295, Train Acc:0.8324\n",
      "Epoch [10/10], Step [179/600], Loss: 0.4300, Train Acc:0.8324\n",
      "Epoch [10/10], Step [180/600], Loss: 0.5038, Train Acc:0.8324\n",
      "Epoch [10/10], Step [181/600], Loss: 0.6196, Train Acc:0.8320\n",
      "Epoch [10/10], Step [182/600], Loss: 0.4359, Train Acc:0.8323\n",
      "Epoch [10/10], Step [183/600], Loss: 0.6568, Train Acc:0.8319\n",
      "Epoch [10/10], Step [184/600], Loss: 0.5498, Train Acc:0.8319\n",
      "Epoch [10/10], Step [185/600], Loss: 0.5515, Train Acc:0.8317\n",
      "Epoch [10/10], Step [186/600], Loss: 0.6200, Train Acc:0.8315\n",
      "Epoch [10/10], Step [187/600], Loss: 0.5342, Train Acc:0.8315\n",
      "Epoch [10/10], Step [188/600], Loss: 0.5326, Train Acc:0.8314\n",
      "Epoch [10/10], Step [189/600], Loss: 0.4939, Train Acc:0.8314\n",
      "Epoch [10/10], Step [190/600], Loss: 0.5079, Train Acc:0.8314\n",
      "Epoch [10/10], Step [191/600], Loss: 0.4247, Train Acc:0.8315\n",
      "Epoch [10/10], Step [192/600], Loss: 0.4195, Train Acc:0.8317\n",
      "Epoch [10/10], Step [193/600], Loss: 0.4225, Train Acc:0.8320\n",
      "Epoch [10/10], Step [194/600], Loss: 0.5246, Train Acc:0.8320\n",
      "Epoch [10/10], Step [195/600], Loss: 0.4887, Train Acc:0.8319\n",
      "Epoch [10/10], Step [196/600], Loss: 0.4974, Train Acc:0.8319\n",
      "Epoch [10/10], Step [197/600], Loss: 0.5603, Train Acc:0.8317\n",
      "Epoch [10/10], Step [198/600], Loss: 0.5457, Train Acc:0.8317\n",
      "Epoch [10/10], Step [199/600], Loss: 0.5804, Train Acc:0.8316\n",
      "Epoch [10/10], Step [200/600], Loss: 0.4990, Train Acc:0.8318\n",
      "Epoch [10/10], Step [201/600], Loss: 0.7034, Train Acc:0.8316\n",
      "Epoch [10/10], Step [202/600], Loss: 0.4388, Train Acc:0.8317\n",
      "Epoch [10/10], Step [203/600], Loss: 0.3105, Train Acc:0.8321\n",
      "Epoch [10/10], Step [204/600], Loss: 0.6666, Train Acc:0.8317\n",
      "Epoch [10/10], Step [205/600], Loss: 0.5305, Train Acc:0.8317\n",
      "Epoch [10/10], Step [206/600], Loss: 0.4413, Train Acc:0.8319\n",
      "Epoch [10/10], Step [207/600], Loss: 0.5375, Train Acc:0.8318\n",
      "Epoch [10/10], Step [208/600], Loss: 0.5843, Train Acc:0.8318\n",
      "Epoch [10/10], Step [209/600], Loss: 0.5022, Train Acc:0.8317\n",
      "Epoch [10/10], Step [210/600], Loss: 0.4045, Train Acc:0.8320\n",
      "Epoch [10/10], Step [211/600], Loss: 0.5969, Train Acc:0.8317\n",
      "Epoch [10/10], Step [212/600], Loss: 0.5793, Train Acc:0.8317\n",
      "Epoch [10/10], Step [213/600], Loss: 0.6810, Train Acc:0.8315\n",
      "Epoch [10/10], Step [214/600], Loss: 0.4686, Train Acc:0.8317\n",
      "Epoch [10/10], Step [215/600], Loss: 0.4954, Train Acc:0.8316\n",
      "Epoch [10/10], Step [216/600], Loss: 0.6340, Train Acc:0.8314\n",
      "Epoch [10/10], Step [217/600], Loss: 0.6014, Train Acc:0.8311\n",
      "Epoch [10/10], Step [218/600], Loss: 0.4128, Train Acc:0.8313\n",
      "Epoch [10/10], Step [219/600], Loss: 0.5975, Train Acc:0.8311\n",
      "Epoch [10/10], Step [220/600], Loss: 0.4234, Train Acc:0.8313\n",
      "Epoch [10/10], Step [221/600], Loss: 0.4591, Train Acc:0.8315\n",
      "Epoch [10/10], Step [222/600], Loss: 0.4468, Train Acc:0.8317\n",
      "Epoch [10/10], Step [223/600], Loss: 0.4766, Train Acc:0.8318\n",
      "Epoch [10/10], Step [224/600], Loss: 0.4726, Train Acc:0.8320\n",
      "Epoch [10/10], Step [225/600], Loss: 0.4942, Train Acc:0.8318\n",
      "Epoch [10/10], Step [226/600], Loss: 0.5314, Train Acc:0.8317\n",
      "Epoch [10/10], Step [227/600], Loss: 0.5594, Train Acc:0.8316\n",
      "Epoch [10/10], Step [228/600], Loss: 0.6243, Train Acc:0.8317\n",
      "Epoch [10/10], Step [229/600], Loss: 0.5311, Train Acc:0.8317\n",
      "Epoch [10/10], Step [230/600], Loss: 0.4925, Train Acc:0.8318\n",
      "Epoch [10/10], Step [231/600], Loss: 0.5909, Train Acc:0.8317\n",
      "Epoch [10/10], Step [232/600], Loss: 0.5834, Train Acc:0.8316\n",
      "Epoch [10/10], Step [233/600], Loss: 0.5470, Train Acc:0.8315\n",
      "Epoch [10/10], Step [234/600], Loss: 0.5184, Train Acc:0.8318\n",
      "Epoch [10/10], Step [235/600], Loss: 0.4781, Train Acc:0.8318\n",
      "Epoch [10/10], Step [236/600], Loss: 0.5091, Train Acc:0.8319\n",
      "Epoch [10/10], Step [237/600], Loss: 0.4884, Train Acc:0.8320\n",
      "Epoch [10/10], Step [238/600], Loss: 0.5893, Train Acc:0.8320\n",
      "Epoch [10/10], Step [239/600], Loss: 0.6287, Train Acc:0.8318\n",
      "Epoch [10/10], Step [240/600], Loss: 0.5922, Train Acc:0.8317\n",
      "Epoch [10/10], Step [241/600], Loss: 0.4110, Train Acc:0.8318\n",
      "Epoch [10/10], Step [242/600], Loss: 0.3690, Train Acc:0.8319\n",
      "Epoch [10/10], Step [243/600], Loss: 0.5235, Train Acc:0.8318\n",
      "Epoch [10/10], Step [244/600], Loss: 0.4692, Train Acc:0.8320\n",
      "Epoch [10/10], Step [245/600], Loss: 0.4419, Train Acc:0.8322\n",
      "Epoch [10/10], Step [246/600], Loss: 0.5452, Train Acc:0.8322\n",
      "Epoch [10/10], Step [247/600], Loss: 0.5743, Train Acc:0.8321\n",
      "Epoch [10/10], Step [248/600], Loss: 0.6150, Train Acc:0.8319\n",
      "Epoch [10/10], Step [249/600], Loss: 0.5398, Train Acc:0.8318\n",
      "Epoch [10/10], Step [250/600], Loss: 0.5277, Train Acc:0.8316\n",
      "Epoch [10/10], Step [251/600], Loss: 0.5510, Train Acc:0.8316\n",
      "Epoch [10/10], Step [252/600], Loss: 0.4921, Train Acc:0.8316\n",
      "Epoch [10/10], Step [253/600], Loss: 0.5839, Train Acc:0.8317\n",
      "Epoch [10/10], Step [254/600], Loss: 0.6934, Train Acc:0.8313\n",
      "Epoch [10/10], Step [255/600], Loss: 0.5343, Train Acc:0.8315\n",
      "Epoch [10/10], Step [256/600], Loss: 0.5365, Train Acc:0.8316\n",
      "Epoch [10/10], Step [257/600], Loss: 0.5960, Train Acc:0.8315\n",
      "Epoch [10/10], Step [258/600], Loss: 0.6493, Train Acc:0.8313\n",
      "Epoch [10/10], Step [259/600], Loss: 0.4660, Train Acc:0.8314\n",
      "Epoch [10/10], Step [260/600], Loss: 0.4664, Train Acc:0.8314\n",
      "Epoch [10/10], Step [261/600], Loss: 0.4803, Train Acc:0.8315\n",
      "Epoch [10/10], Step [262/600], Loss: 0.5023, Train Acc:0.8316\n",
      "Epoch [10/10], Step [263/600], Loss: 0.4658, Train Acc:0.8317\n",
      "Epoch [10/10], Step [264/600], Loss: 0.6366, Train Acc:0.8316\n",
      "Epoch [10/10], Step [265/600], Loss: 0.4316, Train Acc:0.8316\n",
      "Epoch [10/10], Step [266/600], Loss: 0.5071, Train Acc:0.8317\n",
      "Epoch [10/10], Step [267/600], Loss: 0.5304, Train Acc:0.8318\n",
      "Epoch [10/10], Step [268/600], Loss: 0.6524, Train Acc:0.8317\n",
      "Epoch [10/10], Step [269/600], Loss: 0.5558, Train Acc:0.8317\n",
      "Epoch [10/10], Step [270/600], Loss: 0.6298, Train Acc:0.8315\n",
      "Epoch [10/10], Step [271/600], Loss: 0.4878, Train Acc:0.8316\n",
      "Epoch [10/10], Step [272/600], Loss: 0.4666, Train Acc:0.8318\n",
      "Epoch [10/10], Step [273/600], Loss: 0.6122, Train Acc:0.8316\n",
      "Epoch [10/10], Step [274/600], Loss: 0.5694, Train Acc:0.8317\n",
      "Epoch [10/10], Step [275/600], Loss: 0.4416, Train Acc:0.8318\n",
      "Epoch [10/10], Step [276/600], Loss: 0.6450, Train Acc:0.8316\n",
      "Epoch [10/10], Step [277/600], Loss: 0.4819, Train Acc:0.8315\n",
      "Epoch [10/10], Step [278/600], Loss: 0.6233, Train Acc:0.8314\n",
      "Epoch [10/10], Step [279/600], Loss: 0.4948, Train Acc:0.8314\n",
      "Epoch [10/10], Step [280/600], Loss: 0.3713, Train Acc:0.8317\n",
      "Epoch [10/10], Step [281/600], Loss: 0.5982, Train Acc:0.8316\n",
      "Epoch [10/10], Step [282/600], Loss: 0.5285, Train Acc:0.8317\n",
      "Epoch [10/10], Step [283/600], Loss: 0.5137, Train Acc:0.8316\n",
      "Epoch [10/10], Step [284/600], Loss: 0.6988, Train Acc:0.8315\n",
      "Epoch [10/10], Step [285/600], Loss: 0.5053, Train Acc:0.8315\n",
      "Epoch [10/10], Step [286/600], Loss: 0.5843, Train Acc:0.8314\n",
      "Epoch [10/10], Step [287/600], Loss: 0.6676, Train Acc:0.8312\n",
      "Epoch [10/10], Step [288/600], Loss: 0.4944, Train Acc:0.8312\n",
      "Epoch [10/10], Step [289/600], Loss: 0.6695, Train Acc:0.8310\n",
      "Epoch [10/10], Step [290/600], Loss: 0.4557, Train Acc:0.8311\n",
      "Epoch [10/10], Step [291/600], Loss: 0.3854, Train Acc:0.8313\n",
      "Epoch [10/10], Step [292/600], Loss: 0.5416, Train Acc:0.8312\n",
      "Epoch [10/10], Step [293/600], Loss: 0.4708, Train Acc:0.8314\n",
      "Epoch [10/10], Step [294/600], Loss: 0.5182, Train Acc:0.8313\n",
      "Epoch [10/10], Step [295/600], Loss: 0.5732, Train Acc:0.8311\n",
      "Epoch [10/10], Step [296/600], Loss: 0.3725, Train Acc:0.8314\n",
      "Epoch [10/10], Step [297/600], Loss: 0.5276, Train Acc:0.8314\n",
      "Epoch [10/10], Step [298/600], Loss: 0.3987, Train Acc:0.8316\n",
      "Epoch [10/10], Step [299/600], Loss: 0.5153, Train Acc:0.8316\n",
      "Epoch [10/10], Step [300/600], Loss: 0.5407, Train Acc:0.8317\n",
      "Epoch [10/10], Step [301/600], Loss: 0.5611, Train Acc:0.8317\n",
      "Epoch [10/10], Step [302/600], Loss: 0.5077, Train Acc:0.8318\n",
      "Epoch [10/10], Step [303/600], Loss: 0.5387, Train Acc:0.8318\n",
      "Epoch [10/10], Step [304/600], Loss: 0.5764, Train Acc:0.8317\n",
      "Epoch [10/10], Step [305/600], Loss: 0.5721, Train Acc:0.8317\n",
      "Epoch [10/10], Step [306/600], Loss: 0.5345, Train Acc:0.8317\n",
      "Epoch [10/10], Step [307/600], Loss: 0.4991, Train Acc:0.8315\n",
      "Epoch [10/10], Step [308/600], Loss: 0.4713, Train Acc:0.8316\n",
      "Epoch [10/10], Step [309/600], Loss: 0.4508, Train Acc:0.8316\n",
      "Epoch [10/10], Step [310/600], Loss: 0.4565, Train Acc:0.8315\n",
      "Epoch [10/10], Step [311/600], Loss: 0.5425, Train Acc:0.8315\n",
      "Epoch [10/10], Step [312/600], Loss: 0.4091, Train Acc:0.8316\n",
      "Epoch [10/10], Step [313/600], Loss: 0.5709, Train Acc:0.8316\n",
      "Epoch [10/10], Step [314/600], Loss: 0.4328, Train Acc:0.8316\n",
      "Epoch [10/10], Step [315/600], Loss: 0.5792, Train Acc:0.8316\n",
      "Epoch [10/10], Step [316/600], Loss: 0.5565, Train Acc:0.8314\n",
      "Epoch [10/10], Step [317/600], Loss: 0.5607, Train Acc:0.8313\n",
      "Epoch [10/10], Step [318/600], Loss: 0.5551, Train Acc:0.8312\n",
      "Epoch [10/10], Step [319/600], Loss: 0.5661, Train Acc:0.8312\n",
      "Epoch [10/10], Step [320/600], Loss: 0.5334, Train Acc:0.8311\n",
      "Epoch [10/10], Step [321/600], Loss: 0.5182, Train Acc:0.8311\n",
      "Epoch [10/10], Step [322/600], Loss: 0.5735, Train Acc:0.8311\n",
      "Epoch [10/10], Step [323/600], Loss: 0.4477, Train Acc:0.8311\n",
      "Epoch [10/10], Step [324/600], Loss: 0.4623, Train Acc:0.8312\n",
      "Epoch [10/10], Step [325/600], Loss: 0.5158, Train Acc:0.8312\n",
      "Epoch [10/10], Step [326/600], Loss: 0.4667, Train Acc:0.8313\n",
      "Epoch [10/10], Step [327/600], Loss: 0.4583, Train Acc:0.8314\n",
      "Epoch [10/10], Step [328/600], Loss: 0.5732, Train Acc:0.8314\n",
      "Epoch [10/10], Step [329/600], Loss: 0.4348, Train Acc:0.8315\n",
      "Epoch [10/10], Step [330/600], Loss: 0.5747, Train Acc:0.8313\n",
      "Epoch [10/10], Step [331/600], Loss: 0.4129, Train Acc:0.8314\n",
      "Epoch [10/10], Step [332/600], Loss: 0.5493, Train Acc:0.8312\n",
      "Epoch [10/10], Step [333/600], Loss: 0.5185, Train Acc:0.8313\n",
      "Epoch [10/10], Step [334/600], Loss: 0.4639, Train Acc:0.8313\n",
      "Epoch [10/10], Step [335/600], Loss: 0.5420, Train Acc:0.8313\n",
      "Epoch [10/10], Step [336/600], Loss: 0.4953, Train Acc:0.8314\n",
      "Epoch [10/10], Step [337/600], Loss: 0.4711, Train Acc:0.8315\n",
      "Epoch [10/10], Step [338/600], Loss: 0.5332, Train Acc:0.8314\n",
      "Epoch [10/10], Step [339/600], Loss: 0.6321, Train Acc:0.8311\n",
      "Epoch [10/10], Step [340/600], Loss: 0.5361, Train Acc:0.8312\n",
      "Epoch [10/10], Step [341/600], Loss: 0.5358, Train Acc:0.8311\n",
      "Epoch [10/10], Step [342/600], Loss: 0.5205, Train Acc:0.8311\n",
      "Epoch [10/10], Step [343/600], Loss: 0.6767, Train Acc:0.8309\n",
      "Epoch [10/10], Step [344/600], Loss: 0.4705, Train Acc:0.8310\n",
      "Epoch [10/10], Step [345/600], Loss: 0.4746, Train Acc:0.8309\n",
      "Epoch [10/10], Step [346/600], Loss: 0.4653, Train Acc:0.8310\n",
      "Epoch [10/10], Step [347/600], Loss: 0.3137, Train Acc:0.8313\n",
      "Epoch [10/10], Step [348/600], Loss: 0.4297, Train Acc:0.8315\n",
      "Epoch [10/10], Step [349/600], Loss: 0.4726, Train Acc:0.8315\n",
      "Epoch [10/10], Step [350/600], Loss: 0.4209, Train Acc:0.8316\n",
      "Epoch [10/10], Step [351/600], Loss: 0.5300, Train Acc:0.8315\n",
      "Epoch [10/10], Step [352/600], Loss: 0.4640, Train Acc:0.8316\n",
      "Epoch [10/10], Step [353/600], Loss: 0.5211, Train Acc:0.8316\n",
      "Epoch [10/10], Step [354/600], Loss: 0.6098, Train Acc:0.8315\n",
      "Epoch [10/10], Step [355/600], Loss: 0.5866, Train Acc:0.8314\n",
      "Epoch [10/10], Step [356/600], Loss: 0.6485, Train Acc:0.8313\n",
      "Epoch [10/10], Step [357/600], Loss: 0.4755, Train Acc:0.8314\n",
      "Epoch [10/10], Step [358/600], Loss: 0.5049, Train Acc:0.8314\n",
      "Epoch [10/10], Step [359/600], Loss: 0.5062, Train Acc:0.8315\n",
      "Epoch [10/10], Step [360/600], Loss: 0.3860, Train Acc:0.8317\n",
      "Epoch [10/10], Step [361/600], Loss: 0.5697, Train Acc:0.8316\n",
      "Epoch [10/10], Step [362/600], Loss: 0.5797, Train Acc:0.8316\n",
      "Epoch [10/10], Step [363/600], Loss: 0.6150, Train Acc:0.8315\n",
      "Epoch [10/10], Step [364/600], Loss: 0.4541, Train Acc:0.8315\n",
      "Epoch [10/10], Step [365/600], Loss: 0.5786, Train Acc:0.8314\n",
      "Epoch [10/10], Step [366/600], Loss: 0.4650, Train Acc:0.8314\n",
      "Epoch [10/10], Step [367/600], Loss: 0.4721, Train Acc:0.8315\n",
      "Epoch [10/10], Step [368/600], Loss: 0.3479, Train Acc:0.8316\n",
      "Epoch [10/10], Step [369/600], Loss: 0.5562, Train Acc:0.8315\n",
      "Epoch [10/10], Step [370/600], Loss: 0.5287, Train Acc:0.8316\n",
      "Epoch [10/10], Step [371/600], Loss: 0.5225, Train Acc:0.8317\n",
      "Epoch [10/10], Step [372/600], Loss: 0.6287, Train Acc:0.8315\n",
      "Epoch [10/10], Step [373/600], Loss: 0.3987, Train Acc:0.8316\n",
      "Epoch [10/10], Step [374/600], Loss: 0.6024, Train Acc:0.8314\n",
      "Epoch [10/10], Step [375/600], Loss: 0.4634, Train Acc:0.8316\n",
      "Epoch [10/10], Step [376/600], Loss: 0.4712, Train Acc:0.8317\n",
      "Epoch [10/10], Step [377/600], Loss: 0.5283, Train Acc:0.8317\n",
      "Epoch [10/10], Step [378/600], Loss: 0.5633, Train Acc:0.8317\n",
      "Epoch [10/10], Step [379/600], Loss: 0.4261, Train Acc:0.8318\n",
      "Epoch [10/10], Step [380/600], Loss: 0.4754, Train Acc:0.8318\n",
      "Epoch [10/10], Step [381/600], Loss: 0.5796, Train Acc:0.8317\n",
      "Epoch [10/10], Step [382/600], Loss: 0.5506, Train Acc:0.8318\n",
      "Epoch [10/10], Step [383/600], Loss: 0.6107, Train Acc:0.8316\n",
      "Epoch [10/10], Step [384/600], Loss: 0.7473, Train Acc:0.8315\n",
      "Epoch [10/10], Step [385/600], Loss: 0.6566, Train Acc:0.8315\n",
      "Epoch [10/10], Step [386/600], Loss: 0.4687, Train Acc:0.8314\n",
      "Epoch [10/10], Step [387/600], Loss: 0.6019, Train Acc:0.8313\n",
      "Epoch [10/10], Step [388/600], Loss: 0.4622, Train Acc:0.8314\n",
      "Epoch [10/10], Step [389/600], Loss: 0.5556, Train Acc:0.8314\n",
      "Epoch [10/10], Step [390/600], Loss: 0.4301, Train Acc:0.8315\n",
      "Epoch [10/10], Step [391/600], Loss: 0.5142, Train Acc:0.8315\n",
      "Epoch [10/10], Step [392/600], Loss: 0.5606, Train Acc:0.8315\n",
      "Epoch [10/10], Step [393/600], Loss: 0.5226, Train Acc:0.8315\n",
      "Epoch [10/10], Step [394/600], Loss: 0.4056, Train Acc:0.8318\n",
      "Epoch [10/10], Step [395/600], Loss: 0.3822, Train Acc:0.8319\n",
      "Epoch [10/10], Step [396/600], Loss: 0.5184, Train Acc:0.8318\n",
      "Epoch [10/10], Step [397/600], Loss: 0.7287, Train Acc:0.8317\n",
      "Epoch [10/10], Step [398/600], Loss: 0.4878, Train Acc:0.8317\n",
      "Epoch [10/10], Step [399/600], Loss: 0.4486, Train Acc:0.8317\n",
      "Epoch [10/10], Step [400/600], Loss: 0.5521, Train Acc:0.8316\n",
      "Epoch [10/10], Step [401/600], Loss: 0.6020, Train Acc:0.8315\n",
      "Epoch [10/10], Step [402/600], Loss: 0.5487, Train Acc:0.8315\n",
      "Epoch [10/10], Step [403/600], Loss: 0.4780, Train Acc:0.8314\n",
      "Epoch [10/10], Step [404/600], Loss: 0.5446, Train Acc:0.8314\n",
      "Epoch [10/10], Step [405/600], Loss: 0.3999, Train Acc:0.8315\n",
      "Epoch [10/10], Step [406/600], Loss: 0.5154, Train Acc:0.8315\n",
      "Epoch [10/10], Step [407/600], Loss: 0.6329, Train Acc:0.8314\n",
      "Epoch [10/10], Step [408/600], Loss: 0.5954, Train Acc:0.8313\n",
      "Epoch [10/10], Step [409/600], Loss: 0.4623, Train Acc:0.8313\n",
      "Epoch [10/10], Step [410/600], Loss: 0.4940, Train Acc:0.8313\n",
      "Epoch [10/10], Step [411/600], Loss: 0.5140, Train Acc:0.8313\n",
      "Epoch [10/10], Step [412/600], Loss: 0.5745, Train Acc:0.8313\n",
      "Epoch [10/10], Step [413/600], Loss: 0.5819, Train Acc:0.8312\n",
      "Epoch [10/10], Step [414/600], Loss: 0.4428, Train Acc:0.8313\n",
      "Epoch [10/10], Step [415/600], Loss: 0.6586, Train Acc:0.8313\n",
      "Epoch [10/10], Step [416/600], Loss: 0.5452, Train Acc:0.8313\n",
      "Epoch [10/10], Step [417/600], Loss: 0.5727, Train Acc:0.8312\n",
      "Epoch [10/10], Step [418/600], Loss: 0.5657, Train Acc:0.8312\n",
      "Epoch [10/10], Step [419/600], Loss: 0.4517, Train Acc:0.8313\n",
      "Epoch [10/10], Step [420/600], Loss: 0.5473, Train Acc:0.8313\n",
      "Epoch [10/10], Step [421/600], Loss: 0.5006, Train Acc:0.8313\n",
      "Epoch [10/10], Step [422/600], Loss: 0.5873, Train Acc:0.8312\n",
      "Epoch [10/10], Step [423/600], Loss: 0.5032, Train Acc:0.8312\n",
      "Epoch [10/10], Step [424/600], Loss: 0.6119, Train Acc:0.8312\n",
      "Epoch [10/10], Step [425/600], Loss: 0.4019, Train Acc:0.8313\n",
      "Epoch [10/10], Step [426/600], Loss: 0.6047, Train Acc:0.8312\n",
      "Epoch [10/10], Step [427/600], Loss: 0.7109, Train Acc:0.8310\n",
      "Epoch [10/10], Step [428/600], Loss: 0.4653, Train Acc:0.8310\n",
      "Epoch [10/10], Step [429/600], Loss: 0.5996, Train Acc:0.8309\n",
      "Epoch [10/10], Step [430/600], Loss: 0.4186, Train Acc:0.8310\n",
      "Epoch [10/10], Step [431/600], Loss: 0.5417, Train Acc:0.8310\n",
      "Epoch [10/10], Step [432/600], Loss: 0.4057, Train Acc:0.8311\n",
      "Epoch [10/10], Step [433/600], Loss: 0.5830, Train Acc:0.8309\n",
      "Epoch [10/10], Step [434/600], Loss: 0.5415, Train Acc:0.8309\n",
      "Epoch [10/10], Step [435/600], Loss: 0.4885, Train Acc:0.8309\n",
      "Epoch [10/10], Step [436/600], Loss: 0.6663, Train Acc:0.8308\n",
      "Epoch [10/10], Step [437/600], Loss: 0.4883, Train Acc:0.8308\n",
      "Epoch [10/10], Step [438/600], Loss: 0.5824, Train Acc:0.8307\n",
      "Epoch [10/10], Step [439/600], Loss: 0.4864, Train Acc:0.8308\n",
      "Epoch [10/10], Step [440/600], Loss: 0.5388, Train Acc:0.8307\n",
      "Epoch [10/10], Step [441/600], Loss: 0.5555, Train Acc:0.8307\n",
      "Epoch [10/10], Step [442/600], Loss: 0.5208, Train Acc:0.8307\n",
      "Epoch [10/10], Step [443/600], Loss: 0.4973, Train Acc:0.8308\n",
      "Epoch [10/10], Step [444/600], Loss: 0.5658, Train Acc:0.8307\n",
      "Epoch [10/10], Step [445/600], Loss: 0.4130, Train Acc:0.8309\n",
      "Epoch [10/10], Step [446/600], Loss: 0.4630, Train Acc:0.8309\n",
      "Epoch [10/10], Step [447/600], Loss: 0.6098, Train Acc:0.8308\n",
      "Epoch [10/10], Step [448/600], Loss: 0.4430, Train Acc:0.8309\n",
      "Epoch [10/10], Step [449/600], Loss: 0.6027, Train Acc:0.8309\n",
      "Epoch [10/10], Step [450/600], Loss: 0.4711, Train Acc:0.8309\n",
      "Epoch [10/10], Step [451/600], Loss: 0.7073, Train Acc:0.8308\n",
      "Epoch [10/10], Step [452/600], Loss: 0.6220, Train Acc:0.8306\n",
      "Epoch [10/10], Step [453/600], Loss: 0.6314, Train Acc:0.8306\n",
      "Epoch [10/10], Step [454/600], Loss: 0.4918, Train Acc:0.8306\n",
      "Epoch [10/10], Step [455/600], Loss: 0.5770, Train Acc:0.8305\n",
      "Epoch [10/10], Step [456/600], Loss: 0.5840, Train Acc:0.8305\n",
      "Epoch [10/10], Step [457/600], Loss: 0.5245, Train Acc:0.8305\n",
      "Epoch [10/10], Step [458/600], Loss: 0.6618, Train Acc:0.8303\n",
      "Epoch [10/10], Step [459/600], Loss: 0.4608, Train Acc:0.8304\n",
      "Epoch [10/10], Step [460/600], Loss: 0.6151, Train Acc:0.8303\n",
      "Epoch [10/10], Step [461/600], Loss: 0.6255, Train Acc:0.8302\n",
      "Epoch [10/10], Step [462/600], Loss: 0.4848, Train Acc:0.8302\n",
      "Epoch [10/10], Step [463/600], Loss: 0.4778, Train Acc:0.8304\n",
      "Epoch [10/10], Step [464/600], Loss: 0.6570, Train Acc:0.8303\n",
      "Epoch [10/10], Step [465/600], Loss: 0.4850, Train Acc:0.8304\n",
      "Epoch [10/10], Step [466/600], Loss: 0.6075, Train Acc:0.8303\n",
      "Epoch [10/10], Step [467/600], Loss: 0.4657, Train Acc:0.8304\n",
      "Epoch [10/10], Step [468/600], Loss: 0.4946, Train Acc:0.8303\n",
      "Epoch [10/10], Step [469/600], Loss: 0.5976, Train Acc:0.8303\n",
      "Epoch [10/10], Step [470/600], Loss: 0.4263, Train Acc:0.8303\n",
      "Epoch [10/10], Step [471/600], Loss: 0.4555, Train Acc:0.8303\n",
      "Epoch [10/10], Step [472/600], Loss: 0.5555, Train Acc:0.8303\n",
      "Epoch [10/10], Step [473/600], Loss: 0.4478, Train Acc:0.8305\n",
      "Epoch [10/10], Step [474/600], Loss: 0.4947, Train Acc:0.8305\n",
      "Epoch [10/10], Step [475/600], Loss: 0.5725, Train Acc:0.8305\n",
      "Epoch [10/10], Step [476/600], Loss: 0.6126, Train Acc:0.8304\n",
      "Epoch [10/10], Step [477/600], Loss: 0.7559, Train Acc:0.8303\n",
      "Epoch [10/10], Step [478/600], Loss: 0.4912, Train Acc:0.8303\n",
      "Epoch [10/10], Step [479/600], Loss: 0.4344, Train Acc:0.8303\n",
      "Epoch [10/10], Step [480/600], Loss: 0.4909, Train Acc:0.8303\n",
      "Epoch [10/10], Step [481/600], Loss: 0.4446, Train Acc:0.8304\n",
      "Epoch [10/10], Step [482/600], Loss: 0.4286, Train Acc:0.8305\n",
      "Epoch [10/10], Step [483/600], Loss: 0.5091, Train Acc:0.8304\n",
      "Epoch [10/10], Step [484/600], Loss: 0.5435, Train Acc:0.8304\n",
      "Epoch [10/10], Step [485/600], Loss: 0.4052, Train Acc:0.8304\n",
      "Epoch [10/10], Step [486/600], Loss: 0.4769, Train Acc:0.8303\n",
      "Epoch [10/10], Step [487/600], Loss: 0.5124, Train Acc:0.8303\n",
      "Epoch [10/10], Step [488/600], Loss: 0.4979, Train Acc:0.8303\n",
      "Epoch [10/10], Step [489/600], Loss: 0.3751, Train Acc:0.8304\n",
      "Epoch [10/10], Step [490/600], Loss: 0.5008, Train Acc:0.8304\n",
      "Epoch [10/10], Step [491/600], Loss: 0.5385, Train Acc:0.8304\n",
      "Epoch [10/10], Step [492/600], Loss: 0.5425, Train Acc:0.8304\n",
      "Epoch [10/10], Step [493/600], Loss: 0.4917, Train Acc:0.8304\n",
      "Epoch [10/10], Step [494/600], Loss: 0.4625, Train Acc:0.8305\n",
      "Epoch [10/10], Step [495/600], Loss: 0.4856, Train Acc:0.8305\n",
      "Epoch [10/10], Step [496/600], Loss: 0.5101, Train Acc:0.8304\n",
      "Epoch [10/10], Step [497/600], Loss: 0.5447, Train Acc:0.8304\n",
      "Epoch [10/10], Step [498/600], Loss: 0.5496, Train Acc:0.8303\n",
      "Epoch [10/10], Step [499/600], Loss: 0.4063, Train Acc:0.8303\n",
      "Epoch [10/10], Step [500/600], Loss: 0.5629, Train Acc:0.8304\n",
      "Epoch [10/10], Step [501/600], Loss: 0.4544, Train Acc:0.8304\n",
      "Epoch [10/10], Step [502/600], Loss: 0.4166, Train Acc:0.8304\n",
      "Epoch [10/10], Step [503/600], Loss: 0.6265, Train Acc:0.8304\n",
      "Epoch [10/10], Step [504/600], Loss: 0.4945, Train Acc:0.8304\n",
      "Epoch [10/10], Step [505/600], Loss: 0.4056, Train Acc:0.8304\n",
      "Epoch [10/10], Step [506/600], Loss: 0.3651, Train Acc:0.8305\n",
      "Epoch [10/10], Step [507/600], Loss: 0.5490, Train Acc:0.8305\n",
      "Epoch [10/10], Step [508/600], Loss: 0.5165, Train Acc:0.8305\n",
      "Epoch [10/10], Step [509/600], Loss: 0.4681, Train Acc:0.8305\n",
      "Epoch [10/10], Step [510/600], Loss: 0.5687, Train Acc:0.8305\n",
      "Epoch [10/10], Step [511/600], Loss: 0.7310, Train Acc:0.8304\n",
      "Epoch [10/10], Step [512/600], Loss: 0.5536, Train Acc:0.8303\n",
      "Epoch [10/10], Step [513/600], Loss: 0.6419, Train Acc:0.8304\n",
      "Epoch [10/10], Step [514/600], Loss: 0.6004, Train Acc:0.8303\n",
      "Epoch [10/10], Step [515/600], Loss: 0.5063, Train Acc:0.8303\n",
      "Epoch [10/10], Step [516/600], Loss: 0.4506, Train Acc:0.8304\n",
      "Epoch [10/10], Step [517/600], Loss: 0.4499, Train Acc:0.8304\n",
      "Epoch [10/10], Step [518/600], Loss: 0.5442, Train Acc:0.8304\n",
      "Epoch [10/10], Step [519/600], Loss: 0.6462, Train Acc:0.8302\n",
      "Epoch [10/10], Step [520/600], Loss: 0.6202, Train Acc:0.8301\n",
      "Epoch [10/10], Step [521/600], Loss: 0.5467, Train Acc:0.8302\n",
      "Epoch [10/10], Step [522/600], Loss: 0.5734, Train Acc:0.8302\n",
      "Epoch [10/10], Step [523/600], Loss: 0.5801, Train Acc:0.8301\n",
      "Epoch [10/10], Step [524/600], Loss: 0.6333, Train Acc:0.8300\n",
      "Epoch [10/10], Step [525/600], Loss: 0.6021, Train Acc:0.8300\n",
      "Epoch [10/10], Step [526/600], Loss: 0.5247, Train Acc:0.8300\n",
      "Epoch [10/10], Step [527/600], Loss: 0.3880, Train Acc:0.8301\n",
      "Epoch [10/10], Step [528/600], Loss: 0.4000, Train Acc:0.8302\n",
      "Epoch [10/10], Step [529/600], Loss: 0.5227, Train Acc:0.8301\n",
      "Epoch [10/10], Step [530/600], Loss: 0.4615, Train Acc:0.8302\n",
      "Epoch [10/10], Step [531/600], Loss: 0.3769, Train Acc:0.8303\n",
      "Epoch [10/10], Step [532/600], Loss: 0.4353, Train Acc:0.8303\n",
      "Epoch [10/10], Step [533/600], Loss: 0.4961, Train Acc:0.8303\n",
      "Epoch [10/10], Step [534/600], Loss: 0.5145, Train Acc:0.8303\n",
      "Epoch [10/10], Step [535/600], Loss: 0.5857, Train Acc:0.8302\n",
      "Epoch [10/10], Step [536/600], Loss: 0.5515, Train Acc:0.8302\n",
      "Epoch [10/10], Step [537/600], Loss: 0.4164, Train Acc:0.8302\n",
      "Epoch [10/10], Step [538/600], Loss: 0.4313, Train Acc:0.8302\n",
      "Epoch [10/10], Step [539/600], Loss: 0.5679, Train Acc:0.8301\n",
      "Epoch [10/10], Step [540/600], Loss: 0.5350, Train Acc:0.8301\n",
      "Epoch [10/10], Step [541/600], Loss: 0.5630, Train Acc:0.8300\n",
      "Epoch [10/10], Step [542/600], Loss: 0.6088, Train Acc:0.8299\n",
      "Epoch [10/10], Step [543/600], Loss: 0.4927, Train Acc:0.8299\n",
      "Epoch [10/10], Step [544/600], Loss: 0.4645, Train Acc:0.8299\n",
      "Epoch [10/10], Step [545/600], Loss: 0.5324, Train Acc:0.8299\n",
      "Epoch [10/10], Step [546/600], Loss: 0.4706, Train Acc:0.8299\n",
      "Epoch [10/10], Step [547/600], Loss: 0.5164, Train Acc:0.8298\n",
      "Epoch [10/10], Step [548/600], Loss: 0.5357, Train Acc:0.8297\n",
      "Epoch [10/10], Step [549/600], Loss: 0.5025, Train Acc:0.8297\n",
      "Epoch [10/10], Step [550/600], Loss: 0.5741, Train Acc:0.8296\n",
      "Epoch [10/10], Step [551/600], Loss: 0.5238, Train Acc:0.8296\n",
      "Epoch [10/10], Step [552/600], Loss: 0.4660, Train Acc:0.8295\n",
      "Epoch [10/10], Step [553/600], Loss: 0.4754, Train Acc:0.8295\n",
      "Epoch [10/10], Step [554/600], Loss: 0.4765, Train Acc:0.8296\n",
      "Epoch [10/10], Step [555/600], Loss: 0.4934, Train Acc:0.8296\n",
      "Epoch [10/10], Step [556/600], Loss: 0.5435, Train Acc:0.8296\n",
      "Epoch [10/10], Step [557/600], Loss: 0.4578, Train Acc:0.8297\n",
      "Epoch [10/10], Step [558/600], Loss: 0.4847, Train Acc:0.8297\n",
      "Epoch [10/10], Step [559/600], Loss: 0.4738, Train Acc:0.8297\n",
      "Epoch [10/10], Step [560/600], Loss: 0.5181, Train Acc:0.8297\n",
      "Epoch [10/10], Step [561/600], Loss: 0.4441, Train Acc:0.8297\n",
      "Epoch [10/10], Step [562/600], Loss: 0.5101, Train Acc:0.8297\n",
      "Epoch [10/10], Step [563/600], Loss: 0.3774, Train Acc:0.8298\n",
      "Epoch [10/10], Step [564/600], Loss: 0.6278, Train Acc:0.8298\n",
      "Epoch [10/10], Step [565/600], Loss: 0.4955, Train Acc:0.8298\n",
      "Epoch [10/10], Step [566/600], Loss: 0.5703, Train Acc:0.8298\n",
      "Epoch [10/10], Step [567/600], Loss: 0.4614, Train Acc:0.8298\n",
      "Epoch [10/10], Step [568/600], Loss: 0.6940, Train Acc:0.8295\n",
      "Epoch [10/10], Step [569/600], Loss: 0.4983, Train Acc:0.8295\n",
      "Epoch [10/10], Step [570/600], Loss: 0.6641, Train Acc:0.8295\n",
      "Epoch [10/10], Step [571/600], Loss: 0.6130, Train Acc:0.8294\n",
      "Epoch [10/10], Step [572/600], Loss: 0.5172, Train Acc:0.8295\n",
      "Epoch [10/10], Step [573/600], Loss: 0.5539, Train Acc:0.8294\n",
      "Epoch [10/10], Step [574/600], Loss: 0.5510, Train Acc:0.8294\n",
      "Epoch [10/10], Step [575/600], Loss: 0.4798, Train Acc:0.8295\n",
      "Epoch [10/10], Step [576/600], Loss: 0.6111, Train Acc:0.8294\n",
      "Epoch [10/10], Step [577/600], Loss: 0.5531, Train Acc:0.8294\n",
      "Epoch [10/10], Step [578/600], Loss: 0.5703, Train Acc:0.8293\n",
      "Epoch [10/10], Step [579/600], Loss: 0.5082, Train Acc:0.8293\n",
      "Epoch [10/10], Step [580/600], Loss: 0.5626, Train Acc:0.8292\n",
      "Epoch [10/10], Step [581/600], Loss: 0.4552, Train Acc:0.8292\n",
      "Epoch [10/10], Step [582/600], Loss: 0.5072, Train Acc:0.8292\n",
      "Epoch [10/10], Step [583/600], Loss: 0.5039, Train Acc:0.8292\n",
      "Epoch [10/10], Step [584/600], Loss: 0.4586, Train Acc:0.8292\n",
      "Epoch [10/10], Step [585/600], Loss: 0.4321, Train Acc:0.8292\n",
      "Epoch [10/10], Step [586/600], Loss: 0.5214, Train Acc:0.8293\n",
      "Epoch [10/10], Step [587/600], Loss: 0.4478, Train Acc:0.8294\n",
      "Epoch [10/10], Step [588/600], Loss: 0.4319, Train Acc:0.8294\n",
      "Epoch [10/10], Step [589/600], Loss: 0.4886, Train Acc:0.8294\n",
      "Epoch [10/10], Step [590/600], Loss: 0.4114, Train Acc:0.8294\n",
      "Epoch [10/10], Step [591/600], Loss: 0.4706, Train Acc:0.8295\n",
      "Epoch [10/10], Step [592/600], Loss: 0.6932, Train Acc:0.8294\n",
      "Epoch [10/10], Step [593/600], Loss: 0.5637, Train Acc:0.8294\n",
      "Epoch [10/10], Step [594/600], Loss: 0.5423, Train Acc:0.8294\n",
      "Epoch [10/10], Step [595/600], Loss: 0.5483, Train Acc:0.8293\n",
      "Epoch [10/10], Step [596/600], Loss: 0.6536, Train Acc:0.8292\n",
      "Epoch [10/10], Step [597/600], Loss: 0.5054, Train Acc:0.8292\n",
      "Epoch [10/10], Step [598/600], Loss: 0.4255, Train Acc:0.8292\n",
      "Epoch [10/10], Step [599/600], Loss: 0.4632, Train Acc:0.8292\n",
      "Epoch [10/10], Step [600/600], Loss: 0.4544, Train Acc:0.8293\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #load data to device (dataloader했기 때문에)\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward, loss 계산\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward, 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #로그 출력, acc \n",
    "        if (i+1)%1 ==0:\n",
    "            loss_list.append(loss.item())\n",
    "            _, preodicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preodicted == labels).sum().item()\n",
    "            acc_list.append(correct/total)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}, Train Acc:{acc_list[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG1CAYAAAAFuNXgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8iUlEQVR4nO3dd3hT9dsG8PskbboHLV1AoWWvllGGbGSDIkMFAQVE8AeCiooC+iIKIqCgiCioKEsQF0t2KUvK3quyaRltKaN7J+f947Rp0qZtkma06f25rlxNznxyCDlPvlMQRVEEERERkY2QWTsAIiIiIlNickNEREQ2hckNERER2RQmN0RERGRTmNwQERGRTWFyQ0RERDaFyQ0RERHZFCY3REREZFOY3BAREZFNYXJDRERENsWqyc3cuXPRunVruLm5wdfXFwMHDsSVK1dK3GflypUQBEHr4ejoaKGIiYiIqLyzanJz4MABTJw4EUePHkV4eDhycnLQq1cvpKWllbifu7s7YmNj1Y/o6GgLRUxERETlnZ01T75z506t1ytXroSvry9OnTqFzp07F7ufIAjw9/c36pwqlQr379+Hm5sbBEEw6hhERERkWaIoIiUlBdWqVYNMVnLZjFWTm8KSkpIAAF5eXiVul5qailq1akGlUqFly5b4/PPP0aRJE53bZmVlISsrS/363r17aNy4semCJiIiIou5c+cOatSoUeI2giiKooXiKZFKpcJzzz2HxMREHDp0qNjtjhw5gmvXriE0NBRJSUlYsGABDh48iEuXLul8s5988gk+/fTTIsvv3LkDd3d3k74HIiIiMo/k5GQEBgYiMTERHh4eJW5bbpKbCRMmYMeOHTh06FCpGZmmnJwcNGrUCMOGDcPs2bOLrC9ccpN/cZKSkpjcEBERVRDJycnw8PDQ6/5dLqqlJk2ahK1bt+LgwYMGJTYAYG9vjxYtWuD69es61zs4OMDBwcEUYRIREVEFYNXeUqIoYtKkSdi4cSP27t2L4OBgg4+hVCpx4cIFBAQEmCFCIiIiqmisWnIzceJErFu3Dps3b4abmxvi4uIAAB4eHnBycgIAjBw5EtWrV8fcuXMBALNmzcJTTz2FunXrIjExEV9++SWio6MxduxYq70PIiIiKj+smtwsXboUANC1a1et5StWrMDo0aMBADExMVpdvp48eYJx48YhLi4OVapUQVhYGA4fPsweUEREJVAqlcjJybF2GEQlUigUpXbz1ke5aVBsKYY0SCIiquhEUURcXBwSExOtHQpRqWQyGYKDg6FQKIqsq3ANiomIyDzyExtfX184Oztz8FIqt/IH2Y2NjUXNmjXL9FllckNEZKOUSqU6sfH29rZ2OESl8vHxwf3795Gbmwt7e3ujj8NZwYmIbFR+GxtnZ2crR0Kkn/zqKKVSWabjMLkhIrJxrIqiisJUn1UmN0RERGRTmNwQEVGlEBQUhEWLFum9/f79+yEIAnuaVUBMboiIqFwRBKHExyeffGLUcU+cOIHXX39d7+3bt2+P2NjYUidpLCsmUabH3lImkpWrxMPUbAgAqnk6WTscIqIKKzY2Vv38999/x8cff4wrV66ol7m6uqqfi6IIpVIJO7vSb2c+Pj4GxaFQKODv72/QPlQ+sOTGRC7eS0KHeXsx7Kej1g6FiKhC8/f3Vz88PDwgCIL69X///Qc3Nzfs2LEDYWFhcHBwwKFDh3Djxg0MGDAAfn5+cHV1RevWrbFnzx6t4xaulhIEAcuXL8egQYPg7OyMevXqYcuWLer1hUtUVq5cCU9PT+zatQuNGjWCq6sr+vTpo5WM5ebm4q233oKnpye8vb0xdepUjBo1CgMHDjT6ejx58gQjR45ElSpV4OzsjL59++LatWvq9dHR0ejfvz+qVKkCFxcXNGnSBNu3b1fvO2LECPj4+MDJyQn16tXDihUrjI6lomByYyKyvBbeSlWlGvCZiCoYURSRnp1rlYcpB8SfNm0a5s2bh6ioKISGhiI1NRX9+vVDREQEzpw5gz59+qB///6IiYkp8TiffvophgwZgvPnz6Nfv34YMWIEHj9+XOz26enpWLBgAdasWYODBw8iJiYGU6ZMUa+fP38+1q5dixUrViAyMhLJycnYtGlTmd7r6NGjcfLkSWzZsgVHjhyBKIro16+fuqv/xIkTkZWVhYMHD+LChQuYP3++unRrxowZuHz5Mnbs2IGoqCgsXboUVatWLVM8FQGrpUxELpOSGxWTGyIqxzJylGj88S6rnPvyrN5wVpjmtjNr1iz07NlT/drLywvNmjVTv549ezY2btyILVu2YNKkScUeZ/To0Rg2bBgA4PPPP8fixYtx/Phx9OnTR+f2OTk5WLZsGerUqQMAmDRpEmbNmqVe/+2332L69OkYNGgQAGDJkiXqUhRjXLt2DVu2bEFkZCTat28PAFi7di0CAwOxadMmvPjii4iJicHzzz+PkJAQAEDt2rXV+8fExKBFixZo1aoVAKn0qjJgyY2J5Jfc5DK5ISIyu/ybdb7U1FRMmTIFjRo1gqenJ1xdXREVFVVqyU1oaKj6uYuLC9zd3fHgwYNit3d2dlYnNgAQEBCg3j4pKQnx8fFo06aNer1cLkdYWJhB701TVFQU7Ozs0LZtW/Uyb29vNGjQAFFRUQCAt956C5999hk6dOiAmTNn4vz58+ptJ0yYgPXr16N58+b44IMPcPjwYaNjqUhYcmMi+SU3D1KyrBwJEVHxnOzluDyrt9XObSouLi5ar6dMmYLw8HAsWLAAdevWhZOTE1544QVkZ2eXeJzCQ/wLggCVSmXQ9taef3rs2LHo3bs3tm3bht27d2Pu3LlYuHAh3nzzTfTt2xfR0dHYvn07wsPD0b17d0ycOBELFiywaszmxpIbE9EcVDEpPcd6gRARlUAQBDgr7KzyMOdIyZGRkRg9ejQGDRqEkJAQ+Pv74/bt22Y7ny4eHh7w8/PDiRMn1MuUSiVOnz5t9DEbNWqE3NxcHDt2TL3s0aNHuHLlCho3bqxeFhgYiPHjx2PDhg1477338NNPP6nX+fj4YNSoUfj111+xaNEi/Pjjj0bHU1Gw5MZE0rML5sF4lJYFD2fjJ/wiIiLD1KtXDxs2bED//v0hCAJmzJhRYgmMubz55puYO3cu6tati4YNG+Lbb7/FkydP9ErsLly4ADc3N/VrQRDQrFkzDBgwAOPGjcMPP/wANzc3TJs2DdWrV8eAAQMAAJMnT0bfvn1Rv359PHnyBPv27UOjRo0AAB9//DHCwsLQpEkTZGVlYevWrep1tozJjYmEVC8Y5On3E3cwvZ/tf3iIiMqLr776CmPGjEH79u1RtWpVTJ06FcnJyRaPY+rUqYiLi8PIkSMhl8vx+uuvo3fv3pDLS6+S69y5s9ZruVyO3NxcrFixAm+//TaeffZZZGdno3Pnzti+fbu6ikypVGLixIm4e/cu3N3d0adPH3z99dcApLF6pk+fjtu3b8PJyQmdOnXC+vXrTf/GyxlBtHZloYUlJyfDw8MDSUlJcHd3N+mxg6ZtAwAMbRWI+S+ElrI1EZF5ZWZm4tatWwgODoajo6O1w6mUVCoVGjVqhCFDhmD27NnWDqfcK+kza8j9myU3ZqCsXPkiERHliY6Oxu7du9GlSxdkZWVhyZIluHXrFoYPH27t0CoVNig2A3dHtrchIqqMZDIZVq5cidatW6NDhw64cOEC9uzZUynauZQnLLkxg7BaVawdAhERWUFgYCAiIyOtHUalx5IbE+pQ1xsAkGuFFvpEREQkYXJjQnYy6XLmKNnmhoiIyFqY3JhQ/qSZCRylmIiIyGqY3JjQoesPAQDzd/5n5UiIiIgqLyY3REREZFOY3BAREZFNYXJjQvlTMFR1dbByJEREVFhQUBAWLVqk9/b79++HIAhITEw0W0xkHkxuTGhA82oAgI55XcKJiMhwgiCU+Pjkk0+MOu6JEyfw+uuv6719+/btERsbCw8Pj9I3NpGGDRvCwcEBcXFxFjunLWJyY0L5s76yIzgRkfFiY2PVj0WLFsHd3V1r2ZQpU9TbiqKI3NxcvY7r4+MDZ2dnveNQKBTw9/fXa0ZvUzh06BAyMjLwwgsvYNWqVRY5Z0lycnKsHYLRmNyYUP7HX8XshojIaP7+/uqHh4cHBEFQv/7vv//g5uaGHTt2ICwsDA4ODjh06BBu3LiBAQMGwM/PD66urmjdujX27NmjddzC1VKCIGD58uUYNGgQnJ2dUa9ePWzZskW9vnC11MqVK+Hp6Yldu3ahUaNGcHV1RZ8+fRAbG6veJzc3F2+99RY8PT3h7e2NqVOnYtSoURg4cGCp7/vnn3/G8OHD8corr+CXX34psv7u3bsYNmwYvLy84OLiglatWuHYsWPq9f/88w9at24NR0dHVK1aFYMGDdJ6r5s2bdI6nqenJ1auXAkAuH37NgRBwO+//44uXbrA0dERa9euxaNHjzBs2DBUr14dzs7OCAkJwW+//aZ1HJVKhS+++AJ169aFg4MDatasiTlz5gAAunXrhkmTJmltn5CQAIVCgYiIiFKvibGY3JiQLC+7qWQTrRNRRSKKQHaadR4m/G6cNm0a5s2bh6ioKISGhiI1NRX9+vVDREQEzpw5gz59+qB///6IiYkp8TiffvophgwZgvPnz6Nfv34YMWIEHj9+XOz26enpWLBgAdasWYODBw8iJiZGqyRp/vz5WLt2LVasWIHIyEgkJycXSSp0SUlJwZ9//omXX34ZPXv2RFJSEv7991/1+tTUVHTp0gX37t3Dli1bcO7cOXzwwQdQ5Y2Iv23bNgwaNAj9+vXDmTNnEBERgTZt2pR63sKmTZuGt99+G1FRUejduzcyMzMRFhaGbdu24eLFi3j99dfxyiuv4Pjx4+p9pk+fjnnz5mHGjBm4fPky1q1bBz8/PwDA2LFjsW7dOmRlFYz/9uuvv6J69ero1q2bwfHpi3NLmZC6Woq5DRGVVznpwOfVrHPuD+8DCheTHGrWrFno2bOn+rWXlxeaNWumfj179mxs3LgRW7ZsKVJyoGn06NEYNmwYAODzzz/H4sWLcfz4cfTp00fn9jk5OVi2bBnq1KkDAJg0aRJmzZqlXv/tt99i+vTp6lKTJUuWYPv27aW+n/Xr16NevXpo0qQJAOCll17Czz//jE6dOgEA1q1bh4SEBJw4cQJeXl4AgLp166r3nzNnDl566SV8+umn6mWa10NfkydPxuDBg7WWaSZvb775Jnbt2oU//vgDbdq0QUpKCr755hssWbIEo0aNAgDUqVMHHTt2BAAMHjwYkyZNwubNmzFkyBAAUgnY6NGjzVrdx5IbE8r/d0rOrLj1lEREFUGrVq20XqempmLKlClo1KgRPD094erqiqioqFJLbkJDQ9XPXVxc4O7ujgcPHhS7vbOzszqxAYCAgAD19klJSYiPj9cqMZHL5QgLCyv1/fzyyy94+eWX1a9ffvll/Pnnn0hJSQEAnD17Fi1atFAnNoWdPXsW3bt3L/U8pSl8XZVKJWbPno2QkBB4eXnB1dUVu3btUl/XqKgoZGVlFXtuR0dHrWq206dP4+LFixg9enSZYy0JS25MaN9/0gf832sPrRwJEVEx7J2lEhRrndtEXFy0S4CmTJmC8PBwLFiwAHXr1oWTkxNeeOEFZGdnlxySvb3Wa0EQ1FU9+m5f1qYIly9fxtGjR3H8+HFMnTpVvVypVGL9+vUYN24cnJycSjxGaet1xamrwXDh6/rll1/im2++waJFixASEgIXFxdMnjxZfV1LOy8gVU01b94cd+/exYoVK9CtWzfUqlWr1P3KgiU3JrTvSoK1QyAiKpkgSFVD1niYsRoiMjISo0ePxqBBgxASEgJ/f3/cvn3bbOfTxcPDA35+fjhx4oR6mVKpxOnTp0vc7+eff0bnzp1x7tw5nD17Vv1499138fPPPwOQSpjOnj1bbHug0NDQEhvo+vj4aDV8vnbtGtLT00t9T5GRkRgwYABefvllNGvWDLVr18bVq1fV6+vVqwcnJ6cSzx0SEoJWrVrhp59+wrp16zBmzJhSz1tWTG6IiKjCq1evHjZs2ICzZ8/i3LlzGD58eIklMOby5ptvYu7cudi8eTOuXLmCt99+G0+ePCm2fUlOTg7WrFmDYcOGoWnTplqPsWPH4tixY7h06RKGDRsGf39/DBw4EJGRkbh58yb+/vtvHDlyBAAwc+ZM/Pbbb5g5cyaioqJw4cIFzJ8/X32ebt26YcmSJThz5gxOnjyJ8ePHFymF0qVevXoIDw/H4cOHERUVhf/973+Ij49Xr3d0dMTUqVPxwQcfYPXq1bhx4waOHj2qTsryjR07FvPmzYMoilq9uMyFyY0JfdK/sbVDICKqlL766itUqVIF7du3R//+/dG7d2+0bNnS4nFMnToVw4YNw8iRI9GuXTu4urqid+/ecHR01Ln9li1b8OjRI503/EaNGqFRo0b4+eefoVAosHv3bvj6+qJfv34ICQnBvHnzIJfLAQBdu3bFn3/+iS1btqB58+bo1q2bVo+mhQsXIjAwEJ06dcLw4cMxZcoUvcb8+b//+z+0bNkSvXv3RteuXdUJlqYZM2bgvffew8cff4xGjRph6NChRdotDRs2DHZ2dhg2bFix18KUBLGS9VtOTk6Gh4cHkpKS4O7ubtJjn4p+gueXHkZNL2cc/OBpkx6biMhQmZmZuHXrFoKDgy1yQ6GiVCoVGjVqhCFDhmD27NnWDsdqbt++jTp16uDEiRMlJp0lfWYNuX+zQbEJKeRSQViO0vJFoUREZH3R0dHYvXs3unTpgqysLCxZsgS3bt3C8OHDrR2aVeTk5ODRo0f4v//7Pzz11FMWK01jtZQJKeyky5mdy+SGiKgykslkWLlyJVq3bo0OHTrgwoUL2LNnDxo1amTt0KwiMjISAQEBOHHiBJYtW2ax87LkxoTs5VKDsWyW3BARVUqBgYGIjIy0dhjlRteuXa0yaj9LbkyIJTdERETWx+TGhNjmhojKo0rWb4QqMFN9VpncmFB+yY1KBHKZ4BCRleWPY6LPYG1E5UH+yMf5XdyNxTY3JmQvL8gVc5Qi7Mr2b0NEVCZyuRyenp7qMUecnZ3NOlkhUVmoVCokJCTA2dkZdnZlS0+Y3JhQfskNILW7cVIwuyEi6/L39weAEieDJCovZDIZatasWeYknMmNCdnJCv4x2GOKiMoDQRAQEBAAX19fnRMlEpUnCoUCMlnZW8wwuTEhQRCgsJMhO1fF5IaIyhW5XF7mdgxEFQUbFJtYfo8pdgcnIiKyDiY3Jpbf7obdwYmIiKyDyY2JqUcpZskNERGRVTC5MTGHvP7fWblKK0dCRERUOTG5MTF3J6mNdnJGrpUjISIiqpyY3JiYh5M0ImhSBrtcEhERWQOTGxNzZLUUERGRVTG5MTG7vAbFOUpOVEdERGQNTG5MzJ4zgxMREVkVkxsTy5+CQaliyQ0REZE1MLkxsfycJovj3BAREVkFkxsT23LuPgDgy11XrBwJERFR5cTkhoiIiGwKkxsiIiKyKUxuiIiIyKZYNbmZO3cuWrduDTc3N/j6+mLgwIG4cqX0tip//vknGjZsCEdHR4SEhGD79u0WiJaIiIgqAqsmNwcOHMDEiRNx9OhRhIeHIycnB7169UJaWlqx+xw+fBjDhg3Da6+9hjNnzmDgwIEYOHAgLl68aMHIiYiIqLwSRFEsNwOyJCQkwNfXFwcOHEDnzp11bjN06FCkpaVh69at6mVPPfUUmjdvjmXLlpV6juTkZHh4eCApKQnu7u4miz1f0LRt6ue35z1j8uMTERFVRobcv8tVm5ukpCQAgJeXV7HbHDlyBD169NBa1rt3bxw5ckTn9llZWUhOTtZ6EBERke0qN8mNSqXC5MmT0aFDBzRt2rTY7eLi4uDn56e1zM/PD3FxcTq3nzt3Ljw8PNSPwMBAk8ZNRERE5Uu5SW4mTpyIixcvYv369SY97vTp05GUlKR+3Llzx6THJyIiovKlXCQ3kyZNwtatW7Fv3z7UqFGjxG39/f0RHx+vtSw+Ph7+/v46t3dwcIC7u7vWw5xmDWhi1uMTERFRyaya3IiiiEmTJmHjxo3Yu3cvgoODS92nXbt2iIiI0FoWHh6Odu3amStMg7SqJbUX8nFzsHIkRERElZOdNU8+ceJErFu3Dps3b4abm5u63YyHhwecnJwAACNHjkT16tUxd+5cAMDbb7+NLl26YOHChXjmmWewfv16nDx5Ej/++KPV3ocmed6s4CrOCk5ERGQVVi25Wbp0KZKSktC1a1cEBASoH7///rt6m5iYGMTGxqpft2/fHuvWrcOPP/6IZs2a4a+//sKmTZtKbIRsSfK8K6osPz3siYiIKhWrltzoM8TO/v37iyx78cUX8eKLL5ohorITBJbcEBERWVO5aFBsS+T5yQ1zGyIiIqtgcmNi+W1ulMxuiIiIrILJjYnJ8hsUs80NERGRVTC5MbG83IbJDRERkZUwuTGx/DY3OUpRrwbTREREZFpMbkwsv1oKAP45H1vClkRERGQOTG5MLL/kBgAu3U+yYiRERESVE5MbE5NpJDfxSZlWjISIiKhyYnJjYoLGFX2Ulm29QIiIiCopJjcmplkt9UxIgBUjISIiqpyY3JiYi0PBjBaujlad3YKIiKhSYnJjBh3qegPgKMVERETWwOTGDPIbFTO5ISIisjwmN2aQP79ULpMbIiIii2NyYwb7ryQAAI7efGTlSIiIiCofJjdmtOH0PWuHQEREVOkwuSEiIiKbwuTGDDrX9wEAvBhWw8qREBERVT5MbswgtLoHAO0xb4iIiMgymNyYgZ1c6i2Vo1RZORIiIqLKh8mNGdjLpcvK5IaIiMjymNyYgV3eODf78rqEExERkeUwuTGD/HFuElKyrBwJERFR5cPkxgxiHqern5+KfmzFSIiIiCofJjdmkK3R1uby/WQrRkJERFT5MLkxg2BvF/VzBzu5FSMhIiKqfJjcmMGkbnULXgjWi4OIiKgyYnJjBk6KgtIamcDshoiIyJKY3JiBUiVaOwQiIqJKi8mNGahEUedzIiIiMj8mN2ag0hiY+NMtl6wXCBERUSXE5MYMXBwK2tykZSvxMJWD+REREVkKkxszaB7oqfX63J1Eq8RBRERUGTG5MQNBEBDk7ax+zWY3RERElsPkxkwEjS7gzG2IiIgsh8mNmdx6mKZ+LrLohoiIyGKY3FgAUxsiIiLLYXJjASy4ISIishwmNxbB7IaIiMhSmNxYAEtuiIiILIfJjQUwtyEiIrIcJjcWwJIbIiIiy2FyYwEiy26IiIgshsmNBbDkhoiIyHKY3FgAcxsiIiLLYXJjARyhmIiIyHKY3JhJz8Z+6uff7btuxUiIiIgqFyY3ZtKqVhX186vxqVaMhIiIqHJhcmMmHetVtXYIRERElRKTGzOp7+dm7RCIiIgqJSY3REREZFOY3JiJXBCsHQIREVGlxOTGTGQy7eQmV6myUiRERESVC5MbC3mUlm3tEIiIiCoFJjcWsiLytrVDICIiqhSY3FjIsgM3rB0CERFRpcDkhoiIiGwKkxsiIiKyKUYlN126dMHq1auRkZFh6niIiIiIysSo5KZFixaYMmUK/P39MW7cOBw9etTUcREREREZxajkZtGiRbh//z5WrFiBBw8eoHPnzmjcuDEWLFiA+Ph4U8dIREREpDej29zY2dlh8ODB2Lx5M+7evYvhw4djxowZCAwMxMCBA7F3715TxklERESklzI3KD5+/DhmzpyJhQsXwtfXF9OnT0fVqlXx7LPPYsqUKaaIkYiIiEhvRiU3Dx48wMKFC9G0aVN06tQJCQkJ+O2333D79m18+umnWL58OXbv3o1ly5aVeJyDBw+if//+qFatGgRBwKZNm0rcfv/+/RAEocgjLi7OmLdBRERENsjOmJ1q1KiBOnXqYMyYMRg9ejR8fHyKbBMaGorWrVuXeJy0tDQ0a9YMY8aMweDBg/U+/5UrV+Du7q5+7evrq3/wREREZNOMSm4iIiLQqVOnErdxd3fHvn37Stymb9++6Nu3r8Hn9/X1haenp8H7ERERke0zqlqqRo0auHbtWpHl165dw+3bt8saU6maN2+OgIAA9OzZE5GRkSVum5WVheTkZK0HERER2S6jkpvRo0fj8OHDRZYfO3YMo0ePLmtMxQoICMCyZcvw999/4++//0ZgYCC6du2K06dPF7vP3Llz4eHhoX4EBgaaLT4iIiKyPkEURdHQndzd3XH69GnUrVtXa/n169fRqlUrJCYmGh6IIGDjxo0YOHCgQft16dIFNWvWxJo1a3Suz8rKQlZWlvp1cnIyAgMDkZSUpNVuxxyCpm3Tev3PpI4IqeFh1nMSERHZouTkZHh4eOh1/zaq5EYQBKSkpBRZnpSUBKVSacwhjdamTRtcv3692PUODg5wd3fXeljLoO9LrkIjIiKisjMquencuTPmzp2rlcgolUrMnTsXHTt2NFlw+jh79iwCAgIsek59jWpXS+t1rsrgQjIiIiIykFG9pebPn4/OnTujQYMG6l5T//77L5KTkw0amTg1NVWr1OXWrVs4e/YsvLy8ULNmTUyfPh337t3D6tWrAUjTPgQHB6NJkybIzMzE8uXLsXfvXuzevduYt2F2zzWvhlVHoq0dBhERUaViVHLTuHFjnD9/HkuWLMG5c+fg5OSEkSNHYtKkSfDy8tL7OCdPnsTTTz+tfv3uu+8CAEaNGoWVK1ciNjYWMTEx6vXZ2dl47733cO/ePTg7OyM0NBR79uzROkZ5Yi8v8wDQREREZCCjGhRXZIY0SCorlUpE7Q+3ay27Pe8Zs56TiIjIFhly/zaq5CZfeno6YmJikJ2drbU8NDS0LIe1GTKZYO0QiIiIKh2jkpuEhAS8+uqr2LFjh871lu4xRURERJTPqEYhkydPRmJiIo4dOwYnJyfs3LkTq1atQr169bBlyxZTx0hERESkN6NKbvbu3YvNmzejVatWkMlkqFWrFnr27Al3d3fMnTsXzzzDdiXFSc7MgbujvbXDICIisllGldykpaWpZ+KuUqUKEhISAAAhISElToVAQMd5+neVJyIiIsMZldw0aNAAV65cAQA0a9YMP/zwA+7du4dly5aV2wH1yovkzFxrh0BERGTTjEpu3n77bcTGxgIAZs6ciR07dqBmzZpYvHgxPv/8c5MGWNH9Nb6dtUMgIiKqVIxqc/Pyyy+rn4eFhSE6Ohr//fcfatasiapVq5osOFvQKkj/QQ2JiIio7AwuucnJyUGdOnUQFRWlXubs7IyWLVsysSEiIiKrMzi5sbe3R2ZmpjliISIiIiozo9rcTJw4EfPnz0duLhvHEhERUfliVJubEydOICIiArt370ZISAhcXFy01m/YsMEkwdmqpIwceDhxrBsiIiJzMCq58fT0xPPPP2/qWCqNL3f9h88Ghlg7DCIiIptkVHKzYsUKU8dRqdx5nGHtEIiIiGyWUW1uiIiIiMoro0pugoODIQhCsetv3rxpdECVwYGrCdYOgYiIyGYZldxMnjxZ63VOTg7OnDmDnTt34v333zdFXDalRhUn3H3CqigiIiJLMCq5efvtt3Uu/+6773Dy5MkyBWSL9rzbBQ1n7NRadj8xA9U8nawUERERke0yaZubvn374u+//zblIW2Co728yLL28/Zi16U4K0RDRERk20ya3Pz111/w8uJcSvr64cANa4dARERkc4yqlmrRooVWg2JRFBEXF4eEhAR8//33JgvO1onWDoCIiMgGGZXcDBw4UOu1TCaDj48PunbtioYNG5oirkpBZHZDRERkckYlNzNnzjR1HJWSyOyGiIjI5Ixqc7N9+3bs2rWryPJdu3Zhx44dZQ6qslAxtyEiIjI5o5KbadOmQalUFlkuiiKmTZtW5qAqC5GtboiIiEzOqOTm2rVraNy4cZHlDRs2xPXr18scVGXBWikiIiLTMyq58fDw0DnFwvXr1+Hi4lLmoCqLS/eTrR0CERGRzTEquRkwYAAmT56MGzcKxmm5fv063nvvPTz33HMmC64ySM7MsXYIRERENsWo5OaLL76Ai4sLGjZsiODgYAQHB6NRo0bw9vbGggULTB2jTYtLyrR2CERERDbFqK7gHh4eOHz4MMLDw3Hu3Dk4OTkhNDQUnTt3NnV8Ni9XyYY3REREpmRUcgMAgiCgV69e6NWrlynjsVn2cgE5OhIZuUzQsTUREREZy6hqqbfeeguLFy8usnzJkiWYPHlyWWOyScWNaRMVm8zB/IiIiEzIqOTm77//RocOHYosb9++Pf76668yB2WLVMUkMJN/P4udFzk7OBERkakYldw8evQIHh4eRZa7u7vj4cOHZQ7KFpVUOLPxzD3LBUJERGTjjEpu6tati507dxZZvmPHDtSuXbvMQdkiDyf7YtexUoqIiMh0jGpQ/O6772LSpElISEhAt27dAAARERFYuHAhFi1aZMr4bEaglxOS7nFMGyIiInMzKrkZM2YMsrKyMGfOHMyePRsAEBQUhKVLl2LkyJEmDdBWsM0wERGRZRjdFXzChAmYMGECEhIS4OTkBFdXVwDA48eP4eXlZbIAiYiIiAxhVJsbTT4+PnB1dcXu3bsxZMgQVK9e3RRxVSoxj9KtHQIREZHNKFNyEx0djZkzZyIoKAgvvvgiZDIZVq9ebarYbEpJ1VJX4lMwdtUJXI1PsVxARERENsrgaqns7Gxs2LABy5cvR2RkJHr06IG7d+/izJkzCAkJMUeMNqFtbS9cji1+FvA9UQ9w7m4STnzUw4JRERER2R6DSm7efPNNVKtWDd988w0GDRqEu3fv4p9//oEgCJDL5eaK0Sa837tBqdskpGRZIBIiIiLbZlDJzdKlSzF16lRMmzYNbm5u5orJJjkrjG67TURERAYwqORmzZo1OH78OAICAjB06FBs3boVSqXSXLHZnFeeqmXtEIiIiGyeQcnNsGHDEB4ejgsXLqBhw4aYOHEi/P39oVKpcPnyZXPFaDNmD2xq7RCIiIhsnlG9pYKDg/Hpp5/i9u3b+PXXX/H888/j5ZdfRo0aNfDWW2+ZOkab8uULodYOgYiIyKaVqSu4IAjo3bs3/vjjD9y/fx9TpkzBgQMHTBWbTXqxVSDe6lbX2mEQERHZLIOSm06dOmHBggW4evVqkXVeXl6YPHkyzp07Z7LgbJYgFLvqSVo2cpUqCwZDRERkWwxKbsaNG4cjR44gLCwMjRo1wtSpUxEZGQmREycZxN2x+J5TLWaHo/+SSAtGQ0REZFsMSm5GjhyJv//+Gw8fPsTChQuRmJiIF198Ef7+/hgzZgw2bdqEjIwMc8VqM0a0LbnXVFQJg/0RERFRyYxqc+Pg4IB+/frhhx9+wP3797FlyxYEBARgxowZ8Pb2xrPPPovISJY+FMdJwQEPiYiIzKXME2cCQNu2bTFnzhxcuHABFy5cQPfu3REbG2uKQ1d62bkqfLz5IiKi4q0dChERUYVgVHJz584d3L17V/36+PHjmDx5Mn788UfUqVMH77zzDl544QWTBVkZ9f3mX2RkK7H2WDRWH4nGa6tOIiIqHtGP0nDkxiM8vWA/Dl9/aO0wiYiIyh2jkpvhw4dj3759AIC4uDj06NEDx48fx0cffYRZs2aZNMDKKio2GZvP3sP9xII2TK+tOokuX+7HsJ+O4tbDNAxffsyKERIREZVPRiU3Fy9eRJs2bQAAf/zxB0JCQnD48GGsXbsWK1euNGV8ldq0DReQkplr7TCIiIgqFKOSm5ycHDg4OAAA9uzZg+eeew4A0LBhQ7a1MbH1J+5YOwQiIqIKxajkpkmTJli2bBn+/fdfhIeHo0+fPgCA+/fvw9vb26QBEhERERnCqORm/vz5+OGHH9C1a1cMGzYMzZo1AwBs2bJFXV1FREREZA3FD5Vbgq5du+Lhw4dITk5GlSpV1Mtff/11ODs7myw4IiIiIkMZVXKTkZGBrKwsdWITHR2NRYsW4cqVK/D19TVpgERERESGMCq5GTBgAFavXg0ASExMRNu2bbFw4UIMHDgQS5cuNWmARERERIYwKrk5ffo0OnXqBAD466+/4Ofnh+joaKxevRqLFy82aYBEREREhjAquUlPT4ebmxsAYPfu3Rg8eDBkMhmeeuopREdHmzRAIiIiIkMYldzUrVsXmzZtwp07d7Br1y706tULAPDgwQO4u7ubNECyHFEU8dHGC1j+701rh0JERGQ0o5Kbjz/+GFOmTEFQUBDatGmDdu3aAZBKcVq0aKH3cQ4ePIj+/fujWrVqEAQBmzZtKnWf/fv3o2XLlnBwcEDdunU5IrKGlMwc3H2SbvT+p2OeYO2xGHy2LcqEUREREVmWUcnNCy+8gJiYGJw8eRK7du1SL+/evTu+/vprvY+TlpaGZs2a4bvvvtNr+1u3buGZZ57B008/jbNnz2Ly5MkYO3asVgyVzaPULPXzlrPD0XH+Ptx5bFyCk5qlNFVYREREVmPUODcA4O/vD39/f/Xs4DVq1DB4AL++ffuib9++em+/bNkyBAcHY+HChQCARo0a4dChQ/j666/Ru3dvg85tK8I+24MJXetgSq8GyFGKAIATtx8j0IvjDRERUeVkVMmNSqXCrFmz4OHhgVq1aqFWrVrw9PTE7NmzoVKpTB2j2pEjR9CjRw+tZb1798aRI0eK3ScrKwvJyclaD1uzdP8N/H3qrvq1KJb9mNm5KpyKfoxcpfn+PYmIiMzBqOTmo48+wpIlSzBv3jycOXMGZ86cweeff45vv/0WM2bMMHWManFxcfDz89Na5ufnh+TkZGRkZOjcZ+7cufDw8FA/AgMDzRafNd1L1P3+8/1z7j5GrziOxPRsvY73/l/n8PzSI/hi1xWj4klKz8GTNP3ORUREZEpGJTerVq3C8uXLMWHCBISGhiI0NBRvvPEGfvrpp3LXwHf69OlISkpSP+7cqZyzbL/52xnsv5KARXuu6bX95rP3AQA/HjS855RSJaLZrN1oMTscmTlsx0NERJZlVHLz+PFjNGzYsMjyhg0b4vHjx2UOqjj+/v6Ij4/XWhYfHw93d3c4OTnp3MfBwQHu7u5aj/LkxbAaJjnONxH6JS1P9Cy5KYsMjYTmoUaDZyIiIkswKrlp1qwZlixZUmT5kiVLEBoaWuagitOuXTtERERoLQsPD1d3Ra9IGgVISdagFtVNfuz8JjeHrz/EkGVHcP1BinqdYPKz6Ti/KRr92LCr8SlYdfg22zMREZmJUb2lvvjiCzzzzDPYs2ePOrE4cuQI7ty5g+3bt+t9nNTUVFy/fl39+tatWzh79iy8vLxQs2ZNTJ8+Hffu3VPPYzV+/HgsWbIEH3zwAcaMGYO9e/fijz/+wLZt24x5G1a1eWIHJKRmwctZYZbji6KI4cuPAQBeX3PKLOfQhyBYIp2qWHp9fRCA9G80ukOwlaMhIrI9RpXcdOnSBVevXsWgQYOQmJiIxMREDB48GJcuXcKaNWv0Ps7JkyfRokUL9cB/7777Llq0aIGPP/4YABAbG4uYmBj19sHBwdi2bRvCw8PRrFkzLFy4EMuXL6+Q3cAVdjJU93SCOe79Gdm5eHrBfvXrhykFVUOb8trSkPWdv5dk7RCIiGyS0ePcVKtWDXPmzNFadu7cOfz888/48ccf9TpG165dS6zC0NU4uWvXrjhz5oxBsVY2W8/H4vaj0gfyE0WRJStERGRzjCq5IdMxR26hT4uXsatO4oVlR6BSmb59jOYRmToREZGlMbmxMgc7ucmPefyWdo+15MxcrdeiKGJPVDxORT/B9YRUk59fszDuVPQTkx+fiIioJExuKqGIqAfq55qJSEa27jFpsnKV2HTmHhJSDO/W/eZvrEIsjsByLSIiszCozc3gwYNLXJ+YmFiWWCqtla+2xugVJyx2vrGrT+pc/suhWzqXfxV+FT8cuIkaVZxwaGo39fLl/97EX6fuYu3YtvB2dTBLrERERIYyqORGcxoDXY9atWph5MiR5orVZj1V2xtujka37S4TUaOFTEwxs4nvviQNnHj3SQYWaEzH8Nm2KPwXl4Il+65r76CjGQ/HviEiIksx6I66YsUKc8VRqTnay3Hq/3pi+oYL+Pv03dJ3MLGMbCUepmYhLjlT53rNypMl+65jSu8GWuuzc0sejO6r3Vew/sQd/PNmR/i5O5Y1XKPtuRyPA1cTMOPZxlDYmb9Gdtv5WAR4OqJlzSpmP1dF83+bLuB+YiaWj2wFmYzVc0Q6Jd8Hfn0eaDUGaDPO2tFUKGxzU04o7GSo6+tq8fOKItB6zh50+mJfsdvompQzLStXx5a6Ld57HQ9SsrBk7/XSNy6jTWfuYd+VBzrXjV19EmuORmPtsehSE7KyiopNxsR1pzH4+8NmPU9F9evRGOz97wEu3udYP0TF2vsZ8OAysH2KtSOpcJjclCNjOgZhfJc6Fj2nUiUitZREJUtHIvClkbOFm9Odx+mY/PtZvFpK+6V/zt1H/f/bgSV79ZuPyxjReowzRECuGYYiILIZSvPPBWirmNyUIw52ckzr2xDznw+x2Dmf/faQUfudvZOofl7WsXr2/hePN387g6SMnDIdR99JOk/HJAIAFuy+WqbzlRXHTySiEsnsrR1BhcXkphx6ISwQtbydrR1GsXKVKq02w49Ss7HtfCxy8iaCFIsZRrDwzfzQtYeYufkixqw8iX/O3cfX4dZNNoiIyhWZ6cdBqyyY3JRDcpmAXZM7W7QExxAqEVoD5Oy4GIeJ605j6f4bUBpQzfDyz8ew6ki0+nVcku4GzfriVBKVU1JGDtYcua13yR1RhSGzTi9aW8DkppxytJfDSVE+P9gZ2UqdZTNfhV9FnQ+3o82cCKOOW7jE5/bDNGw7H2uRbuTZuSpcjU+pkF3WD19/iO/2XTfLVBoVwXt/nMWMzZfw2krLjRVFZBFyVksZi8kNGex/v55ESTlAtrL4nkifb4/CKz8f01nCU/iYXRfsx8R1p7Hl3H3klnDMgv2Nv7mPW30Svb4+iD9O3jH6GNYyfPkxfLnrCnZcjLN2KFaxJ2/E7XN32fOKbAxLbozG5KYck5fTapajNx8X266mJAKAHw/exL/XHuL/Nl0osv7IzUfYfPZekeVvrz+Lzl/s00peHqdl499rCSYrrThwNQEAsCLytkmOZw3FDcJY3pTWO4+I8jC5MRqTm3KsR2NfhNbwsHYYOhlTSKK5y2/Hi5aQpGTm4u31Z3EtPgWXCo1/cr9Qe5xeXx/AKz8fx8YzRZMhMsyVuBSMW30SUbHJZjn+V7uv4H9rTkKlEvFV+FU0nblLva4stYBfh1/F9/vNP3aS2WU8AXZOB+6ftXYkVN5oJjcnllsvjgqIyU055mAnx5ZJHa0dhsms1mg8XJKdF+PwzOKiXdQ1b4QPU6XxH8Ivx6uXaTYorkhtZ6wd65AfjiD8cjyGLDtiluMv3nsduy7F4/CNR1gcYZqxhR4kZ+KbiGv4Ymf5G2/JYLv/Dzj6PfBjF2tHor+0h8CBL4DHN60dif5UKuDYj0DseWtHoj87jRHdt71nvTgqIJZ5kVHMeT/+99pDg7Y/fOMhIALODgUfZ1E0bhwZc74vpUrE+3+dU78WACSl56Df4n/Rq4kfZvZvUqbjG1uLmT++UIqZq4uycnXPOm/cscw7wrRFxV+ydgSG++tV4NZBYN8cYOQWoHYFSMzO/w7seF96/kkFaZ/l6qv9+tswYNw+IDEGOPIdUL83kBoP5KRLY+JkJQNOVQAHd6kbucJF2k+QA9lp0nN7J0BUSttAlNYJMkCVAyjzHnKFtMxOISVYdg7SX3snaXtRJe0rs5eWye2l53L7cjOAF5ObCqBDXW9EXn9k7TC05OjRwNfUdOUdadm5GP7TMQDAurFtS9xWly93/Yd+IQFlji2/rdCA5tWL3WbnxThsOK1djbb2eDTuJWZgReTtMic3xrjARrjWl6bxfzv8Y6DnLOvFoq9bBwuer34OeC0cCGgO5KQBdk6AvZnnkBNFQKWUbsD58m+qKqX0PCcdyE6X/uZkAGd+1T6GMhfIzZDiFWSATAbkZkvJwpPbQGI0kJYgPc/JlLYRBOn4okq68csVgKuf9NfeKS85yAZUudLf/GRBlbdcmbcconRMUSUlGvnJg1whHV+V90Pj4l/aMT+6DswLLHh9bp2JL6wJCHIpyanRGhi91WphMLmpAH4e1RoNZ+y0dhharj1Itcp5c5UqrZIdzeeaDVWlqp7Sf0F8t+8Gvtt3o8RtLt1PQlqWEm2CvXSuT8uS2goBQPdGfnB10P3fStcIzJolRZ9tvYxAL2eMah9UatymYsneYRb7QffbcODKNilJaDUGcHAz3bFVKulmmf4ISLoj3aDSH0nLBEFqI5H/Cze/vYSdI4C8m7EoSr+o83/pyuyApJiC40d+A1zcIB0bABSu0v4hL0jJg7M3kPFYuvk9vApAkG6emYlAbhagzJJu0KJSOn7GY+kGr3CRbqCqXCArpeBXvCAUFHMqXKTjCzJpu/ybrCCX1itzpPeh6//Vzz21X3ebIZ0nN6vgl72Du1SqoMwqGHk3LUHaJuMJkHQXyM274edm5cWQqxFLjvReslKk660q/P9JkK6tSo8SyE/KZ1vGYlVrCdw/Xcy6FtJfQQZ41pKuXW6mdJ0ASJ+93IL/B9l5HQ+yU6XPn0opfV7kDnmfS7mUgImi9Dc3UzpmTkbBv48gg/TZy/usaRKVQK4y7/NiPUxuKgBH+8o1SuXx2491LhdFET8fuoW5O/7TvV7j+bm7SQirVTAb98lijln0GEXLfPLb/4zpEIyP+zcusj4zp+A/d1aOstjkpjTLD90CAKOTm/Rs01T7qFQi7iVmINDLBKNkiyLckYYs2ENQZqOGkABXZEAGFbJgD8fH/0lfuu7VpO2VOdKvaZm99CvWTgFAkL5Uc9KlZEKZBcXDR3hKdhkyqGAHJWQQIUIATsZLiQ0glYKEfwx0/RBo2E/6Ur+xF5DbAQ+vAw6ugL0zkHxPSiAEGZCZJN34k+5KMzKrcgGI0he63EGKTZ+bZ1kkaSSc2anS49gy857T1PbOtsJJRR3/NoL0b2zvKCWh+hBkgFs1wCsYcK8ufTYd3PJu6GJBgpqbJR0zP5lQZhdUzcjzPr8yO+mv1rK8xM7eEeqELDcrL5HIkl7L7KR1okrar8kgqYoq4SoQ8SkQ1BFoO15KCp11/+iyCFHUKJnKySuxyntt5Z5eTG6oQvnn/H1UQTLG2m3Hb8ruuCv6qNdpNsx9fulh3J73DACpCu0FPRvLPkjJQnauConp2dh5KQ6DW9ZQr/sl8pbO5EbTx1suYWrvhsWul0MJJcyTrC6OuIZ3e9YveSNRzPtFnQmocuGRHYdAIR5yqKTkIDEGC7Zfws4Ld/Fe99p4pkNL6dd2vie3gZsHALcAqUj88S3pizYxGshKBVyqSl9uyfelL7iH13HeMe8X5J/AIYdC8Ww27r36AViv0LFCVyn4/s+lR1lpTmIoswc8A6Ubp7N3QdsGVa50bbPTpV+woii9zm/XkF+tkV9NoVJqJzPFqd1V2vZJNODmJyVjXrXzqjMcpKoVJ0/pudxBOlduBuBcVYoxO1WKTW4P2LtIiR0EqEtiRJWU1OUnADI7QCZH9JNsJKalo1l197x2GHklPX+8onEt7IBecwA3f8DRQ6qyuv2vFGO1FtJxs5KlX/5pD6U4c7Ok/Vx8pJgdPQCPGlJJlSBIy/JLtvJikZJdRynRUDhL7wvIK/4UC0qa7J2kZQrnvKQ1r6RJV2lN67FSVc8zX+UlEgopmSmvg+f51AdeWlvw2pqJDZD3b6UAoOs/o3UxuaEKQ/oaFnDGcTwAYKLdFtTLXI2cvI9xcUPeGNI+KDE9B32+OQiVSsTtR+k4mzfJpr62nY/F+buJ+KhfoSTo0iYM3zEKwx2BBNEDrbOW6ndAUZRuCqnxUmlCZpL0JezgBqTEAhmJmGYXCU+kwlHIBtatlko4crML2hqoSz00ipXzTAEwRTPhWAR8AOADBwCHABxRFNzUPQJ134hjz+p7eZAl2iEZLhAhwAHZcFLYI0spwk2V1w1dkEk36vybvyZ7Z/XNO8fOGdGPM5ELOZSQQQUBAoCmstva+8gdpGoQTTVaS1UkgiAV43vXka6NSindZB3cpATCMyivgWReUpKTKd04nTwBuQJKwQ6ZOUq4GFlSp5Z/01W4AR/elX6NayaUVtRl2jYAbtjxdic0CnAvWDHpJLBmENBxspQgaKrztCVD1J9HzYIqwPdvSIk42SwmN1Rh3ExIK1JtdM1xJIIy1wIQoNIoufFCMhB9BLh7AoqEa/jG/jqckYnHojti4YUU0QlPRDckwwWpcIJSlCEBHkgXHfEwIRUOyEF15OJW1H00F9LhgBw4CNkQo0QIymzpl2Te+ewysjFUfhxyqJADOYQkETWvn8J4+XUp4dh3DjgwXx2bj5CEaniIGvcvIPh8BD63c0UCPJAryhEquwUsniH96k97JP3iLqV59HjN/8WGzD0qyJEDOXJUAnIhgxwquNgLSMkRoIQM7kiHTDPBKJTYKO2ckSQ6w75hb7i5V5FKMLJTpQShSpBUWuFdBw2++g8K5OK7l5pi5Pob0Gyz4SzKkZ6thAOycWXOs9Kv9Pxf2vlF3qJS+xc4gAeJGegxb2+Rt3TbcXjBi/weMbHnCtqO+DWVqqV0WHX4NtYfu4PVY9rAx61wEVNR/b/5F5djk3Hiox56bV8qWV7j2HKS2Gi69TANdX1dMX3DBXSsWxUDW9QD3rlYZDtRFLH0wA2EVvdEx3rlLHnQnISyHCc2WblKPEjOMk21cCXG5IYqjN6LDgIQgUIdMW47jpCebACe1Vy3QvpjB2BAWWqCNO9bvxdd7QFgfuFS7LNA4/xlB4ruc9jxLeAJoLOGSlfzILlCKjlx9pJu+tmpUjWAkxdWnEtDguiBHNjho8FPFbQxsHOSShrsnTSWOUqlFvZOgCBg1qaLWHO0YPyh258+g5BpUpsVrURB06RTgFcw6nwoNXIPjffAlheLH48pCzeRBQVyHLwAaI+Lkt9OKAuKolUB6iJvIwR1Knge0EyvXWZukbpkfxV+FXMHlz5p7eW8QQ/3XXmAIa0CS9laD5o9f8oZlSji71N38VfeY2AL3b0C90Q9UI89lF8tXG7kVx2Wc899G4kr8Sn4e0I7hNWycrVTBcbkpoJ4MawG/jx119phWJ09DGw0W60lcoOfxtz98RAhwEdIhI+QBGdkwhUZcBfS4IYMCBBRTXgEe+RCLohQiQJyIEc6nJAqOiIL9siCPRoH+kKwc0Bubg6yclVwcbDD6ZhEPFY6QgUZ7JELJWRoUssfh25LDWlffioIOPmzzvDiRU+kiw4IlsXjkLIJqgsPUaXvR/D0D5baSzi4SQ9Hj2K7G316apv6+UdhZrqhjNsndXl1r1YkjliN0aNVKhEymfm7RRV7hme+Ao4uBQZ8Z/SxNRuIm0J6di7sZDIo7EpJXsp1cgM8Sssudbs75XkKkME/Ar+/Ajz9YbGb5ChVkAkC5Bb4DBfnSrzURm3TmftMbsqAyU0F8XRDX63kJvydzuj59cES9rAdMqjggGy4IhMBQkGPh/aZi6USkDy3/Hri97veOKmqj5tiNfz57rOo4+uGx8mZ+DlC35nKRTggRypJAOCskGv1QtraryM++Ou8+lf7urFtMem3M3icqf3Fv+ypMLx//RQAIPyBD1ZBSm4uqWrhnKo2htvtw8KcF/CtcnCRCJ7+zwcr2rVBUkYOPJzscf5uIlwd0lDbx1WKUBSxOOI6gn1c8Fyzanq+rzKq3rLUTQ5de4gJa09h7uAQPBtqobgKa/2a9DAzfefHSsvKRZOZu+Dn7oBjH/bQWqdSiUjJykWmayP4pUYBIUPMEapJiKJYXsZmM55fE+CtYrpTQ0ps2s3dCy8Xe+x+p3wOSpiZo4RCLrPID4iKjslNBVF45FxXR9P801VHArrJz+APZVf1Dd1YcihRFUmoKiQjB3IkiS7IgAJ+QiJ8hSfwQgoUyEVVIQluQjpckQE3IQPeSIaHkAZnZMIOSjgJWVIbl7yHvVD0l7RKFHAf3qid+StU+b/jo7X/w3f/6iC+HdYCa/Sc9kEiaF2Hwt2rn/1We1qI4cuPlXrEA1cTsPWZcPjH7sULp5sCEPBh7rhit09IzcKSvdewYPdVTO3TEPN3Sl3f84v5T0U/wdd7pMY1hiY3GdlKOCkMrKNzLmifcCUuBTM2X8R7Gr2yVCoR0Y/S8PLP0rWYtO6MyZObpPQcXIpNwlPB3mb/YtfsdbfzYixqVHFG0+raPW3mbLtc7L63HqYhyNsFMpmAC/ekdj/xyVlFtn3xhyM4Ff0EHngHHWUX8V2Pj0z4LixLFEXM33kFR24YNrq4Nf1+IgbrjsXgp1Gt4OvmiBsJqXiYmoWHqUX/rcqDxPRsNJ8VjhY1PbHxjQ7WDqfcY3JTQXRp4KP1Wmain1G/O8xGDeEhagnx+Cz3lSLrnZCJhsIdtJRdQ4DwCNmwhxOyECA8RoDwCH7CEzggB07IghwqnYmIqahEAalwRBqcsEPZBoBQkNgU483fzpgtnpIUntDxib0fkmsNB04XnQ29sKSMHCzYLSUv+YmNJmO/fH87HoPpGy7gyxdC8aIhbURqPqV++uKyw0jOzMXQH4+qlz1Ky0aXL/cbFZO++i85hJjH6fjihVDTtG/Rw7k7iRj/q/RLv3D7keO3dI+b9POhW/hsWxSGt62JzweFlPjpPBX9BACQBFdsUz2F78o4qu+j1Cxk5ChRo4rpG6KWNi1J5PVHWHag+MEwM3OU5W68rql/S/8X5++4goVD9GuXpUtCShbeWHsKL7WuiefDapS+g5H2X0kAAJwppgdnUnoORq04jgHNq+HVDsFmi0OpEq1abacvJjcVhKuDHU581AOt5+wBIDV78HJR4HFePbhcJkBZXF/oEtQQpF9aY+12YKzdjjLHmSvK8BjusEcu3JAOO0GFZNEJsaI3EuGKLNEeD+GBZNEZKXCWei3BDcmiC1LgBKUoRxockAWF1M5FtFe3d8mEAmIFmev1fKFpDbJylHBW6Pff7c7jjBLXGzv/1fQN0pf5+3+dNyi5iXmSgfBDt/Bax2AkZxo/gJ0hcWdkK6Gwk6m/RGPy2nJsOx+LIa0Ci60iUapEXI1PQQM/N8hkAjJzlEjOzIGvm/6Jw/Fbj7H2WDQc7Iq/GWtO0qoZyoLdUmPadcdipOTGgnU5YZ9J3w1nZvREFRfdpbCP07LhrJAbnGhIQyQW/14epRWfcM/Zdhk//Xur3DaQzcgp+pmWquH0+7dbsOsKTtx+ghO3n5g1uSnN0gM3cPZOIs7eSTRbcrPtfCze/eMslgxviZ6N/cxyDlNhclOB2GlkyzJBwOaJHbBk73W0q+ON0Boe6LZQR7ccE0gTHeAiZGGPsgXuiL7IggL3RS/Eit6IE72QCQUy4IBcUYZ4VNFIQEQokIts5I22WYl9ti0KL5roi8/Sc4hfvJ+C2dGXDaoC23kxDrsvx2HOwNJ7HRWWlJGDZp/uRgM/N+x6p7NB+7b9PAIPU7PwVre6eLdXA7SftxeP07IROa0bqns66dxn6/n7+Dq8oA/9/aRMfLTxot7/Xpr/HiUlAJZyIyEVrVyKJhEPUjLRZk4EvFwUOD2jp449i6cqZqio/AbkJSUCP/0rjbw9f+cV/PG/dgad1xLKOlmuvu2vzC0j2zRxRMUm46vwq3ivV3009HfXWjdxnVSSOW71yfLXG64QJjcViKezPTrX94EoivB2UUBwFTD/hVAA0heXqUUqm+BH5bM4oAqFccmJgGyU05E+rcAUvd1EUSyxWkqlEhGbnIm526Pg7aLA2E61dY6XEZ+cCT93xyIlIO/8frbItldUUilPmgFf4uN/lRpTB3sb3v32yA2p0Xh+rxFN+feh4pKI/GuzeO91vNurgbpkM/LaQwxprbu0atI63VWX0Ub0/MksNPO5NRrhbj0fi+X/3sKCIc20pgLJv66P9ej1VJiIou9l96U4vPfnOXzzUnOtdkqGylGqcOthGur5ulq0pCufruQkf7otvRgZ8uO0bLg72sFOrl9ptKUuzZAfjiAlMxfHbz3GuZm9LHNSM2ByU4EIgoDVY9qY9RxBmeVwllkCICU2IZ/sLvGX4tu/n8U/5+6rX686Eo3rc/oW+QJt+3kEZg9sWmQyz41nCmYtH5g1C389/QTL9krtEYyZCT5eI+muaL1tSgpXKOZ5WUsBNO2+FIfX15zCqHa18OmApvj9RAw2nrmHH15uBQ/n4n80rDx8GwBQ74Ar3uvVAJfvJ+NJenaZEocNp++ic33tdn+vr5ES2DErT+Kbl5oX2efg1QTtfYq5NhN+PYU9UQ8wd3AIhrWpKW1qQLVQafb+F48bD9IwrnNtnev/vfawSPd/c5eOXn+Qih5fHUCzGh7YPKn4MaKsISWv6lnXRL+abiakIlupKlK6U15UjAYMVDoT/G98Lssak92RvhrO2FkksRn+01Gt15qJTb7MXN1JyYxNF7H5bNHt850V6yL36Rnq3mPmHnpg3OqTOBX9BDsuxmotL9w4+7+4ZGy/oL1Nqcp4nxzywxEs3V/y7PGGnHZF5K0S90tMz1YnD6uOROOt385g6t8XcPTmYyzZd02vc+eX0PRb/C9GLD+Gu0+MH4Pm8I1HiLxuWE+o/CqM0uyJegAAWP6vNMDjtvOxaPN5hN6T3ZZmzMqTmLM9qthG4EBBmy5jGPPR2nxW+hFxrlDbPK3jlnDgmwmpOrYv2OH2wzQjoirq0LXi/827LTyAPov+RVK6dWf/Lg6TGxtR+FfOM6EBpe6jQMGH8v9yXsV5sY7J4yLTydKRpBy+UfpMxxkmmi28rEor1Qi/HI/nlx7WSrgu3E1Sj3ibr8+ifzFrq+6u2JpKSyBKo/lf6vitx5i/8z8sjriGc3cStab62Ho+FjM3Xyy1Qb/mAHef/lNy/IV/NW/RSFpTjGzUXVpD9cKyClWxXdVRTWgIfXv5TVx3GgkpWXhh2RGjOkkUJz5Z/6p7XdVsl+4nYdGeq1hzNBpztl0uU1WcplylCufuJCLXgJLR0tpXGvsZKSx/eIeSxJuhSYQpMLmxEVVdtXtHfPtSi1L3cUTBl80hVVOTx0Tlw9L9N4wedffDjaV3XdfXgasJBu/Tf8mh0jcqhmYC8cFf59U3o6xcJV5cdhhB07apu2Pr66vwqxjwXSRuJBT8Mj5wNQGrjkRj6/mipWCaCVJZ3osmU1Z9lWT5v9rJYUnn/SRv6oqS3CylNEHX4Yf8cKTU46r3zwvw/T/P4YO/zhVZH345Xu9jJWbk4MONF3AquqC055nFh7BozzXM2HQRP/17C0duSj8sjKk+07yWn/5zGQO+i0Tdj3Zgh6ElkhZw+MZDPCohMS2vtc1MbmyEIAgI8Cjo7qo50NnIdrUAAF0K1ZkrNKYyuC36mzlCspZfIm+h2ae7jdp3w+l7pW9Ugr80GlFrzmFlrLL8Wr72QCrKX/7vLZy4LSU1r+jxy1RfCSlFbwDbL8SpnyeaqPhehIhcpQprj0Xjho7qCWP9cOAGxqw8gey8EsIvd10pZY8CT3S9NxMkYaUln5fuJ2H6hvP4ePNFtPpsD87EPMGfp+7ij5N3kZiu3XB6i44qW02aH60526Kw7lgMnl9afHK19mgMRiw/iscldIPXh+b/iwlr9avK09fth2llnk5k+E/HEPbZHoxYfrT0jcsRNii2YevGtUVKZi56NfbD6PZBCK7qgoioBxi7+iQAwB5S0WWWyK7atk5XlZYlZOaY9ryqMiQ3WXmxnL+bqF5WeARqTXFJhhW3F65CuZmQip8PGVc1VlKXclEEfj0ajU/ySqb07ZJbWgHD3B3SgJEbTt/FS3kNe8tq4xntHoJX41PwIDkLHetVxXf7rmuXpuj5T3v3STou3ktG7yZ+eGaxdmnYvB0Fg17qqtGauPY0vnmpeZEG9oeuPUS7Ot7q15pJ474rD3QOmrqtDKUsogUGdDh68xFe+vEo6vm6Ivxd7ekktpy7D4VcQJ+mpTdfyBd5vfQq8PKEyY0NKfy9375OwbD5+fMS9Wjsh41vtMeg7w/DTpCSmxx+DKiCyFEaf1OIiktGSA0PXH+gX2nH7UeGNTItfDOdraNd0N+n7urVHq6kREQlAqc0Rqk9eDUBVV0dit8h/5glrNNslzVtwwWdyY2hVz4lKxfv/K5dPdQrr1H6nnc7G1QypKnj/H0AgMXDila9l5bAbbsQi56N/YoMMzBr62XsnFwwk7zmd+mrK04YFac1CQKwKa/nY36JZUpmDqZvuIDO9Xzwwd/nAQBXPutT4mCVhth6/j5WH45GuzreeJyWjVkDmlila38+3tVsiL6/BvJHylWAyQ1VLMZ0R8/3wV/n0aleVa32MqZUuFRJ1//G9/48hzN3dFe15E+UWprC/89H/nJc7xiLHCuvy3XnL/fpsa3RpylCV4JZXJucS/eTsPV8LN7oWgdujgXX59jNoiUJmiVexVVhXryXhOU6StQ0Ny9rycruS3GYtfUyvnmpBcJqVSnxXMUxdduqJfuuY+v5WGw9X1DiZIoG2/n5S/54Ucfzerl1b+SLrg18y3x8Y7HNjQ0J1HdOGWUO2ghReFkuDdfO5IYqirJ+4a8/fsc0geiQW6hUKaaYkp9fj8boXN7s0934Ly4Z2bkqzCypga6e1+BWoWSh8I/o47ceo83nEVh24IbO9kIl0TXkgCGO3tS/m/cziw9h6f4bRXrN6aL5Hou7TPv1aNhe1s/Z62tO4e6TDLy6Qnfiue647s+ApsJt1MpSCrLxzF3El1DNmp2rwvm7iVCZsHdaaePkmBvvajbk66HNMXvr5aKDVYkiEHsOuLkfuBGBundP4w+Hgl9O98SqWpu3q+2t7glAVJ6U9av3mwj9xogxRv5M7flK6x2kS59F/5a6jb7tjg7feKRVepGeVVD1dPj6Q/WM9prtVNTb6hzKv+BYZZ2QNn+gQUP8ffou3upeT/167bGiCYJmyV5cUqbO6rriqiW1EiMjP2hrj0VrNaJPzszF7YdpCKqqPVK3MY3LS2tMX9L6d34/h4HNi58+5a3fzmDnpThM7dPQ4LjKKyY3NiTQyxk/jmwlvRBFIPYscOEv4PIWIKngi0AO4KHojsOqJjisapI3w3aB315/CkHTtlkucCJ9WaofdDlmyBXQHA9og8bo0/mJTXH0SbLMoaRqkvRspXri4OLk94IDgFG/HMcpA+bQ0q6WMs5HGy8WWTZi+TFETutm1PG+23cdqVm5mNKrAd5ef7bEbY2J+Y8TdzC6QzB2XpJ69f186KYRRykmHiv/V2VyY0tUSuB6BHBpA3BjL5Cq0RPB3hmo3RUI7oJotxZ4es0DqFgrSVThGHLTWBF526hz6Bqx92Gq4XNSGcqUXdsfGTGHljncSzRs8ERN+Y2ua+qYH64kU/48h//itBtN6/rYfPLPZYwu8wziuqvLLNEjrCRMbmxB8n3g5Arg7FogWWNcEjsnoEEfoOnzQJ3ugEL6D5LzIAUqGDaUOlF5UNJw9ZWFLZdd9TLzFB/6MtXow/mUKhHHbj5C0xoecHc0fDJhXdVY+Y3BU7NysXT/dfwXW5DMFE5spO1LP0+WCYdu0BWDJTG5qahEEYiOBCK/kUprxLz6dKcqQOhLQIO+QM2nALuSu4j6uzsiTsew5M0CPXHuTqIZAieislCJIkelMgPN70FTV6nM+ucSVh2JRk0vZ7z8lOFjCOkqBcmfuXz+jv/0GiBTn7ZaKSVMylucIzcfoa6va5HlPxy4iel9Gxl8PFNhclMRXQsH9s8D7p0sWFarA9D6NaDhs6UmNJrFiP9OfRrpWUo0m6U9gu2vr7XBuTtJyFWpMLqEcR6Gt62JdToa9hGReVyPTy0yTgvpdseACTHNOZ7NqiNS8hHzOB2fby/agLs0uvKSJ+nZOHrzMU7H6DeFiLkG8pyx6SJeeaqWWY5dFkxuKpL4y0D4DOB6XqM6uQPQYgTQdgLgU9+oQ8oEAR7ORYtJ3Rzt0bFeVR17aOMvSCLLYmKjv05flD5+jy7Wbi9SmK5qsqE/HtV7QEoAJc4PZYuY3FQE0UeAQ18B1/JKV2T2QNv/Ae3fAtz8DD6cdpdHEZopyuQe9YruUAy5jKkNEdkeEw73YhK64jEksQGA0xqjWptabJLxjabNhd1lyrOUeGD9CGBFHymxEeRStdMbR4Hec4xKbAAg2NsFLWp6onN9nyJzrITW8Ciy/bs9pVKher6u2PBGewxpVQMA8Hb3eqUOd05ERGVTljnVLCE10/C2OubGkpvy6noEsHkikBILyOyA5sOlkpqq+pesFEcmE7BhQnu9t3+rez2twbOa1fDEmI7BaODnhhmbi47rQERUkZm6t1RZLdpjvsEnTeG+gZPMWgKTm/Lo/J/ApvGAKhfwrgcMWQX4NTHpKYobyruk2YjzyWUCGvq76709EVFFUs5ym3JvVBnmNzMXJjfliUoFRHwKRC6SXocMAZ5bDNg7WSwEdz0m7iMismXGTJ1B5Qvb3JQXWanA7y8XJDbt3wIG/WCxxGbe4BD8r0tttKzpadB+xbW5OW3AsOf6MDSukjQKcDfZsYiIqPxhyU15kHwfWDcEiLsgde8esAQIHWLREF5qY/jAUsU5PK0bvFwU8HJR4HHeEOiHp3XDlnP3dU7SV5Im1dwxqEV1HNBjJl991fdzRVRsssmOR0RE5QtLbqwtNQH4ubeU2Lj4AKO3WjyxKYtxnbRnIG9R0xMBHo4AgJn9G6uXV/N0wvgudbS2/SRvvV0JXcq3vdUJYzvVLraNkDFYn05EJBn+01Frh2AWLLmxJmUu8OcoacZu9xpSYuNV1knMLCvQyxnX5vTF59uj4O5oj8k96qkTERdFyR+v0R2C8WyzapALAjp/uQ8pmbnoUNcbkdcfFdnWlM2WmdsQEUkO3yj6fWsLmNxY04H50vxQCjfglQ0VLrHJZy+XYWZ/43pzVXWVpoo4M6Mn7OQy3EvMQId5e00ZXhHlrZsnERGZFqulrOXGXuDgF9LzZ78GfBpYNx4rKzyYoDkxtSEism1Mbqwh4wmwcYL0PGw0EPKCVcMxF2OayRRXqlL4WO/3LkMyyOyGiMimMbmxhojZQGoc4F0X6P25cVlABdU6qAoAoGsDH53rDakx2jyxQ5FlNz7vp3Nbrfm0TJjdNApwh5eLwmTHIyKismNyY2n3TgEnf5GeP/s1oHCxbjwW9uMrrTBnUFN881KLUrd1cyhoEjaweXWtdfZyAS4O8iL76DOZp0qlR6B68HS2x7fDWuDwtG6mOWCeTnrMxk5ERMVjcmNJKiWw9V0AIhA6FAjubO2ILK6KiwIj2taChx4jIf/2+lPq5wOaV8PfE9pjVLtaqOfrimFtamqV8tjJBAxuqZ0AVXEuOMfUPg0BACPb1UK/0AAAQDUPR/w5vp3+sWsc78bn/XD2416o6+sKR3u5umG0KZiy2zsRkTUUVzpvKewtZUknfwFizwIOHkCvz6wdTblXx8dV/VwQBITVqoKwWlXUy+I0Jms7O7MXXBRSSc4f/2uH2w/T0KKmJ3p+fRDt63jjf51ro3cTf9TycoYgSIlNPT83vZKsfPX93HDs1mMARUuIFHLt15oDGBbWqlYVnIx+Uux59Ch8IiIq15wVRUvWLalclNx89913CAoKgqOjI9q2bYvjx4ufhGvlypUQBEHr4ejoaMFojaTMBQ7k9Y7qPgNw9bVuPDZAs+WMk71cXeLRJtgLQ1oHop6fG8593Au/vtYWgiAguKoLZDLpM9MqyMugxAYA2tepinmDQ/C3HjOql1Q95uxQ8m8K5jZEVNFZe8QNqyc3v//+O959913MnDkTp0+fRrNmzdC7d288ePCg2H3c3d0RGxurfkRHR1swYiPdPgikPQCcvKQeUpWAMbUrmtU7CruSP54qjf89xZ3Kw9keMhMWhbzUpqZW6ZG+lr0cpn4e5O1c4rYyVksREZWJ1ZObr776CuPGjcOrr76Kxo0bY9myZXB2dsYvv/xS7D6CIMDf31/98PPzs2DERrr4t/S3yUBAXjlm3g7wMHzSTyeFHJHTuuHo9O6lNg7W/GVgiXygLL2s+jT113qtORXF4mEt0CzQE/ZyAZ7O9pzYk4gqvEpdcpOdnY1Tp06hR48e6mUymQw9evTAkSNHit0vNTUVtWrVQmBgIAYMGIBLly4Vu21WVhaSk5O1HhaXlQJc3iI9b/q85c9vJY0C3PHlC6H49bW2Bu1X3dMJ/h6lVzVqJzemzW7K2mPJkATluWbVsHliB1yZ3RcnPupRaomVqeyc3AlNqjGRIiLTM+WQG8awanLz8OFDKJXKIiUvfn5+iIuL07lPgwYN8Msvv2Dz5s349ddfoVKp0L59e9y9e1fn9nPnzoWHh4f6ERgYaPL3UaqorUBWMuBVB6hZensNW/Jiq0B0NFPXZj930/VQAqSBAaNm9cGa19pgsY6u6oKerWH2vNsZ/kbEJpMJsJfLoFRZ5kuhob+7XiNDtwnyskA0RESmY/VqKUO1a9cOI0eORPPmzdGlSxds2LABPj4++OGHH3RuP336dCQlJakfd+7csXDEAK7ukP42fR6QVbhLXm55uzrgj/+1w9Y3O5bpOLV9XCATgNHtg+CkkKNTPR+46Gj0W9IvEc01dX3dyhRPrp4D8dSoUlDt52gvw8H3nzb4XH0LVZfp8sULoQYft7wIrlq5xpEiKi8qdbVU1apVIZfLER8fr7U8Pj4e/v6lf+kCgL29PVq0aIHr16/rXO/g4AB3d3eth0XlZgHXI6TnDfpY9tyVQJtgLzSt7lGmY4S/0wWXZ/XRSmgUdjLMGdRUK4EwldLKf3L1KLn594OnsX9KV/VrUSw++ZrxbONijzOibc0Sz/PZwKYIqupSYauvTJXcuFi5WysRGcaqyY1CoUBYWBgiIiLUy1QqFSIiItCunX6DqymVSly4cAEBAQHmCrNs/tsKZKcCrn5AQOmj8pLlyWUCHO2L3rxGtK2F7W93Mvn5qldxKrEBdC2v0m/IDnYy2MllmPR0XQDArAHFz8r+bGjx/zfs9CxJbFHTU6/tyhuViX4+Hvmwu0mOQ1RZWHsKP6vXkbz77rv46aefsGrVKkRFRWHChAlIS0vDq6++CgAYOXIkpk+frt5+1qxZ2L17N27evInTp0/j5ZdfRnR0NMaOHWutt1CymGPS39pPs0qqAnJ3tFe3ORncoobe+4XU8CyybNWYNhjdPgij2geVuO+QVjUwuEV1zBrQBD+8Eob/damNCV0Leld1beADHzepTc+U3g1w6v96YGjr4ktgfN0c8GxoALrULzpiqKl7meUnW+ZS3dOwkjRTNV+SG3ChSko0iSqLq/EpVj2/1UcoHjp0KBISEvDxxx8jLi4OzZs3x86dO9WNjGNiYiDTSAqePHmCcePGIS4uDlWqVEFYWBgOHz6Mxo2LL3q3qrsnpL/1elo3DjLa+tefQmp2Ltwd9e/CP6x1IFYdvo3rD1LVy7rU99GZYBRmJ5fhq6HN1a97N/GHKIp4IawGald1KdIzzDtvbKD8hEfTS60DIQgClgxvCQAYveI49l9JwIth+idqgHb9eZ8m/th5KU79nga2qIZ3fj8HQKrOuzyrNxp/vAsA0DbYC7+ObYumM3chK7fsk3q1CfbCxjP3DIhbd3bzXs/6WBh+tczx6GKOUQluz3sGPx28iTnbo8xwdOtwUciRlq20dhhkJtGP0q16fqsnNwAwadIkTJo0See6/fv3a73++uuv8fXXX1sgKhPIzQLiLkjPa7SybixkNJlMMCixAaQEZdfkzth89h5am6C3kSAIWtNR6OKssMPB95+GXC6gw7y9AAD3QqMwLxneEoeuPVTP+1JagYSu1KBJNXf4ujvgSXoOFr/UHIIgqJMbIS+Oo9O7469Td/BSm5qwl8twakZPvLz8GM7eSUSHut4QReDwjUd6vfey0JXbHHz/adT0dkbPJn545/dzmNyjHtYcicah6w+LP44ZY9TXkFaBNpXc/DG+HZ5ZfMjaYZCNKhfJjc16eA1Q5QCOHoBnLWtHQxYmlwkY3NKwEpKyqpk3+rFMkKpkCo/X4+pgpzWgoGb39k71quLfa7pv8G4ayV3zmp7oVK/kEih/D0dM6lZP67ybJnbAg+RM+Lg5QBAEBE3bVmS/ur6uWqVdJWng54YrpRR9V/MsOl6SPG8esIb+7tiR16Zq9ZHbxR7j22EtSmy706eJP2pVdcYPB27qEbXE09keiek5em07tJU0fIWd3LwjVdb3c8XVeP2uvSk0qeaB74a3xMR1py12Tqo82AjEnB7k/crybWyZIXSJ8hz7sAf++F+7UpMQzY9l/szpukzoWgcBHo7o3tC31GOWxNfdsdgBFw9NfRob3miPgBIGcHyueTX18zbBpZeI1dZR2qVr4OvicpcxHYLRv1k1KEoYDyjA0xEB7obNb/fn/9rBWSHH4BbVsXhYyR0N5ud1xXdxsEOfJvr1Iu3RyPC566zRdZdfi2QuTG7M6cFl6a9vI+vGQeXOMyFSD6ZapcwzZSwfNwe9bv6a9xb7Em7gHk72ODK9O34e3brk45XhZlWjijPcHe0RObUbXmotlVbMGxyCb15qDgDo0cgPTzfwVY/+/GqHoFKPKYrAD6+EaS3TNXeXrht7j0Z++Li/1JZPV2+6fEUGd9TjItTzc8PlWX3w1dDmeK5ZtWK36xeincyMbF96CXDLmp5YPqo1Pn3OsIbNxeU2pU2DcvzD7mjob9zYTpYasJIqH1ZLmZNmyQ2RhqbVPXBo6tNaE4Vag2YpijWHS/d2Uaify2QC5g4OwaRudVGjipT8NavhqR5zaPtbHZGVq9KZcHzSvzE++eey+vULYTXg4+aAYW1q4rfjMQBKTlTy1fZxwU8jtZOiWt7OOhtJCgLQv1k1zNp6GZ3r+5RYBDKhax30alx0Lrzqnk64l5iBQS2qI8DDEd/vvwEAkBvYw7JZDQ8sHNIcADAqr2fe2mPR+GjjxVL3NbbbvIO9vNgE6PSMnmg5O9zk5yQqDUtuzOlB3pxXTG5sXv5geO1qe+u9T40qznrdaM3J1LUCxs7xVXguLkEQ1IkNAARVdVFPFSEIusclAoBWGo23/9e5troH2fR+UpVbvxB/eDgVbRxeJLET9X8vMkHqsRY1uw9WlFKyNbVPQ7SoWXRW+b1TuuDk//XA10Ob4wON6sEivb1KyQU2T+po/MCFeuQZa8e2hVOhay+XCVqFVe3rFPwf8NJIWjXllyrqym1q+3BUaQD4eVSrIqWvpphypr5fyR0TSmJXSimepg/6NDD6PKbA5MZcslKBROmXIqulbN+ErnXx++tP4ZdSbm7ljeZNSRSh88ZvCaZK8jRvlprVbO6O9rg97xl8PyJMx14o0pZFn1Gi8+UnQQ52cul5MUlRVVfdN/r8fXWV4hWOwphyjl6N9Wuno3nsF/KGCpAJgJtjQQF/h7pVETmtm9Z+MgFoVavgJrzopebo08Qfa15rA6BghGxdo33rKrnZPbmzuto238tP1cTmiR30eh/GKm20bk1htYomqKXp3rBoO6iSpj/p1tBXq0RT2l6/wWpLqpL2djE+QVo37im9tx3VLsjo85gCkxtzeZQ3HYRzVcCZEw/aOrlMQNva3nCqYMP0Fy6dODT1aex5t7P6dbC3ZX5Fz+xvmtJNzV/9hhQivdIuCL+MLhiuQdeNuLjD6Xuad3rW1z+gYoTUMHyqER83hxIbizvZy7FlUgetUqKP+zfGOz3qI/zdLlg9pg0a+rth1RgpWfFyUeCv8QUjyMsEAe/3boD3ezfAnne7wNfNEcteCVM3PH+tYzDOzeyFl58qaC9UxVlKogu3uXFWyGEnl+HbQo2smwdWQbNAT4PfuyH8DWgU/mG/kn+w3vy8H5oXivf1zrWLbPd+7wbFJm26Sg71/Uz/8b92WPZyS/02NoC+SZ29XChSwmdpTG7M5bFUZw7vOiVvR1ROiKLU5buurxv+ntAOcwY1RYe6+lezGWtoq0AEepWtYbWbox3OzeylNT9YcVUiushlAro19MNf49uhX4g/FrzYrMg2DnbFfFnrecPpUKdq6RsVVqhgw93RHuc/6YVQA5MczRtr4X/TiPe6ILSGp9Zozu6O9ni7Rz3U8XFFaA1P7JzcWWsASm+NUiaZIMDFwQ4Tn66Lur66qzwKlwh+ktfYuUejou2PAKndlbE+7Fd8IleS9gZ91ksuQ5PJBFTXSJAjp3XTOQeeCJgkaevbtKCkrEDBNayn8e9SOEF6v3cDvQb1vPJZn1IblwPA5Vm9ceGT3mX6NzQFJjfm8ihvzAsvJjdUMdTwKvgyDqvlhRFtaxnchsaYJjea1R7GcrKXq2+g3w5rgcEtq2O4AdUM+VoFeeH7EWGopmOah2+GNdfZu61wbylXh4Ik6MyMnvB1c8DzLWsgyIi2MLoaebs72uOHV8IwrE2g3seRywTMGxyCLvV9tKpOp/VtqH6v7/WSSpbyx9UpiWZ1kr73sPzkyMlejgAP6ZxVXBT4b3bBhMJlLR0DgK4NDO8Gr7CTIayWF/6e0A5/T2hfZH2gl/bnwdWhaPXtildbw9PZHj/m9c77pH8TPBsagHXj2qK6pxNcHOwwe0ATrfZl+dVOH/ZrqFePsyI98/KMLnVKl+L/Tcd3qVNiqjalV338+8HTxSf3hTgr7KzelhBgbynzeXJL+usVbN04iEpx/KPuyM5VGTwKc1l9PbQZNpy+hzc1BvszVH4vo+4a47r0b1YN/UvoXm2shv7uOPD+0wiZuQspWbnq5S+20v7V+2xoNey6GI82wV6o4qLA8Y96mDyWAA8nzB0cipjH6Yi8rt9Izy+1qYmX2kgJ35ZJHXDwagLGdCj4fhrQvDqequ0NXx3TeBSmOdeWvglwowB3qdqqUKPY4m6EQ1rVwB8n72otW//6Uxiz8gTSS5i2QVdiWtjasW0xYrk075+rgx3WjWsLQErq7zwu2iNufJc6Wj3OGvi7YeLTdfD9/hvqdl5PN/DFmRk91dfDx81BPe1JvlfaBWF421r49J9LaFrNA57OUnLzeuc6eL1zHZ0DW2oydqiFkvYrqTSmY92qWoNxViRMbszlSbT0t0qQVcMgKo2vm2ED0JXEy1n/qqBBLWpgkAGTkeqy4Y32CL8cj8Etq5fpOIbQLLU4N7NXkSoXe7kMy17R3XDZUKX1lP71tbZo+3kEHqRkGXTc0BqeCNUxuaufnu1Oank7Y0DzavBwsterqiJfcdVW+TQTpfnPhxZJbp6q7Y2Pn22MaRsuFNl3/vMhaF+nKlw1qiabBXpizsCmWLj7CvZdSVAv71C3Kna/0xn3nmTgaR0NfQtztJNjZv/G+PSfy+qu/O/3bogxHYLx6soTeDGvZESfRE8uEzBrQNNStzNU40I9DoujK8TiPmdt9Rgrq7xicmMuiUxuqPL4akgzRF5/hOcNnJCzrPzcHbUaqlrCpwOaYsqf5/BG1zpm713WqpR5yQRBwMpX22DKn+fwfm/Ldb0VBAHfvFTyyMqGGN0+CCsP31Z32c8/R8Hzgm0d7AtaU1ye1RtxSZmIS8pEuzreRZILAdKYUjV1tOmq7+eG+n5Fq4I0D/Fmt7o4eydRGqXaToZnQgPgo9HeyNvVAVsmdTTkrZbo42cbY9bWyzrX1a7qUmwVriAI8HZ1wNHp3dWdGgwp5Slc/dks0BMvhtXQWZ3lZC9HRo52yVm/EH9svxCn/wktgMmNOeRmA8l5MxczuaFKYHDLGhafR8taXgirgacb+Gg1qjW1A+93xdGbj/S6po2ruWN73hxZFdXM/o3xvy611W1xCtMcVbpfSADWHIlG62AvOCvsUNvHtcg0G37uDohPzkKvJlIpyzs96+NqfCqO3HxUas88zVKMCV3rwFlRcJs0ZSmnLmM6BuPOk/RiS0DHdqqNRXuuFbu/fzFTlzSpVtCYeUqvBoi8fhhA8W11Aqs4FfujYefkTujy5X71680TO6BxNXfMGZgLe7vy04yXyY05pCUAogqQ2QEuxs/DQ0TlkzkTGwCo5e2CWhbqhl8eCIKgM7EZ1a4Wjt16rDXZq4OdHBveKHnMm61vdsKJ24/RM68KydNZgd9e13+MFmua2b9g2gzNIQk+G9gUrg52uDW3H347fgeNAtww6PvDeh2zXR1vLB3REnV8XVHfzw1Rs/poD1thwFiRtbxdsHNyJ7y49Agmdaur7u1VxYDeiZbA5MYc0vLqdl18ODMcEZGRPjWybYqPmwP6heg34F1hmjfpkuZbs4S3e9RHYnoOejfxR/u60lACgiDo1ROw8J2nr8b1KDweV5FkppS2Xg393XFuZi+rd/cuCZMbc0h7KP11NmJcCyIishpXBzv8M6kj5DLB6smNq4MdvtQx5lJhZf0NXXiajwZ6dEsvz4kNwOTGPNQlN0xuiIgqGmNGgq7INFObd3rU1zmackXD5MYc0vNKbtjehoiIrMDYSWzf7lExx7UpjMmNObDkhoiIzKxbQ1/EPE4vMo+VoUobT6kiYnJjDul5I4ZywkwiIjKTn0e1giiW//Yv1sDkxhwyEqW/TkxuiIjIPARBKLYxsSHpTpCOOdMqOiY35pDxRPrrpN/08ERERKbUOm/qBF2jMxc2vmsdJGfmao0nVNExuTEHJjdERGRFHk72uPRpbyj0GDXYWWGHT55rUup2FQmTG3NgckNERFbm4lB5b/HlZyIIW8LkhoiIyGqY3JhaTgaQmyk9Z3JDRERkcUxuTC2/1EaQAw6lD2FNREREpsXkxtQ0q6Q4aSYREZHFMbkxNba3ISIisiomN6bG5IaIiMiqmNyYGpMbIiIiq2JyY2pMboiIiKyKyY2pqeeVYnJDRERkDUxuTI0lN0RERFbF5MbUmNwQERFZFZMbU0t/JP119rJuHERERJUUkxtTS4mT/rr6WTcOIiKiSorJjamlxkt/3fytGwcREVElxeTGlLLTgKxk6TlLboiIiKyCyY0p5VdJ2Ttz0kwiIiIrYXJjSvlVUq5+nDSTiIjISpjcmFJ+yQ3b2xAREVkNkxtT0iy5ISIiIqtgcmNK6pKbAOvGQUREVIkxuTEldcmNr3XjICIiqsSY3JhS8j3pL5MbIiIiq2FyYyrZacCtg9Jz56rWjYWIiKgSY3JjKjf2FTyX21kvDiIiokqOyY2paHb/rtXRenEQERFVcixiMJUarYBPkqwdBRERUaXHkhsiIiKyKUxuiIiIyKYwuSEiIiKbwuSGiIiIbAqTGyIiIrIpTG6IiIjIpjC5ISIiIpvC5IaIiIhsCpMbIiIisilMboiIiMimMLkhIiIim8LkhoiIiGwKkxsiIiKyKUxuiIiIyKbYWTsASxNFEQCQnJxs5UiIiIhIX/n37fz7eEkqXXKTkpICAAgMDLRyJERERGSolJQUeHh4lLiNIOqTAtkQlUqF+/fvw83NDYIgmPTYycnJCAwMxJ07d+Du7m7SY9saXiv98Vrpj9dKf7xWhuH10p+5rpUoikhJSUG1atUgk5XcqqbSldzIZDLUqFHDrOdwd3fnh19PvFb647XSH6+V/nitDMPrpT9zXKvSSmzysUExERER2RQmN0RERGRTmNyYkIODA2bOnAkHBwdrh1Lu8Vrpj9dKf7xW+uO1Mgyvl/7Kw7WqdA2KiYiIyLax5IaIiIhsCpMbIiIisilMboiIiMimMLkhIiIim8LkxkS+++47BAUFwdHREW3btsXx48etHZLZHTx4EP3790e1atUgCAI2bdqktV4URXz88ccICAiAk5MTevTogWvXrmlt8/jxY4wYMQLu7u7w9PTEa6+9htTUVK1tzp8/j06dOsHR0RGBgYH44osvzP3WTG7u3Llo3bo13Nzc4Ovri4EDB+LKlSta22RmZmLixInw9vaGq6srnn/+ecTHx2ttExMTg2eeeQbOzs7w9fXF+++/j9zcXK1t9u/fj5YtW8LBwQF169bFypUrzf32TGrp0qUIDQ1VDwDWrl077NixQ72e16l48+bNgyAImDx5snoZr5fkk08+gSAIWo+GDRuq1/M6abt37x5efvlleHt7w8nJCSEhITh58qR6fbn/fhepzNavXy8qFArxl19+ES9duiSOGzdO9PT0FOPj460dmllt375d/Oijj8QNGzaIAMSNGzdqrZ83b57o4eEhbtq0STx37pz43HPPicHBwWJGRoZ6mz59+ojNmjUTjx49Kv77779i3bp1xWHDhqnXJyUliX5+fuKIESPEixcvir/99pvo5OQk/vDDD5Z6mybRu3dvccWKFeLFixfFs2fPiv369RNr1qwppqamqrcZP368GBgYKEZERIgnT54Un3rqKbF9+/bq9bm5uWLTpk3FHj16iGfOnBG3b98uVq1aVZw+fbp6m5s3b4rOzs7iu+++K16+fFn89ttvRblcLu7cudOi77cstmzZIm7btk28evWqeOXKFfHDDz8U7e3txYsXL4qiyOtUnOPHj4tBQUFiaGio+Pbbb6uX83pJZs6cKTZp0kSMjY1VPxISEtTreZ0KPH78WKxVq5Y4evRo8dixY+LNmzfFXbt2idevX1dvU96/35ncmECbNm3EiRMnql8rlUqxWrVq4ty5c60YlWUVTm5UKpXo7+8vfvnll+pliYmJooODg/jbb7+JoiiKly9fFgGIJ06cUG+zY8cOURAE8d69e6IoiuL3338vVqlSRczKylJvM3XqVLFBgwZmfkfm9eDBAxGAeODAAVEUpWtjb28v/vnnn+ptoqKiRADikSNHRFGUkkmZTCbGxcWpt1m6dKno7u6uvj4ffPCB2KRJE61zDR06VOzdu7e535JZValSRVy+fDmvUzFSUlLEevXqieHh4WKXLl3UyQ2vV4GZM2eKzZo107mO10nb1KlTxY4dOxa7viJ8v7Naqoyys7Nx6tQp9OjRQ71MJpOhR48eOHLkiBUjs65bt24hLi5O67p4eHigbdu26uty5MgReHp6olWrVuptevToAZlMhmPHjqm36dy5MxQKhXqb3r1748qVK3jy5ImF3o3pJSUlAQC8vLwAAKdOnUJOTo7W9WrYsCFq1qypdb1CQkLg5+en3qZ3795ITk7GpUuX1NtoHiN/m4r6WVQqlVi/fj3S0tLQrl07XqdiTJw4Ec8880yR98Trpe3atWuoVq0aateujREjRiAmJgYAr1NhW7ZsQatWrfDiiy/C19cXLVq0wE8//aReXxG+35nclNHDhw+hVCq1PvAA4Ofnh7i4OCtFZX35772k6xIXFwdfX1+t9XZ2dvDy8tLaRtcxNM9R0ahUKkyePBkdOnRA06ZNAUjvRaFQwNPTU2vbwtertGtR3DbJycnIyMgwx9sxiwsXLsDV1RUODg4YP348Nm7ciMaNG/M66bB+/XqcPn0ac+fOLbKO16tA27ZtsXLlSuzcuRNLly7FrVu30KlTJ6SkpPA6FXLz5k0sXboU9erVw65duzBhwgS89dZbWLVqFYCK8f1e6WYFJ7K2iRMn4uLFizh06JC1Qym3GjRogLNnzyIpKQl//fUXRo0ahQMHDlg7rHLnzp07ePvttxEeHg5HR0drh1Ou9e3bV/08NDQUbdu2Ra1atfDHH3/AycnJipGVPyqVCq1atcLnn38OAGjRogUuXryIZcuWYdSoUVaOTj8suSmjqlWrQi6XF2lVHx8fD39/fytFZX35772k6+Lv748HDx5orc/NzcXjx4+1ttF1DM1zVCSTJk3C1q1bsW/fPtSoUUO93N/fH9nZ2UhMTNTavvD1Ku1aFLeNu7t7hfoCVygUqFu3LsLCwjB37lw0a9YM33zzDa9TIadOncKDBw/QsmVL2NnZwc7ODgcOHMDixYthZ2cHPz8/Xq9ieHp6on79+rh+/To/V4UEBASgcePGWssaNWqkrsarCN/vTG7KSKFQICwsDBEREeplKpUKERERaNeunRUjs67g4GD4+/trXZfk5GQcO3ZMfV3atWuHxMREnDp1Sr3N3r17oVKp0LZtW/U2Bw8eRE5Ojnqb8PBwNGjQAFWqVLHQuyk7URQxadIkbNy4EXv37kVwcLDW+rCwMNjb22tdrytXriAmJkbrel24cEHrCyM8PBzu7u7qL6J27dppHSN/m4r+WVSpVMjKyuJ1KqR79+64cOECzp49q360atUKI0aMUD/n9dItNTUVN27cQEBAAD9XhXTo0KHIUBVXr15FrVq1AFSQ7/cyN0kmcf369aKDg4O4cuVK8fLly+Lrr78uenp6arWqt0UpKSnimTNnxDNnzogAxK+++ko8c+aMGB0dLYqi1FXQ09NT3Lx5s3j+/HlxwIABOrsKtmjRQjx27Jh46NAhsV69elpdBRMTE0U/Pz/xlVdeES9evCiuX79edHZ2rnBdwSdMmCB6eHiI+/fv1+qKmp6ert5m/PjxYs2aNcW9e/eKJ0+eFNu1aye2a9dOvT6/K2qvXr3Es2fPijt37hR9fHx0dkV9//33xaioKPG7776rcF1Rp02bJh44cEC8deuWeP78eXHatGmiIAji7t27RVHkdSqNZm8pUeT1yvfee++J+/fvF2/duiVGRkaKPXr0EKtWrSo+ePBAFEVeJ03Hjx8X7ezsxDlz5ojXrl0T165dKzo7O4u//vqrepvy/v3O5MZEvv32W7FmzZqiQqEQ27RpIx49etTaIZndvn37RABFHqNGjRJFUeouOGPGDNHPz090cHAQu3fvLl65ckXrGI8ePRKHDRsmurq6iu7u7uKrr74qpqSkaG1z7tw5sWPHjqKDg4NYvXp1cd68eZZ6iyaj6zoBEFesWKHeJiMjQ3zjjTfEKlWqiM7OzuKgQYPE2NhYrePcvn1b7Nu3r+jk5CRWrVpVfO+998ScnBytbfbt2yc2b95cVCgUYu3atbXOURGMGTNGrFWrlqhQKEQfHx+xe/fu6sRGFHmdSlM4ueH1kgwdOlQMCAgQFQqFWL16dXHo0KFa47bwOmn7559/xKZNm4oODg5iw4YNxR9//FFrfXn/fhdEURTLVvZDREREVH6wzQ0RERHZFCY3REREZFOY3BAREZFNYXJDRERENoXJDREREdkUJjdERERkU5jcEBERkU1hckNEREQ2hckNEVU6QUFBWLRokbXDICIzYXJDRGY1evRoDBw4EADQtWtXTJ482WLnXrlyJTw9PYssP3HiBF5//XWLxUFElmVn7QCIiAyVnZ0NhUJh9P4+Pj4mjIaIyhuW3BCRRYwePRoHDhzAN998A0EQIAgCbt++DQC4ePEi+vbtC1dXV/j5+eGVV17Bw4cP1ft27doVkyZNwuTJk1G1alX07t0bAPDVV18hJCQELi4uCAwMxBtvvIHU1FQAwP79+/Hqq68iKSlJfb5PPvkEQNFqqZiYGAwYMACurq5wd3fHkCFDEB8fr17/ySefoHnz5lizZg2CgoLg4eGBl156CSkpKea9aERkFCY3RGQR33zzDdq1a4dx48YhNjYWsbGxCAwMRGJiIrp164YWLVrg5MmT2LlzJ+Lj4zFkyBCt/VetWgWFQoHIyEgsW7YMACCTybB48WJcunQJq1atwt69e/HBBx8AANq3b49FixbB3d1dfb4pU6YUiUulUmHAgAF4/PgxDhw4gPDwcNy8eRNDhw7V2u7GjRvYtGkTtm7diq1bt+LAgQOYN2+ema4WEZUFq6WIyCI8PDygUCjg7OwMf39/9fIlS5agRYsW+Pzzz9XLfvnlFwQGBuLq1auoX78+AKBevXr44osvtI6p2X4nKCgIn332GcaPH4/vv/8eCoUCHh4eEARB63yFRURE4MKFC7h16xYCAwMBAKtXr0aTJk1w4sQJtG7dGoCUBK1cuRJubm4AgFdeeQURERGYM2dO2S4MEZkcS26IyKrOnTuHffv2wdXVVf1o2LAhAKm0JF9YWFiRfffs2YPu3bujevXqcHNzwyuvvIJHjx4hPT1d7/NHRUUhMDBQndgAQOPGjeHp6YmoqCj1sqCgIHViAwABAQF48OCBQe+ViCyDJTdEZFWpqano378/5s+fX2RdQECA+rmLi4vWutu3b+PZZ5/FhAkTMGfOHHh5eeHQoUN47bXXkJ2dDWdnZ5PGaW9vr/VaEASoVCqTnoOITIPJDRFZjEKhgFKp1FrWsmVL/P333wgKCoKdnf5fSadOnYJKpcLChQshk0mF0H/88Uep5yusUaNGuHPnDu7cuaMuvbl8+TISExPRuHFjveMhovKD1VJEZDFBQUE4duwYbt++jYcPH0KlUmHixIl4/Pgxhg0bhhMnTuDGjRvYtWsXXn311RITk7p16yInJwfffvstbt68iTVr1qgbGmueLzU1FREREXj48KHO6qoePXogJCQEI0aMwOnTp3H8+HGMHDkSXbp0QatWrUx+DYjI/JjcEJHFTJkyBXK5HI0bN4aPjw9iYmJQrVo1REZGQqlUolevXggJCcHkyZPh6empLpHRpVmzZvjqq68wf/58NG3aFGvXrsXcuXO1tmnfvj3Gjx+PoUOHwsfHp0iDZECqXtq8eTOqVKmCzp07o0ePHqhduzZ+//13k79/IrIMQRRF0dpBEBEREZkKS26IiIjIpjC5ISIiIpvC5IaIiIhsCpMbIiIisilMboiIiMimMLkhIiIim8LkhoiIiGwKkxsiIiKyKUxuiIiIyKYwuSEiIiKbwuSGiIiIbMr/A+e+ezs7hArwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss, acc visualize\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.plot(acc_list, label='Training Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mspytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
