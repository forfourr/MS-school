{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression Test\n",
    ": 각 이미지는 28x28/ gray/ 10 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Label\\n0: 티셔츠\\n1: 바지\\n2: 스웨터\\n3: 드레스\\n4: 코트\\n5: 샌들\\n6: 셔츠\\n7: 운동화\\n8: 가방\\n9: 앵클 부츠\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Label\n",
    "0: 티셔츠\n",
    "1: 바지\n",
    "2: 스웨터\n",
    "3: 드레스\n",
    "4: 코트\n",
    "5: 샌들\n",
    "6: 셔츠\n",
    "7: 운동화\n",
    "8: 가방\n",
    "9: 앵클 부츠\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download dataset / dataloader 생성\n",
    "train_dataset = dataset.FashionMNIST(root = './data', train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "test_dataset = dataset.FashionMNIST(root = './data', train=False,\n",
    "                                     transform=transforms.ToTensor())\n",
    "\n",
    "# create train/test Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = 100,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = 100,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LosigiticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LosigiticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        #sigmoid X : 이진분류가 아니라 다중분류이기 때문이다.\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 *28     #이미지 크기가 28*28/ 흑백 아니라면 28*28*3\n",
    "num_classes = 10        #label이 10개\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "total_step = len(train_loader)\n",
    "print(total_step)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Loss, Optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LosigiticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LosigiticRegression(input_size=input_size,\n",
    "                            num_classes=num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device >> cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LosigiticRegression(\n",
       "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "#device setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device >>', device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/600], Loss: 2.3085, Train Acc:0.0700\n",
      "Epoch [1/10], Step [2/600], Loss: 2.3447, Train Acc:0.0600\n",
      "Epoch [1/10], Step [3/600], Loss: 2.3455, Train Acc:0.0633\n",
      "Epoch [1/10], Step [4/600], Loss: 2.2982, Train Acc:0.0700\n",
      "Epoch [1/10], Step [5/600], Loss: 2.2925, Train Acc:0.0720\n",
      "Epoch [1/10], Step [6/600], Loss: 2.2627, Train Acc:0.0733\n",
      "Epoch [1/10], Step [7/600], Loss: 2.2689, Train Acc:0.0800\n",
      "Epoch [1/10], Step [8/600], Loss: 2.2696, Train Acc:0.0862\n",
      "Epoch [1/10], Step [9/600], Loss: 2.2674, Train Acc:0.0933\n",
      "Epoch [1/10], Step [10/600], Loss: 2.2378, Train Acc:0.1030\n",
      "Epoch [1/10], Step [11/600], Loss: 2.1682, Train Acc:0.1182\n",
      "Epoch [1/10], Step [12/600], Loss: 2.2200, Train Acc:0.1283\n",
      "Epoch [1/10], Step [13/600], Loss: 2.1702, Train Acc:0.1469\n",
      "Epoch [1/10], Step [14/600], Loss: 2.1695, Train Acc:0.1586\n",
      "Epoch [1/10], Step [15/600], Loss: 2.1171, Train Acc:0.1707\n",
      "Epoch [1/10], Step [16/600], Loss: 2.1024, Train Acc:0.1844\n",
      "Epoch [1/10], Step [17/600], Loss: 2.1086, Train Acc:0.1918\n",
      "Epoch [1/10], Step [18/600], Loss: 2.0881, Train Acc:0.1994\n",
      "Epoch [1/10], Step [19/600], Loss: 2.1049, Train Acc:0.2095\n",
      "Epoch [1/10], Step [20/600], Loss: 2.0576, Train Acc:0.2240\n",
      "Epoch [1/10], Step [21/600], Loss: 2.0418, Train Acc:0.2314\n",
      "Epoch [1/10], Step [22/600], Loss: 2.0173, Train Acc:0.2418\n",
      "Epoch [1/10], Step [23/600], Loss: 1.9944, Train Acc:0.2535\n",
      "Epoch [1/10], Step [24/600], Loss: 2.0184, Train Acc:0.2567\n",
      "Epoch [1/10], Step [25/600], Loss: 2.0258, Train Acc:0.2624\n",
      "Epoch [1/10], Step [26/600], Loss: 2.0002, Train Acc:0.2658\n",
      "Epoch [1/10], Step [27/600], Loss: 1.9855, Train Acc:0.2711\n",
      "Epoch [1/10], Step [28/600], Loss: 1.9719, Train Acc:0.2754\n",
      "Epoch [1/10], Step [29/600], Loss: 1.9289, Train Acc:0.2800\n",
      "Epoch [1/10], Step [30/600], Loss: 1.9070, Train Acc:0.2867\n",
      "Epoch [1/10], Step [31/600], Loss: 1.8542, Train Acc:0.2942\n",
      "Epoch [1/10], Step [32/600], Loss: 1.9129, Train Acc:0.2991\n",
      "Epoch [1/10], Step [33/600], Loss: 1.8530, Train Acc:0.3048\n",
      "Epoch [1/10], Step [34/600], Loss: 1.8487, Train Acc:0.3091\n",
      "Epoch [1/10], Step [35/600], Loss: 1.8464, Train Acc:0.3134\n",
      "Epoch [1/10], Step [36/600], Loss: 1.8253, Train Acc:0.3189\n",
      "Epoch [1/10], Step [37/600], Loss: 1.8711, Train Acc:0.3227\n",
      "Epoch [1/10], Step [38/600], Loss: 1.8918, Train Acc:0.3255\n",
      "Epoch [1/10], Step [39/600], Loss: 1.7958, Train Acc:0.3310\n",
      "Epoch [1/10], Step [40/600], Loss: 1.7775, Train Acc:0.3355\n",
      "Epoch [1/10], Step [41/600], Loss: 1.7782, Train Acc:0.3390\n",
      "Epoch [1/10], Step [42/600], Loss: 1.7579, Train Acc:0.3421\n",
      "Epoch [1/10], Step [43/600], Loss: 1.6531, Train Acc:0.3470\n",
      "Epoch [1/10], Step [44/600], Loss: 1.7358, Train Acc:0.3514\n",
      "Epoch [1/10], Step [45/600], Loss: 1.7584, Train Acc:0.3549\n",
      "Epoch [1/10], Step [46/600], Loss: 1.7376, Train Acc:0.3609\n",
      "Epoch [1/10], Step [47/600], Loss: 1.7271, Train Acc:0.3649\n",
      "Epoch [1/10], Step [48/600], Loss: 1.6735, Train Acc:0.3700\n",
      "Epoch [1/10], Step [49/600], Loss: 1.6362, Train Acc:0.3747\n",
      "Epoch [1/10], Step [50/600], Loss: 1.6926, Train Acc:0.3784\n",
      "Epoch [1/10], Step [51/600], Loss: 1.6817, Train Acc:0.3839\n",
      "Epoch [1/10], Step [52/600], Loss: 1.6839, Train Acc:0.3873\n",
      "Epoch [1/10], Step [53/600], Loss: 1.6609, Train Acc:0.3908\n",
      "Epoch [1/10], Step [54/600], Loss: 1.6942, Train Acc:0.3935\n",
      "Epoch [1/10], Step [55/600], Loss: 1.6751, Train Acc:0.3955\n",
      "Epoch [1/10], Step [56/600], Loss: 1.6519, Train Acc:0.3991\n",
      "Epoch [1/10], Step [57/600], Loss: 1.6524, Train Acc:0.4018\n",
      "Epoch [1/10], Step [58/600], Loss: 1.6250, Train Acc:0.4045\n",
      "Epoch [1/10], Step [59/600], Loss: 1.6160, Train Acc:0.4076\n",
      "Epoch [1/10], Step [60/600], Loss: 1.5627, Train Acc:0.4118\n",
      "Epoch [1/10], Step [61/600], Loss: 1.5125, Train Acc:0.4156\n",
      "Epoch [1/10], Step [62/600], Loss: 1.5251, Train Acc:0.4200\n",
      "Epoch [1/10], Step [63/600], Loss: 1.6178, Train Acc:0.4225\n",
      "Epoch [1/10], Step [64/600], Loss: 1.5284, Train Acc:0.4259\n",
      "Epoch [1/10], Step [65/600], Loss: 1.5474, Train Acc:0.4286\n",
      "Epoch [1/10], Step [66/600], Loss: 1.5294, Train Acc:0.4314\n",
      "Epoch [1/10], Step [67/600], Loss: 1.5398, Train Acc:0.4346\n",
      "Epoch [1/10], Step [68/600], Loss: 1.5200, Train Acc:0.4372\n",
      "Epoch [1/10], Step [69/600], Loss: 1.4783, Train Acc:0.4414\n",
      "Epoch [1/10], Step [70/600], Loss: 1.5387, Train Acc:0.4430\n",
      "Epoch [1/10], Step [71/600], Loss: 1.4746, Train Acc:0.4461\n",
      "Epoch [1/10], Step [72/600], Loss: 1.4819, Train Acc:0.4486\n",
      "Epoch [1/10], Step [73/600], Loss: 1.5877, Train Acc:0.4504\n",
      "Epoch [1/10], Step [74/600], Loss: 1.4986, Train Acc:0.4532\n",
      "Epoch [1/10], Step [75/600], Loss: 1.5030, Train Acc:0.4559\n",
      "Epoch [1/10], Step [76/600], Loss: 1.5010, Train Acc:0.4584\n",
      "Epoch [1/10], Step [77/600], Loss: 1.5117, Train Acc:0.4604\n",
      "Epoch [1/10], Step [78/600], Loss: 1.5127, Train Acc:0.4619\n",
      "Epoch [1/10], Step [79/600], Loss: 1.4530, Train Acc:0.4642\n",
      "Epoch [1/10], Step [80/600], Loss: 1.3981, Train Acc:0.4665\n",
      "Epoch [1/10], Step [81/600], Loss: 1.5099, Train Acc:0.4680\n",
      "Epoch [1/10], Step [82/600], Loss: 1.4098, Train Acc:0.4702\n",
      "Epoch [1/10], Step [83/600], Loss: 1.4326, Train Acc:0.4723\n",
      "Epoch [1/10], Step [84/600], Loss: 1.4055, Train Acc:0.4746\n",
      "Epoch [1/10], Step [85/600], Loss: 1.3954, Train Acc:0.4769\n",
      "Epoch [1/10], Step [86/600], Loss: 1.3754, Train Acc:0.4792\n",
      "Epoch [1/10], Step [87/600], Loss: 1.4032, Train Acc:0.4810\n",
      "Epoch [1/10], Step [88/600], Loss: 1.4451, Train Acc:0.4826\n",
      "Epoch [1/10], Step [89/600], Loss: 1.4376, Train Acc:0.4840\n",
      "Epoch [1/10], Step [90/600], Loss: 1.3567, Train Acc:0.4866\n",
      "Epoch [1/10], Step [91/600], Loss: 1.5037, Train Acc:0.4870\n",
      "Epoch [1/10], Step [92/600], Loss: 1.3270, Train Acc:0.4887\n",
      "Epoch [1/10], Step [93/600], Loss: 1.3553, Train Acc:0.4912\n",
      "Epoch [1/10], Step [94/600], Loss: 1.3169, Train Acc:0.4932\n",
      "Epoch [1/10], Step [95/600], Loss: 1.4006, Train Acc:0.4952\n",
      "Epoch [1/10], Step [96/600], Loss: 1.3511, Train Acc:0.4968\n",
      "Epoch [1/10], Step [97/600], Loss: 1.3632, Train Acc:0.4991\n",
      "Epoch [1/10], Step [98/600], Loss: 1.3551, Train Acc:0.5007\n",
      "Epoch [1/10], Step [99/600], Loss: 1.2377, Train Acc:0.5029\n",
      "Epoch [1/10], Step [100/600], Loss: 1.2904, Train Acc:0.5045\n",
      "Epoch [1/10], Step [101/600], Loss: 1.3480, Train Acc:0.5055\n",
      "Epoch [1/10], Step [102/600], Loss: 1.3746, Train Acc:0.5066\n",
      "Epoch [1/10], Step [103/600], Loss: 1.4341, Train Acc:0.5077\n",
      "Epoch [1/10], Step [104/600], Loss: 1.2506, Train Acc:0.5099\n",
      "Epoch [1/10], Step [105/600], Loss: 1.2923, Train Acc:0.5115\n",
      "Epoch [1/10], Step [106/600], Loss: 1.3603, Train Acc:0.5124\n",
      "Epoch [1/10], Step [107/600], Loss: 1.3335, Train Acc:0.5138\n",
      "Epoch [1/10], Step [108/600], Loss: 1.2965, Train Acc:0.5158\n",
      "Epoch [1/10], Step [109/600], Loss: 1.3850, Train Acc:0.5164\n",
      "Epoch [1/10], Step [110/600], Loss: 1.3064, Train Acc:0.5177\n",
      "Epoch [1/10], Step [111/600], Loss: 1.2541, Train Acc:0.5191\n",
      "Epoch [1/10], Step [112/600], Loss: 1.2746, Train Acc:0.5202\n",
      "Epoch [1/10], Step [113/600], Loss: 1.3086, Train Acc:0.5212\n",
      "Epoch [1/10], Step [114/600], Loss: 1.2420, Train Acc:0.5225\n",
      "Epoch [1/10], Step [115/600], Loss: 1.3250, Train Acc:0.5230\n",
      "Epoch [1/10], Step [116/600], Loss: 1.3302, Train Acc:0.5235\n",
      "Epoch [1/10], Step [117/600], Loss: 1.3201, Train Acc:0.5239\n",
      "Epoch [1/10], Step [118/600], Loss: 1.3423, Train Acc:0.5247\n",
      "Epoch [1/10], Step [119/600], Loss: 1.3789, Train Acc:0.5255\n",
      "Epoch [1/10], Step [120/600], Loss: 1.2945, Train Acc:0.5266\n",
      "Epoch [1/10], Step [121/600], Loss: 1.2719, Train Acc:0.5279\n",
      "Epoch [1/10], Step [122/600], Loss: 1.2391, Train Acc:0.5290\n",
      "Epoch [1/10], Step [123/600], Loss: 1.2524, Train Acc:0.5303\n",
      "Epoch [1/10], Step [124/600], Loss: 1.2499, Train Acc:0.5314\n",
      "Epoch [1/10], Step [125/600], Loss: 1.3226, Train Acc:0.5316\n",
      "Epoch [1/10], Step [126/600], Loss: 1.2012, Train Acc:0.5328\n",
      "Epoch [1/10], Step [127/600], Loss: 1.1998, Train Acc:0.5337\n",
      "Epoch [1/10], Step [128/600], Loss: 1.2529, Train Acc:0.5349\n",
      "Epoch [1/10], Step [129/600], Loss: 1.3329, Train Acc:0.5354\n",
      "Epoch [1/10], Step [130/600], Loss: 1.2075, Train Acc:0.5368\n",
      "Epoch [1/10], Step [131/600], Loss: 1.1648, Train Acc:0.5384\n",
      "Epoch [1/10], Step [132/600], Loss: 1.2295, Train Acc:0.5391\n",
      "Epoch [1/10], Step [133/600], Loss: 1.3015, Train Acc:0.5399\n",
      "Epoch [1/10], Step [134/600], Loss: 1.2559, Train Acc:0.5405\n",
      "Epoch [1/10], Step [135/600], Loss: 1.2116, Train Acc:0.5414\n",
      "Epoch [1/10], Step [136/600], Loss: 1.2897, Train Acc:0.5419\n",
      "Epoch [1/10], Step [137/600], Loss: 1.1761, Train Acc:0.5431\n",
      "Epoch [1/10], Step [138/600], Loss: 1.1567, Train Acc:0.5441\n",
      "Epoch [1/10], Step [139/600], Loss: 1.1723, Train Acc:0.5452\n",
      "Epoch [1/10], Step [140/600], Loss: 1.1289, Train Acc:0.5463\n",
      "Epoch [1/10], Step [141/600], Loss: 1.2546, Train Acc:0.5466\n",
      "Epoch [1/10], Step [142/600], Loss: 1.1102, Train Acc:0.5478\n",
      "Epoch [1/10], Step [143/600], Loss: 1.2912, Train Acc:0.5483\n",
      "Epoch [1/10], Step [144/600], Loss: 1.1444, Train Acc:0.5493\n",
      "Epoch [1/10], Step [145/600], Loss: 1.1471, Train Acc:0.5503\n",
      "Epoch [1/10], Step [146/600], Loss: 1.2265, Train Acc:0.5505\n",
      "Epoch [1/10], Step [147/600], Loss: 1.2490, Train Acc:0.5511\n",
      "Epoch [1/10], Step [148/600], Loss: 1.1992, Train Acc:0.5516\n",
      "Epoch [1/10], Step [149/600], Loss: 1.1403, Train Acc:0.5523\n",
      "Epoch [1/10], Step [150/600], Loss: 1.2541, Train Acc:0.5525\n",
      "Epoch [1/10], Step [151/600], Loss: 1.1696, Train Acc:0.5534\n",
      "Epoch [1/10], Step [152/600], Loss: 1.1833, Train Acc:0.5540\n",
      "Epoch [1/10], Step [153/600], Loss: 1.2561, Train Acc:0.5542\n",
      "Epoch [1/10], Step [154/600], Loss: 1.1041, Train Acc:0.5555\n",
      "Epoch [1/10], Step [155/600], Loss: 1.1670, Train Acc:0.5559\n",
      "Epoch [1/10], Step [156/600], Loss: 1.1442, Train Acc:0.5570\n",
      "Epoch [1/10], Step [157/600], Loss: 1.2435, Train Acc:0.5574\n",
      "Epoch [1/10], Step [158/600], Loss: 1.2202, Train Acc:0.5578\n",
      "Epoch [1/10], Step [159/600], Loss: 1.2132, Train Acc:0.5585\n",
      "Epoch [1/10], Step [160/600], Loss: 1.0877, Train Acc:0.5594\n",
      "Epoch [1/10], Step [161/600], Loss: 1.1063, Train Acc:0.5602\n",
      "Epoch [1/10], Step [162/600], Loss: 1.0832, Train Acc:0.5612\n",
      "Epoch [1/10], Step [163/600], Loss: 1.1241, Train Acc:0.5620\n",
      "Epoch [1/10], Step [164/600], Loss: 1.0924, Train Acc:0.5628\n",
      "Epoch [1/10], Step [165/600], Loss: 1.1009, Train Acc:0.5638\n",
      "Epoch [1/10], Step [166/600], Loss: 1.1586, Train Acc:0.5645\n",
      "Epoch [1/10], Step [167/600], Loss: 1.1643, Train Acc:0.5653\n",
      "Epoch [1/10], Step [168/600], Loss: 1.2151, Train Acc:0.5658\n",
      "Epoch [1/10], Step [169/600], Loss: 1.1168, Train Acc:0.5664\n",
      "Epoch [1/10], Step [170/600], Loss: 1.0836, Train Acc:0.5674\n",
      "Epoch [1/10], Step [171/600], Loss: 1.2716, Train Acc:0.5675\n",
      "Epoch [1/10], Step [172/600], Loss: 1.0774, Train Acc:0.5683\n",
      "Epoch [1/10], Step [173/600], Loss: 1.0980, Train Acc:0.5692\n",
      "Epoch [1/10], Step [174/600], Loss: 1.2062, Train Acc:0.5695\n",
      "Epoch [1/10], Step [175/600], Loss: 1.1262, Train Acc:0.5699\n",
      "Epoch [1/10], Step [176/600], Loss: 1.0824, Train Acc:0.5707\n",
      "Epoch [1/10], Step [177/600], Loss: 1.1229, Train Acc:0.5714\n",
      "Epoch [1/10], Step [178/600], Loss: 1.0778, Train Acc:0.5722\n",
      "Epoch [1/10], Step [179/600], Loss: 1.1806, Train Acc:0.5725\n",
      "Epoch [1/10], Step [180/600], Loss: 1.1977, Train Acc:0.5726\n",
      "Epoch [1/10], Step [181/600], Loss: 1.1177, Train Acc:0.5733\n",
      "Epoch [1/10], Step [182/600], Loss: 1.1430, Train Acc:0.5734\n",
      "Epoch [1/10], Step [183/600], Loss: 1.1392, Train Acc:0.5740\n",
      "Epoch [1/10], Step [184/600], Loss: 1.1200, Train Acc:0.5744\n",
      "Epoch [1/10], Step [185/600], Loss: 1.0673, Train Acc:0.5752\n",
      "Epoch [1/10], Step [186/600], Loss: 1.0571, Train Acc:0.5757\n",
      "Epoch [1/10], Step [187/600], Loss: 1.0574, Train Acc:0.5764\n",
      "Epoch [1/10], Step [188/600], Loss: 1.0322, Train Acc:0.5770\n",
      "Epoch [1/10], Step [189/600], Loss: 1.1685, Train Acc:0.5775\n",
      "Epoch [1/10], Step [190/600], Loss: 1.0179, Train Acc:0.5781\n",
      "Epoch [1/10], Step [191/600], Loss: 1.1222, Train Acc:0.5784\n",
      "Epoch [1/10], Step [192/600], Loss: 1.0008, Train Acc:0.5794\n",
      "Epoch [1/10], Step [193/600], Loss: 1.0226, Train Acc:0.5799\n",
      "Epoch [1/10], Step [194/600], Loss: 1.0975, Train Acc:0.5804\n",
      "Epoch [1/10], Step [195/600], Loss: 1.1284, Train Acc:0.5807\n",
      "Epoch [1/10], Step [196/600], Loss: 1.0983, Train Acc:0.5812\n",
      "Epoch [1/10], Step [197/600], Loss: 1.1061, Train Acc:0.5817\n",
      "Epoch [1/10], Step [198/600], Loss: 0.9956, Train Acc:0.5825\n",
      "Epoch [1/10], Step [199/600], Loss: 1.0729, Train Acc:0.5830\n",
      "Epoch [1/10], Step [200/600], Loss: 1.1197, Train Acc:0.5832\n",
      "Epoch [1/10], Step [201/600], Loss: 1.1194, Train Acc:0.5835\n",
      "Epoch [1/10], Step [202/600], Loss: 1.0119, Train Acc:0.5843\n",
      "Epoch [1/10], Step [203/600], Loss: 0.9680, Train Acc:0.5849\n",
      "Epoch [1/10], Step [204/600], Loss: 1.0775, Train Acc:0.5852\n",
      "Epoch [1/10], Step [205/600], Loss: 1.1209, Train Acc:0.5856\n",
      "Epoch [1/10], Step [206/600], Loss: 1.0431, Train Acc:0.5861\n",
      "Epoch [1/10], Step [207/600], Loss: 1.0122, Train Acc:0.5865\n",
      "Epoch [1/10], Step [208/600], Loss: 1.0045, Train Acc:0.5870\n",
      "Epoch [1/10], Step [209/600], Loss: 1.0203, Train Acc:0.5873\n",
      "Epoch [1/10], Step [210/600], Loss: 0.9884, Train Acc:0.5879\n",
      "Epoch [1/10], Step [211/600], Loss: 1.0443, Train Acc:0.5884\n",
      "Epoch [1/10], Step [212/600], Loss: 1.0706, Train Acc:0.5891\n",
      "Epoch [1/10], Step [213/600], Loss: 1.1708, Train Acc:0.5891\n",
      "Epoch [1/10], Step [214/600], Loss: 0.9578, Train Acc:0.5897\n",
      "Epoch [1/10], Step [215/600], Loss: 1.2063, Train Acc:0.5897\n",
      "Epoch [1/10], Step [216/600], Loss: 1.0446, Train Acc:0.5900\n",
      "Epoch [1/10], Step [217/600], Loss: 1.0011, Train Acc:0.5908\n",
      "Epoch [1/10], Step [218/600], Loss: 1.0461, Train Acc:0.5914\n",
      "Epoch [1/10], Step [219/600], Loss: 0.9774, Train Acc:0.5921\n",
      "Epoch [1/10], Step [220/600], Loss: 1.1898, Train Acc:0.5923\n",
      "Epoch [1/10], Step [221/600], Loss: 1.0792, Train Acc:0.5927\n",
      "Epoch [1/10], Step [222/600], Loss: 1.0022, Train Acc:0.5930\n",
      "Epoch [1/10], Step [223/600], Loss: 1.0569, Train Acc:0.5935\n",
      "Epoch [1/10], Step [224/600], Loss: 1.0857, Train Acc:0.5939\n",
      "Epoch [1/10], Step [225/600], Loss: 0.9707, Train Acc:0.5944\n",
      "Epoch [1/10], Step [226/600], Loss: 1.0451, Train Acc:0.5946\n",
      "Epoch [1/10], Step [227/600], Loss: 0.9775, Train Acc:0.5949\n",
      "Epoch [1/10], Step [228/600], Loss: 0.9393, Train Acc:0.5954\n",
      "Epoch [1/10], Step [229/600], Loss: 0.9972, Train Acc:0.5961\n",
      "Epoch [1/10], Step [230/600], Loss: 1.0741, Train Acc:0.5963\n",
      "Epoch [1/10], Step [231/600], Loss: 0.9776, Train Acc:0.5969\n",
      "Epoch [1/10], Step [232/600], Loss: 1.0673, Train Acc:0.5973\n",
      "Epoch [1/10], Step [233/600], Loss: 1.1014, Train Acc:0.5976\n",
      "Epoch [1/10], Step [234/600], Loss: 0.9461, Train Acc:0.5979\n",
      "Epoch [1/10], Step [235/600], Loss: 1.0081, Train Acc:0.5983\n",
      "Epoch [1/10], Step [236/600], Loss: 1.0730, Train Acc:0.5983\n",
      "Epoch [1/10], Step [237/600], Loss: 1.0839, Train Acc:0.5983\n",
      "Epoch [1/10], Step [238/600], Loss: 0.9174, Train Acc:0.5990\n",
      "Epoch [1/10], Step [239/600], Loss: 0.9036, Train Acc:0.5997\n",
      "Epoch [1/10], Step [240/600], Loss: 0.8911, Train Acc:0.6002\n",
      "Epoch [1/10], Step [241/600], Loss: 1.0863, Train Acc:0.6004\n",
      "Epoch [1/10], Step [242/600], Loss: 1.0720, Train Acc:0.6005\n",
      "Epoch [1/10], Step [243/600], Loss: 1.0085, Train Acc:0.6009\n",
      "Epoch [1/10], Step [244/600], Loss: 1.0039, Train Acc:0.6014\n",
      "Epoch [1/10], Step [245/600], Loss: 0.9769, Train Acc:0.6016\n",
      "Epoch [1/10], Step [246/600], Loss: 1.0191, Train Acc:0.6019\n",
      "Epoch [1/10], Step [247/600], Loss: 1.0286, Train Acc:0.6022\n",
      "Epoch [1/10], Step [248/600], Loss: 1.0067, Train Acc:0.6025\n",
      "Epoch [1/10], Step [249/600], Loss: 1.0777, Train Acc:0.6026\n",
      "Epoch [1/10], Step [250/600], Loss: 1.0568, Train Acc:0.6028\n",
      "Epoch [1/10], Step [251/600], Loss: 1.2135, Train Acc:0.6027\n",
      "Epoch [1/10], Step [252/600], Loss: 1.0540, Train Acc:0.6028\n",
      "Epoch [1/10], Step [253/600], Loss: 0.9856, Train Acc:0.6032\n",
      "Epoch [1/10], Step [254/600], Loss: 1.0128, Train Acc:0.6034\n",
      "Epoch [1/10], Step [255/600], Loss: 0.9888, Train Acc:0.6037\n",
      "Epoch [1/10], Step [256/600], Loss: 1.0398, Train Acc:0.6038\n",
      "Epoch [1/10], Step [257/600], Loss: 0.9591, Train Acc:0.6043\n",
      "Epoch [1/10], Step [258/600], Loss: 0.9602, Train Acc:0.6048\n",
      "Epoch [1/10], Step [259/600], Loss: 0.9876, Train Acc:0.6050\n",
      "Epoch [1/10], Step [260/600], Loss: 0.9619, Train Acc:0.6053\n",
      "Epoch [1/10], Step [261/600], Loss: 0.9715, Train Acc:0.6056\n",
      "Epoch [1/10], Step [262/600], Loss: 0.9499, Train Acc:0.6060\n",
      "Epoch [1/10], Step [263/600], Loss: 1.0390, Train Acc:0.6063\n",
      "Epoch [1/10], Step [264/600], Loss: 0.9276, Train Acc:0.6069\n",
      "Epoch [1/10], Step [265/600], Loss: 1.0372, Train Acc:0.6069\n",
      "Epoch [1/10], Step [266/600], Loss: 0.9271, Train Acc:0.6073\n",
      "Epoch [1/10], Step [267/600], Loss: 0.8787, Train Acc:0.6078\n",
      "Epoch [1/10], Step [268/600], Loss: 0.9485, Train Acc:0.6080\n",
      "Epoch [1/10], Step [269/600], Loss: 1.0224, Train Acc:0.6084\n",
      "Epoch [1/10], Step [270/600], Loss: 0.9757, Train Acc:0.6085\n",
      "Epoch [1/10], Step [271/600], Loss: 0.9543, Train Acc:0.6089\n",
      "Epoch [1/10], Step [272/600], Loss: 0.8446, Train Acc:0.6097\n",
      "Epoch [1/10], Step [273/600], Loss: 0.8938, Train Acc:0.6103\n",
      "Epoch [1/10], Step [274/600], Loss: 0.9677, Train Acc:0.6105\n",
      "Epoch [1/10], Step [275/600], Loss: 1.0054, Train Acc:0.6107\n",
      "Epoch [1/10], Step [276/600], Loss: 0.8171, Train Acc:0.6114\n",
      "Epoch [1/10], Step [277/600], Loss: 1.0239, Train Acc:0.6115\n",
      "Epoch [1/10], Step [278/600], Loss: 0.9499, Train Acc:0.6119\n",
      "Epoch [1/10], Step [279/600], Loss: 1.0229, Train Acc:0.6122\n",
      "Epoch [1/10], Step [280/600], Loss: 0.9624, Train Acc:0.6125\n",
      "Epoch [1/10], Step [281/600], Loss: 0.9576, Train Acc:0.6129\n",
      "Epoch [1/10], Step [282/600], Loss: 0.9039, Train Acc:0.6134\n",
      "Epoch [1/10], Step [283/600], Loss: 0.9789, Train Acc:0.6137\n",
      "Epoch [1/10], Step [284/600], Loss: 1.0034, Train Acc:0.6140\n",
      "Epoch [1/10], Step [285/600], Loss: 1.0018, Train Acc:0.6140\n",
      "Epoch [1/10], Step [286/600], Loss: 0.9068, Train Acc:0.6144\n",
      "Epoch [1/10], Step [287/600], Loss: 0.9143, Train Acc:0.6147\n",
      "Epoch [1/10], Step [288/600], Loss: 0.9141, Train Acc:0.6152\n",
      "Epoch [1/10], Step [289/600], Loss: 0.8889, Train Acc:0.6155\n",
      "Epoch [1/10], Step [290/600], Loss: 0.9645, Train Acc:0.6160\n",
      "Epoch [1/10], Step [291/600], Loss: 0.9907, Train Acc:0.6161\n",
      "Epoch [1/10], Step [292/600], Loss: 1.0730, Train Acc:0.6160\n",
      "Epoch [1/10], Step [293/600], Loss: 1.0634, Train Acc:0.6161\n",
      "Epoch [1/10], Step [294/600], Loss: 0.9991, Train Acc:0.6163\n",
      "Epoch [1/10], Step [295/600], Loss: 0.9498, Train Acc:0.6167\n",
      "Epoch [1/10], Step [296/600], Loss: 0.9228, Train Acc:0.6170\n",
      "Epoch [1/10], Step [297/600], Loss: 0.9542, Train Acc:0.6172\n",
      "Epoch [1/10], Step [298/600], Loss: 0.9437, Train Acc:0.6173\n",
      "Epoch [1/10], Step [299/600], Loss: 1.0177, Train Acc:0.6175\n",
      "Epoch [1/10], Step [300/600], Loss: 0.9479, Train Acc:0.6177\n",
      "Epoch [1/10], Step [301/600], Loss: 0.9284, Train Acc:0.6180\n",
      "Epoch [1/10], Step [302/600], Loss: 0.9264, Train Acc:0.6184\n",
      "Epoch [1/10], Step [303/600], Loss: 0.9898, Train Acc:0.6187\n",
      "Epoch [1/10], Step [304/600], Loss: 1.0213, Train Acc:0.6190\n",
      "Epoch [1/10], Step [305/600], Loss: 1.0085, Train Acc:0.6191\n",
      "Epoch [1/10], Step [306/600], Loss: 0.9432, Train Acc:0.6192\n",
      "Epoch [1/10], Step [307/600], Loss: 0.9370, Train Acc:0.6194\n",
      "Epoch [1/10], Step [308/600], Loss: 0.8893, Train Acc:0.6198\n",
      "Epoch [1/10], Step [309/600], Loss: 1.0021, Train Acc:0.6201\n",
      "Epoch [1/10], Step [310/600], Loss: 0.9316, Train Acc:0.6205\n",
      "Epoch [1/10], Step [311/600], Loss: 1.0424, Train Acc:0.6205\n",
      "Epoch [1/10], Step [312/600], Loss: 0.9177, Train Acc:0.6211\n",
      "Epoch [1/10], Step [313/600], Loss: 0.9332, Train Acc:0.6214\n",
      "Epoch [1/10], Step [314/600], Loss: 0.9477, Train Acc:0.6215\n",
      "Epoch [1/10], Step [315/600], Loss: 0.8334, Train Acc:0.6220\n",
      "Epoch [1/10], Step [316/600], Loss: 0.9702, Train Acc:0.6221\n",
      "Epoch [1/10], Step [317/600], Loss: 1.0175, Train Acc:0.6221\n",
      "Epoch [1/10], Step [318/600], Loss: 0.9278, Train Acc:0.6224\n",
      "Epoch [1/10], Step [319/600], Loss: 0.9363, Train Acc:0.6228\n",
      "Epoch [1/10], Step [320/600], Loss: 0.8586, Train Acc:0.6231\n",
      "Epoch [1/10], Step [321/600], Loss: 0.8716, Train Acc:0.6235\n",
      "Epoch [1/10], Step [322/600], Loss: 0.9684, Train Acc:0.6238\n",
      "Epoch [1/10], Step [323/600], Loss: 0.9096, Train Acc:0.6239\n",
      "Epoch [1/10], Step [324/600], Loss: 1.0302, Train Acc:0.6240\n",
      "Epoch [1/10], Step [325/600], Loss: 1.0682, Train Acc:0.6241\n",
      "Epoch [1/10], Step [326/600], Loss: 1.1210, Train Acc:0.6240\n",
      "Epoch [1/10], Step [327/600], Loss: 0.9711, Train Acc:0.6243\n",
      "Epoch [1/10], Step [328/600], Loss: 0.8850, Train Acc:0.6245\n",
      "Epoch [1/10], Step [329/600], Loss: 0.8879, Train Acc:0.6247\n",
      "Epoch [1/10], Step [330/600], Loss: 0.9149, Train Acc:0.6249\n",
      "Epoch [1/10], Step [331/600], Loss: 0.9672, Train Acc:0.6252\n",
      "Epoch [1/10], Step [332/600], Loss: 0.8787, Train Acc:0.6256\n",
      "Epoch [1/10], Step [333/600], Loss: 0.9053, Train Acc:0.6259\n",
      "Epoch [1/10], Step [334/600], Loss: 0.8822, Train Acc:0.6261\n",
      "Epoch [1/10], Step [335/600], Loss: 0.8824, Train Acc:0.6265\n",
      "Epoch [1/10], Step [336/600], Loss: 0.8284, Train Acc:0.6268\n",
      "Epoch [1/10], Step [337/600], Loss: 0.7661, Train Acc:0.6271\n",
      "Epoch [1/10], Step [338/600], Loss: 0.9942, Train Acc:0.6272\n",
      "Epoch [1/10], Step [339/600], Loss: 0.8214, Train Acc:0.6276\n",
      "Epoch [1/10], Step [340/600], Loss: 0.9760, Train Acc:0.6278\n",
      "Epoch [1/10], Step [341/600], Loss: 0.9224, Train Acc:0.6279\n",
      "Epoch [1/10], Step [342/600], Loss: 0.8769, Train Acc:0.6282\n",
      "Epoch [1/10], Step [343/600], Loss: 0.9041, Train Acc:0.6285\n",
      "Epoch [1/10], Step [344/600], Loss: 0.9411, Train Acc:0.6286\n",
      "Epoch [1/10], Step [345/600], Loss: 0.9229, Train Acc:0.6288\n",
      "Epoch [1/10], Step [346/600], Loss: 0.8859, Train Acc:0.6290\n",
      "Epoch [1/10], Step [347/600], Loss: 0.9531, Train Acc:0.6292\n",
      "Epoch [1/10], Step [348/600], Loss: 0.8805, Train Acc:0.6293\n",
      "Epoch [1/10], Step [349/600], Loss: 0.9140, Train Acc:0.6295\n",
      "Epoch [1/10], Step [350/600], Loss: 0.8291, Train Acc:0.6300\n",
      "Epoch [1/10], Step [351/600], Loss: 0.9152, Train Acc:0.6302\n",
      "Epoch [1/10], Step [352/600], Loss: 1.0926, Train Acc:0.6301\n",
      "Epoch [1/10], Step [353/600], Loss: 0.8371, Train Acc:0.6304\n",
      "Epoch [1/10], Step [354/600], Loss: 0.8638, Train Acc:0.6308\n",
      "Epoch [1/10], Step [355/600], Loss: 0.9305, Train Acc:0.6309\n",
      "Epoch [1/10], Step [356/600], Loss: 0.9217, Train Acc:0.6314\n",
      "Epoch [1/10], Step [357/600], Loss: 1.0171, Train Acc:0.6315\n",
      "Epoch [1/10], Step [358/600], Loss: 0.8193, Train Acc:0.6318\n",
      "Epoch [1/10], Step [359/600], Loss: 0.8538, Train Acc:0.6321\n",
      "Epoch [1/10], Step [360/600], Loss: 0.7601, Train Acc:0.6325\n",
      "Epoch [1/10], Step [361/600], Loss: 0.8263, Train Acc:0.6328\n",
      "Epoch [1/10], Step [362/600], Loss: 0.8921, Train Acc:0.6331\n",
      "Epoch [1/10], Step [363/600], Loss: 0.9719, Train Acc:0.6332\n",
      "Epoch [1/10], Step [364/600], Loss: 0.8546, Train Acc:0.6335\n",
      "Epoch [1/10], Step [365/600], Loss: 0.9983, Train Acc:0.6337\n",
      "Epoch [1/10], Step [366/600], Loss: 0.8496, Train Acc:0.6340\n",
      "Epoch [1/10], Step [367/600], Loss: 0.9134, Train Acc:0.6341\n",
      "Epoch [1/10], Step [368/600], Loss: 0.8447, Train Acc:0.6345\n",
      "Epoch [1/10], Step [369/600], Loss: 0.9511, Train Acc:0.6344\n",
      "Epoch [1/10], Step [370/600], Loss: 0.8352, Train Acc:0.6346\n",
      "Epoch [1/10], Step [371/600], Loss: 0.8647, Train Acc:0.6349\n",
      "Epoch [1/10], Step [372/600], Loss: 0.9052, Train Acc:0.6351\n",
      "Epoch [1/10], Step [373/600], Loss: 0.9021, Train Acc:0.6353\n",
      "Epoch [1/10], Step [374/600], Loss: 0.8271, Train Acc:0.6356\n",
      "Epoch [1/10], Step [375/600], Loss: 1.0389, Train Acc:0.6356\n",
      "Epoch [1/10], Step [376/600], Loss: 1.0198, Train Acc:0.6357\n",
      "Epoch [1/10], Step [377/600], Loss: 0.9306, Train Acc:0.6359\n",
      "Epoch [1/10], Step [378/600], Loss: 0.8436, Train Acc:0.6362\n",
      "Epoch [1/10], Step [379/600], Loss: 1.0706, Train Acc:0.6361\n",
      "Epoch [1/10], Step [380/600], Loss: 0.8674, Train Acc:0.6364\n",
      "Epoch [1/10], Step [381/600], Loss: 0.9543, Train Acc:0.6365\n",
      "Epoch [1/10], Step [382/600], Loss: 0.9237, Train Acc:0.6367\n",
      "Epoch [1/10], Step [383/600], Loss: 0.7851, Train Acc:0.6371\n",
      "Epoch [1/10], Step [384/600], Loss: 0.8912, Train Acc:0.6373\n",
      "Epoch [1/10], Step [385/600], Loss: 0.8476, Train Acc:0.6375\n",
      "Epoch [1/10], Step [386/600], Loss: 0.8927, Train Acc:0.6376\n",
      "Epoch [1/10], Step [387/600], Loss: 0.8618, Train Acc:0.6380\n",
      "Epoch [1/10], Step [388/600], Loss: 0.7614, Train Acc:0.6384\n",
      "Epoch [1/10], Step [389/600], Loss: 0.9216, Train Acc:0.6385\n",
      "Epoch [1/10], Step [390/600], Loss: 0.7931, Train Acc:0.6389\n",
      "Epoch [1/10], Step [391/600], Loss: 0.8923, Train Acc:0.6391\n",
      "Epoch [1/10], Step [392/600], Loss: 1.0069, Train Acc:0.6391\n",
      "Epoch [1/10], Step [393/600], Loss: 1.0484, Train Acc:0.6390\n",
      "Epoch [1/10], Step [394/600], Loss: 0.7885, Train Acc:0.6393\n",
      "Epoch [1/10], Step [395/600], Loss: 0.8856, Train Acc:0.6396\n",
      "Epoch [1/10], Step [396/600], Loss: 0.8268, Train Acc:0.6399\n",
      "Epoch [1/10], Step [397/600], Loss: 0.8870, Train Acc:0.6401\n",
      "Epoch [1/10], Step [398/600], Loss: 0.8537, Train Acc:0.6403\n",
      "Epoch [1/10], Step [399/600], Loss: 0.8691, Train Acc:0.6403\n",
      "Epoch [1/10], Step [400/600], Loss: 0.9225, Train Acc:0.6404\n",
      "Epoch [1/10], Step [401/600], Loss: 0.8731, Train Acc:0.6405\n",
      "Epoch [1/10], Step [402/600], Loss: 0.7919, Train Acc:0.6408\n",
      "Epoch [1/10], Step [403/600], Loss: 0.9879, Train Acc:0.6408\n",
      "Epoch [1/10], Step [404/600], Loss: 0.8292, Train Acc:0.6410\n",
      "Epoch [1/10], Step [405/600], Loss: 0.8972, Train Acc:0.6412\n",
      "Epoch [1/10], Step [406/600], Loss: 0.7802, Train Acc:0.6415\n",
      "Epoch [1/10], Step [407/600], Loss: 0.7581, Train Acc:0.6419\n",
      "Epoch [1/10], Step [408/600], Loss: 0.9449, Train Acc:0.6420\n",
      "Epoch [1/10], Step [409/600], Loss: 0.9149, Train Acc:0.6422\n",
      "Epoch [1/10], Step [410/600], Loss: 0.8325, Train Acc:0.6424\n",
      "Epoch [1/10], Step [411/600], Loss: 0.8968, Train Acc:0.6426\n",
      "Epoch [1/10], Step [412/600], Loss: 0.9316, Train Acc:0.6426\n",
      "Epoch [1/10], Step [413/600], Loss: 0.9332, Train Acc:0.6429\n",
      "Epoch [1/10], Step [414/600], Loss: 0.8164, Train Acc:0.6433\n",
      "Epoch [1/10], Step [415/600], Loss: 0.8492, Train Acc:0.6435\n",
      "Epoch [1/10], Step [416/600], Loss: 0.8041, Train Acc:0.6438\n",
      "Epoch [1/10], Step [417/600], Loss: 0.9296, Train Acc:0.6439\n",
      "Epoch [1/10], Step [418/600], Loss: 0.8134, Train Acc:0.6443\n",
      "Epoch [1/10], Step [419/600], Loss: 0.9360, Train Acc:0.6444\n",
      "Epoch [1/10], Step [420/600], Loss: 0.8523, Train Acc:0.6446\n",
      "Epoch [1/10], Step [421/600], Loss: 0.7663, Train Acc:0.6449\n",
      "Epoch [1/10], Step [422/600], Loss: 0.8190, Train Acc:0.6452\n",
      "Epoch [1/10], Step [423/600], Loss: 0.8364, Train Acc:0.6454\n",
      "Epoch [1/10], Step [424/600], Loss: 0.8138, Train Acc:0.6458\n",
      "Epoch [1/10], Step [425/600], Loss: 0.9605, Train Acc:0.6459\n",
      "Epoch [1/10], Step [426/600], Loss: 0.8326, Train Acc:0.6461\n",
      "Epoch [1/10], Step [427/600], Loss: 0.8720, Train Acc:0.6463\n",
      "Epoch [1/10], Step [428/600], Loss: 1.0300, Train Acc:0.6464\n",
      "Epoch [1/10], Step [429/600], Loss: 0.7920, Train Acc:0.6467\n",
      "Epoch [1/10], Step [430/600], Loss: 0.8910, Train Acc:0.6470\n",
      "Epoch [1/10], Step [431/600], Loss: 0.8649, Train Acc:0.6472\n",
      "Epoch [1/10], Step [432/600], Loss: 0.8509, Train Acc:0.6474\n",
      "Epoch [1/10], Step [433/600], Loss: 0.8312, Train Acc:0.6476\n",
      "Epoch [1/10], Step [434/600], Loss: 0.8679, Train Acc:0.6477\n",
      "Epoch [1/10], Step [435/600], Loss: 0.9135, Train Acc:0.6477\n",
      "Epoch [1/10], Step [436/600], Loss: 0.9234, Train Acc:0.6480\n",
      "Epoch [1/10], Step [437/600], Loss: 0.8339, Train Acc:0.6482\n",
      "Epoch [1/10], Step [438/600], Loss: 0.9329, Train Acc:0.6485\n",
      "Epoch [1/10], Step [439/600], Loss: 0.8661, Train Acc:0.6486\n",
      "Epoch [1/10], Step [440/600], Loss: 0.9405, Train Acc:0.6487\n",
      "Epoch [1/10], Step [441/600], Loss: 0.8687, Train Acc:0.6488\n",
      "Epoch [1/10], Step [442/600], Loss: 0.7372, Train Acc:0.6491\n",
      "Epoch [1/10], Step [443/600], Loss: 0.9835, Train Acc:0.6493\n",
      "Epoch [1/10], Step [444/600], Loss: 0.8073, Train Acc:0.6495\n",
      "Epoch [1/10], Step [445/600], Loss: 0.8362, Train Acc:0.6497\n",
      "Epoch [1/10], Step [446/600], Loss: 0.8213, Train Acc:0.6498\n",
      "Epoch [1/10], Step [447/600], Loss: 0.8074, Train Acc:0.6500\n",
      "Epoch [1/10], Step [448/600], Loss: 0.9608, Train Acc:0.6501\n",
      "Epoch [1/10], Step [449/600], Loss: 0.8891, Train Acc:0.6501\n",
      "Epoch [1/10], Step [450/600], Loss: 0.8177, Train Acc:0.6503\n",
      "Epoch [1/10], Step [451/600], Loss: 0.9183, Train Acc:0.6504\n",
      "Epoch [1/10], Step [452/600], Loss: 0.7751, Train Acc:0.6507\n",
      "Epoch [1/10], Step [453/600], Loss: 0.9106, Train Acc:0.6509\n",
      "Epoch [1/10], Step [454/600], Loss: 0.9164, Train Acc:0.6510\n",
      "Epoch [1/10], Step [455/600], Loss: 0.8835, Train Acc:0.6511\n",
      "Epoch [1/10], Step [456/600], Loss: 0.7904, Train Acc:0.6513\n",
      "Epoch [1/10], Step [457/600], Loss: 0.8621, Train Acc:0.6514\n",
      "Epoch [1/10], Step [458/600], Loss: 0.8064, Train Acc:0.6516\n",
      "Epoch [1/10], Step [459/600], Loss: 0.7528, Train Acc:0.6519\n",
      "Epoch [1/10], Step [460/600], Loss: 0.8525, Train Acc:0.6521\n",
      "Epoch [1/10], Step [461/600], Loss: 0.8278, Train Acc:0.6523\n",
      "Epoch [1/10], Step [462/600], Loss: 0.7645, Train Acc:0.6525\n",
      "Epoch [1/10], Step [463/600], Loss: 0.9020, Train Acc:0.6526\n",
      "Epoch [1/10], Step [464/600], Loss: 0.6671, Train Acc:0.6529\n",
      "Epoch [1/10], Step [465/600], Loss: 0.7670, Train Acc:0.6530\n",
      "Epoch [1/10], Step [466/600], Loss: 0.8939, Train Acc:0.6532\n",
      "Epoch [1/10], Step [467/600], Loss: 0.9822, Train Acc:0.6532\n",
      "Epoch [1/10], Step [468/600], Loss: 0.7953, Train Acc:0.6535\n",
      "Epoch [1/10], Step [469/600], Loss: 0.8648, Train Acc:0.6537\n",
      "Epoch [1/10], Step [470/600], Loss: 0.8424, Train Acc:0.6538\n",
      "Epoch [1/10], Step [471/600], Loss: 0.9068, Train Acc:0.6540\n",
      "Epoch [1/10], Step [472/600], Loss: 0.7957, Train Acc:0.6543\n",
      "Epoch [1/10], Step [473/600], Loss: 0.8832, Train Acc:0.6544\n",
      "Epoch [1/10], Step [474/600], Loss: 0.8241, Train Acc:0.6545\n",
      "Epoch [1/10], Step [475/600], Loss: 0.9139, Train Acc:0.6545\n",
      "Epoch [1/10], Step [476/600], Loss: 0.9056, Train Acc:0.6546\n",
      "Epoch [1/10], Step [477/600], Loss: 0.8145, Train Acc:0.6548\n",
      "Epoch [1/10], Step [478/600], Loss: 0.9195, Train Acc:0.6548\n",
      "Epoch [1/10], Step [479/600], Loss: 0.8384, Train Acc:0.6549\n",
      "Epoch [1/10], Step [480/600], Loss: 0.9358, Train Acc:0.6550\n",
      "Epoch [1/10], Step [481/600], Loss: 0.8888, Train Acc:0.6550\n",
      "Epoch [1/10], Step [482/600], Loss: 0.8981, Train Acc:0.6551\n",
      "Epoch [1/10], Step [483/600], Loss: 0.7797, Train Acc:0.6552\n",
      "Epoch [1/10], Step [484/600], Loss: 0.7648, Train Acc:0.6554\n",
      "Epoch [1/10], Step [485/600], Loss: 1.0564, Train Acc:0.6554\n",
      "Epoch [1/10], Step [486/600], Loss: 0.8555, Train Acc:0.6556\n",
      "Epoch [1/10], Step [487/600], Loss: 0.7318, Train Acc:0.6559\n",
      "Epoch [1/10], Step [488/600], Loss: 0.6925, Train Acc:0.6562\n",
      "Epoch [1/10], Step [489/600], Loss: 0.7919, Train Acc:0.6563\n",
      "Epoch [1/10], Step [490/600], Loss: 0.7628, Train Acc:0.6565\n",
      "Epoch [1/10], Step [491/600], Loss: 0.8760, Train Acc:0.6567\n",
      "Epoch [1/10], Step [492/600], Loss: 0.8331, Train Acc:0.6568\n",
      "Epoch [1/10], Step [493/600], Loss: 0.8836, Train Acc:0.6569\n",
      "Epoch [1/10], Step [494/600], Loss: 0.7745, Train Acc:0.6572\n",
      "Epoch [1/10], Step [495/600], Loss: 0.8408, Train Acc:0.6573\n",
      "Epoch [1/10], Step [496/600], Loss: 0.9210, Train Acc:0.6574\n",
      "Epoch [1/10], Step [497/600], Loss: 0.8732, Train Acc:0.6576\n",
      "Epoch [1/10], Step [498/600], Loss: 0.8133, Train Acc:0.6578\n",
      "Epoch [1/10], Step [499/600], Loss: 0.8492, Train Acc:0.6579\n",
      "Epoch [1/10], Step [500/600], Loss: 0.8792, Train Acc:0.6579\n",
      "Epoch [1/10], Step [501/600], Loss: 0.8626, Train Acc:0.6580\n",
      "Epoch [1/10], Step [502/600], Loss: 0.8654, Train Acc:0.6582\n",
      "Epoch [1/10], Step [503/600], Loss: 0.9029, Train Acc:0.6582\n",
      "Epoch [1/10], Step [504/600], Loss: 0.8498, Train Acc:0.6584\n",
      "Epoch [1/10], Step [505/600], Loss: 0.7543, Train Acc:0.6587\n",
      "Epoch [1/10], Step [506/600], Loss: 0.7592, Train Acc:0.6588\n",
      "Epoch [1/10], Step [507/600], Loss: 0.8788, Train Acc:0.6588\n",
      "Epoch [1/10], Step [508/600], Loss: 0.7848, Train Acc:0.6591\n",
      "Epoch [1/10], Step [509/600], Loss: 0.8313, Train Acc:0.6593\n",
      "Epoch [1/10], Step [510/600], Loss: 0.8010, Train Acc:0.6595\n",
      "Epoch [1/10], Step [511/600], Loss: 0.9261, Train Acc:0.6595\n",
      "Epoch [1/10], Step [512/600], Loss: 0.8781, Train Acc:0.6597\n",
      "Epoch [1/10], Step [513/600], Loss: 0.7537, Train Acc:0.6599\n",
      "Epoch [1/10], Step [514/600], Loss: 0.8925, Train Acc:0.6599\n",
      "Epoch [1/10], Step [515/600], Loss: 0.7241, Train Acc:0.6603\n",
      "Epoch [1/10], Step [516/600], Loss: 0.8357, Train Acc:0.6604\n",
      "Epoch [1/10], Step [517/600], Loss: 0.9053, Train Acc:0.6605\n",
      "Epoch [1/10], Step [518/600], Loss: 1.0193, Train Acc:0.6604\n",
      "Epoch [1/10], Step [519/600], Loss: 0.7768, Train Acc:0.6606\n",
      "Epoch [1/10], Step [520/600], Loss: 0.8230, Train Acc:0.6608\n",
      "Epoch [1/10], Step [521/600], Loss: 0.7562, Train Acc:0.6610\n",
      "Epoch [1/10], Step [522/600], Loss: 0.7886, Train Acc:0.6612\n",
      "Epoch [1/10], Step [523/600], Loss: 0.7287, Train Acc:0.6614\n",
      "Epoch [1/10], Step [524/600], Loss: 0.8408, Train Acc:0.6616\n",
      "Epoch [1/10], Step [525/600], Loss: 0.7681, Train Acc:0.6619\n",
      "Epoch [1/10], Step [526/600], Loss: 0.9784, Train Acc:0.6619\n",
      "Epoch [1/10], Step [527/600], Loss: 0.6922, Train Acc:0.6622\n",
      "Epoch [1/10], Step [528/600], Loss: 0.7520, Train Acc:0.6624\n",
      "Epoch [1/10], Step [529/600], Loss: 0.7761, Train Acc:0.6625\n",
      "Epoch [1/10], Step [530/600], Loss: 0.7876, Train Acc:0.6628\n",
      "Epoch [1/10], Step [531/600], Loss: 0.8005, Train Acc:0.6628\n",
      "Epoch [1/10], Step [532/600], Loss: 0.7474, Train Acc:0.6631\n",
      "Epoch [1/10], Step [533/600], Loss: 0.8191, Train Acc:0.6632\n",
      "Epoch [1/10], Step [534/600], Loss: 0.8539, Train Acc:0.6634\n",
      "Epoch [1/10], Step [535/600], Loss: 0.8352, Train Acc:0.6636\n",
      "Epoch [1/10], Step [536/600], Loss: 0.8967, Train Acc:0.6637\n",
      "Epoch [1/10], Step [537/600], Loss: 0.8338, Train Acc:0.6638\n",
      "Epoch [1/10], Step [538/600], Loss: 0.7216, Train Acc:0.6640\n",
      "Epoch [1/10], Step [539/600], Loss: 0.7690, Train Acc:0.6641\n",
      "Epoch [1/10], Step [540/600], Loss: 0.7851, Train Acc:0.6644\n",
      "Epoch [1/10], Step [541/600], Loss: 0.8153, Train Acc:0.6645\n",
      "Epoch [1/10], Step [542/600], Loss: 0.9665, Train Acc:0.6645\n",
      "Epoch [1/10], Step [543/600], Loss: 0.8126, Train Acc:0.6648\n",
      "Epoch [1/10], Step [544/600], Loss: 0.9040, Train Acc:0.6649\n",
      "Epoch [1/10], Step [545/600], Loss: 0.8205, Train Acc:0.6650\n",
      "Epoch [1/10], Step [546/600], Loss: 0.7669, Train Acc:0.6652\n",
      "Epoch [1/10], Step [547/600], Loss: 0.7174, Train Acc:0.6655\n",
      "Epoch [1/10], Step [548/600], Loss: 0.8881, Train Acc:0.6657\n",
      "Epoch [1/10], Step [549/600], Loss: 0.8952, Train Acc:0.6658\n",
      "Epoch [1/10], Step [550/600], Loss: 0.7775, Train Acc:0.6660\n",
      "Epoch [1/10], Step [551/600], Loss: 0.7446, Train Acc:0.6663\n",
      "Epoch [1/10], Step [552/600], Loss: 0.8144, Train Acc:0.6664\n",
      "Epoch [1/10], Step [553/600], Loss: 0.8747, Train Acc:0.6666\n",
      "Epoch [1/10], Step [554/600], Loss: 0.8824, Train Acc:0.6667\n",
      "Epoch [1/10], Step [555/600], Loss: 0.7938, Train Acc:0.6669\n",
      "Epoch [1/10], Step [556/600], Loss: 0.7206, Train Acc:0.6671\n",
      "Epoch [1/10], Step [557/600], Loss: 0.7423, Train Acc:0.6672\n",
      "Epoch [1/10], Step [558/600], Loss: 0.7901, Train Acc:0.6674\n",
      "Epoch [1/10], Step [559/600], Loss: 0.7613, Train Acc:0.6675\n",
      "Epoch [1/10], Step [560/600], Loss: 0.8510, Train Acc:0.6675\n",
      "Epoch [1/10], Step [561/600], Loss: 0.7809, Train Acc:0.6676\n",
      "Epoch [1/10], Step [562/600], Loss: 0.7012, Train Acc:0.6679\n",
      "Epoch [1/10], Step [563/600], Loss: 0.7852, Train Acc:0.6680\n",
      "Epoch [1/10], Step [564/600], Loss: 0.8183, Train Acc:0.6681\n",
      "Epoch [1/10], Step [565/600], Loss: 0.8006, Train Acc:0.6682\n",
      "Epoch [1/10], Step [566/600], Loss: 0.7552, Train Acc:0.6683\n",
      "Epoch [1/10], Step [567/600], Loss: 0.8120, Train Acc:0.6684\n",
      "Epoch [1/10], Step [568/600], Loss: 0.7854, Train Acc:0.6685\n",
      "Epoch [1/10], Step [569/600], Loss: 0.8195, Train Acc:0.6687\n",
      "Epoch [1/10], Step [570/600], Loss: 0.6931, Train Acc:0.6689\n",
      "Epoch [1/10], Step [571/600], Loss: 0.7860, Train Acc:0.6691\n",
      "Epoch [1/10], Step [572/600], Loss: 0.7458, Train Acc:0.6694\n",
      "Epoch [1/10], Step [573/600], Loss: 0.8190, Train Acc:0.6694\n",
      "Epoch [1/10], Step [574/600], Loss: 0.8652, Train Acc:0.6695\n",
      "Epoch [1/10], Step [575/600], Loss: 0.8067, Train Acc:0.6695\n",
      "Epoch [1/10], Step [576/600], Loss: 0.8360, Train Acc:0.6695\n",
      "Epoch [1/10], Step [577/600], Loss: 0.7622, Train Acc:0.6698\n",
      "Epoch [1/10], Step [578/600], Loss: 0.9211, Train Acc:0.6698\n",
      "Epoch [1/10], Step [579/600], Loss: 0.7811, Train Acc:0.6699\n",
      "Epoch [1/10], Step [580/600], Loss: 0.7858, Train Acc:0.6701\n",
      "Epoch [1/10], Step [581/600], Loss: 0.7677, Train Acc:0.6702\n",
      "Epoch [1/10], Step [582/600], Loss: 0.9608, Train Acc:0.6701\n",
      "Epoch [1/10], Step [583/600], Loss: 0.8638, Train Acc:0.6702\n",
      "Epoch [1/10], Step [584/600], Loss: 0.7430, Train Acc:0.6703\n",
      "Epoch [1/10], Step [585/600], Loss: 0.7447, Train Acc:0.6704\n",
      "Epoch [1/10], Step [586/600], Loss: 0.7395, Train Acc:0.6706\n",
      "Epoch [1/10], Step [587/600], Loss: 0.8456, Train Acc:0.6707\n",
      "Epoch [1/10], Step [588/600], Loss: 0.8283, Train Acc:0.6707\n",
      "Epoch [1/10], Step [589/600], Loss: 0.8339, Train Acc:0.6709\n",
      "Epoch [1/10], Step [590/600], Loss: 0.7993, Train Acc:0.6711\n",
      "Epoch [1/10], Step [591/600], Loss: 0.7788, Train Acc:0.6713\n",
      "Epoch [1/10], Step [592/600], Loss: 0.7365, Train Acc:0.6715\n",
      "Epoch [1/10], Step [593/600], Loss: 0.7912, Train Acc:0.6717\n",
      "Epoch [1/10], Step [594/600], Loss: 0.7734, Train Acc:0.6719\n",
      "Epoch [1/10], Step [595/600], Loss: 0.6865, Train Acc:0.6722\n",
      "Epoch [1/10], Step [596/600], Loss: 0.8782, Train Acc:0.6723\n",
      "Epoch [1/10], Step [597/600], Loss: 0.6198, Train Acc:0.6726\n",
      "Epoch [1/10], Step [598/600], Loss: 0.9896, Train Acc:0.6726\n",
      "Epoch [1/10], Step [599/600], Loss: 0.7791, Train Acc:0.6727\n",
      "Epoch [1/10], Step [600/600], Loss: 0.8391, Train Acc:0.6729\n",
      "Epoch [2/10], Step [1/600], Loss: 0.8216, Train Acc:0.7100\n",
      "Epoch [2/10], Step [2/600], Loss: 0.7737, Train Acc:0.7400\n",
      "Epoch [2/10], Step [3/600], Loss: 0.9136, Train Acc:0.7200\n",
      "Epoch [2/10], Step [4/600], Loss: 0.7259, Train Acc:0.7425\n",
      "Epoch [2/10], Step [5/600], Loss: 0.7805, Train Acc:0.7420\n",
      "Epoch [2/10], Step [6/600], Loss: 0.7584, Train Acc:0.7550\n",
      "Epoch [2/10], Step [7/600], Loss: 0.8489, Train Acc:0.7500\n",
      "Epoch [2/10], Step [8/600], Loss: 0.9255, Train Acc:0.7412\n",
      "Epoch [2/10], Step [9/600], Loss: 0.9028, Train Acc:0.7378\n",
      "Epoch [2/10], Step [10/600], Loss: 0.8839, Train Acc:0.7330\n",
      "Epoch [2/10], Step [11/600], Loss: 0.7846, Train Acc:0.7355\n",
      "Epoch [2/10], Step [12/600], Loss: 0.7990, Train Acc:0.7342\n",
      "Epoch [2/10], Step [13/600], Loss: 0.7547, Train Acc:0.7369\n",
      "Epoch [2/10], Step [14/600], Loss: 0.7225, Train Acc:0.7379\n",
      "Epoch [2/10], Step [15/600], Loss: 0.7801, Train Acc:0.7380\n",
      "Epoch [2/10], Step [16/600], Loss: 0.7889, Train Acc:0.7381\n",
      "Epoch [2/10], Step [17/600], Loss: 0.7465, Train Acc:0.7394\n",
      "Epoch [2/10], Step [18/600], Loss: 0.8279, Train Acc:0.7394\n",
      "Epoch [2/10], Step [19/600], Loss: 0.6786, Train Acc:0.7463\n",
      "Epoch [2/10], Step [20/600], Loss: 0.7017, Train Acc:0.7490\n",
      "Epoch [2/10], Step [21/600], Loss: 0.8459, Train Acc:0.7486\n",
      "Epoch [2/10], Step [22/600], Loss: 0.8017, Train Acc:0.7455\n",
      "Epoch [2/10], Step [23/600], Loss: 0.7394, Train Acc:0.7439\n",
      "Epoch [2/10], Step [24/600], Loss: 0.7177, Train Acc:0.7442\n",
      "Epoch [2/10], Step [25/600], Loss: 0.7988, Train Acc:0.7456\n",
      "Epoch [2/10], Step [26/600], Loss: 0.7716, Train Acc:0.7481\n",
      "Epoch [2/10], Step [27/600], Loss: 0.8077, Train Acc:0.7463\n",
      "Epoch [2/10], Step [28/600], Loss: 0.9555, Train Acc:0.7436\n",
      "Epoch [2/10], Step [29/600], Loss: 0.6644, Train Acc:0.7469\n",
      "Epoch [2/10], Step [30/600], Loss: 0.8377, Train Acc:0.7483\n",
      "Epoch [2/10], Step [31/600], Loss: 0.7194, Train Acc:0.7503\n",
      "Epoch [2/10], Step [32/600], Loss: 0.8061, Train Acc:0.7506\n",
      "Epoch [2/10], Step [33/600], Loss: 0.7126, Train Acc:0.7521\n",
      "Epoch [2/10], Step [34/600], Loss: 0.8066, Train Acc:0.7524\n",
      "Epoch [2/10], Step [35/600], Loss: 0.7044, Train Acc:0.7537\n",
      "Epoch [2/10], Step [36/600], Loss: 0.7747, Train Acc:0.7539\n",
      "Epoch [2/10], Step [37/600], Loss: 0.8244, Train Acc:0.7530\n",
      "Epoch [2/10], Step [38/600], Loss: 0.7506, Train Acc:0.7542\n",
      "Epoch [2/10], Step [39/600], Loss: 0.7477, Train Acc:0.7538\n",
      "Epoch [2/10], Step [40/600], Loss: 0.7241, Train Acc:0.7552\n",
      "Epoch [2/10], Step [41/600], Loss: 0.7571, Train Acc:0.7551\n",
      "Epoch [2/10], Step [42/600], Loss: 0.8477, Train Acc:0.7543\n",
      "Epoch [2/10], Step [43/600], Loss: 0.7803, Train Acc:0.7551\n",
      "Epoch [2/10], Step [44/600], Loss: 0.9194, Train Acc:0.7520\n",
      "Epoch [2/10], Step [45/600], Loss: 0.6805, Train Acc:0.7520\n",
      "Epoch [2/10], Step [46/600], Loss: 0.7177, Train Acc:0.7528\n",
      "Epoch [2/10], Step [47/600], Loss: 0.8062, Train Acc:0.7528\n",
      "Epoch [2/10], Step [48/600], Loss: 0.6909, Train Acc:0.7527\n",
      "Epoch [2/10], Step [49/600], Loss: 0.7345, Train Acc:0.7535\n",
      "Epoch [2/10], Step [50/600], Loss: 0.7755, Train Acc:0.7540\n",
      "Epoch [2/10], Step [51/600], Loss: 0.7469, Train Acc:0.7545\n",
      "Epoch [2/10], Step [52/600], Loss: 0.7782, Train Acc:0.7544\n",
      "Epoch [2/10], Step [53/600], Loss: 0.7009, Train Acc:0.7551\n",
      "Epoch [2/10], Step [54/600], Loss: 0.6898, Train Acc:0.7557\n",
      "Epoch [2/10], Step [55/600], Loss: 0.7851, Train Acc:0.7553\n",
      "Epoch [2/10], Step [56/600], Loss: 0.8533, Train Acc:0.7546\n",
      "Epoch [2/10], Step [57/600], Loss: 0.7855, Train Acc:0.7542\n",
      "Epoch [2/10], Step [58/600], Loss: 0.8131, Train Acc:0.7533\n",
      "Epoch [2/10], Step [59/600], Loss: 0.8631, Train Acc:0.7532\n",
      "Epoch [2/10], Step [60/600], Loss: 0.8018, Train Acc:0.7530\n",
      "Epoch [2/10], Step [61/600], Loss: 0.7627, Train Acc:0.7536\n",
      "Epoch [2/10], Step [62/600], Loss: 0.8234, Train Acc:0.7529\n",
      "Epoch [2/10], Step [63/600], Loss: 0.8259, Train Acc:0.7525\n",
      "Epoch [2/10], Step [64/600], Loss: 0.7818, Train Acc:0.7520\n",
      "Epoch [2/10], Step [65/600], Loss: 0.8432, Train Acc:0.7511\n",
      "Epoch [2/10], Step [66/600], Loss: 0.6607, Train Acc:0.7521\n",
      "Epoch [2/10], Step [67/600], Loss: 0.7259, Train Acc:0.7528\n",
      "Epoch [2/10], Step [68/600], Loss: 0.7363, Train Acc:0.7532\n",
      "Epoch [2/10], Step [69/600], Loss: 0.8070, Train Acc:0.7535\n",
      "Epoch [2/10], Step [70/600], Loss: 0.8726, Train Acc:0.7536\n",
      "Epoch [2/10], Step [71/600], Loss: 0.7253, Train Acc:0.7538\n",
      "Epoch [2/10], Step [72/600], Loss: 0.6622, Train Acc:0.7543\n",
      "Epoch [2/10], Step [73/600], Loss: 0.8838, Train Acc:0.7540\n",
      "Epoch [2/10], Step [74/600], Loss: 0.6909, Train Acc:0.7546\n",
      "Epoch [2/10], Step [75/600], Loss: 0.7688, Train Acc:0.7549\n",
      "Epoch [2/10], Step [76/600], Loss: 0.9114, Train Acc:0.7543\n",
      "Epoch [2/10], Step [77/600], Loss: 0.9565, Train Acc:0.7535\n",
      "Epoch [2/10], Step [78/600], Loss: 0.7109, Train Acc:0.7541\n",
      "Epoch [2/10], Step [79/600], Loss: 0.8800, Train Acc:0.7539\n",
      "Epoch [2/10], Step [80/600], Loss: 0.8623, Train Acc:0.7531\n",
      "Epoch [2/10], Step [81/600], Loss: 0.7112, Train Acc:0.7531\n",
      "Epoch [2/10], Step [82/600], Loss: 0.7700, Train Acc:0.7527\n",
      "Epoch [2/10], Step [83/600], Loss: 0.7354, Train Acc:0.7528\n",
      "Epoch [2/10], Step [84/600], Loss: 0.7713, Train Acc:0.7535\n",
      "Epoch [2/10], Step [85/600], Loss: 0.7048, Train Acc:0.7539\n",
      "Epoch [2/10], Step [86/600], Loss: 0.7729, Train Acc:0.7537\n",
      "Epoch [2/10], Step [87/600], Loss: 0.8131, Train Acc:0.7537\n",
      "Epoch [2/10], Step [88/600], Loss: 0.8437, Train Acc:0.7525\n",
      "Epoch [2/10], Step [89/600], Loss: 0.8333, Train Acc:0.7528\n",
      "Epoch [2/10], Step [90/600], Loss: 0.7645, Train Acc:0.7523\n",
      "Epoch [2/10], Step [91/600], Loss: 0.7933, Train Acc:0.7524\n",
      "Epoch [2/10], Step [92/600], Loss: 0.7486, Train Acc:0.7524\n",
      "Epoch [2/10], Step [93/600], Loss: 0.6811, Train Acc:0.7528\n",
      "Epoch [2/10], Step [94/600], Loss: 0.7809, Train Acc:0.7526\n",
      "Epoch [2/10], Step [95/600], Loss: 0.7420, Train Acc:0.7528\n",
      "Epoch [2/10], Step [96/600], Loss: 0.7177, Train Acc:0.7533\n",
      "Epoch [2/10], Step [97/600], Loss: 0.7215, Train Acc:0.7541\n",
      "Epoch [2/10], Step [98/600], Loss: 0.7818, Train Acc:0.7535\n",
      "Epoch [2/10], Step [99/600], Loss: 0.8300, Train Acc:0.7531\n",
      "Epoch [2/10], Step [100/600], Loss: 0.7595, Train Acc:0.7536\n",
      "Epoch [2/10], Step [101/600], Loss: 0.7466, Train Acc:0.7538\n",
      "Epoch [2/10], Step [102/600], Loss: 0.6415, Train Acc:0.7540\n",
      "Epoch [2/10], Step [103/600], Loss: 0.8594, Train Acc:0.7533\n",
      "Epoch [2/10], Step [104/600], Loss: 0.7747, Train Acc:0.7535\n",
      "Epoch [2/10], Step [105/600], Loss: 0.8372, Train Acc:0.7526\n",
      "Epoch [2/10], Step [106/600], Loss: 0.8493, Train Acc:0.7524\n",
      "Epoch [2/10], Step [107/600], Loss: 0.8244, Train Acc:0.7522\n",
      "Epoch [2/10], Step [108/600], Loss: 0.8140, Train Acc:0.7520\n",
      "Epoch [2/10], Step [109/600], Loss: 0.7438, Train Acc:0.7520\n",
      "Epoch [2/10], Step [110/600], Loss: 0.7076, Train Acc:0.7520\n",
      "Epoch [2/10], Step [111/600], Loss: 0.6676, Train Acc:0.7524\n",
      "Epoch [2/10], Step [112/600], Loss: 0.7918, Train Acc:0.7525\n",
      "Epoch [2/10], Step [113/600], Loss: 0.8692, Train Acc:0.7526\n",
      "Epoch [2/10], Step [114/600], Loss: 0.7506, Train Acc:0.7523\n",
      "Epoch [2/10], Step [115/600], Loss: 0.7876, Train Acc:0.7522\n",
      "Epoch [2/10], Step [116/600], Loss: 0.6982, Train Acc:0.7526\n",
      "Epoch [2/10], Step [117/600], Loss: 0.7127, Train Acc:0.7527\n",
      "Epoch [2/10], Step [118/600], Loss: 0.7795, Train Acc:0.7528\n",
      "Epoch [2/10], Step [119/600], Loss: 0.7707, Train Acc:0.7529\n",
      "Epoch [2/10], Step [120/600], Loss: 0.8480, Train Acc:0.7530\n",
      "Epoch [2/10], Step [121/600], Loss: 0.7426, Train Acc:0.7535\n",
      "Epoch [2/10], Step [122/600], Loss: 0.8323, Train Acc:0.7530\n",
      "Epoch [2/10], Step [123/600], Loss: 0.8725, Train Acc:0.7522\n",
      "Epoch [2/10], Step [124/600], Loss: 0.7294, Train Acc:0.7523\n",
      "Epoch [2/10], Step [125/600], Loss: 0.7244, Train Acc:0.7526\n",
      "Epoch [2/10], Step [126/600], Loss: 0.7422, Train Acc:0.7532\n",
      "Epoch [2/10], Step [127/600], Loss: 0.7238, Train Acc:0.7533\n",
      "Epoch [2/10], Step [128/600], Loss: 0.8776, Train Acc:0.7531\n",
      "Epoch [2/10], Step [129/600], Loss: 0.6917, Train Acc:0.7537\n",
      "Epoch [2/10], Step [130/600], Loss: 0.8441, Train Acc:0.7532\n",
      "Epoch [2/10], Step [131/600], Loss: 0.7525, Train Acc:0.7529\n",
      "Epoch [2/10], Step [132/600], Loss: 0.7394, Train Acc:0.7529\n",
      "Epoch [2/10], Step [133/600], Loss: 0.8530, Train Acc:0.7527\n",
      "Epoch [2/10], Step [134/600], Loss: 0.7488, Train Acc:0.7526\n",
      "Epoch [2/10], Step [135/600], Loss: 0.6053, Train Acc:0.7531\n",
      "Epoch [2/10], Step [136/600], Loss: 0.7984, Train Acc:0.7531\n",
      "Epoch [2/10], Step [137/600], Loss: 0.8656, Train Acc:0.7523\n",
      "Epoch [2/10], Step [138/600], Loss: 0.7681, Train Acc:0.7525\n",
      "Epoch [2/10], Step [139/600], Loss: 0.6621, Train Acc:0.7526\n",
      "Epoch [2/10], Step [140/600], Loss: 0.7035, Train Acc:0.7526\n",
      "Epoch [2/10], Step [141/600], Loss: 0.8192, Train Acc:0.7528\n",
      "Epoch [2/10], Step [142/600], Loss: 0.8741, Train Acc:0.7525\n",
      "Epoch [2/10], Step [143/600], Loss: 0.6257, Train Acc:0.7528\n",
      "Epoch [2/10], Step [144/600], Loss: 0.8081, Train Acc:0.7526\n",
      "Epoch [2/10], Step [145/600], Loss: 0.7580, Train Acc:0.7525\n",
      "Epoch [2/10], Step [146/600], Loss: 0.6852, Train Acc:0.7529\n",
      "Epoch [2/10], Step [147/600], Loss: 0.6629, Train Acc:0.7533\n",
      "Epoch [2/10], Step [148/600], Loss: 0.9562, Train Acc:0.7530\n",
      "Epoch [2/10], Step [149/600], Loss: 0.8148, Train Acc:0.7531\n",
      "Epoch [2/10], Step [150/600], Loss: 0.8580, Train Acc:0.7527\n",
      "Epoch [2/10], Step [151/600], Loss: 0.8288, Train Acc:0.7528\n",
      "Epoch [2/10], Step [152/600], Loss: 0.7691, Train Acc:0.7530\n",
      "Epoch [2/10], Step [153/600], Loss: 0.7209, Train Acc:0.7531\n",
      "Epoch [2/10], Step [154/600], Loss: 0.7918, Train Acc:0.7532\n",
      "Epoch [2/10], Step [155/600], Loss: 0.6985, Train Acc:0.7534\n",
      "Epoch [2/10], Step [156/600], Loss: 0.7281, Train Acc:0.7535\n",
      "Epoch [2/10], Step [157/600], Loss: 0.7743, Train Acc:0.7533\n",
      "Epoch [2/10], Step [158/600], Loss: 0.7868, Train Acc:0.7533\n",
      "Epoch [2/10], Step [159/600], Loss: 0.8247, Train Acc:0.7532\n",
      "Epoch [2/10], Step [160/600], Loss: 0.7254, Train Acc:0.7534\n",
      "Epoch [2/10], Step [161/600], Loss: 0.7856, Train Acc:0.7534\n",
      "Epoch [2/10], Step [162/600], Loss: 0.7512, Train Acc:0.7539\n",
      "Epoch [2/10], Step [163/600], Loss: 0.7076, Train Acc:0.7540\n",
      "Epoch [2/10], Step [164/600], Loss: 0.6629, Train Acc:0.7541\n",
      "Epoch [2/10], Step [165/600], Loss: 0.6692, Train Acc:0.7544\n",
      "Epoch [2/10], Step [166/600], Loss: 0.6402, Train Acc:0.7547\n",
      "Epoch [2/10], Step [167/600], Loss: 0.6835, Train Acc:0.7550\n",
      "Epoch [2/10], Step [168/600], Loss: 0.7245, Train Acc:0.7553\n",
      "Epoch [2/10], Step [169/600], Loss: 0.5713, Train Acc:0.7558\n",
      "Epoch [2/10], Step [170/600], Loss: 0.8104, Train Acc:0.7554\n",
      "Epoch [2/10], Step [171/600], Loss: 0.7784, Train Acc:0.7557\n",
      "Epoch [2/10], Step [172/600], Loss: 0.7872, Train Acc:0.7559\n",
      "Epoch [2/10], Step [173/600], Loss: 0.6838, Train Acc:0.7561\n",
      "Epoch [2/10], Step [174/600], Loss: 0.7927, Train Acc:0.7562\n",
      "Epoch [2/10], Step [175/600], Loss: 0.6973, Train Acc:0.7564\n",
      "Epoch [2/10], Step [176/600], Loss: 0.7420, Train Acc:0.7564\n",
      "Epoch [2/10], Step [177/600], Loss: 0.7822, Train Acc:0.7564\n",
      "Epoch [2/10], Step [178/600], Loss: 0.7120, Train Acc:0.7566\n",
      "Epoch [2/10], Step [179/600], Loss: 0.7779, Train Acc:0.7565\n",
      "Epoch [2/10], Step [180/600], Loss: 0.7931, Train Acc:0.7564\n",
      "Epoch [2/10], Step [181/600], Loss: 0.8052, Train Acc:0.7562\n",
      "Epoch [2/10], Step [182/600], Loss: 0.7686, Train Acc:0.7561\n",
      "Epoch [2/10], Step [183/600], Loss: 0.7100, Train Acc:0.7563\n",
      "Epoch [2/10], Step [184/600], Loss: 0.7216, Train Acc:0.7564\n",
      "Epoch [2/10], Step [185/600], Loss: 0.6652, Train Acc:0.7565\n",
      "Epoch [2/10], Step [186/600], Loss: 0.7393, Train Acc:0.7568\n",
      "Epoch [2/10], Step [187/600], Loss: 0.8629, Train Acc:0.7568\n",
      "Epoch [2/10], Step [188/600], Loss: 0.7025, Train Acc:0.7569\n",
      "Epoch [2/10], Step [189/600], Loss: 0.7953, Train Acc:0.7567\n",
      "Epoch [2/10], Step [190/600], Loss: 0.8038, Train Acc:0.7564\n",
      "Epoch [2/10], Step [191/600], Loss: 0.6325, Train Acc:0.7568\n",
      "Epoch [2/10], Step [192/600], Loss: 0.8514, Train Acc:0.7567\n",
      "Epoch [2/10], Step [193/600], Loss: 0.8227, Train Acc:0.7564\n",
      "Epoch [2/10], Step [194/600], Loss: 0.8078, Train Acc:0.7559\n",
      "Epoch [2/10], Step [195/600], Loss: 0.7982, Train Acc:0.7558\n",
      "Epoch [2/10], Step [196/600], Loss: 0.7481, Train Acc:0.7556\n",
      "Epoch [2/10], Step [197/600], Loss: 0.8136, Train Acc:0.7556\n",
      "Epoch [2/10], Step [198/600], Loss: 0.7456, Train Acc:0.7558\n",
      "Epoch [2/10], Step [199/600], Loss: 0.6017, Train Acc:0.7561\n",
      "Epoch [2/10], Step [200/600], Loss: 0.7666, Train Acc:0.7560\n",
      "Epoch [2/10], Step [201/600], Loss: 0.7246, Train Acc:0.7566\n",
      "Epoch [2/10], Step [202/600], Loss: 0.7966, Train Acc:0.7565\n",
      "Epoch [2/10], Step [203/600], Loss: 0.6780, Train Acc:0.7571\n",
      "Epoch [2/10], Step [204/600], Loss: 0.6711, Train Acc:0.7574\n",
      "Epoch [2/10], Step [205/600], Loss: 0.6742, Train Acc:0.7575\n",
      "Epoch [2/10], Step [206/600], Loss: 0.7882, Train Acc:0.7577\n",
      "Epoch [2/10], Step [207/600], Loss: 0.8128, Train Acc:0.7577\n",
      "Epoch [2/10], Step [208/600], Loss: 0.7655, Train Acc:0.7578\n",
      "Epoch [2/10], Step [209/600], Loss: 0.7623, Train Acc:0.7579\n",
      "Epoch [2/10], Step [210/600], Loss: 0.7807, Train Acc:0.7580\n",
      "Epoch [2/10], Step [211/600], Loss: 0.7281, Train Acc:0.7580\n",
      "Epoch [2/10], Step [212/600], Loss: 0.6675, Train Acc:0.7583\n",
      "Epoch [2/10], Step [213/600], Loss: 0.6897, Train Acc:0.7585\n",
      "Epoch [2/10], Step [214/600], Loss: 0.6549, Train Acc:0.7588\n",
      "Epoch [2/10], Step [215/600], Loss: 0.7521, Train Acc:0.7591\n",
      "Epoch [2/10], Step [216/600], Loss: 0.7734, Train Acc:0.7589\n",
      "Epoch [2/10], Step [217/600], Loss: 0.6619, Train Acc:0.7593\n",
      "Epoch [2/10], Step [218/600], Loss: 0.8444, Train Acc:0.7592\n",
      "Epoch [2/10], Step [219/600], Loss: 0.6772, Train Acc:0.7591\n",
      "Epoch [2/10], Step [220/600], Loss: 0.6850, Train Acc:0.7595\n",
      "Epoch [2/10], Step [221/600], Loss: 0.7999, Train Acc:0.7595\n",
      "Epoch [2/10], Step [222/600], Loss: 0.7827, Train Acc:0.7594\n",
      "Epoch [2/10], Step [223/600], Loss: 0.8373, Train Acc:0.7589\n",
      "Epoch [2/10], Step [224/600], Loss: 0.6277, Train Acc:0.7590\n",
      "Epoch [2/10], Step [225/600], Loss: 0.6446, Train Acc:0.7592\n",
      "Epoch [2/10], Step [226/600], Loss: 0.7418, Train Acc:0.7594\n",
      "Epoch [2/10], Step [227/600], Loss: 0.7120, Train Acc:0.7595\n",
      "Epoch [2/10], Step [228/600], Loss: 0.7268, Train Acc:0.7597\n",
      "Epoch [2/10], Step [229/600], Loss: 0.6658, Train Acc:0.7598\n",
      "Epoch [2/10], Step [230/600], Loss: 0.7378, Train Acc:0.7597\n",
      "Epoch [2/10], Step [231/600], Loss: 0.7916, Train Acc:0.7597\n",
      "Epoch [2/10], Step [232/600], Loss: 0.5814, Train Acc:0.7600\n",
      "Epoch [2/10], Step [233/600], Loss: 0.8261, Train Acc:0.7598\n",
      "Epoch [2/10], Step [234/600], Loss: 0.7280, Train Acc:0.7597\n",
      "Epoch [2/10], Step [235/600], Loss: 0.7262, Train Acc:0.7597\n",
      "Epoch [2/10], Step [236/600], Loss: 0.7257, Train Acc:0.7599\n",
      "Epoch [2/10], Step [237/600], Loss: 0.8443, Train Acc:0.7597\n",
      "Epoch [2/10], Step [238/600], Loss: 0.7458, Train Acc:0.7596\n",
      "Epoch [2/10], Step [239/600], Loss: 0.7254, Train Acc:0.7597\n",
      "Epoch [2/10], Step [240/600], Loss: 0.6948, Train Acc:0.7598\n",
      "Epoch [2/10], Step [241/600], Loss: 0.7872, Train Acc:0.7595\n",
      "Epoch [2/10], Step [242/600], Loss: 0.7822, Train Acc:0.7595\n",
      "Epoch [2/10], Step [243/600], Loss: 0.7480, Train Acc:0.7596\n",
      "Epoch [2/10], Step [244/600], Loss: 0.7725, Train Acc:0.7594\n",
      "Epoch [2/10], Step [245/600], Loss: 0.8327, Train Acc:0.7591\n",
      "Epoch [2/10], Step [246/600], Loss: 0.7768, Train Acc:0.7591\n",
      "Epoch [2/10], Step [247/600], Loss: 0.6667, Train Acc:0.7591\n",
      "Epoch [2/10], Step [248/600], Loss: 0.8536, Train Acc:0.7590\n",
      "Epoch [2/10], Step [249/600], Loss: 0.6607, Train Acc:0.7589\n",
      "Epoch [2/10], Step [250/600], Loss: 0.7026, Train Acc:0.7590\n",
      "Epoch [2/10], Step [251/600], Loss: 0.7784, Train Acc:0.7590\n",
      "Epoch [2/10], Step [252/600], Loss: 0.6597, Train Acc:0.7592\n",
      "Epoch [2/10], Step [253/600], Loss: 0.9191, Train Acc:0.7589\n",
      "Epoch [2/10], Step [254/600], Loss: 0.7190, Train Acc:0.7591\n",
      "Epoch [2/10], Step [255/600], Loss: 0.7324, Train Acc:0.7592\n",
      "Epoch [2/10], Step [256/600], Loss: 0.8289, Train Acc:0.7590\n",
      "Epoch [2/10], Step [257/600], Loss: 0.7409, Train Acc:0.7590\n",
      "Epoch [2/10], Step [258/600], Loss: 0.6921, Train Acc:0.7591\n",
      "Epoch [2/10], Step [259/600], Loss: 0.6777, Train Acc:0.7593\n",
      "Epoch [2/10], Step [260/600], Loss: 0.7872, Train Acc:0.7592\n",
      "Epoch [2/10], Step [261/600], Loss: 0.7391, Train Acc:0.7593\n",
      "Epoch [2/10], Step [262/600], Loss: 0.6723, Train Acc:0.7596\n",
      "Epoch [2/10], Step [263/600], Loss: 0.7665, Train Acc:0.7596\n",
      "Epoch [2/10], Step [264/600], Loss: 0.6615, Train Acc:0.7598\n",
      "Epoch [2/10], Step [265/600], Loss: 0.6669, Train Acc:0.7600\n",
      "Epoch [2/10], Step [266/600], Loss: 0.7651, Train Acc:0.7600\n",
      "Epoch [2/10], Step [267/600], Loss: 0.6976, Train Acc:0.7600\n",
      "Epoch [2/10], Step [268/600], Loss: 0.6634, Train Acc:0.7603\n",
      "Epoch [2/10], Step [269/600], Loss: 0.8074, Train Acc:0.7600\n",
      "Epoch [2/10], Step [270/600], Loss: 0.6156, Train Acc:0.7601\n",
      "Epoch [2/10], Step [271/600], Loss: 0.6780, Train Acc:0.7602\n",
      "Epoch [2/10], Step [272/600], Loss: 0.7134, Train Acc:0.7603\n",
      "Epoch [2/10], Step [273/600], Loss: 0.7649, Train Acc:0.7605\n",
      "Epoch [2/10], Step [274/600], Loss: 0.7727, Train Acc:0.7603\n",
      "Epoch [2/10], Step [275/600], Loss: 0.6747, Train Acc:0.7604\n",
      "Epoch [2/10], Step [276/600], Loss: 0.7609, Train Acc:0.7605\n",
      "Epoch [2/10], Step [277/600], Loss: 0.8109, Train Acc:0.7604\n",
      "Epoch [2/10], Step [278/600], Loss: 0.7400, Train Acc:0.7603\n",
      "Epoch [2/10], Step [279/600], Loss: 0.6114, Train Acc:0.7606\n",
      "Epoch [2/10], Step [280/600], Loss: 0.7562, Train Acc:0.7606\n",
      "Epoch [2/10], Step [281/600], Loss: 0.6735, Train Acc:0.7607\n",
      "Epoch [2/10], Step [282/600], Loss: 0.6834, Train Acc:0.7607\n",
      "Epoch [2/10], Step [283/600], Loss: 0.6825, Train Acc:0.7609\n",
      "Epoch [2/10], Step [284/600], Loss: 0.6874, Train Acc:0.7609\n",
      "Epoch [2/10], Step [285/600], Loss: 0.9190, Train Acc:0.7607\n",
      "Epoch [2/10], Step [286/600], Loss: 0.7372, Train Acc:0.7606\n",
      "Epoch [2/10], Step [287/600], Loss: 0.7831, Train Acc:0.7603\n",
      "Epoch [2/10], Step [288/600], Loss: 0.7329, Train Acc:0.7604\n",
      "Epoch [2/10], Step [289/600], Loss: 0.5926, Train Acc:0.7606\n",
      "Epoch [2/10], Step [290/600], Loss: 0.7083, Train Acc:0.7606\n",
      "Epoch [2/10], Step [291/600], Loss: 0.6740, Train Acc:0.7607\n",
      "Epoch [2/10], Step [292/600], Loss: 0.7321, Train Acc:0.7607\n",
      "Epoch [2/10], Step [293/600], Loss: 0.8211, Train Acc:0.7604\n",
      "Epoch [2/10], Step [294/600], Loss: 0.6814, Train Acc:0.7604\n",
      "Epoch [2/10], Step [295/600], Loss: 0.7458, Train Acc:0.7605\n",
      "Epoch [2/10], Step [296/600], Loss: 0.9181, Train Acc:0.7603\n",
      "Epoch [2/10], Step [297/600], Loss: 0.7433, Train Acc:0.7602\n",
      "Epoch [2/10], Step [298/600], Loss: 0.7772, Train Acc:0.7602\n",
      "Epoch [2/10], Step [299/600], Loss: 0.5914, Train Acc:0.7605\n",
      "Epoch [2/10], Step [300/600], Loss: 0.7562, Train Acc:0.7605\n",
      "Epoch [2/10], Step [301/600], Loss: 0.7186, Train Acc:0.7605\n",
      "Epoch [2/10], Step [302/600], Loss: 0.5597, Train Acc:0.7607\n",
      "Epoch [2/10], Step [303/600], Loss: 0.7044, Train Acc:0.7608\n",
      "Epoch [2/10], Step [304/600], Loss: 0.6651, Train Acc:0.7610\n",
      "Epoch [2/10], Step [305/600], Loss: 0.6468, Train Acc:0.7612\n",
      "Epoch [2/10], Step [306/600], Loss: 0.7971, Train Acc:0.7610\n",
      "Epoch [2/10], Step [307/600], Loss: 0.7757, Train Acc:0.7608\n",
      "Epoch [2/10], Step [308/600], Loss: 0.7282, Train Acc:0.7609\n",
      "Epoch [2/10], Step [309/600], Loss: 0.7357, Train Acc:0.7609\n",
      "Epoch [2/10], Step [310/600], Loss: 0.7506, Train Acc:0.7610\n",
      "Epoch [2/10], Step [311/600], Loss: 0.7598, Train Acc:0.7608\n",
      "Epoch [2/10], Step [312/600], Loss: 0.6633, Train Acc:0.7611\n",
      "Epoch [2/10], Step [313/600], Loss: 0.8430, Train Acc:0.7609\n",
      "Epoch [2/10], Step [314/600], Loss: 0.7714, Train Acc:0.7610\n",
      "Epoch [2/10], Step [315/600], Loss: 0.6310, Train Acc:0.7611\n",
      "Epoch [2/10], Step [316/600], Loss: 0.6677, Train Acc:0.7613\n",
      "Epoch [2/10], Step [317/600], Loss: 0.8581, Train Acc:0.7611\n",
      "Epoch [2/10], Step [318/600], Loss: 0.6663, Train Acc:0.7613\n",
      "Epoch [2/10], Step [319/600], Loss: 0.8095, Train Acc:0.7611\n",
      "Epoch [2/10], Step [320/600], Loss: 0.7026, Train Acc:0.7610\n",
      "Epoch [2/10], Step [321/600], Loss: 0.6708, Train Acc:0.7611\n",
      "Epoch [2/10], Step [322/600], Loss: 0.6436, Train Acc:0.7613\n",
      "Epoch [2/10], Step [323/600], Loss: 0.7636, Train Acc:0.7613\n",
      "Epoch [2/10], Step [324/600], Loss: 0.6919, Train Acc:0.7612\n",
      "Epoch [2/10], Step [325/600], Loss: 0.9053, Train Acc:0.7609\n",
      "Epoch [2/10], Step [326/600], Loss: 0.7946, Train Acc:0.7608\n",
      "Epoch [2/10], Step [327/600], Loss: 0.7348, Train Acc:0.7609\n",
      "Epoch [2/10], Step [328/600], Loss: 0.6714, Train Acc:0.7610\n",
      "Epoch [2/10], Step [329/600], Loss: 0.7746, Train Acc:0.7610\n",
      "Epoch [2/10], Step [330/600], Loss: 0.7281, Train Acc:0.7609\n",
      "Epoch [2/10], Step [331/600], Loss: 0.8226, Train Acc:0.7606\n",
      "Epoch [2/10], Step [332/600], Loss: 0.8175, Train Acc:0.7605\n",
      "Epoch [2/10], Step [333/600], Loss: 0.6633, Train Acc:0.7605\n",
      "Epoch [2/10], Step [334/600], Loss: 0.7126, Train Acc:0.7607\n",
      "Epoch [2/10], Step [335/600], Loss: 0.6696, Train Acc:0.7607\n",
      "Epoch [2/10], Step [336/600], Loss: 0.6875, Train Acc:0.7607\n",
      "Epoch [2/10], Step [337/600], Loss: 0.7857, Train Acc:0.7606\n",
      "Epoch [2/10], Step [338/600], Loss: 0.6146, Train Acc:0.7608\n",
      "Epoch [2/10], Step [339/600], Loss: 0.7553, Train Acc:0.7609\n",
      "Epoch [2/10], Step [340/600], Loss: 0.7092, Train Acc:0.7610\n",
      "Epoch [2/10], Step [341/600], Loss: 0.6343, Train Acc:0.7612\n",
      "Epoch [2/10], Step [342/600], Loss: 0.7420, Train Acc:0.7611\n",
      "Epoch [2/10], Step [343/600], Loss: 0.7323, Train Acc:0.7611\n",
      "Epoch [2/10], Step [344/600], Loss: 0.6159, Train Acc:0.7613\n",
      "Epoch [2/10], Step [345/600], Loss: 0.7250, Train Acc:0.7613\n",
      "Epoch [2/10], Step [346/600], Loss: 0.7597, Train Acc:0.7612\n",
      "Epoch [2/10], Step [347/600], Loss: 0.6472, Train Acc:0.7614\n",
      "Epoch [2/10], Step [348/600], Loss: 0.8223, Train Acc:0.7612\n",
      "Epoch [2/10], Step [349/600], Loss: 0.8049, Train Acc:0.7611\n",
      "Epoch [2/10], Step [350/600], Loss: 0.7361, Train Acc:0.7612\n",
      "Epoch [2/10], Step [351/600], Loss: 0.7285, Train Acc:0.7613\n",
      "Epoch [2/10], Step [352/600], Loss: 0.7506, Train Acc:0.7614\n",
      "Epoch [2/10], Step [353/600], Loss: 0.7850, Train Acc:0.7614\n",
      "Epoch [2/10], Step [354/600], Loss: 0.7130, Train Acc:0.7614\n",
      "Epoch [2/10], Step [355/600], Loss: 0.8131, Train Acc:0.7612\n",
      "Epoch [2/10], Step [356/600], Loss: 0.7733, Train Acc:0.7611\n",
      "Epoch [2/10], Step [357/600], Loss: 0.6939, Train Acc:0.7610\n",
      "Epoch [2/10], Step [358/600], Loss: 0.7375, Train Acc:0.7610\n",
      "Epoch [2/10], Step [359/600], Loss: 0.7698, Train Acc:0.7609\n",
      "Epoch [2/10], Step [360/600], Loss: 0.7458, Train Acc:0.7611\n",
      "Epoch [2/10], Step [361/600], Loss: 0.6396, Train Acc:0.7612\n",
      "Epoch [2/10], Step [362/600], Loss: 0.7533, Train Acc:0.7611\n",
      "Epoch [2/10], Step [363/600], Loss: 0.6663, Train Acc:0.7612\n",
      "Epoch [2/10], Step [364/600], Loss: 0.5930, Train Acc:0.7614\n",
      "Epoch [2/10], Step [365/600], Loss: 0.8118, Train Acc:0.7614\n",
      "Epoch [2/10], Step [366/600], Loss: 0.7092, Train Acc:0.7614\n",
      "Epoch [2/10], Step [367/600], Loss: 0.7432, Train Acc:0.7614\n",
      "Epoch [2/10], Step [368/600], Loss: 0.7639, Train Acc:0.7613\n",
      "Epoch [2/10], Step [369/600], Loss: 0.6545, Train Acc:0.7614\n",
      "Epoch [2/10], Step [370/600], Loss: 0.8956, Train Acc:0.7614\n",
      "Epoch [2/10], Step [371/600], Loss: 0.5392, Train Acc:0.7617\n",
      "Epoch [2/10], Step [372/600], Loss: 0.6793, Train Acc:0.7617\n",
      "Epoch [2/10], Step [373/600], Loss: 0.5484, Train Acc:0.7619\n",
      "Epoch [2/10], Step [374/600], Loss: 0.6796, Train Acc:0.7620\n",
      "Epoch [2/10], Step [375/600], Loss: 0.7805, Train Acc:0.7618\n",
      "Epoch [2/10], Step [376/600], Loss: 0.6624, Train Acc:0.7619\n",
      "Epoch [2/10], Step [377/600], Loss: 0.7592, Train Acc:0.7619\n",
      "Epoch [2/10], Step [378/600], Loss: 0.8184, Train Acc:0.7618\n",
      "Epoch [2/10], Step [379/600], Loss: 0.8004, Train Acc:0.7617\n",
      "Epoch [2/10], Step [380/600], Loss: 0.6357, Train Acc:0.7617\n",
      "Epoch [2/10], Step [381/600], Loss: 0.7437, Train Acc:0.7616\n",
      "Epoch [2/10], Step [382/600], Loss: 0.8349, Train Acc:0.7613\n",
      "Epoch [2/10], Step [383/600], Loss: 0.8313, Train Acc:0.7612\n",
      "Epoch [2/10], Step [384/600], Loss: 0.6571, Train Acc:0.7613\n",
      "Epoch [2/10], Step [385/600], Loss: 0.6929, Train Acc:0.7614\n",
      "Epoch [2/10], Step [386/600], Loss: 0.6873, Train Acc:0.7613\n",
      "Epoch [2/10], Step [387/600], Loss: 0.7767, Train Acc:0.7612\n",
      "Epoch [2/10], Step [388/600], Loss: 0.6034, Train Acc:0.7613\n",
      "Epoch [2/10], Step [389/600], Loss: 0.6971, Train Acc:0.7613\n",
      "Epoch [2/10], Step [390/600], Loss: 0.7534, Train Acc:0.7613\n",
      "Epoch [2/10], Step [391/600], Loss: 0.7544, Train Acc:0.7613\n",
      "Epoch [2/10], Step [392/600], Loss: 0.7037, Train Acc:0.7614\n",
      "Epoch [2/10], Step [393/600], Loss: 0.7176, Train Acc:0.7616\n",
      "Epoch [2/10], Step [394/600], Loss: 0.8073, Train Acc:0.7615\n",
      "Epoch [2/10], Step [395/600], Loss: 0.6500, Train Acc:0.7617\n",
      "Epoch [2/10], Step [396/600], Loss: 0.6956, Train Acc:0.7619\n",
      "Epoch [2/10], Step [397/600], Loss: 0.6735, Train Acc:0.7618\n",
      "Epoch [2/10], Step [398/600], Loss: 0.6544, Train Acc:0.7620\n",
      "Epoch [2/10], Step [399/600], Loss: 0.6785, Train Acc:0.7622\n",
      "Epoch [2/10], Step [400/600], Loss: 0.8339, Train Acc:0.7621\n",
      "Epoch [2/10], Step [401/600], Loss: 0.7458, Train Acc:0.7620\n",
      "Epoch [2/10], Step [402/600], Loss: 0.7134, Train Acc:0.7621\n",
      "Epoch [2/10], Step [403/600], Loss: 0.7750, Train Acc:0.7620\n",
      "Epoch [2/10], Step [404/600], Loss: 0.6566, Train Acc:0.7622\n",
      "Epoch [2/10], Step [405/600], Loss: 0.7348, Train Acc:0.7622\n",
      "Epoch [2/10], Step [406/600], Loss: 0.6296, Train Acc:0.7624\n",
      "Epoch [2/10], Step [407/600], Loss: 0.5661, Train Acc:0.7626\n",
      "Epoch [2/10], Step [408/600], Loss: 0.6763, Train Acc:0.7626\n",
      "Epoch [2/10], Step [409/600], Loss: 0.7555, Train Acc:0.7627\n",
      "Epoch [2/10], Step [410/600], Loss: 0.7854, Train Acc:0.7626\n",
      "Epoch [2/10], Step [411/600], Loss: 0.6976, Train Acc:0.7627\n",
      "Epoch [2/10], Step [412/600], Loss: 0.7220, Train Acc:0.7628\n",
      "Epoch [2/10], Step [413/600], Loss: 0.7123, Train Acc:0.7629\n",
      "Epoch [2/10], Step [414/600], Loss: 0.6840, Train Acc:0.7630\n",
      "Epoch [2/10], Step [415/600], Loss: 0.7096, Train Acc:0.7630\n",
      "Epoch [2/10], Step [416/600], Loss: 0.7175, Train Acc:0.7630\n",
      "Epoch [2/10], Step [417/600], Loss: 0.8265, Train Acc:0.7629\n",
      "Epoch [2/10], Step [418/600], Loss: 0.7223, Train Acc:0.7629\n",
      "Epoch [2/10], Step [419/600], Loss: 0.6150, Train Acc:0.7630\n",
      "Epoch [2/10], Step [420/600], Loss: 0.6463, Train Acc:0.7630\n",
      "Epoch [2/10], Step [421/600], Loss: 0.6837, Train Acc:0.7630\n",
      "Epoch [2/10], Step [422/600], Loss: 0.7052, Train Acc:0.7630\n",
      "Epoch [2/10], Step [423/600], Loss: 0.7206, Train Acc:0.7631\n",
      "Epoch [2/10], Step [424/600], Loss: 0.7581, Train Acc:0.7632\n",
      "Epoch [2/10], Step [425/600], Loss: 0.7632, Train Acc:0.7632\n",
      "Epoch [2/10], Step [426/600], Loss: 0.6557, Train Acc:0.7634\n",
      "Epoch [2/10], Step [427/600], Loss: 0.7065, Train Acc:0.7633\n",
      "Epoch [2/10], Step [428/600], Loss: 0.8088, Train Acc:0.7632\n",
      "Epoch [2/10], Step [429/600], Loss: 0.7076, Train Acc:0.7632\n",
      "Epoch [2/10], Step [430/600], Loss: 0.5577, Train Acc:0.7634\n",
      "Epoch [2/10], Step [431/600], Loss: 0.8173, Train Acc:0.7634\n",
      "Epoch [2/10], Step [432/600], Loss: 0.6389, Train Acc:0.7635\n",
      "Epoch [2/10], Step [433/600], Loss: 0.8128, Train Acc:0.7634\n",
      "Epoch [2/10], Step [434/600], Loss: 0.6846, Train Acc:0.7634\n",
      "Epoch [2/10], Step [435/600], Loss: 0.7469, Train Acc:0.7634\n",
      "Epoch [2/10], Step [436/600], Loss: 0.8119, Train Acc:0.7634\n",
      "Epoch [2/10], Step [437/600], Loss: 0.8149, Train Acc:0.7632\n",
      "Epoch [2/10], Step [438/600], Loss: 0.6337, Train Acc:0.7633\n",
      "Epoch [2/10], Step [439/600], Loss: 0.7013, Train Acc:0.7635\n",
      "Epoch [2/10], Step [440/600], Loss: 0.7324, Train Acc:0.7636\n",
      "Epoch [2/10], Step [441/600], Loss: 0.7027, Train Acc:0.7637\n",
      "Epoch [2/10], Step [442/600], Loss: 0.6429, Train Acc:0.7637\n",
      "Epoch [2/10], Step [443/600], Loss: 0.8090, Train Acc:0.7637\n",
      "Epoch [2/10], Step [444/600], Loss: 0.6441, Train Acc:0.7638\n",
      "Epoch [2/10], Step [445/600], Loss: 0.8494, Train Acc:0.7637\n",
      "Epoch [2/10], Step [446/600], Loss: 0.7055, Train Acc:0.7639\n",
      "Epoch [2/10], Step [447/600], Loss: 0.5881, Train Acc:0.7640\n",
      "Epoch [2/10], Step [448/600], Loss: 0.6894, Train Acc:0.7639\n",
      "Epoch [2/10], Step [449/600], Loss: 0.7162, Train Acc:0.7639\n",
      "Epoch [2/10], Step [450/600], Loss: 0.8755, Train Acc:0.7638\n",
      "Epoch [2/10], Step [451/600], Loss: 0.6597, Train Acc:0.7638\n",
      "Epoch [2/10], Step [452/600], Loss: 0.5647, Train Acc:0.7640\n",
      "Epoch [2/10], Step [453/600], Loss: 0.7592, Train Acc:0.7640\n",
      "Epoch [2/10], Step [454/600], Loss: 0.6752, Train Acc:0.7640\n",
      "Epoch [2/10], Step [455/600], Loss: 0.7151, Train Acc:0.7641\n",
      "Epoch [2/10], Step [456/600], Loss: 0.6449, Train Acc:0.7641\n",
      "Epoch [2/10], Step [457/600], Loss: 0.8190, Train Acc:0.7640\n",
      "Epoch [2/10], Step [458/600], Loss: 0.7200, Train Acc:0.7640\n",
      "Epoch [2/10], Step [459/600], Loss: 0.7063, Train Acc:0.7642\n",
      "Epoch [2/10], Step [460/600], Loss: 0.6321, Train Acc:0.7643\n",
      "Epoch [2/10], Step [461/600], Loss: 0.6740, Train Acc:0.7644\n",
      "Epoch [2/10], Step [462/600], Loss: 0.7856, Train Acc:0.7643\n",
      "Epoch [2/10], Step [463/600], Loss: 0.8757, Train Acc:0.7641\n",
      "Epoch [2/10], Step [464/600], Loss: 0.6629, Train Acc:0.7641\n",
      "Epoch [2/10], Step [465/600], Loss: 0.6086, Train Acc:0.7641\n",
      "Epoch [2/10], Step [466/600], Loss: 0.8719, Train Acc:0.7640\n",
      "Epoch [2/10], Step [467/600], Loss: 0.7152, Train Acc:0.7640\n",
      "Epoch [2/10], Step [468/600], Loss: 0.6631, Train Acc:0.7641\n",
      "Epoch [2/10], Step [469/600], Loss: 0.6610, Train Acc:0.7642\n",
      "Epoch [2/10], Step [470/600], Loss: 0.5999, Train Acc:0.7643\n",
      "Epoch [2/10], Step [471/600], Loss: 0.7182, Train Acc:0.7643\n",
      "Epoch [2/10], Step [472/600], Loss: 0.5544, Train Acc:0.7645\n",
      "Epoch [2/10], Step [473/600], Loss: 0.6881, Train Acc:0.7645\n",
      "Epoch [2/10], Step [474/600], Loss: 0.6481, Train Acc:0.7646\n",
      "Epoch [2/10], Step [475/600], Loss: 0.7018, Train Acc:0.7645\n",
      "Epoch [2/10], Step [476/600], Loss: 0.6803, Train Acc:0.7646\n",
      "Epoch [2/10], Step [477/600], Loss: 0.5566, Train Acc:0.7647\n",
      "Epoch [2/10], Step [478/600], Loss: 0.6760, Train Acc:0.7648\n",
      "Epoch [2/10], Step [479/600], Loss: 0.6517, Train Acc:0.7649\n",
      "Epoch [2/10], Step [480/600], Loss: 0.6365, Train Acc:0.7649\n",
      "Epoch [2/10], Step [481/600], Loss: 0.5990, Train Acc:0.7652\n",
      "Epoch [2/10], Step [482/600], Loss: 0.6659, Train Acc:0.7651\n",
      "Epoch [2/10], Step [483/600], Loss: 0.7138, Train Acc:0.7651\n",
      "Epoch [2/10], Step [484/600], Loss: 0.6011, Train Acc:0.7652\n",
      "Epoch [2/10], Step [485/600], Loss: 0.7590, Train Acc:0.7651\n",
      "Epoch [2/10], Step [486/600], Loss: 0.7581, Train Acc:0.7650\n",
      "Epoch [2/10], Step [487/600], Loss: 0.7910, Train Acc:0.7649\n",
      "Epoch [2/10], Step [488/600], Loss: 0.6259, Train Acc:0.7649\n",
      "Epoch [2/10], Step [489/600], Loss: 0.7331, Train Acc:0.7649\n",
      "Epoch [2/10], Step [490/600], Loss: 0.7367, Train Acc:0.7649\n",
      "Epoch [2/10], Step [491/600], Loss: 0.7244, Train Acc:0.7649\n",
      "Epoch [2/10], Step [492/600], Loss: 0.6213, Train Acc:0.7650\n",
      "Epoch [2/10], Step [493/600], Loss: 0.5987, Train Acc:0.7651\n",
      "Epoch [2/10], Step [494/600], Loss: 0.6556, Train Acc:0.7652\n",
      "Epoch [2/10], Step [495/600], Loss: 0.7459, Train Acc:0.7653\n",
      "Epoch [2/10], Step [496/600], Loss: 0.7348, Train Acc:0.7653\n",
      "Epoch [2/10], Step [497/600], Loss: 0.7862, Train Acc:0.7652\n",
      "Epoch [2/10], Step [498/600], Loss: 0.7143, Train Acc:0.7652\n",
      "Epoch [2/10], Step [499/600], Loss: 0.6711, Train Acc:0.7652\n",
      "Epoch [2/10], Step [500/600], Loss: 0.8124, Train Acc:0.7651\n",
      "Epoch [2/10], Step [501/600], Loss: 0.7693, Train Acc:0.7651\n",
      "Epoch [2/10], Step [502/600], Loss: 0.7244, Train Acc:0.7651\n",
      "Epoch [2/10], Step [503/600], Loss: 0.6823, Train Acc:0.7651\n",
      "Epoch [2/10], Step [504/600], Loss: 0.6015, Train Acc:0.7652\n",
      "Epoch [2/10], Step [505/600], Loss: 0.7258, Train Acc:0.7653\n",
      "Epoch [2/10], Step [506/600], Loss: 0.6154, Train Acc:0.7654\n",
      "Epoch [2/10], Step [507/600], Loss: 0.6368, Train Acc:0.7654\n",
      "Epoch [2/10], Step [508/600], Loss: 0.8009, Train Acc:0.7654\n",
      "Epoch [2/10], Step [509/600], Loss: 0.7698, Train Acc:0.7654\n",
      "Epoch [2/10], Step [510/600], Loss: 0.6934, Train Acc:0.7653\n",
      "Epoch [2/10], Step [511/600], Loss: 0.6599, Train Acc:0.7654\n",
      "Epoch [2/10], Step [512/600], Loss: 0.7276, Train Acc:0.7654\n",
      "Epoch [2/10], Step [513/600], Loss: 0.7138, Train Acc:0.7654\n",
      "Epoch [2/10], Step [514/600], Loss: 0.8854, Train Acc:0.7654\n",
      "Epoch [2/10], Step [515/600], Loss: 0.7357, Train Acc:0.7653\n",
      "Epoch [2/10], Step [516/600], Loss: 0.6744, Train Acc:0.7653\n",
      "Epoch [2/10], Step [517/600], Loss: 0.7393, Train Acc:0.7654\n",
      "Epoch [2/10], Step [518/600], Loss: 0.7522, Train Acc:0.7653\n",
      "Epoch [2/10], Step [519/600], Loss: 0.5950, Train Acc:0.7655\n",
      "Epoch [2/10], Step [520/600], Loss: 0.7806, Train Acc:0.7655\n",
      "Epoch [2/10], Step [521/600], Loss: 0.6781, Train Acc:0.7655\n",
      "Epoch [2/10], Step [522/600], Loss: 0.7624, Train Acc:0.7654\n",
      "Epoch [2/10], Step [523/600], Loss: 0.7056, Train Acc:0.7654\n",
      "Epoch [2/10], Step [524/600], Loss: 0.7798, Train Acc:0.7653\n",
      "Epoch [2/10], Step [525/600], Loss: 0.7186, Train Acc:0.7653\n",
      "Epoch [2/10], Step [526/600], Loss: 0.7716, Train Acc:0.7653\n",
      "Epoch [2/10], Step [527/600], Loss: 0.7626, Train Acc:0.7652\n",
      "Epoch [2/10], Step [528/600], Loss: 0.7760, Train Acc:0.7651\n",
      "Epoch [2/10], Step [529/600], Loss: 0.7384, Train Acc:0.7651\n",
      "Epoch [2/10], Step [530/600], Loss: 0.6107, Train Acc:0.7652\n",
      "Epoch [2/10], Step [531/600], Loss: 0.6687, Train Acc:0.7653\n",
      "Epoch [2/10], Step [532/600], Loss: 0.5885, Train Acc:0.7653\n",
      "Epoch [2/10], Step [533/600], Loss: 0.6105, Train Acc:0.7655\n",
      "Epoch [2/10], Step [534/600], Loss: 0.8674, Train Acc:0.7654\n",
      "Epoch [2/10], Step [535/600], Loss: 0.6538, Train Acc:0.7654\n",
      "Epoch [2/10], Step [536/600], Loss: 0.7269, Train Acc:0.7655\n",
      "Epoch [2/10], Step [537/600], Loss: 0.7915, Train Acc:0.7655\n",
      "Epoch [2/10], Step [538/600], Loss: 0.6909, Train Acc:0.7655\n",
      "Epoch [2/10], Step [539/600], Loss: 0.6980, Train Acc:0.7655\n",
      "Epoch [2/10], Step [540/600], Loss: 0.7320, Train Acc:0.7655\n",
      "Epoch [2/10], Step [541/600], Loss: 0.7061, Train Acc:0.7656\n",
      "Epoch [2/10], Step [542/600], Loss: 0.8070, Train Acc:0.7655\n",
      "Epoch [2/10], Step [543/600], Loss: 0.6920, Train Acc:0.7655\n",
      "Epoch [2/10], Step [544/600], Loss: 0.6590, Train Acc:0.7656\n",
      "Epoch [2/10], Step [545/600], Loss: 0.6125, Train Acc:0.7658\n",
      "Epoch [2/10], Step [546/600], Loss: 0.8252, Train Acc:0.7657\n",
      "Epoch [2/10], Step [547/600], Loss: 0.6901, Train Acc:0.7657\n",
      "Epoch [2/10], Step [548/600], Loss: 0.7328, Train Acc:0.7657\n",
      "Epoch [2/10], Step [549/600], Loss: 0.8260, Train Acc:0.7655\n",
      "Epoch [2/10], Step [550/600], Loss: 0.7039, Train Acc:0.7655\n",
      "Epoch [2/10], Step [551/600], Loss: 0.6530, Train Acc:0.7656\n",
      "Epoch [2/10], Step [552/600], Loss: 0.6505, Train Acc:0.7656\n",
      "Epoch [2/10], Step [553/600], Loss: 0.7086, Train Acc:0.7656\n",
      "Epoch [2/10], Step [554/600], Loss: 0.5485, Train Acc:0.7659\n",
      "Epoch [2/10], Step [555/600], Loss: 0.6546, Train Acc:0.7659\n",
      "Epoch [2/10], Step [556/600], Loss: 0.5765, Train Acc:0.7661\n",
      "Epoch [2/10], Step [557/600], Loss: 0.6546, Train Acc:0.7662\n",
      "Epoch [2/10], Step [558/600], Loss: 0.7217, Train Acc:0.7662\n",
      "Epoch [2/10], Step [559/600], Loss: 0.7054, Train Acc:0.7662\n",
      "Epoch [2/10], Step [560/600], Loss: 0.6052, Train Acc:0.7662\n",
      "Epoch [2/10], Step [561/600], Loss: 0.8681, Train Acc:0.7661\n",
      "Epoch [2/10], Step [562/600], Loss: 0.6013, Train Acc:0.7664\n",
      "Epoch [2/10], Step [563/600], Loss: 0.6166, Train Acc:0.7664\n",
      "Epoch [2/10], Step [564/600], Loss: 0.6836, Train Acc:0.7665\n",
      "Epoch [2/10], Step [565/600], Loss: 0.7096, Train Acc:0.7665\n",
      "Epoch [2/10], Step [566/600], Loss: 0.6553, Train Acc:0.7666\n",
      "Epoch [2/10], Step [567/600], Loss: 0.7337, Train Acc:0.7667\n",
      "Epoch [2/10], Step [568/600], Loss: 0.7004, Train Acc:0.7666\n",
      "Epoch [2/10], Step [569/600], Loss: 0.6582, Train Acc:0.7667\n",
      "Epoch [2/10], Step [570/600], Loss: 0.6095, Train Acc:0.7667\n",
      "Epoch [2/10], Step [571/600], Loss: 0.7150, Train Acc:0.7667\n",
      "Epoch [2/10], Step [572/600], Loss: 0.7003, Train Acc:0.7668\n",
      "Epoch [2/10], Step [573/600], Loss: 0.7176, Train Acc:0.7669\n",
      "Epoch [2/10], Step [574/600], Loss: 0.7585, Train Acc:0.7669\n",
      "Epoch [2/10], Step [575/600], Loss: 0.7192, Train Acc:0.7668\n",
      "Epoch [2/10], Step [576/600], Loss: 0.7498, Train Acc:0.7668\n",
      "Epoch [2/10], Step [577/600], Loss: 0.8515, Train Acc:0.7668\n",
      "Epoch [2/10], Step [578/600], Loss: 0.7825, Train Acc:0.7667\n",
      "Epoch [2/10], Step [579/600], Loss: 0.5745, Train Acc:0.7668\n",
      "Epoch [2/10], Step [580/600], Loss: 0.7025, Train Acc:0.7668\n",
      "Epoch [2/10], Step [581/600], Loss: 0.7466, Train Acc:0.7668\n",
      "Epoch [2/10], Step [582/600], Loss: 0.8126, Train Acc:0.7668\n",
      "Epoch [2/10], Step [583/600], Loss: 0.6889, Train Acc:0.7668\n",
      "Epoch [2/10], Step [584/600], Loss: 0.7188, Train Acc:0.7668\n",
      "Epoch [2/10], Step [585/600], Loss: 0.7405, Train Acc:0.7668\n",
      "Epoch [2/10], Step [586/600], Loss: 0.5814, Train Acc:0.7669\n",
      "Epoch [2/10], Step [587/600], Loss: 0.7102, Train Acc:0.7669\n",
      "Epoch [2/10], Step [588/600], Loss: 0.6194, Train Acc:0.7671\n",
      "Epoch [2/10], Step [589/600], Loss: 0.6828, Train Acc:0.7670\n",
      "Epoch [2/10], Step [590/600], Loss: 0.7149, Train Acc:0.7671\n",
      "Epoch [2/10], Step [591/600], Loss: 0.7211, Train Acc:0.7670\n",
      "Epoch [2/10], Step [592/600], Loss: 0.6746, Train Acc:0.7670\n",
      "Epoch [2/10], Step [593/600], Loss: 0.7520, Train Acc:0.7671\n",
      "Epoch [2/10], Step [594/600], Loss: 0.5295, Train Acc:0.7672\n",
      "Epoch [2/10], Step [595/600], Loss: 0.5894, Train Acc:0.7673\n",
      "Epoch [2/10], Step [596/600], Loss: 0.5751, Train Acc:0.7674\n",
      "Epoch [2/10], Step [597/600], Loss: 0.7416, Train Acc:0.7673\n",
      "Epoch [2/10], Step [598/600], Loss: 0.7821, Train Acc:0.7673\n",
      "Epoch [2/10], Step [599/600], Loss: 0.6366, Train Acc:0.7674\n",
      "Epoch [2/10], Step [600/600], Loss: 0.6333, Train Acc:0.7674\n",
      "Epoch [3/10], Step [1/600], Loss: 0.6326, Train Acc:0.8100\n",
      "Epoch [3/10], Step [2/600], Loss: 0.7223, Train Acc:0.7700\n",
      "Epoch [3/10], Step [3/600], Loss: 0.6133, Train Acc:0.7767\n",
      "Epoch [3/10], Step [4/600], Loss: 0.7043, Train Acc:0.7775\n",
      "Epoch [3/10], Step [5/600], Loss: 0.7403, Train Acc:0.7680\n",
      "Epoch [3/10], Step [6/600], Loss: 0.7625, Train Acc:0.7717\n",
      "Epoch [3/10], Step [7/600], Loss: 0.6355, Train Acc:0.7771\n",
      "Epoch [3/10], Step [8/600], Loss: 0.6896, Train Acc:0.7863\n",
      "Epoch [3/10], Step [9/600], Loss: 0.6809, Train Acc:0.7856\n",
      "Epoch [3/10], Step [10/600], Loss: 0.7587, Train Acc:0.7820\n",
      "Epoch [3/10], Step [11/600], Loss: 0.7406, Train Acc:0.7809\n",
      "Epoch [3/10], Step [12/600], Loss: 0.7001, Train Acc:0.7792\n",
      "Epoch [3/10], Step [13/600], Loss: 0.5541, Train Acc:0.7815\n",
      "Epoch [3/10], Step [14/600], Loss: 0.6503, Train Acc:0.7843\n",
      "Epoch [3/10], Step [15/600], Loss: 0.6557, Train Acc:0.7847\n",
      "Epoch [3/10], Step [16/600], Loss: 0.7748, Train Acc:0.7825\n",
      "Epoch [3/10], Step [17/600], Loss: 0.6500, Train Acc:0.7841\n",
      "Epoch [3/10], Step [18/600], Loss: 0.7180, Train Acc:0.7867\n",
      "Epoch [3/10], Step [19/600], Loss: 0.6496, Train Acc:0.7858\n",
      "Epoch [3/10], Step [20/600], Loss: 0.7869, Train Acc:0.7845\n",
      "Epoch [3/10], Step [21/600], Loss: 0.6491, Train Acc:0.7871\n",
      "Epoch [3/10], Step [22/600], Loss: 0.7755, Train Acc:0.7841\n",
      "Epoch [3/10], Step [23/600], Loss: 0.6656, Train Acc:0.7843\n",
      "Epoch [3/10], Step [24/600], Loss: 0.9161, Train Acc:0.7800\n",
      "Epoch [3/10], Step [25/600], Loss: 0.6512, Train Acc:0.7820\n",
      "Epoch [3/10], Step [26/600], Loss: 0.7319, Train Acc:0.7804\n",
      "Epoch [3/10], Step [27/600], Loss: 0.6082, Train Acc:0.7807\n",
      "Epoch [3/10], Step [28/600], Loss: 0.8353, Train Acc:0.7775\n",
      "Epoch [3/10], Step [29/600], Loss: 0.6351, Train Acc:0.7779\n",
      "Epoch [3/10], Step [30/600], Loss: 0.5480, Train Acc:0.7813\n",
      "Epoch [3/10], Step [31/600], Loss: 0.6106, Train Acc:0.7823\n",
      "Epoch [3/10], Step [32/600], Loss: 0.6364, Train Acc:0.7834\n",
      "Epoch [3/10], Step [33/600], Loss: 0.6889, Train Acc:0.7842\n",
      "Epoch [3/10], Step [34/600], Loss: 0.7687, Train Acc:0.7847\n",
      "Epoch [3/10], Step [35/600], Loss: 0.5994, Train Acc:0.7854\n",
      "Epoch [3/10], Step [36/600], Loss: 0.6681, Train Acc:0.7850\n",
      "Epoch [3/10], Step [37/600], Loss: 0.6615, Train Acc:0.7854\n",
      "Epoch [3/10], Step [38/600], Loss: 0.6623, Train Acc:0.7855\n",
      "Epoch [3/10], Step [39/600], Loss: 0.5874, Train Acc:0.7869\n",
      "Epoch [3/10], Step [40/600], Loss: 0.7480, Train Acc:0.7870\n",
      "Epoch [3/10], Step [41/600], Loss: 0.7088, Train Acc:0.7863\n",
      "Epoch [3/10], Step [42/600], Loss: 0.6886, Train Acc:0.7860\n",
      "Epoch [3/10], Step [43/600], Loss: 0.6401, Train Acc:0.7856\n",
      "Epoch [3/10], Step [44/600], Loss: 0.6249, Train Acc:0.7875\n",
      "Epoch [3/10], Step [45/600], Loss: 0.6238, Train Acc:0.7882\n",
      "Epoch [3/10], Step [46/600], Loss: 0.6860, Train Acc:0.7880\n",
      "Epoch [3/10], Step [47/600], Loss: 0.6448, Train Acc:0.7885\n",
      "Epoch [3/10], Step [48/600], Loss: 0.6999, Train Acc:0.7871\n",
      "Epoch [3/10], Step [49/600], Loss: 0.6044, Train Acc:0.7869\n",
      "Epoch [3/10], Step [50/600], Loss: 0.6006, Train Acc:0.7880\n",
      "Epoch [3/10], Step [51/600], Loss: 0.7545, Train Acc:0.7884\n",
      "Epoch [3/10], Step [52/600], Loss: 0.7539, Train Acc:0.7875\n",
      "Epoch [3/10], Step [53/600], Loss: 0.6529, Train Acc:0.7877\n",
      "Epoch [3/10], Step [54/600], Loss: 0.6624, Train Acc:0.7883\n",
      "Epoch [3/10], Step [55/600], Loss: 0.6985, Train Acc:0.7891\n",
      "Epoch [3/10], Step [56/600], Loss: 0.7738, Train Acc:0.7884\n",
      "Epoch [3/10], Step [57/600], Loss: 0.7447, Train Acc:0.7874\n",
      "Epoch [3/10], Step [58/600], Loss: 0.7757, Train Acc:0.7871\n",
      "Epoch [3/10], Step [59/600], Loss: 0.6434, Train Acc:0.7868\n",
      "Epoch [3/10], Step [60/600], Loss: 0.7142, Train Acc:0.7865\n",
      "Epoch [3/10], Step [61/600], Loss: 0.7565, Train Acc:0.7854\n",
      "Epoch [3/10], Step [62/600], Loss: 0.5812, Train Acc:0.7860\n",
      "Epoch [3/10], Step [63/600], Loss: 0.7680, Train Acc:0.7849\n",
      "Epoch [3/10], Step [64/600], Loss: 0.6393, Train Acc:0.7855\n",
      "Epoch [3/10], Step [65/600], Loss: 0.6216, Train Acc:0.7855\n",
      "Epoch [3/10], Step [66/600], Loss: 0.7289, Train Acc:0.7845\n",
      "Epoch [3/10], Step [67/600], Loss: 0.6914, Train Acc:0.7845\n",
      "Epoch [3/10], Step [68/600], Loss: 0.6431, Train Acc:0.7838\n",
      "Epoch [3/10], Step [69/600], Loss: 0.6932, Train Acc:0.7836\n",
      "Epoch [3/10], Step [70/600], Loss: 0.7525, Train Acc:0.7834\n",
      "Epoch [3/10], Step [71/600], Loss: 0.8198, Train Acc:0.7830\n",
      "Epoch [3/10], Step [72/600], Loss: 0.5259, Train Acc:0.7837\n",
      "Epoch [3/10], Step [73/600], Loss: 0.6939, Train Acc:0.7840\n",
      "Epoch [3/10], Step [74/600], Loss: 0.6675, Train Acc:0.7838\n",
      "Epoch [3/10], Step [75/600], Loss: 0.6467, Train Acc:0.7841\n",
      "Epoch [3/10], Step [76/600], Loss: 0.6544, Train Acc:0.7842\n",
      "Epoch [3/10], Step [77/600], Loss: 0.6950, Train Acc:0.7847\n",
      "Epoch [3/10], Step [78/600], Loss: 0.5848, Train Acc:0.7849\n",
      "Epoch [3/10], Step [79/600], Loss: 0.7443, Train Acc:0.7844\n",
      "Epoch [3/10], Step [80/600], Loss: 0.6013, Train Acc:0.7853\n",
      "Epoch [3/10], Step [81/600], Loss: 0.6789, Train Acc:0.7848\n",
      "Epoch [3/10], Step [82/600], Loss: 0.6002, Train Acc:0.7857\n",
      "Epoch [3/10], Step [83/600], Loss: 0.6338, Train Acc:0.7864\n",
      "Epoch [3/10], Step [84/600], Loss: 0.4402, Train Acc:0.7876\n",
      "Epoch [3/10], Step [85/600], Loss: 0.6296, Train Acc:0.7882\n",
      "Epoch [3/10], Step [86/600], Loss: 0.4913, Train Acc:0.7891\n",
      "Epoch [3/10], Step [87/600], Loss: 0.6320, Train Acc:0.7892\n",
      "Epoch [3/10], Step [88/600], Loss: 0.6679, Train Acc:0.7892\n",
      "Epoch [3/10], Step [89/600], Loss: 0.6284, Train Acc:0.7900\n",
      "Epoch [3/10], Step [90/600], Loss: 0.6068, Train Acc:0.7900\n",
      "Epoch [3/10], Step [91/600], Loss: 0.6088, Train Acc:0.7900\n",
      "Epoch [3/10], Step [92/600], Loss: 0.6319, Train Acc:0.7899\n",
      "Epoch [3/10], Step [93/600], Loss: 0.6568, Train Acc:0.7901\n",
      "Epoch [3/10], Step [94/600], Loss: 0.7258, Train Acc:0.7901\n",
      "Epoch [3/10], Step [95/600], Loss: 0.7328, Train Acc:0.7902\n",
      "Epoch [3/10], Step [96/600], Loss: 0.6800, Train Acc:0.7901\n",
      "Epoch [3/10], Step [97/600], Loss: 0.6001, Train Acc:0.7899\n",
      "Epoch [3/10], Step [98/600], Loss: 0.6375, Train Acc:0.7901\n",
      "Epoch [3/10], Step [99/600], Loss: 0.6550, Train Acc:0.7898\n",
      "Epoch [3/10], Step [100/600], Loss: 0.6759, Train Acc:0.7900\n",
      "Epoch [3/10], Step [101/600], Loss: 0.6010, Train Acc:0.7905\n",
      "Epoch [3/10], Step [102/600], Loss: 0.6878, Train Acc:0.7902\n",
      "Epoch [3/10], Step [103/600], Loss: 0.5784, Train Acc:0.7908\n",
      "Epoch [3/10], Step [104/600], Loss: 0.6799, Train Acc:0.7910\n",
      "Epoch [3/10], Step [105/600], Loss: 0.6740, Train Acc:0.7911\n",
      "Epoch [3/10], Step [106/600], Loss: 0.6246, Train Acc:0.7914\n",
      "Epoch [3/10], Step [107/600], Loss: 0.6185, Train Acc:0.7919\n",
      "Epoch [3/10], Step [108/600], Loss: 0.7908, Train Acc:0.7912\n",
      "Epoch [3/10], Step [109/600], Loss: 0.6893, Train Acc:0.7909\n",
      "Epoch [3/10], Step [110/600], Loss: 0.7748, Train Acc:0.7905\n",
      "Epoch [3/10], Step [111/600], Loss: 0.7201, Train Acc:0.7901\n",
      "Epoch [3/10], Step [112/600], Loss: 0.6899, Train Acc:0.7900\n",
      "Epoch [3/10], Step [113/600], Loss: 0.6493, Train Acc:0.7903\n",
      "Epoch [3/10], Step [114/600], Loss: 0.6549, Train Acc:0.7900\n",
      "Epoch [3/10], Step [115/600], Loss: 0.7364, Train Acc:0.7897\n",
      "Epoch [3/10], Step [116/600], Loss: 0.5613, Train Acc:0.7899\n",
      "Epoch [3/10], Step [117/600], Loss: 0.6450, Train Acc:0.7903\n",
      "Epoch [3/10], Step [118/600], Loss: 0.7616, Train Acc:0.7900\n",
      "Epoch [3/10], Step [119/600], Loss: 0.6580, Train Acc:0.7903\n",
      "Epoch [3/10], Step [120/600], Loss: 0.6321, Train Acc:0.7906\n",
      "Epoch [3/10], Step [121/600], Loss: 0.7866, Train Acc:0.7902\n",
      "Epoch [3/10], Step [122/600], Loss: 0.6979, Train Acc:0.7903\n",
      "Epoch [3/10], Step [123/600], Loss: 0.6284, Train Acc:0.7905\n",
      "Epoch [3/10], Step [124/600], Loss: 0.5724, Train Acc:0.7910\n",
      "Epoch [3/10], Step [125/600], Loss: 0.6992, Train Acc:0.7906\n",
      "Epoch [3/10], Step [126/600], Loss: 0.6625, Train Acc:0.7905\n",
      "Epoch [3/10], Step [127/600], Loss: 0.6671, Train Acc:0.7901\n",
      "Epoch [3/10], Step [128/600], Loss: 0.5860, Train Acc:0.7904\n",
      "Epoch [3/10], Step [129/600], Loss: 0.5973, Train Acc:0.7906\n",
      "Epoch [3/10], Step [130/600], Loss: 0.8053, Train Acc:0.7901\n",
      "Epoch [3/10], Step [131/600], Loss: 0.6479, Train Acc:0.7905\n",
      "Epoch [3/10], Step [132/600], Loss: 0.5888, Train Acc:0.7908\n",
      "Epoch [3/10], Step [133/600], Loss: 0.5451, Train Acc:0.7914\n",
      "Epoch [3/10], Step [134/600], Loss: 0.6056, Train Acc:0.7915\n",
      "Epoch [3/10], Step [135/600], Loss: 0.6479, Train Acc:0.7913\n",
      "Epoch [3/10], Step [136/600], Loss: 0.6795, Train Acc:0.7913\n",
      "Epoch [3/10], Step [137/600], Loss: 0.6266, Train Acc:0.7912\n",
      "Epoch [3/10], Step [138/600], Loss: 0.7566, Train Acc:0.7911\n",
      "Epoch [3/10], Step [139/600], Loss: 0.6510, Train Acc:0.7909\n",
      "Epoch [3/10], Step [140/600], Loss: 0.6074, Train Acc:0.7908\n",
      "Epoch [3/10], Step [141/600], Loss: 0.5731, Train Acc:0.7911\n",
      "Epoch [3/10], Step [142/600], Loss: 0.6954, Train Acc:0.7911\n",
      "Epoch [3/10], Step [143/600], Loss: 0.6234, Train Acc:0.7915\n",
      "Epoch [3/10], Step [144/600], Loss: 0.7173, Train Acc:0.7913\n",
      "Epoch [3/10], Step [145/600], Loss: 0.6425, Train Acc:0.7917\n",
      "Epoch [3/10], Step [146/600], Loss: 0.6746, Train Acc:0.7910\n",
      "Epoch [3/10], Step [147/600], Loss: 0.6427, Train Acc:0.7912\n",
      "Epoch [3/10], Step [148/600], Loss: 0.7003, Train Acc:0.7909\n",
      "Epoch [3/10], Step [149/600], Loss: 0.7098, Train Acc:0.7910\n",
      "Epoch [3/10], Step [150/600], Loss: 0.8041, Train Acc:0.7905\n",
      "Epoch [3/10], Step [151/600], Loss: 0.5795, Train Acc:0.7907\n",
      "Epoch [3/10], Step [152/600], Loss: 0.5089, Train Acc:0.7913\n",
      "Epoch [3/10], Step [153/600], Loss: 0.7115, Train Acc:0.7910\n",
      "Epoch [3/10], Step [154/600], Loss: 0.7532, Train Acc:0.7906\n",
      "Epoch [3/10], Step [155/600], Loss: 0.8642, Train Acc:0.7901\n",
      "Epoch [3/10], Step [156/600], Loss: 0.7251, Train Acc:0.7896\n",
      "Epoch [3/10], Step [157/600], Loss: 0.5718, Train Acc:0.7897\n",
      "Epoch [3/10], Step [158/600], Loss: 0.6988, Train Acc:0.7896\n",
      "Epoch [3/10], Step [159/600], Loss: 0.6326, Train Acc:0.7897\n",
      "Epoch [3/10], Step [160/600], Loss: 0.5635, Train Acc:0.7903\n",
      "Epoch [3/10], Step [161/600], Loss: 0.8822, Train Acc:0.7899\n",
      "Epoch [3/10], Step [162/600], Loss: 0.6173, Train Acc:0.7902\n",
      "Epoch [3/10], Step [163/600], Loss: 0.6689, Train Acc:0.7902\n",
      "Epoch [3/10], Step [164/600], Loss: 0.8137, Train Acc:0.7898\n",
      "Epoch [3/10], Step [165/600], Loss: 0.7151, Train Acc:0.7895\n",
      "Epoch [3/10], Step [166/600], Loss: 0.6862, Train Acc:0.7892\n",
      "Epoch [3/10], Step [167/600], Loss: 0.6158, Train Acc:0.7892\n",
      "Epoch [3/10], Step [168/600], Loss: 0.6778, Train Acc:0.7890\n",
      "Epoch [3/10], Step [169/600], Loss: 0.5215, Train Acc:0.7893\n",
      "Epoch [3/10], Step [170/600], Loss: 0.5146, Train Acc:0.7901\n",
      "Epoch [3/10], Step [171/600], Loss: 0.6771, Train Acc:0.7899\n",
      "Epoch [3/10], Step [172/600], Loss: 0.7354, Train Acc:0.7898\n",
      "Epoch [3/10], Step [173/600], Loss: 0.6926, Train Acc:0.7898\n",
      "Epoch [3/10], Step [174/600], Loss: 0.6279, Train Acc:0.7899\n",
      "Epoch [3/10], Step [175/600], Loss: 0.8270, Train Acc:0.7895\n",
      "Epoch [3/10], Step [176/600], Loss: 0.6511, Train Acc:0.7897\n",
      "Epoch [3/10], Step [177/600], Loss: 0.6931, Train Acc:0.7897\n",
      "Epoch [3/10], Step [178/600], Loss: 0.5807, Train Acc:0.7900\n",
      "Epoch [3/10], Step [179/600], Loss: 0.6219, Train Acc:0.7903\n",
      "Epoch [3/10], Step [180/600], Loss: 0.7188, Train Acc:0.7899\n",
      "Epoch [3/10], Step [181/600], Loss: 0.6446, Train Acc:0.7901\n",
      "Epoch [3/10], Step [182/600], Loss: 0.5720, Train Acc:0.7903\n",
      "Epoch [3/10], Step [183/600], Loss: 0.7142, Train Acc:0.7901\n",
      "Epoch [3/10], Step [184/600], Loss: 0.6446, Train Acc:0.7901\n",
      "Epoch [3/10], Step [185/600], Loss: 0.6639, Train Acc:0.7902\n",
      "Epoch [3/10], Step [186/600], Loss: 0.6288, Train Acc:0.7903\n",
      "Epoch [3/10], Step [187/600], Loss: 0.6253, Train Acc:0.7903\n",
      "Epoch [3/10], Step [188/600], Loss: 0.7590, Train Acc:0.7901\n",
      "Epoch [3/10], Step [189/600], Loss: 0.7430, Train Acc:0.7901\n",
      "Epoch [3/10], Step [190/600], Loss: 0.7877, Train Acc:0.7899\n",
      "Epoch [3/10], Step [191/600], Loss: 0.7907, Train Acc:0.7899\n",
      "Epoch [3/10], Step [192/600], Loss: 0.7009, Train Acc:0.7897\n",
      "Epoch [3/10], Step [193/600], Loss: 0.6670, Train Acc:0.7895\n",
      "Epoch [3/10], Step [194/600], Loss: 0.6640, Train Acc:0.7896\n",
      "Epoch [3/10], Step [195/600], Loss: 0.6285, Train Acc:0.7896\n",
      "Epoch [3/10], Step [196/600], Loss: 0.7928, Train Acc:0.7892\n",
      "Epoch [3/10], Step [197/600], Loss: 0.7135, Train Acc:0.7888\n",
      "Epoch [3/10], Step [198/600], Loss: 0.5584, Train Acc:0.7891\n",
      "Epoch [3/10], Step [199/600], Loss: 0.7199, Train Acc:0.7889\n",
      "Epoch [3/10], Step [200/600], Loss: 0.6933, Train Acc:0.7888\n",
      "Epoch [3/10], Step [201/600], Loss: 0.6399, Train Acc:0.7890\n",
      "Epoch [3/10], Step [202/600], Loss: 0.6523, Train Acc:0.7890\n",
      "Epoch [3/10], Step [203/600], Loss: 0.5984, Train Acc:0.7892\n",
      "Epoch [3/10], Step [204/600], Loss: 0.7577, Train Acc:0.7892\n",
      "Epoch [3/10], Step [205/600], Loss: 0.6247, Train Acc:0.7891\n",
      "Epoch [3/10], Step [206/600], Loss: 0.7569, Train Acc:0.7888\n",
      "Epoch [3/10], Step [207/600], Loss: 0.7232, Train Acc:0.7883\n",
      "Epoch [3/10], Step [208/600], Loss: 0.6814, Train Acc:0.7881\n",
      "Epoch [3/10], Step [209/600], Loss: 0.8742, Train Acc:0.7879\n",
      "Epoch [3/10], Step [210/600], Loss: 0.5839, Train Acc:0.7882\n",
      "Epoch [3/10], Step [211/600], Loss: 0.6917, Train Acc:0.7882\n",
      "Epoch [3/10], Step [212/600], Loss: 0.6672, Train Acc:0.7882\n",
      "Epoch [3/10], Step [213/600], Loss: 0.7226, Train Acc:0.7881\n",
      "Epoch [3/10], Step [214/600], Loss: 0.6948, Train Acc:0.7881\n",
      "Epoch [3/10], Step [215/600], Loss: 0.6350, Train Acc:0.7880\n",
      "Epoch [3/10], Step [216/600], Loss: 0.5817, Train Acc:0.7883\n",
      "Epoch [3/10], Step [217/600], Loss: 0.7278, Train Acc:0.7882\n",
      "Epoch [3/10], Step [218/600], Loss: 0.7244, Train Acc:0.7880\n",
      "Epoch [3/10], Step [219/600], Loss: 0.6984, Train Acc:0.7879\n",
      "Epoch [3/10], Step [220/600], Loss: 0.7222, Train Acc:0.7878\n",
      "Epoch [3/10], Step [221/600], Loss: 0.5972, Train Acc:0.7879\n",
      "Epoch [3/10], Step [222/600], Loss: 0.6310, Train Acc:0.7879\n",
      "Epoch [3/10], Step [223/600], Loss: 0.7139, Train Acc:0.7878\n",
      "Epoch [3/10], Step [224/600], Loss: 0.6977, Train Acc:0.7878\n",
      "Epoch [3/10], Step [225/600], Loss: 0.8148, Train Acc:0.7873\n",
      "Epoch [3/10], Step [226/600], Loss: 0.6973, Train Acc:0.7872\n",
      "Epoch [3/10], Step [227/600], Loss: 0.6086, Train Acc:0.7875\n",
      "Epoch [3/10], Step [228/600], Loss: 0.6175, Train Acc:0.7878\n",
      "Epoch [3/10], Step [229/600], Loss: 0.6054, Train Acc:0.7879\n",
      "Epoch [3/10], Step [230/600], Loss: 0.6765, Train Acc:0.7880\n",
      "Epoch [3/10], Step [231/600], Loss: 0.5196, Train Acc:0.7882\n",
      "Epoch [3/10], Step [232/600], Loss: 0.5762, Train Acc:0.7884\n",
      "Epoch [3/10], Step [233/600], Loss: 0.6406, Train Acc:0.7885\n",
      "Epoch [3/10], Step [234/600], Loss: 0.6554, Train Acc:0.7886\n",
      "Epoch [3/10], Step [235/600], Loss: 0.6281, Train Acc:0.7889\n",
      "Epoch [3/10], Step [236/600], Loss: 0.6640, Train Acc:0.7889\n",
      "Epoch [3/10], Step [237/600], Loss: 0.5587, Train Acc:0.7891\n",
      "Epoch [3/10], Step [238/600], Loss: 0.7524, Train Acc:0.7891\n",
      "Epoch [3/10], Step [239/600], Loss: 0.6259, Train Acc:0.7892\n",
      "Epoch [3/10], Step [240/600], Loss: 0.7111, Train Acc:0.7892\n",
      "Epoch [3/10], Step [241/600], Loss: 0.5396, Train Acc:0.7895\n",
      "Epoch [3/10], Step [242/600], Loss: 0.5542, Train Acc:0.7898\n",
      "Epoch [3/10], Step [243/600], Loss: 0.7279, Train Acc:0.7899\n",
      "Epoch [3/10], Step [244/600], Loss: 0.6462, Train Acc:0.7898\n",
      "Epoch [3/10], Step [245/600], Loss: 0.6921, Train Acc:0.7898\n",
      "Epoch [3/10], Step [246/600], Loss: 0.7368, Train Acc:0.7896\n",
      "Epoch [3/10], Step [247/600], Loss: 0.5988, Train Acc:0.7897\n",
      "Epoch [3/10], Step [248/600], Loss: 0.4932, Train Acc:0.7902\n",
      "Epoch [3/10], Step [249/600], Loss: 0.6237, Train Acc:0.7901\n",
      "Epoch [3/10], Step [250/600], Loss: 0.7509, Train Acc:0.7900\n",
      "Epoch [3/10], Step [251/600], Loss: 0.6648, Train Acc:0.7900\n",
      "Epoch [3/10], Step [252/600], Loss: 0.6259, Train Acc:0.7901\n",
      "Epoch [3/10], Step [253/600], Loss: 0.7284, Train Acc:0.7899\n",
      "Epoch [3/10], Step [254/600], Loss: 0.5837, Train Acc:0.7901\n",
      "Epoch [3/10], Step [255/600], Loss: 0.6195, Train Acc:0.7901\n",
      "Epoch [3/10], Step [256/600], Loss: 0.6154, Train Acc:0.7901\n",
      "Epoch [3/10], Step [257/600], Loss: 0.6485, Train Acc:0.7901\n",
      "Epoch [3/10], Step [258/600], Loss: 0.6461, Train Acc:0.7901\n",
      "Epoch [3/10], Step [259/600], Loss: 0.5832, Train Acc:0.7901\n",
      "Epoch [3/10], Step [260/600], Loss: 0.7577, Train Acc:0.7900\n",
      "Epoch [3/10], Step [261/600], Loss: 0.5680, Train Acc:0.7902\n",
      "Epoch [3/10], Step [262/600], Loss: 0.7180, Train Acc:0.7902\n",
      "Epoch [3/10], Step [263/600], Loss: 0.5722, Train Acc:0.7903\n",
      "Epoch [3/10], Step [264/600], Loss: 0.7192, Train Acc:0.7901\n",
      "Epoch [3/10], Step [265/600], Loss: 0.6359, Train Acc:0.7902\n",
      "Epoch [3/10], Step [266/600], Loss: 0.6811, Train Acc:0.7901\n",
      "Epoch [3/10], Step [267/600], Loss: 0.6111, Train Acc:0.7901\n",
      "Epoch [3/10], Step [268/600], Loss: 0.6462, Train Acc:0.7902\n",
      "Epoch [3/10], Step [269/600], Loss: 0.7339, Train Acc:0.7900\n",
      "Epoch [3/10], Step [270/600], Loss: 0.8328, Train Acc:0.7896\n",
      "Epoch [3/10], Step [271/600], Loss: 0.6641, Train Acc:0.7896\n",
      "Epoch [3/10], Step [272/600], Loss: 0.6060, Train Acc:0.7897\n",
      "Epoch [3/10], Step [273/600], Loss: 0.6523, Train Acc:0.7897\n",
      "Epoch [3/10], Step [274/600], Loss: 0.6491, Train Acc:0.7898\n",
      "Epoch [3/10], Step [275/600], Loss: 0.6736, Train Acc:0.7898\n",
      "Epoch [3/10], Step [276/600], Loss: 0.7357, Train Acc:0.7898\n",
      "Epoch [3/10], Step [277/600], Loss: 0.6773, Train Acc:0.7897\n",
      "Epoch [3/10], Step [278/600], Loss: 0.6715, Train Acc:0.7897\n",
      "Epoch [3/10], Step [279/600], Loss: 0.6794, Train Acc:0.7898\n",
      "Epoch [3/10], Step [280/600], Loss: 0.7011, Train Acc:0.7896\n",
      "Epoch [3/10], Step [281/600], Loss: 0.6404, Train Acc:0.7896\n",
      "Epoch [3/10], Step [282/600], Loss: 0.6627, Train Acc:0.7897\n",
      "Epoch [3/10], Step [283/600], Loss: 0.6549, Train Acc:0.7897\n",
      "Epoch [3/10], Step [284/600], Loss: 0.6040, Train Acc:0.7900\n",
      "Epoch [3/10], Step [285/600], Loss: 0.7034, Train Acc:0.7899\n",
      "Epoch [3/10], Step [286/600], Loss: 0.6458, Train Acc:0.7898\n",
      "Epoch [3/10], Step [287/600], Loss: 0.7077, Train Acc:0.7897\n",
      "Epoch [3/10], Step [288/600], Loss: 0.6896, Train Acc:0.7896\n",
      "Epoch [3/10], Step [289/600], Loss: 0.6403, Train Acc:0.7897\n",
      "Epoch [3/10], Step [290/600], Loss: 0.5511, Train Acc:0.7899\n",
      "Epoch [3/10], Step [291/600], Loss: 0.6595, Train Acc:0.7899\n",
      "Epoch [3/10], Step [292/600], Loss: 0.7258, Train Acc:0.7900\n",
      "Epoch [3/10], Step [293/600], Loss: 0.6369, Train Acc:0.7899\n",
      "Epoch [3/10], Step [294/600], Loss: 0.5772, Train Acc:0.7900\n",
      "Epoch [3/10], Step [295/600], Loss: 0.8302, Train Acc:0.7898\n",
      "Epoch [3/10], Step [296/600], Loss: 0.6315, Train Acc:0.7900\n",
      "Epoch [3/10], Step [297/600], Loss: 0.4568, Train Acc:0.7903\n",
      "Epoch [3/10], Step [298/600], Loss: 0.5725, Train Acc:0.7905\n",
      "Epoch [3/10], Step [299/600], Loss: 0.6076, Train Acc:0.7906\n",
      "Epoch [3/10], Step [300/600], Loss: 0.7226, Train Acc:0.7904\n",
      "Epoch [3/10], Step [301/600], Loss: 0.7016, Train Acc:0.7903\n",
      "Epoch [3/10], Step [302/600], Loss: 0.6960, Train Acc:0.7903\n",
      "Epoch [3/10], Step [303/600], Loss: 0.4756, Train Acc:0.7908\n",
      "Epoch [3/10], Step [304/600], Loss: 0.6453, Train Acc:0.7909\n",
      "Epoch [3/10], Step [305/600], Loss: 0.5856, Train Acc:0.7910\n",
      "Epoch [3/10], Step [306/600], Loss: 0.6550, Train Acc:0.7910\n",
      "Epoch [3/10], Step [307/600], Loss: 0.6678, Train Acc:0.7910\n",
      "Epoch [3/10], Step [308/600], Loss: 0.7159, Train Acc:0.7910\n",
      "Epoch [3/10], Step [309/600], Loss: 0.6735, Train Acc:0.7911\n",
      "Epoch [3/10], Step [310/600], Loss: 0.6728, Train Acc:0.7911\n",
      "Epoch [3/10], Step [311/600], Loss: 0.5821, Train Acc:0.7912\n",
      "Epoch [3/10], Step [312/600], Loss: 0.6444, Train Acc:0.7913\n",
      "Epoch [3/10], Step [313/600], Loss: 0.6721, Train Acc:0.7912\n",
      "Epoch [3/10], Step [314/600], Loss: 0.6889, Train Acc:0.7911\n",
      "Epoch [3/10], Step [315/600], Loss: 0.5684, Train Acc:0.7912\n",
      "Epoch [3/10], Step [316/600], Loss: 0.6227, Train Acc:0.7913\n",
      "Epoch [3/10], Step [317/600], Loss: 0.6624, Train Acc:0.7912\n",
      "Epoch [3/10], Step [318/600], Loss: 0.8159, Train Acc:0.7911\n",
      "Epoch [3/10], Step [319/600], Loss: 0.7061, Train Acc:0.7909\n",
      "Epoch [3/10], Step [320/600], Loss: 0.7223, Train Acc:0.7908\n",
      "Epoch [3/10], Step [321/600], Loss: 0.6350, Train Acc:0.7907\n",
      "Epoch [3/10], Step [322/600], Loss: 0.5730, Train Acc:0.7909\n",
      "Epoch [3/10], Step [323/600], Loss: 0.6395, Train Acc:0.7910\n",
      "Epoch [3/10], Step [324/600], Loss: 0.6576, Train Acc:0.7910\n",
      "Epoch [3/10], Step [325/600], Loss: 0.7178, Train Acc:0.7909\n",
      "Epoch [3/10], Step [326/600], Loss: 0.5970, Train Acc:0.7910\n",
      "Epoch [3/10], Step [327/600], Loss: 0.5533, Train Acc:0.7911\n",
      "Epoch [3/10], Step [328/600], Loss: 0.7148, Train Acc:0.7910\n",
      "Epoch [3/10], Step [329/600], Loss: 0.7337, Train Acc:0.7909\n",
      "Epoch [3/10], Step [330/600], Loss: 0.6867, Train Acc:0.7908\n",
      "Epoch [3/10], Step [331/600], Loss: 0.6911, Train Acc:0.7908\n",
      "Epoch [3/10], Step [332/600], Loss: 0.5824, Train Acc:0.7908\n",
      "Epoch [3/10], Step [333/600], Loss: 0.7779, Train Acc:0.7908\n",
      "Epoch [3/10], Step [334/600], Loss: 0.7143, Train Acc:0.7906\n",
      "Epoch [3/10], Step [335/600], Loss: 0.6273, Train Acc:0.7906\n",
      "Epoch [3/10], Step [336/600], Loss: 0.6449, Train Acc:0.7906\n",
      "Epoch [3/10], Step [337/600], Loss: 0.7135, Train Acc:0.7905\n",
      "Epoch [3/10], Step [338/600], Loss: 0.6289, Train Acc:0.7906\n",
      "Epoch [3/10], Step [339/600], Loss: 0.7022, Train Acc:0.7905\n",
      "Epoch [3/10], Step [340/600], Loss: 0.8277, Train Acc:0.7905\n",
      "Epoch [3/10], Step [341/600], Loss: 0.6230, Train Acc:0.7906\n",
      "Epoch [3/10], Step [342/600], Loss: 0.6927, Train Acc:0.7905\n",
      "Epoch [3/10], Step [343/600], Loss: 0.5757, Train Acc:0.7907\n",
      "Epoch [3/10], Step [344/600], Loss: 0.6823, Train Acc:0.7908\n",
      "Epoch [3/10], Step [345/600], Loss: 0.6986, Train Acc:0.7909\n",
      "Epoch [3/10], Step [346/600], Loss: 0.5929, Train Acc:0.7910\n",
      "Epoch [3/10], Step [347/600], Loss: 0.6062, Train Acc:0.7912\n",
      "Epoch [3/10], Step [348/600], Loss: 0.6924, Train Acc:0.7910\n",
      "Epoch [3/10], Step [349/600], Loss: 0.6521, Train Acc:0.7911\n",
      "Epoch [3/10], Step [350/600], Loss: 0.6129, Train Acc:0.7912\n",
      "Epoch [3/10], Step [351/600], Loss: 0.5616, Train Acc:0.7914\n",
      "Epoch [3/10], Step [352/600], Loss: 0.7321, Train Acc:0.7912\n",
      "Epoch [3/10], Step [353/600], Loss: 0.6892, Train Acc:0.7912\n",
      "Epoch [3/10], Step [354/600], Loss: 0.5338, Train Acc:0.7914\n",
      "Epoch [3/10], Step [355/600], Loss: 0.6712, Train Acc:0.7913\n",
      "Epoch [3/10], Step [356/600], Loss: 0.6998, Train Acc:0.7912\n",
      "Epoch [3/10], Step [357/600], Loss: 0.7087, Train Acc:0.7910\n",
      "Epoch [3/10], Step [358/600], Loss: 0.6233, Train Acc:0.7911\n",
      "Epoch [3/10], Step [359/600], Loss: 0.5179, Train Acc:0.7913\n",
      "Epoch [3/10], Step [360/600], Loss: 0.7108, Train Acc:0.7913\n",
      "Epoch [3/10], Step [361/600], Loss: 0.6627, Train Acc:0.7914\n",
      "Epoch [3/10], Step [362/600], Loss: 0.6782, Train Acc:0.7914\n",
      "Epoch [3/10], Step [363/600], Loss: 0.5758, Train Acc:0.7914\n",
      "Epoch [3/10], Step [364/600], Loss: 0.6491, Train Acc:0.7913\n",
      "Epoch [3/10], Step [365/600], Loss: 0.4996, Train Acc:0.7915\n",
      "Epoch [3/10], Step [366/600], Loss: 0.7505, Train Acc:0.7914\n",
      "Epoch [3/10], Step [367/600], Loss: 0.5966, Train Acc:0.7915\n",
      "Epoch [3/10], Step [368/600], Loss: 0.4759, Train Acc:0.7918\n",
      "Epoch [3/10], Step [369/600], Loss: 0.5808, Train Acc:0.7919\n",
      "Epoch [3/10], Step [370/600], Loss: 0.6932, Train Acc:0.7919\n",
      "Epoch [3/10], Step [371/600], Loss: 0.5949, Train Acc:0.7920\n",
      "Epoch [3/10], Step [372/600], Loss: 0.6274, Train Acc:0.7920\n",
      "Epoch [3/10], Step [373/600], Loss: 0.6236, Train Acc:0.7921\n",
      "Epoch [3/10], Step [374/600], Loss: 0.6403, Train Acc:0.7920\n",
      "Epoch [3/10], Step [375/600], Loss: 0.7556, Train Acc:0.7921\n",
      "Epoch [3/10], Step [376/600], Loss: 0.7278, Train Acc:0.7920\n",
      "Epoch [3/10], Step [377/600], Loss: 0.6298, Train Acc:0.7921\n",
      "Epoch [3/10], Step [378/600], Loss: 0.6975, Train Acc:0.7920\n",
      "Epoch [3/10], Step [379/600], Loss: 0.6502, Train Acc:0.7920\n",
      "Epoch [3/10], Step [380/600], Loss: 0.6038, Train Acc:0.7921\n",
      "Epoch [3/10], Step [381/600], Loss: 0.6923, Train Acc:0.7921\n",
      "Epoch [3/10], Step [382/600], Loss: 0.6191, Train Acc:0.7922\n",
      "Epoch [3/10], Step [383/600], Loss: 0.6673, Train Acc:0.7922\n",
      "Epoch [3/10], Step [384/600], Loss: 0.6251, Train Acc:0.7924\n",
      "Epoch [3/10], Step [385/600], Loss: 0.6182, Train Acc:0.7925\n",
      "Epoch [3/10], Step [386/600], Loss: 0.6720, Train Acc:0.7924\n",
      "Epoch [3/10], Step [387/600], Loss: 0.6236, Train Acc:0.7924\n",
      "Epoch [3/10], Step [388/600], Loss: 0.5385, Train Acc:0.7925\n",
      "Epoch [3/10], Step [389/600], Loss: 0.5569, Train Acc:0.7925\n",
      "Epoch [3/10], Step [390/600], Loss: 0.5924, Train Acc:0.7927\n",
      "Epoch [3/10], Step [391/600], Loss: 0.6483, Train Acc:0.7926\n",
      "Epoch [3/10], Step [392/600], Loss: 0.8046, Train Acc:0.7924\n",
      "Epoch [3/10], Step [393/600], Loss: 0.6354, Train Acc:0.7925\n",
      "Epoch [3/10], Step [394/600], Loss: 0.6699, Train Acc:0.7925\n",
      "Epoch [3/10], Step [395/600], Loss: 0.6320, Train Acc:0.7925\n",
      "Epoch [3/10], Step [396/600], Loss: 0.5651, Train Acc:0.7925\n",
      "Epoch [3/10], Step [397/600], Loss: 0.5847, Train Acc:0.7925\n",
      "Epoch [3/10], Step [398/600], Loss: 0.6841, Train Acc:0.7925\n",
      "Epoch [3/10], Step [399/600], Loss: 0.6271, Train Acc:0.7925\n",
      "Epoch [3/10], Step [400/600], Loss: 0.5572, Train Acc:0.7925\n",
      "Epoch [3/10], Step [401/600], Loss: 0.7623, Train Acc:0.7925\n",
      "Epoch [3/10], Step [402/600], Loss: 0.7242, Train Acc:0.7925\n",
      "Epoch [3/10], Step [403/600], Loss: 0.7252, Train Acc:0.7925\n",
      "Epoch [3/10], Step [404/600], Loss: 0.6382, Train Acc:0.7925\n",
      "Epoch [3/10], Step [405/600], Loss: 0.6059, Train Acc:0.7925\n",
      "Epoch [3/10], Step [406/600], Loss: 0.7207, Train Acc:0.7924\n",
      "Epoch [3/10], Step [407/600], Loss: 0.5742, Train Acc:0.7926\n",
      "Epoch [3/10], Step [408/600], Loss: 0.6690, Train Acc:0.7925\n",
      "Epoch [3/10], Step [409/600], Loss: 0.6815, Train Acc:0.7924\n",
      "Epoch [3/10], Step [410/600], Loss: 0.6073, Train Acc:0.7924\n",
      "Epoch [3/10], Step [411/600], Loss: 0.5785, Train Acc:0.7925\n",
      "Epoch [3/10], Step [412/600], Loss: 0.5867, Train Acc:0.7926\n",
      "Epoch [3/10], Step [413/600], Loss: 0.7632, Train Acc:0.7924\n",
      "Epoch [3/10], Step [414/600], Loss: 0.6854, Train Acc:0.7924\n",
      "Epoch [3/10], Step [415/600], Loss: 0.6484, Train Acc:0.7925\n",
      "Epoch [3/10], Step [416/600], Loss: 0.6916, Train Acc:0.7924\n",
      "Epoch [3/10], Step [417/600], Loss: 0.7367, Train Acc:0.7922\n",
      "Epoch [3/10], Step [418/600], Loss: 0.6663, Train Acc:0.7921\n",
      "Epoch [3/10], Step [419/600], Loss: 0.5638, Train Acc:0.7923\n",
      "Epoch [3/10], Step [420/600], Loss: 0.7034, Train Acc:0.7922\n",
      "Epoch [3/10], Step [421/600], Loss: 0.6390, Train Acc:0.7922\n",
      "Epoch [3/10], Step [422/600], Loss: 0.8182, Train Acc:0.7920\n",
      "Epoch [3/10], Step [423/600], Loss: 0.6665, Train Acc:0.7920\n",
      "Epoch [3/10], Step [424/600], Loss: 0.6251, Train Acc:0.7921\n",
      "Epoch [3/10], Step [425/600], Loss: 0.5985, Train Acc:0.7922\n",
      "Epoch [3/10], Step [426/600], Loss: 0.7534, Train Acc:0.7921\n",
      "Epoch [3/10], Step [427/600], Loss: 0.5857, Train Acc:0.7921\n",
      "Epoch [3/10], Step [428/600], Loss: 0.5987, Train Acc:0.7921\n",
      "Epoch [3/10], Step [429/600], Loss: 0.5335, Train Acc:0.7922\n",
      "Epoch [3/10], Step [430/600], Loss: 0.6013, Train Acc:0.7923\n",
      "Epoch [3/10], Step [431/600], Loss: 0.7800, Train Acc:0.7922\n",
      "Epoch [3/10], Step [432/600], Loss: 0.5575, Train Acc:0.7923\n",
      "Epoch [3/10], Step [433/600], Loss: 0.6343, Train Acc:0.7923\n",
      "Epoch [3/10], Step [434/600], Loss: 0.5072, Train Acc:0.7923\n",
      "Epoch [3/10], Step [435/600], Loss: 0.5448, Train Acc:0.7924\n",
      "Epoch [3/10], Step [436/600], Loss: 0.6255, Train Acc:0.7924\n",
      "Epoch [3/10], Step [437/600], Loss: 0.5259, Train Acc:0.7925\n",
      "Epoch [3/10], Step [438/600], Loss: 0.6825, Train Acc:0.7924\n",
      "Epoch [3/10], Step [439/600], Loss: 0.6362, Train Acc:0.7924\n",
      "Epoch [3/10], Step [440/600], Loss: 0.7922, Train Acc:0.7923\n",
      "Epoch [3/10], Step [441/600], Loss: 0.7529, Train Acc:0.7922\n",
      "Epoch [3/10], Step [442/600], Loss: 0.6687, Train Acc:0.7922\n",
      "Epoch [3/10], Step [443/600], Loss: 0.5544, Train Acc:0.7922\n",
      "Epoch [3/10], Step [444/600], Loss: 0.6435, Train Acc:0.7922\n",
      "Epoch [3/10], Step [445/600], Loss: 0.5957, Train Acc:0.7922\n",
      "Epoch [3/10], Step [446/600], Loss: 0.5730, Train Acc:0.7922\n",
      "Epoch [3/10], Step [447/600], Loss: 0.6847, Train Acc:0.7922\n",
      "Epoch [3/10], Step [448/600], Loss: 0.5397, Train Acc:0.7923\n",
      "Epoch [3/10], Step [449/600], Loss: 0.4698, Train Acc:0.7925\n",
      "Epoch [3/10], Step [450/600], Loss: 0.6681, Train Acc:0.7925\n",
      "Epoch [3/10], Step [451/600], Loss: 0.6590, Train Acc:0.7927\n",
      "Epoch [3/10], Step [452/600], Loss: 0.7006, Train Acc:0.7925\n",
      "Epoch [3/10], Step [453/600], Loss: 0.5943, Train Acc:0.7926\n",
      "Epoch [3/10], Step [454/600], Loss: 0.6625, Train Acc:0.7925\n",
      "Epoch [3/10], Step [455/600], Loss: 0.5476, Train Acc:0.7925\n",
      "Epoch [3/10], Step [456/600], Loss: 0.7237, Train Acc:0.7925\n",
      "Epoch [3/10], Step [457/600], Loss: 0.7199, Train Acc:0.7923\n",
      "Epoch [3/10], Step [458/600], Loss: 0.6982, Train Acc:0.7924\n",
      "Epoch [3/10], Step [459/600], Loss: 0.6887, Train Acc:0.7923\n",
      "Epoch [3/10], Step [460/600], Loss: 0.6062, Train Acc:0.7925\n",
      "Epoch [3/10], Step [461/600], Loss: 0.5907, Train Acc:0.7924\n",
      "Epoch [3/10], Step [462/600], Loss: 0.7265, Train Acc:0.7923\n",
      "Epoch [3/10], Step [463/600], Loss: 0.7641, Train Acc:0.7923\n",
      "Epoch [3/10], Step [464/600], Loss: 0.6413, Train Acc:0.7923\n",
      "Epoch [3/10], Step [465/600], Loss: 0.7401, Train Acc:0.7923\n",
      "Epoch [3/10], Step [466/600], Loss: 0.6590, Train Acc:0.7923\n",
      "Epoch [3/10], Step [467/600], Loss: 0.6447, Train Acc:0.7923\n",
      "Epoch [3/10], Step [468/600], Loss: 0.5988, Train Acc:0.7923\n",
      "Epoch [3/10], Step [469/600], Loss: 0.6251, Train Acc:0.7923\n",
      "Epoch [3/10], Step [470/600], Loss: 0.7277, Train Acc:0.7922\n",
      "Epoch [3/10], Step [471/600], Loss: 0.6777, Train Acc:0.7922\n",
      "Epoch [3/10], Step [472/600], Loss: 0.8057, Train Acc:0.7920\n",
      "Epoch [3/10], Step [473/600], Loss: 0.5700, Train Acc:0.7920\n",
      "Epoch [3/10], Step [474/600], Loss: 0.6648, Train Acc:0.7920\n",
      "Epoch [3/10], Step [475/600], Loss: 0.5497, Train Acc:0.7919\n",
      "Epoch [3/10], Step [476/600], Loss: 0.6422, Train Acc:0.7920\n",
      "Epoch [3/10], Step [477/600], Loss: 0.7483, Train Acc:0.7921\n",
      "Epoch [3/10], Step [478/600], Loss: 0.6422, Train Acc:0.7921\n",
      "Epoch [3/10], Step [479/600], Loss: 0.7372, Train Acc:0.7921\n",
      "Epoch [3/10], Step [480/600], Loss: 0.6920, Train Acc:0.7919\n",
      "Epoch [3/10], Step [481/600], Loss: 0.7395, Train Acc:0.7919\n",
      "Epoch [3/10], Step [482/600], Loss: 0.6125, Train Acc:0.7919\n",
      "Epoch [3/10], Step [483/600], Loss: 0.6320, Train Acc:0.7919\n",
      "Epoch [3/10], Step [484/600], Loss: 0.7320, Train Acc:0.7918\n",
      "Epoch [3/10], Step [485/600], Loss: 0.6410, Train Acc:0.7919\n",
      "Epoch [3/10], Step [486/600], Loss: 0.6109, Train Acc:0.7920\n",
      "Epoch [3/10], Step [487/600], Loss: 0.6862, Train Acc:0.7920\n",
      "Epoch [3/10], Step [488/600], Loss: 0.6891, Train Acc:0.7920\n",
      "Epoch [3/10], Step [489/600], Loss: 0.7278, Train Acc:0.7919\n",
      "Epoch [3/10], Step [490/600], Loss: 0.7182, Train Acc:0.7919\n",
      "Epoch [3/10], Step [491/600], Loss: 0.7543, Train Acc:0.7919\n",
      "Epoch [3/10], Step [492/600], Loss: 0.6515, Train Acc:0.7919\n",
      "Epoch [3/10], Step [493/600], Loss: 0.6062, Train Acc:0.7919\n",
      "Epoch [3/10], Step [494/600], Loss: 0.6998, Train Acc:0.7920\n",
      "Epoch [3/10], Step [495/600], Loss: 0.6100, Train Acc:0.7920\n",
      "Epoch [3/10], Step [496/600], Loss: 0.5742, Train Acc:0.7921\n",
      "Epoch [3/10], Step [497/600], Loss: 0.5208, Train Acc:0.7923\n",
      "Epoch [3/10], Step [498/600], Loss: 0.6448, Train Acc:0.7922\n",
      "Epoch [3/10], Step [499/600], Loss: 0.7163, Train Acc:0.7922\n",
      "Epoch [3/10], Step [500/600], Loss: 0.7370, Train Acc:0.7921\n",
      "Epoch [3/10], Step [501/600], Loss: 0.6883, Train Acc:0.7919\n",
      "Epoch [3/10], Step [502/600], Loss: 0.6999, Train Acc:0.7919\n",
      "Epoch [3/10], Step [503/600], Loss: 0.5494, Train Acc:0.7919\n",
      "Epoch [3/10], Step [504/600], Loss: 0.6429, Train Acc:0.7919\n",
      "Epoch [3/10], Step [505/600], Loss: 0.7295, Train Acc:0.7919\n",
      "Epoch [3/10], Step [506/600], Loss: 0.6486, Train Acc:0.7919\n",
      "Epoch [3/10], Step [507/600], Loss: 0.6834, Train Acc:0.7919\n",
      "Epoch [3/10], Step [508/600], Loss: 0.5724, Train Acc:0.7919\n",
      "Epoch [3/10], Step [509/600], Loss: 0.5805, Train Acc:0.7920\n",
      "Epoch [3/10], Step [510/600], Loss: 0.6790, Train Acc:0.7919\n",
      "Epoch [3/10], Step [511/600], Loss: 0.6164, Train Acc:0.7920\n",
      "Epoch [3/10], Step [512/600], Loss: 0.5716, Train Acc:0.7920\n",
      "Epoch [3/10], Step [513/600], Loss: 0.4974, Train Acc:0.7922\n",
      "Epoch [3/10], Step [514/600], Loss: 0.7083, Train Acc:0.7921\n",
      "Epoch [3/10], Step [515/600], Loss: 0.6854, Train Acc:0.7920\n",
      "Epoch [3/10], Step [516/600], Loss: 0.5920, Train Acc:0.7921\n",
      "Epoch [3/10], Step [517/600], Loss: 0.7341, Train Acc:0.7920\n",
      "Epoch [3/10], Step [518/600], Loss: 0.6152, Train Acc:0.7921\n",
      "Epoch [3/10], Step [519/600], Loss: 0.6313, Train Acc:0.7921\n",
      "Epoch [3/10], Step [520/600], Loss: 0.7150, Train Acc:0.7921\n",
      "Epoch [3/10], Step [521/600], Loss: 0.7346, Train Acc:0.7920\n",
      "Epoch [3/10], Step [522/600], Loss: 0.8041, Train Acc:0.7919\n",
      "Epoch [3/10], Step [523/600], Loss: 0.5868, Train Acc:0.7920\n",
      "Epoch [3/10], Step [524/600], Loss: 0.6748, Train Acc:0.7919\n",
      "Epoch [3/10], Step [525/600], Loss: 0.6860, Train Acc:0.7919\n",
      "Epoch [3/10], Step [526/600], Loss: 0.6703, Train Acc:0.7919\n",
      "Epoch [3/10], Step [527/600], Loss: 0.5754, Train Acc:0.7920\n",
      "Epoch [3/10], Step [528/600], Loss: 0.5948, Train Acc:0.7920\n",
      "Epoch [3/10], Step [529/600], Loss: 0.5925, Train Acc:0.7920\n",
      "Epoch [3/10], Step [530/600], Loss: 0.6635, Train Acc:0.7920\n",
      "Epoch [3/10], Step [531/600], Loss: 0.6344, Train Acc:0.7920\n",
      "Epoch [3/10], Step [532/600], Loss: 0.6178, Train Acc:0.7921\n",
      "Epoch [3/10], Step [533/600], Loss: 0.5712, Train Acc:0.7921\n",
      "Epoch [3/10], Step [534/600], Loss: 0.6155, Train Acc:0.7922\n",
      "Epoch [3/10], Step [535/600], Loss: 0.7015, Train Acc:0.7920\n",
      "Epoch [3/10], Step [536/600], Loss: 0.5523, Train Acc:0.7921\n",
      "Epoch [3/10], Step [537/600], Loss: 0.6151, Train Acc:0.7921\n",
      "Epoch [3/10], Step [538/600], Loss: 0.6501, Train Acc:0.7921\n",
      "Epoch [3/10], Step [539/600], Loss: 0.7068, Train Acc:0.7921\n",
      "Epoch [3/10], Step [540/600], Loss: 0.6601, Train Acc:0.7920\n",
      "Epoch [3/10], Step [541/600], Loss: 0.6189, Train Acc:0.7921\n",
      "Epoch [3/10], Step [542/600], Loss: 0.6410, Train Acc:0.7921\n",
      "Epoch [3/10], Step [543/600], Loss: 0.6470, Train Acc:0.7922\n",
      "Epoch [3/10], Step [544/600], Loss: 0.6763, Train Acc:0.7920\n",
      "Epoch [3/10], Step [545/600], Loss: 0.6036, Train Acc:0.7921\n",
      "Epoch [3/10], Step [546/600], Loss: 0.6641, Train Acc:0.7922\n",
      "Epoch [3/10], Step [547/600], Loss: 0.7537, Train Acc:0.7921\n",
      "Epoch [3/10], Step [548/600], Loss: 0.5976, Train Acc:0.7921\n",
      "Epoch [3/10], Step [549/600], Loss: 0.6922, Train Acc:0.7920\n",
      "Epoch [3/10], Step [550/600], Loss: 0.6478, Train Acc:0.7920\n",
      "Epoch [3/10], Step [551/600], Loss: 0.7088, Train Acc:0.7920\n",
      "Epoch [3/10], Step [552/600], Loss: 0.5893, Train Acc:0.7920\n",
      "Epoch [3/10], Step [553/600], Loss: 0.6004, Train Acc:0.7921\n",
      "Epoch [3/10], Step [554/600], Loss: 0.6312, Train Acc:0.7920\n",
      "Epoch [3/10], Step [555/600], Loss: 0.6046, Train Acc:0.7920\n",
      "Epoch [3/10], Step [556/600], Loss: 0.7405, Train Acc:0.7919\n",
      "Epoch [3/10], Step [557/600], Loss: 0.5456, Train Acc:0.7920\n",
      "Epoch [3/10], Step [558/600], Loss: 0.7226, Train Acc:0.7920\n",
      "Epoch [3/10], Step [559/600], Loss: 0.7083, Train Acc:0.7919\n",
      "Epoch [3/10], Step [560/600], Loss: 0.6531, Train Acc:0.7919\n",
      "Epoch [3/10], Step [561/600], Loss: 0.6750, Train Acc:0.7919\n",
      "Epoch [3/10], Step [562/600], Loss: 0.6383, Train Acc:0.7919\n",
      "Epoch [3/10], Step [563/600], Loss: 0.5471, Train Acc:0.7918\n",
      "Epoch [3/10], Step [564/600], Loss: 0.6273, Train Acc:0.7918\n",
      "Epoch [3/10], Step [565/600], Loss: 0.5074, Train Acc:0.7919\n",
      "Epoch [3/10], Step [566/600], Loss: 0.5693, Train Acc:0.7919\n",
      "Epoch [3/10], Step [567/600], Loss: 0.5793, Train Acc:0.7919\n",
      "Epoch [3/10], Step [568/600], Loss: 0.5684, Train Acc:0.7919\n",
      "Epoch [3/10], Step [569/600], Loss: 0.6091, Train Acc:0.7920\n",
      "Epoch [3/10], Step [570/600], Loss: 0.6207, Train Acc:0.7920\n",
      "Epoch [3/10], Step [571/600], Loss: 0.5452, Train Acc:0.7920\n",
      "Epoch [3/10], Step [572/600], Loss: 0.7552, Train Acc:0.7919\n",
      "Epoch [3/10], Step [573/600], Loss: 0.6711, Train Acc:0.7919\n",
      "Epoch [3/10], Step [574/600], Loss: 0.6051, Train Acc:0.7919\n",
      "Epoch [3/10], Step [575/600], Loss: 0.6669, Train Acc:0.7918\n",
      "Epoch [3/10], Step [576/600], Loss: 0.6077, Train Acc:0.7919\n",
      "Epoch [3/10], Step [577/600], Loss: 0.5346, Train Acc:0.7920\n",
      "Epoch [3/10], Step [578/600], Loss: 0.6004, Train Acc:0.7920\n",
      "Epoch [3/10], Step [579/600], Loss: 0.7814, Train Acc:0.7919\n",
      "Epoch [3/10], Step [580/600], Loss: 0.5478, Train Acc:0.7920\n",
      "Epoch [3/10], Step [581/600], Loss: 0.6350, Train Acc:0.7919\n",
      "Epoch [3/10], Step [582/600], Loss: 0.5939, Train Acc:0.7918\n",
      "Epoch [3/10], Step [583/600], Loss: 0.6358, Train Acc:0.7919\n",
      "Epoch [3/10], Step [584/600], Loss: 0.6284, Train Acc:0.7919\n",
      "Epoch [3/10], Step [585/600], Loss: 0.5408, Train Acc:0.7920\n",
      "Epoch [3/10], Step [586/600], Loss: 0.5444, Train Acc:0.7921\n",
      "Epoch [3/10], Step [587/600], Loss: 0.5078, Train Acc:0.7922\n",
      "Epoch [3/10], Step [588/600], Loss: 0.6759, Train Acc:0.7921\n",
      "Epoch [3/10], Step [589/600], Loss: 0.8300, Train Acc:0.7921\n",
      "Epoch [3/10], Step [590/600], Loss: 0.4508, Train Acc:0.7923\n",
      "Epoch [3/10], Step [591/600], Loss: 0.5264, Train Acc:0.7923\n",
      "Epoch [3/10], Step [592/600], Loss: 0.7013, Train Acc:0.7924\n",
      "Epoch [3/10], Step [593/600], Loss: 0.6163, Train Acc:0.7923\n",
      "Epoch [3/10], Step [594/600], Loss: 0.6914, Train Acc:0.7923\n",
      "Epoch [3/10], Step [595/600], Loss: 0.5874, Train Acc:0.7923\n",
      "Epoch [3/10], Step [596/600], Loss: 0.6885, Train Acc:0.7922\n",
      "Epoch [3/10], Step [597/600], Loss: 0.6864, Train Acc:0.7922\n",
      "Epoch [3/10], Step [598/600], Loss: 0.6052, Train Acc:0.7922\n",
      "Epoch [3/10], Step [599/600], Loss: 0.6319, Train Acc:0.7922\n",
      "Epoch [3/10], Step [600/600], Loss: 0.6890, Train Acc:0.7923\n",
      "Epoch [4/10], Step [1/600], Loss: 0.6290, Train Acc:0.7600\n",
      "Epoch [4/10], Step [2/600], Loss: 0.6307, Train Acc:0.7750\n",
      "Epoch [4/10], Step [3/600], Loss: 0.5374, Train Acc:0.8133\n",
      "Epoch [4/10], Step [4/600], Loss: 0.6533, Train Acc:0.8000\n",
      "Epoch [4/10], Step [5/600], Loss: 0.5878, Train Acc:0.8080\n",
      "Epoch [4/10], Step [6/600], Loss: 0.6029, Train Acc:0.8100\n",
      "Epoch [4/10], Step [7/600], Loss: 0.6001, Train Acc:0.8071\n",
      "Epoch [4/10], Step [8/600], Loss: 0.6956, Train Acc:0.7987\n",
      "Epoch [4/10], Step [9/600], Loss: 0.6304, Train Acc:0.8033\n",
      "Epoch [4/10], Step [10/600], Loss: 0.6493, Train Acc:0.7990\n",
      "Epoch [4/10], Step [11/600], Loss: 0.7425, Train Acc:0.7909\n",
      "Epoch [4/10], Step [12/600], Loss: 0.6090, Train Acc:0.7925\n",
      "Epoch [4/10], Step [13/600], Loss: 0.6922, Train Acc:0.7915\n",
      "Epoch [4/10], Step [14/600], Loss: 0.5660, Train Acc:0.7950\n",
      "Epoch [4/10], Step [15/600], Loss: 0.6031, Train Acc:0.7960\n",
      "Epoch [4/10], Step [16/600], Loss: 0.5780, Train Acc:0.7975\n",
      "Epoch [4/10], Step [17/600], Loss: 0.6505, Train Acc:0.7982\n",
      "Epoch [4/10], Step [18/600], Loss: 0.6102, Train Acc:0.7978\n",
      "Epoch [4/10], Step [19/600], Loss: 0.7261, Train Acc:0.7979\n",
      "Epoch [4/10], Step [20/600], Loss: 0.7804, Train Acc:0.7935\n",
      "Epoch [4/10], Step [21/600], Loss: 0.5928, Train Acc:0.7943\n",
      "Epoch [4/10], Step [22/600], Loss: 0.7730, Train Acc:0.7936\n",
      "Epoch [4/10], Step [23/600], Loss: 0.7376, Train Acc:0.7930\n",
      "Epoch [4/10], Step [24/600], Loss: 0.7443, Train Acc:0.7908\n",
      "Epoch [4/10], Step [25/600], Loss: 0.7066, Train Acc:0.7908\n",
      "Epoch [4/10], Step [26/600], Loss: 0.6307, Train Acc:0.7923\n",
      "Epoch [4/10], Step [27/600], Loss: 0.6832, Train Acc:0.7933\n",
      "Epoch [4/10], Step [28/600], Loss: 0.7170, Train Acc:0.7925\n",
      "Epoch [4/10], Step [29/600], Loss: 0.6969, Train Acc:0.7928\n",
      "Epoch [4/10], Step [30/600], Loss: 0.6853, Train Acc:0.7913\n",
      "Epoch [4/10], Step [31/600], Loss: 0.6871, Train Acc:0.7903\n",
      "Epoch [4/10], Step [32/600], Loss: 0.6779, Train Acc:0.7903\n",
      "Epoch [4/10], Step [33/600], Loss: 0.6747, Train Acc:0.7882\n",
      "Epoch [4/10], Step [34/600], Loss: 0.6305, Train Acc:0.7888\n",
      "Epoch [4/10], Step [35/600], Loss: 0.6216, Train Acc:0.7883\n",
      "Epoch [4/10], Step [36/600], Loss: 0.6064, Train Acc:0.7892\n",
      "Epoch [4/10], Step [37/600], Loss: 0.5562, Train Acc:0.7914\n",
      "Epoch [4/10], Step [38/600], Loss: 0.7347, Train Acc:0.7903\n",
      "Epoch [4/10], Step [39/600], Loss: 0.4754, Train Acc:0.7926\n",
      "Epoch [4/10], Step [40/600], Loss: 0.5572, Train Acc:0.7935\n",
      "Epoch [4/10], Step [41/600], Loss: 0.7119, Train Acc:0.7937\n",
      "Epoch [4/10], Step [42/600], Loss: 0.6650, Train Acc:0.7919\n",
      "Epoch [4/10], Step [43/600], Loss: 0.5222, Train Acc:0.7933\n",
      "Epoch [4/10], Step [44/600], Loss: 0.5194, Train Acc:0.7955\n",
      "Epoch [4/10], Step [45/600], Loss: 0.5621, Train Acc:0.7953\n",
      "Epoch [4/10], Step [46/600], Loss: 0.6540, Train Acc:0.7963\n",
      "Epoch [4/10], Step [47/600], Loss: 0.6293, Train Acc:0.7966\n",
      "Epoch [4/10], Step [48/600], Loss: 0.6464, Train Acc:0.7963\n",
      "Epoch [4/10], Step [49/600], Loss: 0.6265, Train Acc:0.7967\n",
      "Epoch [4/10], Step [50/600], Loss: 0.6221, Train Acc:0.7958\n",
      "Epoch [4/10], Step [51/600], Loss: 0.5524, Train Acc:0.7963\n",
      "Epoch [4/10], Step [52/600], Loss: 0.6368, Train Acc:0.7969\n",
      "Epoch [4/10], Step [53/600], Loss: 0.5592, Train Acc:0.7975\n",
      "Epoch [4/10], Step [54/600], Loss: 0.6478, Train Acc:0.7978\n",
      "Epoch [4/10], Step [55/600], Loss: 0.6343, Train Acc:0.7975\n",
      "Epoch [4/10], Step [56/600], Loss: 0.5656, Train Acc:0.7979\n",
      "Epoch [4/10], Step [57/600], Loss: 0.6031, Train Acc:0.7979\n",
      "Epoch [4/10], Step [58/600], Loss: 0.5032, Train Acc:0.7991\n",
      "Epoch [4/10], Step [59/600], Loss: 0.5819, Train Acc:0.7995\n",
      "Epoch [4/10], Step [60/600], Loss: 0.4957, Train Acc:0.8002\n",
      "Epoch [4/10], Step [61/600], Loss: 0.6445, Train Acc:0.8002\n",
      "Epoch [4/10], Step [62/600], Loss: 0.5975, Train Acc:0.8013\n",
      "Epoch [4/10], Step [63/600], Loss: 0.6460, Train Acc:0.8008\n",
      "Epoch [4/10], Step [64/600], Loss: 0.5111, Train Acc:0.8016\n",
      "Epoch [4/10], Step [65/600], Loss: 0.6320, Train Acc:0.8012\n",
      "Epoch [4/10], Step [66/600], Loss: 0.7962, Train Acc:0.7997\n",
      "Epoch [4/10], Step [67/600], Loss: 0.5777, Train Acc:0.8003\n",
      "Epoch [4/10], Step [68/600], Loss: 0.5554, Train Acc:0.8015\n",
      "Epoch [4/10], Step [69/600], Loss: 0.6249, Train Acc:0.8013\n",
      "Epoch [4/10], Step [70/600], Loss: 0.6699, Train Acc:0.8009\n",
      "Epoch [4/10], Step [71/600], Loss: 0.5447, Train Acc:0.8013\n",
      "Epoch [4/10], Step [72/600], Loss: 0.6319, Train Acc:0.8019\n",
      "Epoch [4/10], Step [73/600], Loss: 0.6165, Train Acc:0.8016\n",
      "Epoch [4/10], Step [74/600], Loss: 0.5378, Train Acc:0.8019\n",
      "Epoch [4/10], Step [75/600], Loss: 0.6413, Train Acc:0.8017\n",
      "Epoch [4/10], Step [76/600], Loss: 0.7147, Train Acc:0.8013\n",
      "Epoch [4/10], Step [77/600], Loss: 0.6936, Train Acc:0.8004\n",
      "Epoch [4/10], Step [78/600], Loss: 0.6071, Train Acc:0.8006\n",
      "Epoch [4/10], Step [79/600], Loss: 0.6679, Train Acc:0.8005\n",
      "Epoch [4/10], Step [80/600], Loss: 0.4966, Train Acc:0.8013\n",
      "Epoch [4/10], Step [81/600], Loss: 0.5889, Train Acc:0.8019\n",
      "Epoch [4/10], Step [82/600], Loss: 0.6046, Train Acc:0.8021\n",
      "Epoch [4/10], Step [83/600], Loss: 0.7022, Train Acc:0.8012\n",
      "Epoch [4/10], Step [84/600], Loss: 0.7097, Train Acc:0.8011\n",
      "Epoch [4/10], Step [85/600], Loss: 0.6548, Train Acc:0.8001\n",
      "Epoch [4/10], Step [86/600], Loss: 0.6045, Train Acc:0.8005\n",
      "Epoch [4/10], Step [87/600], Loss: 0.6920, Train Acc:0.8001\n",
      "Epoch [4/10], Step [88/600], Loss: 0.5692, Train Acc:0.8008\n",
      "Epoch [4/10], Step [89/600], Loss: 0.8478, Train Acc:0.7998\n",
      "Epoch [4/10], Step [90/600], Loss: 0.5847, Train Acc:0.8002\n",
      "Epoch [4/10], Step [91/600], Loss: 0.5945, Train Acc:0.8001\n",
      "Epoch [4/10], Step [92/600], Loss: 0.6359, Train Acc:0.8000\n",
      "Epoch [4/10], Step [93/600], Loss: 0.6955, Train Acc:0.8001\n",
      "Epoch [4/10], Step [94/600], Loss: 0.8448, Train Acc:0.7990\n",
      "Epoch [4/10], Step [95/600], Loss: 0.5481, Train Acc:0.7995\n",
      "Epoch [4/10], Step [96/600], Loss: 0.7592, Train Acc:0.7990\n",
      "Epoch [4/10], Step [97/600], Loss: 0.5609, Train Acc:0.7990\n",
      "Epoch [4/10], Step [98/600], Loss: 0.6020, Train Acc:0.7990\n",
      "Epoch [4/10], Step [99/600], Loss: 0.6799, Train Acc:0.7993\n",
      "Epoch [4/10], Step [100/600], Loss: 0.5030, Train Acc:0.7999\n",
      "Epoch [4/10], Step [101/600], Loss: 0.6604, Train Acc:0.7998\n",
      "Epoch [4/10], Step [102/600], Loss: 0.5697, Train Acc:0.8004\n",
      "Epoch [4/10], Step [103/600], Loss: 0.6348, Train Acc:0.8006\n",
      "Epoch [4/10], Step [104/600], Loss: 0.5297, Train Acc:0.8009\n",
      "Epoch [4/10], Step [105/600], Loss: 0.6838, Train Acc:0.8005\n",
      "Epoch [4/10], Step [106/600], Loss: 0.7779, Train Acc:0.8000\n",
      "Epoch [4/10], Step [107/600], Loss: 0.7129, Train Acc:0.7993\n",
      "Epoch [4/10], Step [108/600], Loss: 0.5055, Train Acc:0.8000\n",
      "Epoch [4/10], Step [109/600], Loss: 0.5986, Train Acc:0.8000\n",
      "Epoch [4/10], Step [110/600], Loss: 0.7777, Train Acc:0.7995\n",
      "Epoch [4/10], Step [111/600], Loss: 0.6501, Train Acc:0.7996\n",
      "Epoch [4/10], Step [112/600], Loss: 0.6268, Train Acc:0.7997\n",
      "Epoch [4/10], Step [113/600], Loss: 0.7449, Train Acc:0.7995\n",
      "Epoch [4/10], Step [114/600], Loss: 0.5756, Train Acc:0.7997\n",
      "Epoch [4/10], Step [115/600], Loss: 0.5688, Train Acc:0.7998\n",
      "Epoch [4/10], Step [116/600], Loss: 0.6002, Train Acc:0.7997\n",
      "Epoch [4/10], Step [117/600], Loss: 0.6232, Train Acc:0.7997\n",
      "Epoch [4/10], Step [118/600], Loss: 0.6174, Train Acc:0.7997\n",
      "Epoch [4/10], Step [119/600], Loss: 0.5278, Train Acc:0.8001\n",
      "Epoch [4/10], Step [120/600], Loss: 0.6392, Train Acc:0.8005\n",
      "Epoch [4/10], Step [121/600], Loss: 0.6285, Train Acc:0.8003\n",
      "Epoch [4/10], Step [122/600], Loss: 0.5727, Train Acc:0.8007\n",
      "Epoch [4/10], Step [123/600], Loss: 0.6828, Train Acc:0.8007\n",
      "Epoch [4/10], Step [124/600], Loss: 0.6453, Train Acc:0.8007\n",
      "Epoch [4/10], Step [125/600], Loss: 0.6444, Train Acc:0.8008\n",
      "Epoch [4/10], Step [126/600], Loss: 0.4964, Train Acc:0.8012\n",
      "Epoch [4/10], Step [127/600], Loss: 0.7083, Train Acc:0.8008\n",
      "Epoch [4/10], Step [128/600], Loss: 0.5702, Train Acc:0.8009\n",
      "Epoch [4/10], Step [129/600], Loss: 0.7401, Train Acc:0.8006\n",
      "Epoch [4/10], Step [130/600], Loss: 0.5157, Train Acc:0.8010\n",
      "Epoch [4/10], Step [131/600], Loss: 0.5510, Train Acc:0.8009\n",
      "Epoch [4/10], Step [132/600], Loss: 0.5259, Train Acc:0.8012\n",
      "Epoch [4/10], Step [133/600], Loss: 0.5459, Train Acc:0.8015\n",
      "Epoch [4/10], Step [134/600], Loss: 0.6234, Train Acc:0.8016\n",
      "Epoch [4/10], Step [135/600], Loss: 0.6138, Train Acc:0.8012\n",
      "Epoch [4/10], Step [136/600], Loss: 0.5606, Train Acc:0.8013\n",
      "Epoch [4/10], Step [137/600], Loss: 0.7113, Train Acc:0.8013\n",
      "Epoch [4/10], Step [138/600], Loss: 0.4568, Train Acc:0.8017\n",
      "Epoch [4/10], Step [139/600], Loss: 0.6139, Train Acc:0.8016\n",
      "Epoch [4/10], Step [140/600], Loss: 0.5069, Train Acc:0.8018\n",
      "Epoch [4/10], Step [141/600], Loss: 0.8774, Train Acc:0.8011\n",
      "Epoch [4/10], Step [142/600], Loss: 0.6042, Train Acc:0.8012\n",
      "Epoch [4/10], Step [143/600], Loss: 0.6599, Train Acc:0.8012\n",
      "Epoch [4/10], Step [144/600], Loss: 0.5706, Train Acc:0.8015\n",
      "Epoch [4/10], Step [145/600], Loss: 0.6130, Train Acc:0.8015\n",
      "Epoch [4/10], Step [146/600], Loss: 0.6790, Train Acc:0.8014\n",
      "Epoch [4/10], Step [147/600], Loss: 0.5462, Train Acc:0.8014\n",
      "Epoch [4/10], Step [148/600], Loss: 0.5826, Train Acc:0.8016\n",
      "Epoch [4/10], Step [149/600], Loss: 0.6022, Train Acc:0.8013\n",
      "Epoch [4/10], Step [150/600], Loss: 0.5919, Train Acc:0.8017\n",
      "Epoch [4/10], Step [151/600], Loss: 0.6785, Train Acc:0.8018\n",
      "Epoch [4/10], Step [152/600], Loss: 0.6951, Train Acc:0.8018\n",
      "Epoch [4/10], Step [153/600], Loss: 0.6131, Train Acc:0.8018\n",
      "Epoch [4/10], Step [154/600], Loss: 0.5925, Train Acc:0.8019\n",
      "Epoch [4/10], Step [155/600], Loss: 0.6464, Train Acc:0.8022\n",
      "Epoch [4/10], Step [156/600], Loss: 0.6448, Train Acc:0.8022\n",
      "Epoch [4/10], Step [157/600], Loss: 0.6284, Train Acc:0.8023\n",
      "Epoch [4/10], Step [158/600], Loss: 0.5597, Train Acc:0.8022\n",
      "Epoch [4/10], Step [159/600], Loss: 0.6701, Train Acc:0.8019\n",
      "Epoch [4/10], Step [160/600], Loss: 0.5391, Train Acc:0.8018\n",
      "Epoch [4/10], Step [161/600], Loss: 0.6890, Train Acc:0.8017\n",
      "Epoch [4/10], Step [162/600], Loss: 0.6321, Train Acc:0.8015\n",
      "Epoch [4/10], Step [163/600], Loss: 0.5127, Train Acc:0.8018\n",
      "Epoch [4/10], Step [164/600], Loss: 0.5603, Train Acc:0.8021\n",
      "Epoch [4/10], Step [165/600], Loss: 0.5616, Train Acc:0.8024\n",
      "Epoch [4/10], Step [166/600], Loss: 0.6137, Train Acc:0.8027\n",
      "Epoch [4/10], Step [167/600], Loss: 0.6071, Train Acc:0.8028\n",
      "Epoch [4/10], Step [168/600], Loss: 0.7280, Train Acc:0.8027\n",
      "Epoch [4/10], Step [169/600], Loss: 0.6168, Train Acc:0.8028\n",
      "Epoch [4/10], Step [170/600], Loss: 0.5838, Train Acc:0.8026\n",
      "Epoch [4/10], Step [171/600], Loss: 0.7092, Train Acc:0.8026\n",
      "Epoch [4/10], Step [172/600], Loss: 0.6158, Train Acc:0.8027\n",
      "Epoch [4/10], Step [173/600], Loss: 0.6154, Train Acc:0.8027\n",
      "Epoch [4/10], Step [174/600], Loss: 0.5957, Train Acc:0.8028\n",
      "Epoch [4/10], Step [175/600], Loss: 0.7461, Train Acc:0.8023\n",
      "Epoch [4/10], Step [176/600], Loss: 0.6254, Train Acc:0.8022\n",
      "Epoch [4/10], Step [177/600], Loss: 0.7062, Train Acc:0.8021\n",
      "Epoch [4/10], Step [178/600], Loss: 0.6256, Train Acc:0.8022\n",
      "Epoch [4/10], Step [179/600], Loss: 0.5407, Train Acc:0.8023\n",
      "Epoch [4/10], Step [180/600], Loss: 0.6118, Train Acc:0.8024\n",
      "Epoch [4/10], Step [181/600], Loss: 0.5431, Train Acc:0.8025\n",
      "Epoch [4/10], Step [182/600], Loss: 0.6317, Train Acc:0.8023\n",
      "Epoch [4/10], Step [183/600], Loss: 0.5883, Train Acc:0.8025\n",
      "Epoch [4/10], Step [184/600], Loss: 0.6571, Train Acc:0.8024\n",
      "Epoch [4/10], Step [185/600], Loss: 0.5964, Train Acc:0.8025\n",
      "Epoch [4/10], Step [186/600], Loss: 0.6272, Train Acc:0.8026\n",
      "Epoch [4/10], Step [187/600], Loss: 0.6722, Train Acc:0.8026\n",
      "Epoch [4/10], Step [188/600], Loss: 0.6419, Train Acc:0.8024\n",
      "Epoch [4/10], Step [189/600], Loss: 0.5788, Train Acc:0.8024\n",
      "Epoch [4/10], Step [190/600], Loss: 0.6334, Train Acc:0.8024\n",
      "Epoch [4/10], Step [191/600], Loss: 0.6496, Train Acc:0.8024\n",
      "Epoch [4/10], Step [192/600], Loss: 0.6081, Train Acc:0.8025\n",
      "Epoch [4/10], Step [193/600], Loss: 0.6096, Train Acc:0.8024\n",
      "Epoch [4/10], Step [194/600], Loss: 0.5891, Train Acc:0.8026\n",
      "Epoch [4/10], Step [195/600], Loss: 0.6922, Train Acc:0.8025\n",
      "Epoch [4/10], Step [196/600], Loss: 0.7060, Train Acc:0.8022\n",
      "Epoch [4/10], Step [197/600], Loss: 0.7190, Train Acc:0.8023\n",
      "Epoch [4/10], Step [198/600], Loss: 0.5712, Train Acc:0.8023\n",
      "Epoch [4/10], Step [199/600], Loss: 0.4867, Train Acc:0.8027\n",
      "Epoch [4/10], Step [200/600], Loss: 0.5789, Train Acc:0.8027\n",
      "Epoch [4/10], Step [201/600], Loss: 0.6273, Train Acc:0.8029\n",
      "Epoch [4/10], Step [202/600], Loss: 0.5074, Train Acc:0.8032\n",
      "Epoch [4/10], Step [203/600], Loss: 0.7552, Train Acc:0.8031\n",
      "Epoch [4/10], Step [204/600], Loss: 0.4841, Train Acc:0.8031\n",
      "Epoch [4/10], Step [205/600], Loss: 0.5999, Train Acc:0.8030\n",
      "Epoch [4/10], Step [206/600], Loss: 0.5494, Train Acc:0.8033\n",
      "Epoch [4/10], Step [207/600], Loss: 0.5937, Train Acc:0.8033\n",
      "Epoch [4/10], Step [208/600], Loss: 0.6889, Train Acc:0.8033\n",
      "Epoch [4/10], Step [209/600], Loss: 0.6388, Train Acc:0.8033\n",
      "Epoch [4/10], Step [210/600], Loss: 0.6257, Train Acc:0.8029\n",
      "Epoch [4/10], Step [211/600], Loss: 0.5335, Train Acc:0.8033\n",
      "Epoch [4/10], Step [212/600], Loss: 0.5768, Train Acc:0.8032\n",
      "Epoch [4/10], Step [213/600], Loss: 0.6001, Train Acc:0.8033\n",
      "Epoch [4/10], Step [214/600], Loss: 0.5676, Train Acc:0.8035\n",
      "Epoch [4/10], Step [215/600], Loss: 0.6346, Train Acc:0.8033\n",
      "Epoch [4/10], Step [216/600], Loss: 0.6377, Train Acc:0.8031\n",
      "Epoch [4/10], Step [217/600], Loss: 0.5203, Train Acc:0.8032\n",
      "Epoch [4/10], Step [218/600], Loss: 0.5822, Train Acc:0.8032\n",
      "Epoch [4/10], Step [219/600], Loss: 0.6089, Train Acc:0.8032\n",
      "Epoch [4/10], Step [220/600], Loss: 0.7842, Train Acc:0.8030\n",
      "Epoch [4/10], Step [221/600], Loss: 0.5457, Train Acc:0.8032\n",
      "Epoch [4/10], Step [222/600], Loss: 0.6607, Train Acc:0.8031\n",
      "Epoch [4/10], Step [223/600], Loss: 0.6548, Train Acc:0.8030\n",
      "Epoch [4/10], Step [224/600], Loss: 0.6768, Train Acc:0.8028\n",
      "Epoch [4/10], Step [225/600], Loss: 0.6445, Train Acc:0.8028\n",
      "Epoch [4/10], Step [226/600], Loss: 0.6989, Train Acc:0.8027\n",
      "Epoch [4/10], Step [227/600], Loss: 0.7067, Train Acc:0.8027\n",
      "Epoch [4/10], Step [228/600], Loss: 0.5044, Train Acc:0.8032\n",
      "Epoch [4/10], Step [229/600], Loss: 0.5933, Train Acc:0.8034\n",
      "Epoch [4/10], Step [230/600], Loss: 0.5456, Train Acc:0.8033\n",
      "Epoch [4/10], Step [231/600], Loss: 0.6880, Train Acc:0.8031\n",
      "Epoch [4/10], Step [232/600], Loss: 0.5876, Train Acc:0.8032\n",
      "Epoch [4/10], Step [233/600], Loss: 0.6147, Train Acc:0.8030\n",
      "Epoch [4/10], Step [234/600], Loss: 0.6507, Train Acc:0.8029\n",
      "Epoch [4/10], Step [235/600], Loss: 0.6612, Train Acc:0.8029\n",
      "Epoch [4/10], Step [236/600], Loss: 0.7247, Train Acc:0.8028\n",
      "Epoch [4/10], Step [237/600], Loss: 0.6917, Train Acc:0.8028\n",
      "Epoch [4/10], Step [238/600], Loss: 0.6358, Train Acc:0.8026\n",
      "Epoch [4/10], Step [239/600], Loss: 0.8655, Train Acc:0.8020\n",
      "Epoch [4/10], Step [240/600], Loss: 0.6277, Train Acc:0.8019\n",
      "Epoch [4/10], Step [241/600], Loss: 0.6025, Train Acc:0.8017\n",
      "Epoch [4/10], Step [242/600], Loss: 0.5572, Train Acc:0.8016\n",
      "Epoch [4/10], Step [243/600], Loss: 0.6979, Train Acc:0.8015\n",
      "Epoch [4/10], Step [244/600], Loss: 0.6389, Train Acc:0.8014\n",
      "Epoch [4/10], Step [245/600], Loss: 0.7005, Train Acc:0.8013\n",
      "Epoch [4/10], Step [246/600], Loss: 0.6386, Train Acc:0.8013\n",
      "Epoch [4/10], Step [247/600], Loss: 0.7457, Train Acc:0.8011\n",
      "Epoch [4/10], Step [248/600], Loss: 0.5316, Train Acc:0.8012\n",
      "Epoch [4/10], Step [249/600], Loss: 0.5436, Train Acc:0.8011\n",
      "Epoch [4/10], Step [250/600], Loss: 0.6241, Train Acc:0.8012\n",
      "Epoch [4/10], Step [251/600], Loss: 0.5340, Train Acc:0.8014\n",
      "Epoch [4/10], Step [252/600], Loss: 0.7627, Train Acc:0.8013\n",
      "Epoch [4/10], Step [253/600], Loss: 0.5913, Train Acc:0.8013\n",
      "Epoch [4/10], Step [254/600], Loss: 0.5342, Train Acc:0.8013\n",
      "Epoch [4/10], Step [255/600], Loss: 0.6838, Train Acc:0.8011\n",
      "Epoch [4/10], Step [256/600], Loss: 0.6225, Train Acc:0.8013\n",
      "Epoch [4/10], Step [257/600], Loss: 0.6582, Train Acc:0.8012\n",
      "Epoch [4/10], Step [258/600], Loss: 0.5845, Train Acc:0.8013\n",
      "Epoch [4/10], Step [259/600], Loss: 0.6802, Train Acc:0.8013\n",
      "Epoch [4/10], Step [260/600], Loss: 0.5438, Train Acc:0.8014\n",
      "Epoch [4/10], Step [261/600], Loss: 0.6573, Train Acc:0.8013\n",
      "Epoch [4/10], Step [262/600], Loss: 0.6048, Train Acc:0.8013\n",
      "Epoch [4/10], Step [263/600], Loss: 0.6316, Train Acc:0.8013\n",
      "Epoch [4/10], Step [264/600], Loss: 0.5812, Train Acc:0.8013\n",
      "Epoch [4/10], Step [265/600], Loss: 0.5950, Train Acc:0.8013\n",
      "Epoch [4/10], Step [266/600], Loss: 0.6080, Train Acc:0.8012\n",
      "Epoch [4/10], Step [267/600], Loss: 0.5460, Train Acc:0.8014\n",
      "Epoch [4/10], Step [268/600], Loss: 0.5662, Train Acc:0.8013\n",
      "Epoch [4/10], Step [269/600], Loss: 0.7153, Train Acc:0.8013\n",
      "Epoch [4/10], Step [270/600], Loss: 0.6704, Train Acc:0.8011\n",
      "Epoch [4/10], Step [271/600], Loss: 0.7440, Train Acc:0.8012\n",
      "Epoch [4/10], Step [272/600], Loss: 0.6235, Train Acc:0.8011\n",
      "Epoch [4/10], Step [273/600], Loss: 0.4832, Train Acc:0.8014\n",
      "Epoch [4/10], Step [274/600], Loss: 0.4987, Train Acc:0.8015\n",
      "Epoch [4/10], Step [275/600], Loss: 0.5732, Train Acc:0.8015\n",
      "Epoch [4/10], Step [276/600], Loss: 0.6290, Train Acc:0.8014\n",
      "Epoch [4/10], Step [277/600], Loss: 0.5117, Train Acc:0.8016\n",
      "Epoch [4/10], Step [278/600], Loss: 0.5252, Train Acc:0.8017\n",
      "Epoch [4/10], Step [279/600], Loss: 0.6118, Train Acc:0.8016\n",
      "Epoch [4/10], Step [280/600], Loss: 0.6276, Train Acc:0.8015\n",
      "Epoch [4/10], Step [281/600], Loss: 0.6662, Train Acc:0.8013\n",
      "Epoch [4/10], Step [282/600], Loss: 0.5907, Train Acc:0.8012\n",
      "Epoch [4/10], Step [283/600], Loss: 0.6083, Train Acc:0.8014\n",
      "Epoch [4/10], Step [284/600], Loss: 0.7556, Train Acc:0.8012\n",
      "Epoch [4/10], Step [285/600], Loss: 0.6037, Train Acc:0.8012\n",
      "Epoch [4/10], Step [286/600], Loss: 0.4657, Train Acc:0.8016\n",
      "Epoch [4/10], Step [287/600], Loss: 0.4807, Train Acc:0.8018\n",
      "Epoch [4/10], Step [288/600], Loss: 0.6632, Train Acc:0.8018\n",
      "Epoch [4/10], Step [289/600], Loss: 0.5292, Train Acc:0.8021\n",
      "Epoch [4/10], Step [290/600], Loss: 0.5345, Train Acc:0.8023\n",
      "Epoch [4/10], Step [291/600], Loss: 0.6263, Train Acc:0.8024\n",
      "Epoch [4/10], Step [292/600], Loss: 0.6990, Train Acc:0.8023\n",
      "Epoch [4/10], Step [293/600], Loss: 0.7657, Train Acc:0.8020\n",
      "Epoch [4/10], Step [294/600], Loss: 0.5211, Train Acc:0.8023\n",
      "Epoch [4/10], Step [295/600], Loss: 0.5917, Train Acc:0.8023\n",
      "Epoch [4/10], Step [296/600], Loss: 0.5293, Train Acc:0.8026\n",
      "Epoch [4/10], Step [297/600], Loss: 0.6670, Train Acc:0.8024\n",
      "Epoch [4/10], Step [298/600], Loss: 0.5639, Train Acc:0.8024\n",
      "Epoch [4/10], Step [299/600], Loss: 0.7692, Train Acc:0.8022\n",
      "Epoch [4/10], Step [300/600], Loss: 0.6266, Train Acc:0.8021\n",
      "Epoch [4/10], Step [301/600], Loss: 0.6970, Train Acc:0.8019\n",
      "Epoch [4/10], Step [302/600], Loss: 0.6628, Train Acc:0.8017\n",
      "Epoch [4/10], Step [303/600], Loss: 0.5188, Train Acc:0.8019\n",
      "Epoch [4/10], Step [304/600], Loss: 0.5422, Train Acc:0.8021\n",
      "Epoch [4/10], Step [305/600], Loss: 0.6255, Train Acc:0.8021\n",
      "Epoch [4/10], Step [306/600], Loss: 0.5966, Train Acc:0.8021\n",
      "Epoch [4/10], Step [307/600], Loss: 0.7327, Train Acc:0.8019\n",
      "Epoch [4/10], Step [308/600], Loss: 0.6749, Train Acc:0.8018\n",
      "Epoch [4/10], Step [309/600], Loss: 0.5752, Train Acc:0.8019\n",
      "Epoch [4/10], Step [310/600], Loss: 0.6396, Train Acc:0.8021\n",
      "Epoch [4/10], Step [311/600], Loss: 0.4509, Train Acc:0.8023\n",
      "Epoch [4/10], Step [312/600], Loss: 0.7549, Train Acc:0.8022\n",
      "Epoch [4/10], Step [313/600], Loss: 0.8122, Train Acc:0.8019\n",
      "Epoch [4/10], Step [314/600], Loss: 0.6943, Train Acc:0.8018\n",
      "Epoch [4/10], Step [315/600], Loss: 0.6848, Train Acc:0.8016\n",
      "Epoch [4/10], Step [316/600], Loss: 0.6040, Train Acc:0.8017\n",
      "Epoch [4/10], Step [317/600], Loss: 0.6649, Train Acc:0.8015\n",
      "Epoch [4/10], Step [318/600], Loss: 0.6289, Train Acc:0.8016\n",
      "Epoch [4/10], Step [319/600], Loss: 0.6041, Train Acc:0.8018\n",
      "Epoch [4/10], Step [320/600], Loss: 0.6113, Train Acc:0.8018\n",
      "Epoch [4/10], Step [321/600], Loss: 0.5423, Train Acc:0.8020\n",
      "Epoch [4/10], Step [322/600], Loss: 0.7064, Train Acc:0.8019\n",
      "Epoch [4/10], Step [323/600], Loss: 0.5547, Train Acc:0.8019\n",
      "Epoch [4/10], Step [324/600], Loss: 0.5815, Train Acc:0.8020\n",
      "Epoch [4/10], Step [325/600], Loss: 0.6237, Train Acc:0.8020\n",
      "Epoch [4/10], Step [326/600], Loss: 0.6166, Train Acc:0.8018\n",
      "Epoch [4/10], Step [327/600], Loss: 0.5144, Train Acc:0.8020\n",
      "Epoch [4/10], Step [328/600], Loss: 0.6489, Train Acc:0.8020\n",
      "Epoch [4/10], Step [329/600], Loss: 0.6696, Train Acc:0.8019\n",
      "Epoch [4/10], Step [330/600], Loss: 0.6171, Train Acc:0.8020\n",
      "Epoch [4/10], Step [331/600], Loss: 0.5532, Train Acc:0.8020\n",
      "Epoch [4/10], Step [332/600], Loss: 0.6438, Train Acc:0.8021\n",
      "Epoch [4/10], Step [333/600], Loss: 0.7129, Train Acc:0.8021\n",
      "Epoch [4/10], Step [334/600], Loss: 0.5388, Train Acc:0.8022\n",
      "Epoch [4/10], Step [335/600], Loss: 0.5800, Train Acc:0.8023\n",
      "Epoch [4/10], Step [336/600], Loss: 0.5407, Train Acc:0.8024\n",
      "Epoch [4/10], Step [337/600], Loss: 0.5903, Train Acc:0.8025\n",
      "Epoch [4/10], Step [338/600], Loss: 0.5122, Train Acc:0.8026\n",
      "Epoch [4/10], Step [339/600], Loss: 0.6887, Train Acc:0.8025\n",
      "Epoch [4/10], Step [340/600], Loss: 0.5711, Train Acc:0.8026\n",
      "Epoch [4/10], Step [341/600], Loss: 0.5342, Train Acc:0.8028\n",
      "Epoch [4/10], Step [342/600], Loss: 0.6680, Train Acc:0.8027\n",
      "Epoch [4/10], Step [343/600], Loss: 0.5621, Train Acc:0.8028\n",
      "Epoch [4/10], Step [344/600], Loss: 0.6580, Train Acc:0.8026\n",
      "Epoch [4/10], Step [345/600], Loss: 0.5401, Train Acc:0.8026\n",
      "Epoch [4/10], Step [346/600], Loss: 0.6284, Train Acc:0.8025\n",
      "Epoch [4/10], Step [347/600], Loss: 0.6153, Train Acc:0.8024\n",
      "Epoch [4/10], Step [348/600], Loss: 0.7836, Train Acc:0.8023\n",
      "Epoch [4/10], Step [349/600], Loss: 0.5271, Train Acc:0.8024\n",
      "Epoch [4/10], Step [350/600], Loss: 0.7419, Train Acc:0.8023\n",
      "Epoch [4/10], Step [351/600], Loss: 0.7028, Train Acc:0.8021\n",
      "Epoch [4/10], Step [352/600], Loss: 0.6165, Train Acc:0.8022\n",
      "Epoch [4/10], Step [353/600], Loss: 0.6901, Train Acc:0.8021\n",
      "Epoch [4/10], Step [354/600], Loss: 0.5597, Train Acc:0.8022\n",
      "Epoch [4/10], Step [355/600], Loss: 0.7139, Train Acc:0.8019\n",
      "Epoch [4/10], Step [356/600], Loss: 0.4958, Train Acc:0.8021\n",
      "Epoch [4/10], Step [357/600], Loss: 0.4922, Train Acc:0.8023\n",
      "Epoch [4/10], Step [358/600], Loss: 0.5759, Train Acc:0.8023\n",
      "Epoch [4/10], Step [359/600], Loss: 0.5875, Train Acc:0.8023\n",
      "Epoch [4/10], Step [360/600], Loss: 0.5517, Train Acc:0.8024\n",
      "Epoch [4/10], Step [361/600], Loss: 0.7144, Train Acc:0.8022\n",
      "Epoch [4/10], Step [362/600], Loss: 0.7597, Train Acc:0.8020\n",
      "Epoch [4/10], Step [363/600], Loss: 0.6656, Train Acc:0.8020\n",
      "Epoch [4/10], Step [364/600], Loss: 0.6317, Train Acc:0.8021\n",
      "Epoch [4/10], Step [365/600], Loss: 0.5280, Train Acc:0.8021\n",
      "Epoch [4/10], Step [366/600], Loss: 0.5684, Train Acc:0.8022\n",
      "Epoch [4/10], Step [367/600], Loss: 0.5943, Train Acc:0.8022\n",
      "Epoch [4/10], Step [368/600], Loss: 0.5521, Train Acc:0.8022\n",
      "Epoch [4/10], Step [369/600], Loss: 0.6132, Train Acc:0.8022\n",
      "Epoch [4/10], Step [370/600], Loss: 0.6262, Train Acc:0.8022\n",
      "Epoch [4/10], Step [371/600], Loss: 0.6761, Train Acc:0.8023\n",
      "Epoch [4/10], Step [372/600], Loss: 0.6096, Train Acc:0.8023\n",
      "Epoch [4/10], Step [373/600], Loss: 0.5671, Train Acc:0.8023\n",
      "Epoch [4/10], Step [374/600], Loss: 0.6812, Train Acc:0.8023\n",
      "Epoch [4/10], Step [375/600], Loss: 0.7239, Train Acc:0.8022\n",
      "Epoch [4/10], Step [376/600], Loss: 0.5694, Train Acc:0.8023\n",
      "Epoch [4/10], Step [377/600], Loss: 0.5700, Train Acc:0.8023\n",
      "Epoch [4/10], Step [378/600], Loss: 0.5477, Train Acc:0.8024\n",
      "Epoch [4/10], Step [379/600], Loss: 0.4931, Train Acc:0.8025\n",
      "Epoch [4/10], Step [380/600], Loss: 0.5152, Train Acc:0.8026\n",
      "Epoch [4/10], Step [381/600], Loss: 0.5583, Train Acc:0.8025\n",
      "Epoch [4/10], Step [382/600], Loss: 0.5803, Train Acc:0.8025\n",
      "Epoch [4/10], Step [383/600], Loss: 0.6273, Train Acc:0.8025\n",
      "Epoch [4/10], Step [384/600], Loss: 0.6249, Train Acc:0.8025\n",
      "Epoch [4/10], Step [385/600], Loss: 0.6485, Train Acc:0.8024\n",
      "Epoch [4/10], Step [386/600], Loss: 0.6019, Train Acc:0.8025\n",
      "Epoch [4/10], Step [387/600], Loss: 0.6508, Train Acc:0.8024\n",
      "Epoch [4/10], Step [388/600], Loss: 0.6392, Train Acc:0.8024\n",
      "Epoch [4/10], Step [389/600], Loss: 0.6462, Train Acc:0.8023\n",
      "Epoch [4/10], Step [390/600], Loss: 0.5829, Train Acc:0.8024\n",
      "Epoch [4/10], Step [391/600], Loss: 0.7351, Train Acc:0.8024\n",
      "Epoch [4/10], Step [392/600], Loss: 0.7163, Train Acc:0.8023\n",
      "Epoch [4/10], Step [393/600], Loss: 0.6447, Train Acc:0.8023\n",
      "Epoch [4/10], Step [394/600], Loss: 0.6042, Train Acc:0.8023\n",
      "Epoch [4/10], Step [395/600], Loss: 0.5950, Train Acc:0.8024\n",
      "Epoch [4/10], Step [396/600], Loss: 0.5145, Train Acc:0.8025\n",
      "Epoch [4/10], Step [397/600], Loss: 0.4466, Train Acc:0.8028\n",
      "Epoch [4/10], Step [398/600], Loss: 0.6087, Train Acc:0.8028\n",
      "Epoch [4/10], Step [399/600], Loss: 0.7606, Train Acc:0.8026\n",
      "Epoch [4/10], Step [400/600], Loss: 0.6595, Train Acc:0.8026\n",
      "Epoch [4/10], Step [401/600], Loss: 0.4789, Train Acc:0.8028\n",
      "Epoch [4/10], Step [402/600], Loss: 0.5693, Train Acc:0.8028\n",
      "Epoch [4/10], Step [403/600], Loss: 0.5569, Train Acc:0.8028\n",
      "Epoch [4/10], Step [404/600], Loss: 0.6296, Train Acc:0.8027\n",
      "Epoch [4/10], Step [405/600], Loss: 0.6448, Train Acc:0.8028\n",
      "Epoch [4/10], Step [406/600], Loss: 0.5671, Train Acc:0.8027\n",
      "Epoch [4/10], Step [407/600], Loss: 0.7706, Train Acc:0.8027\n",
      "Epoch [4/10], Step [408/600], Loss: 0.6474, Train Acc:0.8027\n",
      "Epoch [4/10], Step [409/600], Loss: 0.5988, Train Acc:0.8026\n",
      "Epoch [4/10], Step [410/600], Loss: 0.4892, Train Acc:0.8029\n",
      "Epoch [4/10], Step [411/600], Loss: 0.6262, Train Acc:0.8028\n",
      "Epoch [4/10], Step [412/600], Loss: 0.6550, Train Acc:0.8028\n",
      "Epoch [4/10], Step [413/600], Loss: 0.6207, Train Acc:0.8027\n",
      "Epoch [4/10], Step [414/600], Loss: 0.5904, Train Acc:0.8027\n",
      "Epoch [4/10], Step [415/600], Loss: 0.5881, Train Acc:0.8026\n",
      "Epoch [4/10], Step [416/600], Loss: 0.5702, Train Acc:0.8025\n",
      "Epoch [4/10], Step [417/600], Loss: 0.4740, Train Acc:0.8027\n",
      "Epoch [4/10], Step [418/600], Loss: 0.5011, Train Acc:0.8028\n",
      "Epoch [4/10], Step [419/600], Loss: 0.4940, Train Acc:0.8030\n",
      "Epoch [4/10], Step [420/600], Loss: 0.5613, Train Acc:0.8030\n",
      "Epoch [4/10], Step [421/600], Loss: 0.5749, Train Acc:0.8031\n",
      "Epoch [4/10], Step [422/600], Loss: 0.5559, Train Acc:0.8030\n",
      "Epoch [4/10], Step [423/600], Loss: 0.5746, Train Acc:0.8030\n",
      "Epoch [4/10], Step [424/600], Loss: 0.5960, Train Acc:0.8030\n",
      "Epoch [4/10], Step [425/600], Loss: 0.6175, Train Acc:0.8029\n",
      "Epoch [4/10], Step [426/600], Loss: 0.4839, Train Acc:0.8031\n",
      "Epoch [4/10], Step [427/600], Loss: 0.4260, Train Acc:0.8032\n",
      "Epoch [4/10], Step [428/600], Loss: 0.6493, Train Acc:0.8032\n",
      "Epoch [4/10], Step [429/600], Loss: 0.5445, Train Acc:0.8033\n",
      "Epoch [4/10], Step [430/600], Loss: 0.5354, Train Acc:0.8035\n",
      "Epoch [4/10], Step [431/600], Loss: 0.6440, Train Acc:0.8035\n",
      "Epoch [4/10], Step [432/600], Loss: 0.4671, Train Acc:0.8036\n",
      "Epoch [4/10], Step [433/600], Loss: 0.6793, Train Acc:0.8036\n",
      "Epoch [4/10], Step [434/600], Loss: 0.5780, Train Acc:0.8037\n",
      "Epoch [4/10], Step [435/600], Loss: 0.5291, Train Acc:0.8038\n",
      "Epoch [4/10], Step [436/600], Loss: 0.7143, Train Acc:0.8037\n",
      "Epoch [4/10], Step [437/600], Loss: 0.4701, Train Acc:0.8039\n",
      "Epoch [4/10], Step [438/600], Loss: 0.5625, Train Acc:0.8039\n",
      "Epoch [4/10], Step [439/600], Loss: 0.5454, Train Acc:0.8041\n",
      "Epoch [4/10], Step [440/600], Loss: 0.6459, Train Acc:0.8041\n",
      "Epoch [4/10], Step [441/600], Loss: 0.6222, Train Acc:0.8040\n",
      "Epoch [4/10], Step [442/600], Loss: 0.7128, Train Acc:0.8040\n",
      "Epoch [4/10], Step [443/600], Loss: 0.6074, Train Acc:0.8040\n",
      "Epoch [4/10], Step [444/600], Loss: 0.7319, Train Acc:0.8040\n",
      "Epoch [4/10], Step [445/600], Loss: 0.6295, Train Acc:0.8039\n",
      "Epoch [4/10], Step [446/600], Loss: 0.5285, Train Acc:0.8040\n",
      "Epoch [4/10], Step [447/600], Loss: 0.6176, Train Acc:0.8041\n",
      "Epoch [4/10], Step [448/600], Loss: 0.6850, Train Acc:0.8041\n",
      "Epoch [4/10], Step [449/600], Loss: 0.6244, Train Acc:0.8040\n",
      "Epoch [4/10], Step [450/600], Loss: 0.7010, Train Acc:0.8038\n",
      "Epoch [4/10], Step [451/600], Loss: 0.6521, Train Acc:0.8038\n",
      "Epoch [4/10], Step [452/600], Loss: 0.5309, Train Acc:0.8040\n",
      "Epoch [4/10], Step [453/600], Loss: 0.5124, Train Acc:0.8042\n",
      "Epoch [4/10], Step [454/600], Loss: 0.5678, Train Acc:0.8043\n",
      "Epoch [4/10], Step [455/600], Loss: 0.5678, Train Acc:0.8042\n",
      "Epoch [4/10], Step [456/600], Loss: 0.6090, Train Acc:0.8042\n",
      "Epoch [4/10], Step [457/600], Loss: 0.5634, Train Acc:0.8042\n",
      "Epoch [4/10], Step [458/600], Loss: 0.6201, Train Acc:0.8042\n",
      "Epoch [4/10], Step [459/600], Loss: 0.6158, Train Acc:0.8042\n",
      "Epoch [4/10], Step [460/600], Loss: 0.6675, Train Acc:0.8041\n",
      "Epoch [4/10], Step [461/600], Loss: 0.6133, Train Acc:0.8041\n",
      "Epoch [4/10], Step [462/600], Loss: 0.5153, Train Acc:0.8042\n",
      "Epoch [4/10], Step [463/600], Loss: 0.5789, Train Acc:0.8042\n",
      "Epoch [4/10], Step [464/600], Loss: 0.6936, Train Acc:0.8042\n",
      "Epoch [4/10], Step [465/600], Loss: 0.4704, Train Acc:0.8043\n",
      "Epoch [4/10], Step [466/600], Loss: 0.6223, Train Acc:0.8042\n",
      "Epoch [4/10], Step [467/600], Loss: 0.6977, Train Acc:0.8041\n",
      "Epoch [4/10], Step [468/600], Loss: 0.7364, Train Acc:0.8039\n",
      "Epoch [4/10], Step [469/600], Loss: 0.6411, Train Acc:0.8039\n",
      "Epoch [4/10], Step [470/600], Loss: 0.6279, Train Acc:0.8039\n",
      "Epoch [4/10], Step [471/600], Loss: 0.6809, Train Acc:0.8039\n",
      "Epoch [4/10], Step [472/600], Loss: 0.5651, Train Acc:0.8039\n",
      "Epoch [4/10], Step [473/600], Loss: 0.6239, Train Acc:0.8039\n",
      "Epoch [4/10], Step [474/600], Loss: 0.5538, Train Acc:0.8039\n",
      "Epoch [4/10], Step [475/600], Loss: 0.5541, Train Acc:0.8040\n",
      "Epoch [4/10], Step [476/600], Loss: 0.5345, Train Acc:0.8041\n",
      "Epoch [4/10], Step [477/600], Loss: 0.6706, Train Acc:0.8040\n",
      "Epoch [4/10], Step [478/600], Loss: 0.5429, Train Acc:0.8040\n",
      "Epoch [4/10], Step [479/600], Loss: 0.5325, Train Acc:0.8040\n",
      "Epoch [4/10], Step [480/600], Loss: 0.5881, Train Acc:0.8041\n",
      "Epoch [4/10], Step [481/600], Loss: 0.5858, Train Acc:0.8042\n",
      "Epoch [4/10], Step [482/600], Loss: 0.5351, Train Acc:0.8042\n",
      "Epoch [4/10], Step [483/600], Loss: 0.5804, Train Acc:0.8043\n",
      "Epoch [4/10], Step [484/600], Loss: 0.6715, Train Acc:0.8043\n",
      "Epoch [4/10], Step [485/600], Loss: 0.5221, Train Acc:0.8043\n",
      "Epoch [4/10], Step [486/600], Loss: 0.6447, Train Acc:0.8043\n",
      "Epoch [4/10], Step [487/600], Loss: 0.5213, Train Acc:0.8043\n",
      "Epoch [4/10], Step [488/600], Loss: 0.6010, Train Acc:0.8044\n",
      "Epoch [4/10], Step [489/600], Loss: 0.6038, Train Acc:0.8043\n",
      "Epoch [4/10], Step [490/600], Loss: 0.5565, Train Acc:0.8043\n",
      "Epoch [4/10], Step [491/600], Loss: 0.5767, Train Acc:0.8044\n",
      "Epoch [4/10], Step [492/600], Loss: 0.6076, Train Acc:0.8044\n",
      "Epoch [4/10], Step [493/600], Loss: 0.5490, Train Acc:0.8045\n",
      "Epoch [4/10], Step [494/600], Loss: 0.5606, Train Acc:0.8045\n",
      "Epoch [4/10], Step [495/600], Loss: 0.5328, Train Acc:0.8046\n",
      "Epoch [4/10], Step [496/600], Loss: 0.6173, Train Acc:0.8046\n",
      "Epoch [4/10], Step [497/600], Loss: 0.7548, Train Acc:0.8044\n",
      "Epoch [4/10], Step [498/600], Loss: 0.7874, Train Acc:0.8044\n",
      "Epoch [4/10], Step [499/600], Loss: 0.5839, Train Acc:0.8044\n",
      "Epoch [4/10], Step [500/600], Loss: 0.6769, Train Acc:0.8043\n",
      "Epoch [4/10], Step [501/600], Loss: 0.6431, Train Acc:0.8043\n",
      "Epoch [4/10], Step [502/600], Loss: 0.5341, Train Acc:0.8044\n",
      "Epoch [4/10], Step [503/600], Loss: 0.6146, Train Acc:0.8045\n",
      "Epoch [4/10], Step [504/600], Loss: 0.5432, Train Acc:0.8045\n",
      "Epoch [4/10], Step [505/600], Loss: 0.6835, Train Acc:0.8044\n",
      "Epoch [4/10], Step [506/600], Loss: 0.6574, Train Acc:0.8043\n",
      "Epoch [4/10], Step [507/600], Loss: 0.6283, Train Acc:0.8044\n",
      "Epoch [4/10], Step [508/600], Loss: 0.6664, Train Acc:0.8044\n",
      "Epoch [4/10], Step [509/600], Loss: 0.4787, Train Acc:0.8045\n",
      "Epoch [4/10], Step [510/600], Loss: 0.5670, Train Acc:0.8045\n",
      "Epoch [4/10], Step [511/600], Loss: 0.6683, Train Acc:0.8044\n",
      "Epoch [4/10], Step [512/600], Loss: 0.5861, Train Acc:0.8044\n",
      "Epoch [4/10], Step [513/600], Loss: 0.5572, Train Acc:0.8045\n",
      "Epoch [4/10], Step [514/600], Loss: 0.6690, Train Acc:0.8044\n",
      "Epoch [4/10], Step [515/600], Loss: 0.6044, Train Acc:0.8044\n",
      "Epoch [4/10], Step [516/600], Loss: 0.5708, Train Acc:0.8045\n",
      "Epoch [4/10], Step [517/600], Loss: 0.5804, Train Acc:0.8045\n",
      "Epoch [4/10], Step [518/600], Loss: 0.5927, Train Acc:0.8045\n",
      "Epoch [4/10], Step [519/600], Loss: 0.6521, Train Acc:0.8045\n",
      "Epoch [4/10], Step [520/600], Loss: 0.6979, Train Acc:0.8043\n",
      "Epoch [4/10], Step [521/600], Loss: 0.6043, Train Acc:0.8043\n",
      "Epoch [4/10], Step [522/600], Loss: 0.5153, Train Acc:0.8044\n",
      "Epoch [4/10], Step [523/600], Loss: 0.6147, Train Acc:0.8044\n",
      "Epoch [4/10], Step [524/600], Loss: 0.5808, Train Acc:0.8045\n",
      "Epoch [4/10], Step [525/600], Loss: 0.7448, Train Acc:0.8043\n",
      "Epoch [4/10], Step [526/600], Loss: 0.6096, Train Acc:0.8043\n",
      "Epoch [4/10], Step [527/600], Loss: 0.5835, Train Acc:0.8043\n",
      "Epoch [4/10], Step [528/600], Loss: 0.6461, Train Acc:0.8044\n",
      "Epoch [4/10], Step [529/600], Loss: 0.7417, Train Acc:0.8043\n",
      "Epoch [4/10], Step [530/600], Loss: 0.6358, Train Acc:0.8043\n",
      "Epoch [4/10], Step [531/600], Loss: 0.5548, Train Acc:0.8044\n",
      "Epoch [4/10], Step [532/600], Loss: 0.6855, Train Acc:0.8042\n",
      "Epoch [4/10], Step [533/600], Loss: 0.5752, Train Acc:0.8043\n",
      "Epoch [4/10], Step [534/600], Loss: 0.8467, Train Acc:0.8041\n",
      "Epoch [4/10], Step [535/600], Loss: 0.5620, Train Acc:0.8043\n",
      "Epoch [4/10], Step [536/600], Loss: 0.6141, Train Acc:0.8043\n",
      "Epoch [4/10], Step [537/600], Loss: 0.5470, Train Acc:0.8043\n",
      "Epoch [4/10], Step [538/600], Loss: 0.6662, Train Acc:0.8043\n",
      "Epoch [4/10], Step [539/600], Loss: 0.6173, Train Acc:0.8042\n",
      "Epoch [4/10], Step [540/600], Loss: 0.5908, Train Acc:0.8043\n",
      "Epoch [4/10], Step [541/600], Loss: 0.6528, Train Acc:0.8043\n",
      "Epoch [4/10], Step [542/600], Loss: 0.5714, Train Acc:0.8043\n",
      "Epoch [4/10], Step [543/600], Loss: 0.6068, Train Acc:0.8043\n",
      "Epoch [4/10], Step [544/600], Loss: 0.5244, Train Acc:0.8043\n",
      "Epoch [4/10], Step [545/600], Loss: 0.5820, Train Acc:0.8043\n",
      "Epoch [4/10], Step [546/600], Loss: 0.6132, Train Acc:0.8043\n",
      "Epoch [4/10], Step [547/600], Loss: 0.6328, Train Acc:0.8043\n",
      "Epoch [4/10], Step [548/600], Loss: 0.5026, Train Acc:0.8043\n",
      "Epoch [4/10], Step [549/600], Loss: 0.6087, Train Acc:0.8043\n",
      "Epoch [4/10], Step [550/600], Loss: 0.6593, Train Acc:0.8042\n",
      "Epoch [4/10], Step [551/600], Loss: 0.4379, Train Acc:0.8044\n",
      "Epoch [4/10], Step [552/600], Loss: 0.6139, Train Acc:0.8044\n",
      "Epoch [4/10], Step [553/600], Loss: 0.5961, Train Acc:0.8044\n",
      "Epoch [4/10], Step [554/600], Loss: 0.6037, Train Acc:0.8044\n",
      "Epoch [4/10], Step [555/600], Loss: 0.5409, Train Acc:0.8045\n",
      "Epoch [4/10], Step [556/600], Loss: 0.6224, Train Acc:0.8045\n",
      "Epoch [4/10], Step [557/600], Loss: 0.6965, Train Acc:0.8045\n",
      "Epoch [4/10], Step [558/600], Loss: 0.5047, Train Acc:0.8046\n",
      "Epoch [4/10], Step [559/600], Loss: 0.6854, Train Acc:0.8046\n",
      "Epoch [4/10], Step [560/600], Loss: 0.6633, Train Acc:0.8045\n",
      "Epoch [4/10], Step [561/600], Loss: 0.6139, Train Acc:0.8045\n",
      "Epoch [4/10], Step [562/600], Loss: 0.4488, Train Acc:0.8046\n",
      "Epoch [4/10], Step [563/600], Loss: 0.5006, Train Acc:0.8048\n",
      "Epoch [4/10], Step [564/600], Loss: 0.6409, Train Acc:0.8047\n",
      "Epoch [4/10], Step [565/600], Loss: 0.6593, Train Acc:0.8047\n",
      "Epoch [4/10], Step [566/600], Loss: 0.5535, Train Acc:0.8047\n",
      "Epoch [4/10], Step [567/600], Loss: 0.5298, Train Acc:0.8048\n",
      "Epoch [4/10], Step [568/600], Loss: 0.4639, Train Acc:0.8049\n",
      "Epoch [4/10], Step [569/600], Loss: 0.5309, Train Acc:0.8049\n",
      "Epoch [4/10], Step [570/600], Loss: 0.5629, Train Acc:0.8049\n",
      "Epoch [4/10], Step [571/600], Loss: 0.4575, Train Acc:0.8050\n",
      "Epoch [4/10], Step [572/600], Loss: 0.5782, Train Acc:0.8050\n",
      "Epoch [4/10], Step [573/600], Loss: 0.6485, Train Acc:0.8049\n",
      "Epoch [4/10], Step [574/600], Loss: 0.7093, Train Acc:0.8048\n",
      "Epoch [4/10], Step [575/600], Loss: 0.5552, Train Acc:0.8048\n",
      "Epoch [4/10], Step [576/600], Loss: 0.7575, Train Acc:0.8047\n",
      "Epoch [4/10], Step [577/600], Loss: 0.4476, Train Acc:0.8048\n",
      "Epoch [4/10], Step [578/600], Loss: 0.6790, Train Acc:0.8047\n",
      "Epoch [4/10], Step [579/600], Loss: 0.6145, Train Acc:0.8047\n",
      "Epoch [4/10], Step [580/600], Loss: 0.5697, Train Acc:0.8047\n",
      "Epoch [4/10], Step [581/600], Loss: 0.7950, Train Acc:0.8046\n",
      "Epoch [4/10], Step [582/600], Loss: 0.6828, Train Acc:0.8046\n",
      "Epoch [4/10], Step [583/600], Loss: 0.5894, Train Acc:0.8045\n",
      "Epoch [4/10], Step [584/600], Loss: 0.6286, Train Acc:0.8045\n",
      "Epoch [4/10], Step [585/600], Loss: 0.6179, Train Acc:0.8045\n",
      "Epoch [4/10], Step [586/600], Loss: 0.7302, Train Acc:0.8044\n",
      "Epoch [4/10], Step [587/600], Loss: 0.6861, Train Acc:0.8044\n",
      "Epoch [4/10], Step [588/600], Loss: 0.4914, Train Acc:0.8045\n",
      "Epoch [4/10], Step [589/600], Loss: 0.6270, Train Acc:0.8044\n",
      "Epoch [4/10], Step [590/600], Loss: 0.6191, Train Acc:0.8043\n",
      "Epoch [4/10], Step [591/600], Loss: 0.6043, Train Acc:0.8043\n",
      "Epoch [4/10], Step [592/600], Loss: 0.7116, Train Acc:0.8042\n",
      "Epoch [4/10], Step [593/600], Loss: 0.7419, Train Acc:0.8040\n",
      "Epoch [4/10], Step [594/600], Loss: 0.5914, Train Acc:0.8040\n",
      "Epoch [4/10], Step [595/600], Loss: 0.5687, Train Acc:0.8040\n",
      "Epoch [4/10], Step [596/600], Loss: 0.5296, Train Acc:0.8040\n",
      "Epoch [4/10], Step [597/600], Loss: 0.5581, Train Acc:0.8041\n",
      "Epoch [4/10], Step [598/600], Loss: 0.6928, Train Acc:0.8041\n",
      "Epoch [4/10], Step [599/600], Loss: 0.6195, Train Acc:0.8041\n",
      "Epoch [4/10], Step [600/600], Loss: 0.4861, Train Acc:0.8042\n",
      "Epoch [5/10], Step [1/600], Loss: 0.6862, Train Acc:0.8200\n",
      "Epoch [5/10], Step [2/600], Loss: 0.5403, Train Acc:0.8100\n",
      "Epoch [5/10], Step [3/600], Loss: 0.6629, Train Acc:0.8000\n",
      "Epoch [5/10], Step [4/600], Loss: 0.5494, Train Acc:0.8025\n",
      "Epoch [5/10], Step [5/600], Loss: 0.5203, Train Acc:0.8000\n",
      "Epoch [5/10], Step [6/600], Loss: 0.4953, Train Acc:0.8117\n",
      "Epoch [5/10], Step [7/600], Loss: 0.6922, Train Acc:0.8057\n",
      "Epoch [5/10], Step [8/600], Loss: 0.5458, Train Acc:0.8025\n",
      "Epoch [5/10], Step [9/600], Loss: 0.5697, Train Acc:0.8033\n",
      "Epoch [5/10], Step [10/600], Loss: 0.5931, Train Acc:0.8050\n",
      "Epoch [5/10], Step [11/600], Loss: 0.5982, Train Acc:0.8055\n",
      "Epoch [5/10], Step [12/600], Loss: 0.5856, Train Acc:0.8058\n",
      "Epoch [5/10], Step [13/600], Loss: 0.6224, Train Acc:0.8031\n",
      "Epoch [5/10], Step [14/600], Loss: 0.7006, Train Acc:0.7993\n",
      "Epoch [5/10], Step [15/600], Loss: 0.6667, Train Acc:0.7960\n",
      "Epoch [5/10], Step [16/600], Loss: 0.5340, Train Acc:0.8006\n",
      "Epoch [5/10], Step [17/600], Loss: 0.5257, Train Acc:0.8018\n",
      "Epoch [5/10], Step [18/600], Loss: 0.6001, Train Acc:0.8033\n",
      "Epoch [5/10], Step [19/600], Loss: 0.5899, Train Acc:0.8032\n",
      "Epoch [5/10], Step [20/600], Loss: 0.5639, Train Acc:0.8030\n",
      "Epoch [5/10], Step [21/600], Loss: 0.5046, Train Acc:0.8052\n",
      "Epoch [5/10], Step [22/600], Loss: 0.5193, Train Acc:0.8068\n",
      "Epoch [5/10], Step [23/600], Loss: 0.6698, Train Acc:0.8074\n",
      "Epoch [5/10], Step [24/600], Loss: 0.5860, Train Acc:0.8096\n",
      "Epoch [5/10], Step [25/600], Loss: 0.6480, Train Acc:0.8084\n",
      "Epoch [5/10], Step [26/600], Loss: 0.5698, Train Acc:0.8077\n",
      "Epoch [5/10], Step [27/600], Loss: 0.6505, Train Acc:0.8056\n",
      "Epoch [5/10], Step [28/600], Loss: 0.5589, Train Acc:0.8079\n",
      "Epoch [5/10], Step [29/600], Loss: 0.4789, Train Acc:0.8097\n",
      "Epoch [5/10], Step [30/600], Loss: 0.5047, Train Acc:0.8103\n",
      "Epoch [5/10], Step [31/600], Loss: 0.6146, Train Acc:0.8116\n",
      "Epoch [5/10], Step [32/600], Loss: 0.5274, Train Acc:0.8116\n",
      "Epoch [5/10], Step [33/600], Loss: 0.6878, Train Acc:0.8124\n",
      "Epoch [5/10], Step [34/600], Loss: 0.5880, Train Acc:0.8118\n",
      "Epoch [5/10], Step [35/600], Loss: 0.5904, Train Acc:0.8114\n",
      "Epoch [5/10], Step [36/600], Loss: 0.6483, Train Acc:0.8106\n",
      "Epoch [5/10], Step [37/600], Loss: 0.4063, Train Acc:0.8127\n",
      "Epoch [5/10], Step [38/600], Loss: 0.7545, Train Acc:0.8118\n",
      "Epoch [5/10], Step [39/600], Loss: 0.6285, Train Acc:0.8113\n",
      "Epoch [5/10], Step [40/600], Loss: 0.6098, Train Acc:0.8105\n",
      "Epoch [5/10], Step [41/600], Loss: 0.6441, Train Acc:0.8105\n",
      "Epoch [5/10], Step [42/600], Loss: 0.6025, Train Acc:0.8105\n",
      "Epoch [5/10], Step [43/600], Loss: 0.6392, Train Acc:0.8098\n",
      "Epoch [5/10], Step [44/600], Loss: 0.5711, Train Acc:0.8086\n",
      "Epoch [5/10], Step [45/600], Loss: 0.6321, Train Acc:0.8087\n",
      "Epoch [5/10], Step [46/600], Loss: 0.4503, Train Acc:0.8100\n",
      "Epoch [5/10], Step [47/600], Loss: 0.6422, Train Acc:0.8087\n",
      "Epoch [5/10], Step [48/600], Loss: 0.5346, Train Acc:0.8083\n",
      "Epoch [5/10], Step [49/600], Loss: 0.5807, Train Acc:0.8094\n",
      "Epoch [5/10], Step [50/600], Loss: 0.5514, Train Acc:0.8098\n",
      "Epoch [5/10], Step [51/600], Loss: 0.5506, Train Acc:0.8102\n",
      "Epoch [5/10], Step [52/600], Loss: 0.5218, Train Acc:0.8110\n",
      "Epoch [5/10], Step [53/600], Loss: 0.6638, Train Acc:0.8098\n",
      "Epoch [5/10], Step [54/600], Loss: 0.5828, Train Acc:0.8094\n",
      "Epoch [5/10], Step [55/600], Loss: 0.5576, Train Acc:0.8098\n",
      "Epoch [5/10], Step [56/600], Loss: 0.4799, Train Acc:0.8107\n",
      "Epoch [5/10], Step [57/600], Loss: 0.6206, Train Acc:0.8098\n",
      "Epoch [5/10], Step [58/600], Loss: 0.6398, Train Acc:0.8100\n",
      "Epoch [5/10], Step [59/600], Loss: 0.6286, Train Acc:0.8097\n",
      "Epoch [5/10], Step [60/600], Loss: 0.7132, Train Acc:0.8090\n",
      "Epoch [5/10], Step [61/600], Loss: 0.5106, Train Acc:0.8103\n",
      "Epoch [5/10], Step [62/600], Loss: 0.6379, Train Acc:0.8105\n",
      "Epoch [5/10], Step [63/600], Loss: 0.7377, Train Acc:0.8092\n",
      "Epoch [5/10], Step [64/600], Loss: 0.6469, Train Acc:0.8089\n",
      "Epoch [5/10], Step [65/600], Loss: 0.5368, Train Acc:0.8098\n",
      "Epoch [5/10], Step [66/600], Loss: 0.6729, Train Acc:0.8089\n",
      "Epoch [5/10], Step [67/600], Loss: 0.6768, Train Acc:0.8078\n",
      "Epoch [5/10], Step [68/600], Loss: 0.5584, Train Acc:0.8079\n",
      "Epoch [5/10], Step [69/600], Loss: 0.5508, Train Acc:0.8081\n",
      "Epoch [5/10], Step [70/600], Loss: 0.5705, Train Acc:0.8081\n",
      "Epoch [5/10], Step [71/600], Loss: 0.5888, Train Acc:0.8082\n",
      "Epoch [5/10], Step [72/600], Loss: 0.7993, Train Acc:0.8074\n",
      "Epoch [5/10], Step [73/600], Loss: 0.4455, Train Acc:0.8079\n",
      "Epoch [5/10], Step [74/600], Loss: 0.5478, Train Acc:0.8076\n",
      "Epoch [5/10], Step [75/600], Loss: 0.5966, Train Acc:0.8077\n",
      "Epoch [5/10], Step [76/600], Loss: 0.5664, Train Acc:0.8087\n",
      "Epoch [5/10], Step [77/600], Loss: 0.5169, Train Acc:0.8092\n",
      "Epoch [5/10], Step [78/600], Loss: 0.5184, Train Acc:0.8092\n",
      "Epoch [5/10], Step [79/600], Loss: 0.6620, Train Acc:0.8085\n",
      "Epoch [5/10], Step [80/600], Loss: 0.7525, Train Acc:0.8083\n",
      "Epoch [5/10], Step [81/600], Loss: 0.6832, Train Acc:0.8083\n",
      "Epoch [5/10], Step [82/600], Loss: 0.6133, Train Acc:0.8078\n",
      "Epoch [5/10], Step [83/600], Loss: 0.4593, Train Acc:0.8084\n",
      "Epoch [5/10], Step [84/600], Loss: 0.5186, Train Acc:0.8090\n",
      "Epoch [5/10], Step [85/600], Loss: 0.5609, Train Acc:0.8092\n",
      "Epoch [5/10], Step [86/600], Loss: 0.5712, Train Acc:0.8091\n",
      "Epoch [5/10], Step [87/600], Loss: 0.6665, Train Acc:0.8087\n",
      "Epoch [5/10], Step [88/600], Loss: 0.5682, Train Acc:0.8090\n",
      "Epoch [5/10], Step [89/600], Loss: 0.4716, Train Acc:0.8096\n",
      "Epoch [5/10], Step [90/600], Loss: 0.5585, Train Acc:0.8101\n",
      "Epoch [5/10], Step [91/600], Loss: 0.6033, Train Acc:0.8097\n",
      "Epoch [5/10], Step [92/600], Loss: 0.6277, Train Acc:0.8092\n",
      "Epoch [5/10], Step [93/600], Loss: 0.7114, Train Acc:0.8085\n",
      "Epoch [5/10], Step [94/600], Loss: 0.5012, Train Acc:0.8087\n",
      "Epoch [5/10], Step [95/600], Loss: 0.5420, Train Acc:0.8091\n",
      "Epoch [5/10], Step [96/600], Loss: 0.5618, Train Acc:0.8093\n",
      "Epoch [5/10], Step [97/600], Loss: 0.5435, Train Acc:0.8094\n",
      "Epoch [5/10], Step [98/600], Loss: 0.6897, Train Acc:0.8088\n",
      "Epoch [5/10], Step [99/600], Loss: 0.7083, Train Acc:0.8086\n",
      "Epoch [5/10], Step [100/600], Loss: 0.6470, Train Acc:0.8079\n",
      "Epoch [5/10], Step [101/600], Loss: 0.6433, Train Acc:0.8081\n",
      "Epoch [5/10], Step [102/600], Loss: 0.5299, Train Acc:0.8085\n",
      "Epoch [5/10], Step [103/600], Loss: 0.4805, Train Acc:0.8092\n",
      "Epoch [5/10], Step [104/600], Loss: 0.6035, Train Acc:0.8087\n",
      "Epoch [5/10], Step [105/600], Loss: 0.6335, Train Acc:0.8087\n",
      "Epoch [5/10], Step [106/600], Loss: 0.5538, Train Acc:0.8089\n",
      "Epoch [5/10], Step [107/600], Loss: 0.5734, Train Acc:0.8091\n",
      "Epoch [5/10], Step [108/600], Loss: 0.5897, Train Acc:0.8091\n",
      "Epoch [5/10], Step [109/600], Loss: 0.6953, Train Acc:0.8088\n",
      "Epoch [5/10], Step [110/600], Loss: 0.5861, Train Acc:0.8086\n",
      "Epoch [5/10], Step [111/600], Loss: 0.7240, Train Acc:0.8077\n",
      "Epoch [5/10], Step [112/600], Loss: 0.6569, Train Acc:0.8081\n",
      "Epoch [5/10], Step [113/600], Loss: 0.5660, Train Acc:0.8078\n",
      "Epoch [5/10], Step [114/600], Loss: 0.6695, Train Acc:0.8072\n",
      "Epoch [5/10], Step [115/600], Loss: 0.5979, Train Acc:0.8077\n",
      "Epoch [5/10], Step [116/600], Loss: 0.6614, Train Acc:0.8072\n",
      "Epoch [5/10], Step [117/600], Loss: 0.5369, Train Acc:0.8074\n",
      "Epoch [5/10], Step [118/600], Loss: 0.4555, Train Acc:0.8080\n",
      "Epoch [5/10], Step [119/600], Loss: 0.7369, Train Acc:0.8078\n",
      "Epoch [5/10], Step [120/600], Loss: 0.6298, Train Acc:0.8074\n",
      "Epoch [5/10], Step [121/600], Loss: 0.7003, Train Acc:0.8071\n",
      "Epoch [5/10], Step [122/600], Loss: 0.5506, Train Acc:0.8070\n",
      "Epoch [5/10], Step [123/600], Loss: 0.5450, Train Acc:0.8070\n",
      "Epoch [5/10], Step [124/600], Loss: 0.6697, Train Acc:0.8067\n",
      "Epoch [5/10], Step [125/600], Loss: 0.5334, Train Acc:0.8070\n",
      "Epoch [5/10], Step [126/600], Loss: 0.5913, Train Acc:0.8071\n",
      "Epoch [5/10], Step [127/600], Loss: 0.6116, Train Acc:0.8072\n",
      "Epoch [5/10], Step [128/600], Loss: 0.5910, Train Acc:0.8073\n",
      "Epoch [5/10], Step [129/600], Loss: 0.5869, Train Acc:0.8073\n",
      "Epoch [5/10], Step [130/600], Loss: 0.4834, Train Acc:0.8077\n",
      "Epoch [5/10], Step [131/600], Loss: 0.6578, Train Acc:0.8076\n",
      "Epoch [5/10], Step [132/600], Loss: 0.6583, Train Acc:0.8075\n",
      "Epoch [5/10], Step [133/600], Loss: 0.5923, Train Acc:0.8077\n",
      "Epoch [5/10], Step [134/600], Loss: 0.5207, Train Acc:0.8080\n",
      "Epoch [5/10], Step [135/600], Loss: 0.6814, Train Acc:0.8076\n",
      "Epoch [5/10], Step [136/600], Loss: 0.6332, Train Acc:0.8077\n",
      "Epoch [5/10], Step [137/600], Loss: 0.6387, Train Acc:0.8077\n",
      "Epoch [5/10], Step [138/600], Loss: 0.6123, Train Acc:0.8076\n",
      "Epoch [5/10], Step [139/600], Loss: 0.5889, Train Acc:0.8074\n",
      "Epoch [5/10], Step [140/600], Loss: 0.7456, Train Acc:0.8070\n",
      "Epoch [5/10], Step [141/600], Loss: 0.4861, Train Acc:0.8074\n",
      "Epoch [5/10], Step [142/600], Loss: 0.4539, Train Acc:0.8081\n",
      "Epoch [5/10], Step [143/600], Loss: 0.7761, Train Acc:0.8076\n",
      "Epoch [5/10], Step [144/600], Loss: 0.5349, Train Acc:0.8080\n",
      "Epoch [5/10], Step [145/600], Loss: 0.7344, Train Acc:0.8077\n",
      "Epoch [5/10], Step [146/600], Loss: 0.7168, Train Acc:0.8074\n",
      "Epoch [5/10], Step [147/600], Loss: 0.4581, Train Acc:0.8078\n",
      "Epoch [5/10], Step [148/600], Loss: 0.5970, Train Acc:0.8076\n",
      "Epoch [5/10], Step [149/600], Loss: 0.5133, Train Acc:0.8078\n",
      "Epoch [5/10], Step [150/600], Loss: 0.5676, Train Acc:0.8080\n",
      "Epoch [5/10], Step [151/600], Loss: 0.6949, Train Acc:0.8080\n",
      "Epoch [5/10], Step [152/600], Loss: 0.6466, Train Acc:0.8078\n",
      "Epoch [5/10], Step [153/600], Loss: 0.6146, Train Acc:0.8078\n",
      "Epoch [5/10], Step [154/600], Loss: 0.4834, Train Acc:0.8079\n",
      "Epoch [5/10], Step [155/600], Loss: 0.5407, Train Acc:0.8081\n",
      "Epoch [5/10], Step [156/600], Loss: 0.5812, Train Acc:0.8081\n",
      "Epoch [5/10], Step [157/600], Loss: 0.4893, Train Acc:0.8083\n",
      "Epoch [5/10], Step [158/600], Loss: 0.5750, Train Acc:0.8083\n",
      "Epoch [5/10], Step [159/600], Loss: 0.4868, Train Acc:0.8084\n",
      "Epoch [5/10], Step [160/600], Loss: 0.6036, Train Acc:0.8081\n",
      "Epoch [5/10], Step [161/600], Loss: 0.4923, Train Acc:0.8084\n",
      "Epoch [5/10], Step [162/600], Loss: 0.4829, Train Acc:0.8086\n",
      "Epoch [5/10], Step [163/600], Loss: 0.7426, Train Acc:0.8084\n",
      "Epoch [5/10], Step [164/600], Loss: 0.4381, Train Acc:0.8088\n",
      "Epoch [5/10], Step [165/600], Loss: 0.5705, Train Acc:0.8087\n",
      "Epoch [5/10], Step [166/600], Loss: 0.5239, Train Acc:0.8089\n",
      "Epoch [5/10], Step [167/600], Loss: 0.5202, Train Acc:0.8092\n",
      "Epoch [5/10], Step [168/600], Loss: 0.7256, Train Acc:0.8090\n",
      "Epoch [5/10], Step [169/600], Loss: 0.5549, Train Acc:0.8094\n",
      "Epoch [5/10], Step [170/600], Loss: 0.6358, Train Acc:0.8093\n",
      "Epoch [5/10], Step [171/600], Loss: 0.6573, Train Acc:0.8091\n",
      "Epoch [5/10], Step [172/600], Loss: 0.4386, Train Acc:0.8095\n",
      "Epoch [5/10], Step [173/600], Loss: 0.5919, Train Acc:0.8094\n",
      "Epoch [5/10], Step [174/600], Loss: 0.7096, Train Acc:0.8092\n",
      "Epoch [5/10], Step [175/600], Loss: 0.6699, Train Acc:0.8089\n",
      "Epoch [5/10], Step [176/600], Loss: 0.5670, Train Acc:0.8089\n",
      "Epoch [5/10], Step [177/600], Loss: 0.5336, Train Acc:0.8090\n",
      "Epoch [5/10], Step [178/600], Loss: 0.5086, Train Acc:0.8094\n",
      "Epoch [5/10], Step [179/600], Loss: 0.7906, Train Acc:0.8089\n",
      "Epoch [5/10], Step [180/600], Loss: 0.5963, Train Acc:0.8088\n",
      "Epoch [5/10], Step [181/600], Loss: 0.6190, Train Acc:0.8090\n",
      "Epoch [5/10], Step [182/600], Loss: 0.6541, Train Acc:0.8088\n",
      "Epoch [5/10], Step [183/600], Loss: 0.5671, Train Acc:0.8088\n",
      "Epoch [5/10], Step [184/600], Loss: 0.5758, Train Acc:0.8086\n",
      "Epoch [5/10], Step [185/600], Loss: 0.7921, Train Acc:0.8082\n",
      "Epoch [5/10], Step [186/600], Loss: 0.6670, Train Acc:0.8077\n",
      "Epoch [5/10], Step [187/600], Loss: 0.5347, Train Acc:0.8079\n",
      "Epoch [5/10], Step [188/600], Loss: 0.6349, Train Acc:0.8079\n",
      "Epoch [5/10], Step [189/600], Loss: 0.5992, Train Acc:0.8078\n",
      "Epoch [5/10], Step [190/600], Loss: 0.5316, Train Acc:0.8081\n",
      "Epoch [5/10], Step [191/600], Loss: 0.6071, Train Acc:0.8079\n",
      "Epoch [5/10], Step [192/600], Loss: 0.6312, Train Acc:0.8078\n",
      "Epoch [5/10], Step [193/600], Loss: 0.5840, Train Acc:0.8077\n",
      "Epoch [5/10], Step [194/600], Loss: 0.4776, Train Acc:0.8079\n",
      "Epoch [5/10], Step [195/600], Loss: 0.6146, Train Acc:0.8075\n",
      "Epoch [5/10], Step [196/600], Loss: 0.6724, Train Acc:0.8072\n",
      "Epoch [5/10], Step [197/600], Loss: 0.5714, Train Acc:0.8071\n",
      "Epoch [5/10], Step [198/600], Loss: 0.5752, Train Acc:0.8074\n",
      "Epoch [5/10], Step [199/600], Loss: 0.6862, Train Acc:0.8072\n",
      "Epoch [5/10], Step [200/600], Loss: 0.4875, Train Acc:0.8075\n",
      "Epoch [5/10], Step [201/600], Loss: 0.5356, Train Acc:0.8078\n",
      "Epoch [5/10], Step [202/600], Loss: 0.5795, Train Acc:0.8078\n",
      "Epoch [5/10], Step [203/600], Loss: 0.6298, Train Acc:0.8078\n",
      "Epoch [5/10], Step [204/600], Loss: 0.5913, Train Acc:0.8078\n",
      "Epoch [5/10], Step [205/600], Loss: 0.6330, Train Acc:0.8078\n",
      "Epoch [5/10], Step [206/600], Loss: 0.4846, Train Acc:0.8079\n",
      "Epoch [5/10], Step [207/600], Loss: 0.6220, Train Acc:0.8080\n",
      "Epoch [5/10], Step [208/600], Loss: 0.6134, Train Acc:0.8078\n",
      "Epoch [5/10], Step [209/600], Loss: 0.5894, Train Acc:0.8078\n",
      "Epoch [5/10], Step [210/600], Loss: 0.6579, Train Acc:0.8078\n",
      "Epoch [5/10], Step [211/600], Loss: 0.5475, Train Acc:0.8079\n",
      "Epoch [5/10], Step [212/600], Loss: 0.5386, Train Acc:0.8079\n",
      "Epoch [5/10], Step [213/600], Loss: 0.5403, Train Acc:0.8080\n",
      "Epoch [5/10], Step [214/600], Loss: 0.6759, Train Acc:0.8077\n",
      "Epoch [5/10], Step [215/600], Loss: 0.5083, Train Acc:0.8080\n",
      "Epoch [5/10], Step [216/600], Loss: 0.4380, Train Acc:0.8082\n",
      "Epoch [5/10], Step [217/600], Loss: 0.5291, Train Acc:0.8083\n",
      "Epoch [5/10], Step [218/600], Loss: 0.5949, Train Acc:0.8085\n",
      "Epoch [5/10], Step [219/600], Loss: 0.7622, Train Acc:0.8080\n",
      "Epoch [5/10], Step [220/600], Loss: 0.7499, Train Acc:0.8080\n",
      "Epoch [5/10], Step [221/600], Loss: 0.7462, Train Acc:0.8079\n",
      "Epoch [5/10], Step [222/600], Loss: 0.6286, Train Acc:0.8079\n",
      "Epoch [5/10], Step [223/600], Loss: 0.4063, Train Acc:0.8083\n",
      "Epoch [5/10], Step [224/600], Loss: 0.6090, Train Acc:0.8083\n",
      "Epoch [5/10], Step [225/600], Loss: 0.5607, Train Acc:0.8084\n",
      "Epoch [5/10], Step [226/600], Loss: 0.6412, Train Acc:0.8082\n",
      "Epoch [5/10], Step [227/600], Loss: 0.6278, Train Acc:0.8082\n",
      "Epoch [5/10], Step [228/600], Loss: 0.5507, Train Acc:0.8082\n",
      "Epoch [5/10], Step [229/600], Loss: 0.6617, Train Acc:0.8079\n",
      "Epoch [5/10], Step [230/600], Loss: 0.6079, Train Acc:0.8079\n",
      "Epoch [5/10], Step [231/600], Loss: 0.5002, Train Acc:0.8081\n",
      "Epoch [5/10], Step [232/600], Loss: 0.6065, Train Acc:0.8083\n",
      "Epoch [5/10], Step [233/600], Loss: 0.5480, Train Acc:0.8084\n",
      "Epoch [5/10], Step [234/600], Loss: 0.6044, Train Acc:0.8085\n",
      "Epoch [5/10], Step [235/600], Loss: 0.5523, Train Acc:0.8085\n",
      "Epoch [5/10], Step [236/600], Loss: 0.7361, Train Acc:0.8083\n",
      "Epoch [5/10], Step [237/600], Loss: 0.5008, Train Acc:0.8086\n",
      "Epoch [5/10], Step [238/600], Loss: 0.5979, Train Acc:0.8087\n",
      "Epoch [5/10], Step [239/600], Loss: 0.5288, Train Acc:0.8088\n",
      "Epoch [5/10], Step [240/600], Loss: 0.6574, Train Acc:0.8086\n",
      "Epoch [5/10], Step [241/600], Loss: 0.7179, Train Acc:0.8084\n",
      "Epoch [5/10], Step [242/600], Loss: 0.7248, Train Acc:0.8082\n",
      "Epoch [5/10], Step [243/600], Loss: 0.5304, Train Acc:0.8081\n",
      "Epoch [5/10], Step [244/600], Loss: 0.6231, Train Acc:0.8082\n",
      "Epoch [5/10], Step [245/600], Loss: 0.6113, Train Acc:0.8081\n",
      "Epoch [5/10], Step [246/600], Loss: 0.6087, Train Acc:0.8081\n",
      "Epoch [5/10], Step [247/600], Loss: 0.5202, Train Acc:0.8084\n",
      "Epoch [5/10], Step [248/600], Loss: 0.5132, Train Acc:0.8085\n",
      "Epoch [5/10], Step [249/600], Loss: 0.5343, Train Acc:0.8087\n",
      "Epoch [5/10], Step [250/600], Loss: 0.4872, Train Acc:0.8090\n",
      "Epoch [5/10], Step [251/600], Loss: 0.6132, Train Acc:0.8091\n",
      "Epoch [5/10], Step [252/600], Loss: 0.5657, Train Acc:0.8091\n",
      "Epoch [5/10], Step [253/600], Loss: 0.5216, Train Acc:0.8094\n",
      "Epoch [5/10], Step [254/600], Loss: 0.6254, Train Acc:0.8091\n",
      "Epoch [5/10], Step [255/600], Loss: 0.4417, Train Acc:0.8094\n",
      "Epoch [5/10], Step [256/600], Loss: 0.5731, Train Acc:0.8094\n",
      "Epoch [5/10], Step [257/600], Loss: 0.5540, Train Acc:0.8095\n",
      "Epoch [5/10], Step [258/600], Loss: 0.6750, Train Acc:0.8093\n",
      "Epoch [5/10], Step [259/600], Loss: 0.5510, Train Acc:0.8093\n",
      "Epoch [5/10], Step [260/600], Loss: 0.6548, Train Acc:0.8092\n",
      "Epoch [5/10], Step [261/600], Loss: 0.4879, Train Acc:0.8095\n",
      "Epoch [5/10], Step [262/600], Loss: 0.5506, Train Acc:0.8096\n",
      "Epoch [5/10], Step [263/600], Loss: 0.5076, Train Acc:0.8097\n",
      "Epoch [5/10], Step [264/600], Loss: 0.6030, Train Acc:0.8095\n",
      "Epoch [5/10], Step [265/600], Loss: 0.5690, Train Acc:0.8095\n",
      "Epoch [5/10], Step [266/600], Loss: 0.6190, Train Acc:0.8095\n",
      "Epoch [5/10], Step [267/600], Loss: 0.5217, Train Acc:0.8096\n",
      "Epoch [5/10], Step [268/600], Loss: 0.4933, Train Acc:0.8097\n",
      "Epoch [5/10], Step [269/600], Loss: 0.5934, Train Acc:0.8097\n",
      "Epoch [5/10], Step [270/600], Loss: 0.6025, Train Acc:0.8097\n",
      "Epoch [5/10], Step [271/600], Loss: 0.6355, Train Acc:0.8094\n",
      "Epoch [5/10], Step [272/600], Loss: 0.5983, Train Acc:0.8093\n",
      "Epoch [5/10], Step [273/600], Loss: 0.7560, Train Acc:0.8088\n",
      "Epoch [5/10], Step [274/600], Loss: 0.5692, Train Acc:0.8090\n",
      "Epoch [5/10], Step [275/600], Loss: 0.6502, Train Acc:0.8088\n",
      "Epoch [5/10], Step [276/600], Loss: 0.5948, Train Acc:0.8087\n",
      "Epoch [5/10], Step [277/600], Loss: 0.5801, Train Acc:0.8087\n",
      "Epoch [5/10], Step [278/600], Loss: 0.5694, Train Acc:0.8087\n",
      "Epoch [5/10], Step [279/600], Loss: 0.5418, Train Acc:0.8088\n",
      "Epoch [5/10], Step [280/600], Loss: 0.5051, Train Acc:0.8089\n",
      "Epoch [5/10], Step [281/600], Loss: 0.5570, Train Acc:0.8089\n",
      "Epoch [5/10], Step [282/600], Loss: 0.5893, Train Acc:0.8090\n",
      "Epoch [5/10], Step [283/600], Loss: 0.5848, Train Acc:0.8090\n",
      "Epoch [5/10], Step [284/600], Loss: 0.5884, Train Acc:0.8090\n",
      "Epoch [5/10], Step [285/600], Loss: 0.5707, Train Acc:0.8090\n",
      "Epoch [5/10], Step [286/600], Loss: 0.5613, Train Acc:0.8091\n",
      "Epoch [5/10], Step [287/600], Loss: 0.4966, Train Acc:0.8094\n",
      "Epoch [5/10], Step [288/600], Loss: 0.6809, Train Acc:0.8094\n",
      "Epoch [5/10], Step [289/600], Loss: 0.6273, Train Acc:0.8096\n",
      "Epoch [5/10], Step [290/600], Loss: 0.6504, Train Acc:0.8096\n",
      "Epoch [5/10], Step [291/600], Loss: 0.4152, Train Acc:0.8099\n",
      "Epoch [5/10], Step [292/600], Loss: 0.5757, Train Acc:0.8101\n",
      "Epoch [5/10], Step [293/600], Loss: 0.7901, Train Acc:0.8097\n",
      "Epoch [5/10], Step [294/600], Loss: 0.6346, Train Acc:0.8096\n",
      "Epoch [5/10], Step [295/600], Loss: 0.5775, Train Acc:0.8096\n",
      "Epoch [5/10], Step [296/600], Loss: 0.7068, Train Acc:0.8096\n",
      "Epoch [5/10], Step [297/600], Loss: 0.7804, Train Acc:0.8096\n",
      "Epoch [5/10], Step [298/600], Loss: 0.5837, Train Acc:0.8097\n",
      "Epoch [5/10], Step [299/600], Loss: 0.5901, Train Acc:0.8098\n",
      "Epoch [5/10], Step [300/600], Loss: 0.5672, Train Acc:0.8100\n",
      "Epoch [5/10], Step [301/600], Loss: 0.5558, Train Acc:0.8100\n",
      "Epoch [5/10], Step [302/600], Loss: 0.5997, Train Acc:0.8099\n",
      "Epoch [5/10], Step [303/600], Loss: 0.5794, Train Acc:0.8099\n",
      "Epoch [5/10], Step [304/600], Loss: 0.5633, Train Acc:0.8100\n",
      "Epoch [5/10], Step [305/600], Loss: 0.6787, Train Acc:0.8100\n",
      "Epoch [5/10], Step [306/600], Loss: 0.5989, Train Acc:0.8101\n",
      "Epoch [5/10], Step [307/600], Loss: 0.5931, Train Acc:0.8100\n",
      "Epoch [5/10], Step [308/600], Loss: 0.5619, Train Acc:0.8100\n",
      "Epoch [5/10], Step [309/600], Loss: 0.5051, Train Acc:0.8102\n",
      "Epoch [5/10], Step [310/600], Loss: 0.5836, Train Acc:0.8101\n",
      "Epoch [5/10], Step [311/600], Loss: 0.6026, Train Acc:0.8101\n",
      "Epoch [5/10], Step [312/600], Loss: 0.4433, Train Acc:0.8103\n",
      "Epoch [5/10], Step [313/600], Loss: 0.5950, Train Acc:0.8102\n",
      "Epoch [5/10], Step [314/600], Loss: 0.4987, Train Acc:0.8103\n",
      "Epoch [5/10], Step [315/600], Loss: 0.5622, Train Acc:0.8103\n",
      "Epoch [5/10], Step [316/600], Loss: 0.5459, Train Acc:0.8102\n",
      "Epoch [5/10], Step [317/600], Loss: 0.5499, Train Acc:0.8102\n",
      "Epoch [5/10], Step [318/600], Loss: 0.4612, Train Acc:0.8103\n",
      "Epoch [5/10], Step [319/600], Loss: 0.5592, Train Acc:0.8103\n",
      "Epoch [5/10], Step [320/600], Loss: 0.6950, Train Acc:0.8102\n",
      "Epoch [5/10], Step [321/600], Loss: 0.7084, Train Acc:0.8101\n",
      "Epoch [5/10], Step [322/600], Loss: 0.5038, Train Acc:0.8102\n",
      "Epoch [5/10], Step [323/600], Loss: 0.6301, Train Acc:0.8102\n",
      "Epoch [5/10], Step [324/600], Loss: 0.6191, Train Acc:0.8102\n",
      "Epoch [5/10], Step [325/600], Loss: 0.7125, Train Acc:0.8101\n",
      "Epoch [5/10], Step [326/600], Loss: 0.5467, Train Acc:0.8102\n",
      "Epoch [5/10], Step [327/600], Loss: 0.5090, Train Acc:0.8102\n",
      "Epoch [5/10], Step [328/600], Loss: 0.5582, Train Acc:0.8101\n",
      "Epoch [5/10], Step [329/600], Loss: 0.6238, Train Acc:0.8099\n",
      "Epoch [5/10], Step [330/600], Loss: 0.6861, Train Acc:0.8097\n",
      "Epoch [5/10], Step [331/600], Loss: 0.6377, Train Acc:0.8096\n",
      "Epoch [5/10], Step [332/600], Loss: 0.5307, Train Acc:0.8097\n",
      "Epoch [5/10], Step [333/600], Loss: 0.5306, Train Acc:0.8099\n",
      "Epoch [5/10], Step [334/600], Loss: 0.6110, Train Acc:0.8100\n",
      "Epoch [5/10], Step [335/600], Loss: 0.5797, Train Acc:0.8100\n",
      "Epoch [5/10], Step [336/600], Loss: 0.5628, Train Acc:0.8101\n",
      "Epoch [5/10], Step [337/600], Loss: 0.4918, Train Acc:0.8104\n",
      "Epoch [5/10], Step [338/600], Loss: 0.5413, Train Acc:0.8103\n",
      "Epoch [5/10], Step [339/600], Loss: 0.5084, Train Acc:0.8105\n",
      "Epoch [5/10], Step [340/600], Loss: 0.5932, Train Acc:0.8105\n",
      "Epoch [5/10], Step [341/600], Loss: 0.6022, Train Acc:0.8104\n",
      "Epoch [5/10], Step [342/600], Loss: 0.6399, Train Acc:0.8105\n",
      "Epoch [5/10], Step [343/600], Loss: 0.6163, Train Acc:0.8104\n",
      "Epoch [5/10], Step [344/600], Loss: 0.4732, Train Acc:0.8106\n",
      "Epoch [5/10], Step [345/600], Loss: 0.5392, Train Acc:0.8108\n",
      "Epoch [5/10], Step [346/600], Loss: 0.5545, Train Acc:0.8108\n",
      "Epoch [5/10], Step [347/600], Loss: 0.6075, Train Acc:0.8107\n",
      "Epoch [5/10], Step [348/600], Loss: 0.5918, Train Acc:0.8109\n",
      "Epoch [5/10], Step [349/600], Loss: 0.7410, Train Acc:0.8107\n",
      "Epoch [5/10], Step [350/600], Loss: 0.7027, Train Acc:0.8107\n",
      "Epoch [5/10], Step [351/600], Loss: 0.5601, Train Acc:0.8107\n",
      "Epoch [5/10], Step [352/600], Loss: 0.5184, Train Acc:0.8107\n",
      "Epoch [5/10], Step [353/600], Loss: 0.7162, Train Acc:0.8107\n",
      "Epoch [5/10], Step [354/600], Loss: 0.6003, Train Acc:0.8107\n",
      "Epoch [5/10], Step [355/600], Loss: 0.5252, Train Acc:0.8107\n",
      "Epoch [5/10], Step [356/600], Loss: 0.5680, Train Acc:0.8107\n",
      "Epoch [5/10], Step [357/600], Loss: 0.6614, Train Acc:0.8106\n",
      "Epoch [5/10], Step [358/600], Loss: 0.6377, Train Acc:0.8104\n",
      "Epoch [5/10], Step [359/600], Loss: 0.5445, Train Acc:0.8104\n",
      "Epoch [5/10], Step [360/600], Loss: 0.6528, Train Acc:0.8103\n",
      "Epoch [5/10], Step [361/600], Loss: 0.6056, Train Acc:0.8102\n",
      "Epoch [5/10], Step [362/600], Loss: 0.6197, Train Acc:0.8102\n",
      "Epoch [5/10], Step [363/600], Loss: 0.4804, Train Acc:0.8103\n",
      "Epoch [5/10], Step [364/600], Loss: 0.6617, Train Acc:0.8102\n",
      "Epoch [5/10], Step [365/600], Loss: 0.4466, Train Acc:0.8106\n",
      "Epoch [5/10], Step [366/600], Loss: 0.6381, Train Acc:0.8105\n",
      "Epoch [5/10], Step [367/600], Loss: 0.5989, Train Acc:0.8106\n",
      "Epoch [5/10], Step [368/600], Loss: 0.5722, Train Acc:0.8106\n",
      "Epoch [5/10], Step [369/600], Loss: 0.6019, Train Acc:0.8105\n",
      "Epoch [5/10], Step [370/600], Loss: 0.5801, Train Acc:0.8105\n",
      "Epoch [5/10], Step [371/600], Loss: 0.4470, Train Acc:0.8106\n",
      "Epoch [5/10], Step [372/600], Loss: 0.6475, Train Acc:0.8107\n",
      "Epoch [5/10], Step [373/600], Loss: 0.6289, Train Acc:0.8107\n",
      "Epoch [5/10], Step [374/600], Loss: 0.5730, Train Acc:0.8107\n",
      "Epoch [5/10], Step [375/600], Loss: 0.6269, Train Acc:0.8107\n",
      "Epoch [5/10], Step [376/600], Loss: 0.5789, Train Acc:0.8106\n",
      "Epoch [5/10], Step [377/600], Loss: 0.5671, Train Acc:0.8105\n",
      "Epoch [5/10], Step [378/600], Loss: 0.5441, Train Acc:0.8106\n",
      "Epoch [5/10], Step [379/600], Loss: 0.5526, Train Acc:0.8105\n",
      "Epoch [5/10], Step [380/600], Loss: 0.5124, Train Acc:0.8105\n",
      "Epoch [5/10], Step [381/600], Loss: 0.5345, Train Acc:0.8106\n",
      "Epoch [5/10], Step [382/600], Loss: 0.4377, Train Acc:0.8108\n",
      "Epoch [5/10], Step [383/600], Loss: 0.5158, Train Acc:0.8108\n",
      "Epoch [5/10], Step [384/600], Loss: 0.6704, Train Acc:0.8108\n",
      "Epoch [5/10], Step [385/600], Loss: 0.5066, Train Acc:0.8108\n",
      "Epoch [5/10], Step [386/600], Loss: 0.5323, Train Acc:0.8109\n",
      "Epoch [5/10], Step [387/600], Loss: 0.4278, Train Acc:0.8111\n",
      "Epoch [5/10], Step [388/600], Loss: 0.6400, Train Acc:0.8110\n",
      "Epoch [5/10], Step [389/600], Loss: 0.5706, Train Acc:0.8110\n",
      "Epoch [5/10], Step [390/600], Loss: 0.4786, Train Acc:0.8112\n",
      "Epoch [5/10], Step [391/600], Loss: 0.5153, Train Acc:0.8113\n",
      "Epoch [5/10], Step [392/600], Loss: 0.6626, Train Acc:0.8112\n",
      "Epoch [5/10], Step [393/600], Loss: 0.4285, Train Acc:0.8114\n",
      "Epoch [5/10], Step [394/600], Loss: 0.5007, Train Acc:0.8114\n",
      "Epoch [5/10], Step [395/600], Loss: 0.6911, Train Acc:0.8112\n",
      "Epoch [5/10], Step [396/600], Loss: 0.4842, Train Acc:0.8113\n",
      "Epoch [5/10], Step [397/600], Loss: 0.4913, Train Acc:0.8113\n",
      "Epoch [5/10], Step [398/600], Loss: 0.5713, Train Acc:0.8113\n",
      "Epoch [5/10], Step [399/600], Loss: 0.5185, Train Acc:0.8114\n",
      "Epoch [5/10], Step [400/600], Loss: 0.6994, Train Acc:0.8113\n",
      "Epoch [5/10], Step [401/600], Loss: 0.5900, Train Acc:0.8113\n",
      "Epoch [5/10], Step [402/600], Loss: 0.5452, Train Acc:0.8113\n",
      "Epoch [5/10], Step [403/600], Loss: 0.5195, Train Acc:0.8112\n",
      "Epoch [5/10], Step [404/600], Loss: 0.7093, Train Acc:0.8110\n",
      "Epoch [5/10], Step [405/600], Loss: 0.4941, Train Acc:0.8112\n",
      "Epoch [5/10], Step [406/600], Loss: 0.5565, Train Acc:0.8113\n",
      "Epoch [5/10], Step [407/600], Loss: 0.6942, Train Acc:0.8111\n",
      "Epoch [5/10], Step [408/600], Loss: 0.5570, Train Acc:0.8111\n",
      "Epoch [5/10], Step [409/600], Loss: 0.6031, Train Acc:0.8111\n",
      "Epoch [5/10], Step [410/600], Loss: 0.5274, Train Acc:0.8111\n",
      "Epoch [5/10], Step [411/600], Loss: 0.4793, Train Acc:0.8112\n",
      "Epoch [5/10], Step [412/600], Loss: 0.5802, Train Acc:0.8112\n",
      "Epoch [5/10], Step [413/600], Loss: 0.5760, Train Acc:0.8111\n",
      "Epoch [5/10], Step [414/600], Loss: 0.6170, Train Acc:0.8110\n",
      "Epoch [5/10], Step [415/600], Loss: 0.6845, Train Acc:0.8109\n",
      "Epoch [5/10], Step [416/600], Loss: 0.7425, Train Acc:0.8108\n",
      "Epoch [5/10], Step [417/600], Loss: 0.4780, Train Acc:0.8109\n",
      "Epoch [5/10], Step [418/600], Loss: 0.5667, Train Acc:0.8108\n",
      "Epoch [5/10], Step [419/600], Loss: 0.5702, Train Acc:0.8108\n",
      "Epoch [5/10], Step [420/600], Loss: 0.6903, Train Acc:0.8106\n",
      "Epoch [5/10], Step [421/600], Loss: 0.6269, Train Acc:0.8104\n",
      "Epoch [5/10], Step [422/600], Loss: 0.4206, Train Acc:0.8106\n",
      "Epoch [5/10], Step [423/600], Loss: 0.4716, Train Acc:0.8107\n",
      "Epoch [5/10], Step [424/600], Loss: 0.4985, Train Acc:0.8108\n",
      "Epoch [5/10], Step [425/600], Loss: 0.6599, Train Acc:0.8106\n",
      "Epoch [5/10], Step [426/600], Loss: 0.6881, Train Acc:0.8105\n",
      "Epoch [5/10], Step [427/600], Loss: 0.6912, Train Acc:0.8103\n",
      "Epoch [5/10], Step [428/600], Loss: 0.6083, Train Acc:0.8102\n",
      "Epoch [5/10], Step [429/600], Loss: 0.6306, Train Acc:0.8101\n",
      "Epoch [5/10], Step [430/600], Loss: 0.5873, Train Acc:0.8101\n",
      "Epoch [5/10], Step [431/600], Loss: 0.5707, Train Acc:0.8102\n",
      "Epoch [5/10], Step [432/600], Loss: 0.5452, Train Acc:0.8103\n",
      "Epoch [5/10], Step [433/600], Loss: 0.5702, Train Acc:0.8103\n",
      "Epoch [5/10], Step [434/600], Loss: 0.6053, Train Acc:0.8103\n",
      "Epoch [5/10], Step [435/600], Loss: 0.5822, Train Acc:0.8104\n",
      "Epoch [5/10], Step [436/600], Loss: 0.5743, Train Acc:0.8104\n",
      "Epoch [5/10], Step [437/600], Loss: 0.6869, Train Acc:0.8104\n",
      "Epoch [5/10], Step [438/600], Loss: 0.5094, Train Acc:0.8104\n",
      "Epoch [5/10], Step [439/600], Loss: 0.5620, Train Acc:0.8104\n",
      "Epoch [5/10], Step [440/600], Loss: 0.4946, Train Acc:0.8105\n",
      "Epoch [5/10], Step [441/600], Loss: 0.4919, Train Acc:0.8105\n",
      "Epoch [5/10], Step [442/600], Loss: 0.5418, Train Acc:0.8106\n",
      "Epoch [5/10], Step [443/600], Loss: 0.6369, Train Acc:0.8105\n",
      "Epoch [5/10], Step [444/600], Loss: 0.6963, Train Acc:0.8104\n",
      "Epoch [5/10], Step [445/600], Loss: 0.6015, Train Acc:0.8104\n",
      "Epoch [5/10], Step [446/600], Loss: 0.5806, Train Acc:0.8105\n",
      "Epoch [5/10], Step [447/600], Loss: 0.4574, Train Acc:0.8106\n",
      "Epoch [5/10], Step [448/600], Loss: 0.5602, Train Acc:0.8106\n",
      "Epoch [5/10], Step [449/600], Loss: 0.8314, Train Acc:0.8104\n",
      "Epoch [5/10], Step [450/600], Loss: 0.4192, Train Acc:0.8106\n",
      "Epoch [5/10], Step [451/600], Loss: 0.5877, Train Acc:0.8106\n",
      "Epoch [5/10], Step [452/600], Loss: 0.6585, Train Acc:0.8106\n",
      "Epoch [5/10], Step [453/600], Loss: 0.6056, Train Acc:0.8105\n",
      "Epoch [5/10], Step [454/600], Loss: 0.5764, Train Acc:0.8106\n",
      "Epoch [5/10], Step [455/600], Loss: 0.6798, Train Acc:0.8105\n",
      "Epoch [5/10], Step [456/600], Loss: 0.6305, Train Acc:0.8105\n",
      "Epoch [5/10], Step [457/600], Loss: 0.7391, Train Acc:0.8104\n",
      "Epoch [5/10], Step [458/600], Loss: 0.5800, Train Acc:0.8104\n",
      "Epoch [5/10], Step [459/600], Loss: 0.6023, Train Acc:0.8104\n",
      "Epoch [5/10], Step [460/600], Loss: 0.7075, Train Acc:0.8103\n",
      "Epoch [5/10], Step [461/600], Loss: 0.4926, Train Acc:0.8103\n",
      "Epoch [5/10], Step [462/600], Loss: 0.6748, Train Acc:0.8103\n",
      "Epoch [5/10], Step [463/600], Loss: 0.5259, Train Acc:0.8103\n",
      "Epoch [5/10], Step [464/600], Loss: 0.6148, Train Acc:0.8102\n",
      "Epoch [5/10], Step [465/600], Loss: 0.5955, Train Acc:0.8102\n",
      "Epoch [5/10], Step [466/600], Loss: 0.7156, Train Acc:0.8101\n",
      "Epoch [5/10], Step [467/600], Loss: 0.5621, Train Acc:0.8101\n",
      "Epoch [5/10], Step [468/600], Loss: 0.8218, Train Acc:0.8099\n",
      "Epoch [5/10], Step [469/600], Loss: 0.7016, Train Acc:0.8097\n",
      "Epoch [5/10], Step [470/600], Loss: 0.5124, Train Acc:0.8099\n",
      "Epoch [5/10], Step [471/600], Loss: 0.6244, Train Acc:0.8097\n",
      "Epoch [5/10], Step [472/600], Loss: 0.5119, Train Acc:0.8098\n",
      "Epoch [5/10], Step [473/600], Loss: 0.5277, Train Acc:0.8099\n",
      "Epoch [5/10], Step [474/600], Loss: 0.6141, Train Acc:0.8097\n",
      "Epoch [5/10], Step [475/600], Loss: 0.5938, Train Acc:0.8097\n",
      "Epoch [5/10], Step [476/600], Loss: 0.5124, Train Acc:0.8099\n",
      "Epoch [5/10], Step [477/600], Loss: 0.6168, Train Acc:0.8099\n",
      "Epoch [5/10], Step [478/600], Loss: 0.5042, Train Acc:0.8099\n",
      "Epoch [5/10], Step [479/600], Loss: 0.5708, Train Acc:0.8099\n",
      "Epoch [5/10], Step [480/600], Loss: 0.6368, Train Acc:0.8099\n",
      "Epoch [5/10], Step [481/600], Loss: 0.6023, Train Acc:0.8098\n",
      "Epoch [5/10], Step [482/600], Loss: 0.5537, Train Acc:0.8099\n",
      "Epoch [5/10], Step [483/600], Loss: 0.5905, Train Acc:0.8099\n",
      "Epoch [5/10], Step [484/600], Loss: 0.6107, Train Acc:0.8100\n",
      "Epoch [5/10], Step [485/600], Loss: 0.6044, Train Acc:0.8100\n",
      "Epoch [5/10], Step [486/600], Loss: 0.6136, Train Acc:0.8100\n",
      "Epoch [5/10], Step [487/600], Loss: 0.5313, Train Acc:0.8100\n",
      "Epoch [5/10], Step [488/600], Loss: 0.5939, Train Acc:0.8100\n",
      "Epoch [5/10], Step [489/600], Loss: 0.6153, Train Acc:0.8100\n",
      "Epoch [5/10], Step [490/600], Loss: 0.5142, Train Acc:0.8101\n",
      "Epoch [5/10], Step [491/600], Loss: 0.6878, Train Acc:0.8101\n",
      "Epoch [5/10], Step [492/600], Loss: 0.6012, Train Acc:0.8101\n",
      "Epoch [5/10], Step [493/600], Loss: 0.5913, Train Acc:0.8101\n",
      "Epoch [5/10], Step [494/600], Loss: 0.6484, Train Acc:0.8102\n",
      "Epoch [5/10], Step [495/600], Loss: 0.4221, Train Acc:0.8103\n",
      "Epoch [5/10], Step [496/600], Loss: 0.5153, Train Acc:0.8103\n",
      "Epoch [5/10], Step [497/600], Loss: 0.4899, Train Acc:0.8104\n",
      "Epoch [5/10], Step [498/600], Loss: 0.5832, Train Acc:0.8104\n",
      "Epoch [5/10], Step [499/600], Loss: 0.5526, Train Acc:0.8105\n",
      "Epoch [5/10], Step [500/600], Loss: 0.5614, Train Acc:0.8105\n",
      "Epoch [5/10], Step [501/600], Loss: 0.4362, Train Acc:0.8106\n",
      "Epoch [5/10], Step [502/600], Loss: 0.4242, Train Acc:0.8107\n",
      "Epoch [5/10], Step [503/600], Loss: 0.5205, Train Acc:0.8107\n",
      "Epoch [5/10], Step [504/600], Loss: 0.6096, Train Acc:0.8107\n",
      "Epoch [5/10], Step [505/600], Loss: 0.4907, Train Acc:0.8108\n",
      "Epoch [5/10], Step [506/600], Loss: 0.5251, Train Acc:0.8109\n",
      "Epoch [5/10], Step [507/600], Loss: 0.4917, Train Acc:0.8110\n",
      "Epoch [5/10], Step [508/600], Loss: 0.5911, Train Acc:0.8109\n",
      "Epoch [5/10], Step [509/600], Loss: 0.6603, Train Acc:0.8108\n",
      "Epoch [5/10], Step [510/600], Loss: 0.4667, Train Acc:0.8109\n",
      "Epoch [5/10], Step [511/600], Loss: 0.6002, Train Acc:0.8109\n",
      "Epoch [5/10], Step [512/600], Loss: 0.5255, Train Acc:0.8109\n",
      "Epoch [5/10], Step [513/600], Loss: 0.5644, Train Acc:0.8110\n",
      "Epoch [5/10], Step [514/600], Loss: 0.5085, Train Acc:0.8111\n",
      "Epoch [5/10], Step [515/600], Loss: 0.7338, Train Acc:0.8110\n",
      "Epoch [5/10], Step [516/600], Loss: 0.5434, Train Acc:0.8109\n",
      "Epoch [5/10], Step [517/600], Loss: 0.4445, Train Acc:0.8111\n",
      "Epoch [5/10], Step [518/600], Loss: 0.6224, Train Acc:0.8111\n",
      "Epoch [5/10], Step [519/600], Loss: 0.5167, Train Acc:0.8112\n",
      "Epoch [5/10], Step [520/600], Loss: 0.5858, Train Acc:0.8113\n",
      "Epoch [5/10], Step [521/600], Loss: 0.6714, Train Acc:0.8112\n",
      "Epoch [5/10], Step [522/600], Loss: 0.5764, Train Acc:0.8112\n",
      "Epoch [5/10], Step [523/600], Loss: 0.5984, Train Acc:0.8113\n",
      "Epoch [5/10], Step [524/600], Loss: 0.4997, Train Acc:0.8113\n",
      "Epoch [5/10], Step [525/600], Loss: 0.5473, Train Acc:0.8114\n",
      "Epoch [5/10], Step [526/600], Loss: 0.5079, Train Acc:0.8114\n",
      "Epoch [5/10], Step [527/600], Loss: 0.7025, Train Acc:0.8113\n",
      "Epoch [5/10], Step [528/600], Loss: 0.3915, Train Acc:0.8115\n",
      "Epoch [5/10], Step [529/600], Loss: 0.6173, Train Acc:0.8115\n",
      "Epoch [5/10], Step [530/600], Loss: 0.6341, Train Acc:0.8115\n",
      "Epoch [5/10], Step [531/600], Loss: 0.7116, Train Acc:0.8114\n",
      "Epoch [5/10], Step [532/600], Loss: 0.6398, Train Acc:0.8113\n",
      "Epoch [5/10], Step [533/600], Loss: 0.5422, Train Acc:0.8113\n",
      "Epoch [5/10], Step [534/600], Loss: 0.5000, Train Acc:0.8114\n",
      "Epoch [5/10], Step [535/600], Loss: 0.7079, Train Acc:0.8113\n",
      "Epoch [5/10], Step [536/600], Loss: 0.6502, Train Acc:0.8113\n",
      "Epoch [5/10], Step [537/600], Loss: 0.4590, Train Acc:0.8114\n",
      "Epoch [5/10], Step [538/600], Loss: 0.5620, Train Acc:0.8113\n",
      "Epoch [5/10], Step [539/600], Loss: 0.6261, Train Acc:0.8113\n",
      "Epoch [5/10], Step [540/600], Loss: 0.6163, Train Acc:0.8114\n",
      "Epoch [5/10], Step [541/600], Loss: 0.5262, Train Acc:0.8115\n",
      "Epoch [5/10], Step [542/600], Loss: 0.6453, Train Acc:0.8115\n",
      "Epoch [5/10], Step [543/600], Loss: 0.6165, Train Acc:0.8114\n",
      "Epoch [5/10], Step [544/600], Loss: 0.6587, Train Acc:0.8114\n",
      "Epoch [5/10], Step [545/600], Loss: 0.5181, Train Acc:0.8115\n",
      "Epoch [5/10], Step [546/600], Loss: 0.5998, Train Acc:0.8114\n",
      "Epoch [5/10], Step [547/600], Loss: 0.6540, Train Acc:0.8114\n",
      "Epoch [5/10], Step [548/600], Loss: 0.5724, Train Acc:0.8113\n",
      "Epoch [5/10], Step [549/600], Loss: 0.5689, Train Acc:0.8114\n",
      "Epoch [5/10], Step [550/600], Loss: 0.6562, Train Acc:0.8113\n",
      "Epoch [5/10], Step [551/600], Loss: 0.5810, Train Acc:0.8112\n",
      "Epoch [5/10], Step [552/600], Loss: 0.4957, Train Acc:0.8113\n",
      "Epoch [5/10], Step [553/600], Loss: 0.5857, Train Acc:0.8113\n",
      "Epoch [5/10], Step [554/600], Loss: 0.6209, Train Acc:0.8112\n",
      "Epoch [5/10], Step [555/600], Loss: 0.5065, Train Acc:0.8113\n",
      "Epoch [5/10], Step [556/600], Loss: 0.6259, Train Acc:0.8113\n",
      "Epoch [5/10], Step [557/600], Loss: 0.6377, Train Acc:0.8113\n",
      "Epoch [5/10], Step [558/600], Loss: 0.5822, Train Acc:0.8113\n",
      "Epoch [5/10], Step [559/600], Loss: 0.5988, Train Acc:0.8113\n",
      "Epoch [5/10], Step [560/600], Loss: 0.6745, Train Acc:0.8112\n",
      "Epoch [5/10], Step [561/600], Loss: 0.6365, Train Acc:0.8112\n",
      "Epoch [5/10], Step [562/600], Loss: 0.4621, Train Acc:0.8112\n",
      "Epoch [5/10], Step [563/600], Loss: 0.6311, Train Acc:0.8112\n",
      "Epoch [5/10], Step [564/600], Loss: 0.5103, Train Acc:0.8113\n",
      "Epoch [5/10], Step [565/600], Loss: 0.7921, Train Acc:0.8112\n",
      "Epoch [5/10], Step [566/600], Loss: 0.6198, Train Acc:0.8112\n",
      "Epoch [5/10], Step [567/600], Loss: 0.6423, Train Acc:0.8111\n",
      "Epoch [5/10], Step [568/600], Loss: 0.5810, Train Acc:0.8111\n",
      "Epoch [5/10], Step [569/600], Loss: 0.4754, Train Acc:0.8112\n",
      "Epoch [5/10], Step [570/600], Loss: 0.6479, Train Acc:0.8111\n",
      "Epoch [5/10], Step [571/600], Loss: 0.5592, Train Acc:0.8111\n",
      "Epoch [5/10], Step [572/600], Loss: 0.5487, Train Acc:0.8111\n",
      "Epoch [5/10], Step [573/600], Loss: 0.6042, Train Acc:0.8110\n",
      "Epoch [5/10], Step [574/600], Loss: 0.7917, Train Acc:0.8109\n",
      "Epoch [5/10], Step [575/600], Loss: 0.4966, Train Acc:0.8109\n",
      "Epoch [5/10], Step [576/600], Loss: 0.6864, Train Acc:0.8109\n",
      "Epoch [5/10], Step [577/600], Loss: 0.5963, Train Acc:0.8109\n",
      "Epoch [5/10], Step [578/600], Loss: 0.4619, Train Acc:0.8110\n",
      "Epoch [5/10], Step [579/600], Loss: 0.5301, Train Acc:0.8111\n",
      "Epoch [5/10], Step [580/600], Loss: 0.6312, Train Acc:0.8111\n",
      "Epoch [5/10], Step [581/600], Loss: 0.5229, Train Acc:0.8112\n",
      "Epoch [5/10], Step [582/600], Loss: 0.7551, Train Acc:0.8112\n",
      "Epoch [5/10], Step [583/600], Loss: 0.5750, Train Acc:0.8112\n",
      "Epoch [5/10], Step [584/600], Loss: 0.5650, Train Acc:0.8112\n",
      "Epoch [5/10], Step [585/600], Loss: 0.5977, Train Acc:0.8112\n",
      "Epoch [5/10], Step [586/600], Loss: 0.6020, Train Acc:0.8112\n",
      "Epoch [5/10], Step [587/600], Loss: 0.6400, Train Acc:0.8112\n",
      "Epoch [5/10], Step [588/600], Loss: 0.7710, Train Acc:0.8112\n",
      "Epoch [5/10], Step [589/600], Loss: 0.6754, Train Acc:0.8111\n",
      "Epoch [5/10], Step [590/600], Loss: 0.5302, Train Acc:0.8111\n",
      "Epoch [5/10], Step [591/600], Loss: 0.4748, Train Acc:0.8112\n",
      "Epoch [5/10], Step [592/600], Loss: 0.6066, Train Acc:0.8111\n",
      "Epoch [5/10], Step [593/600], Loss: 0.5704, Train Acc:0.8111\n",
      "Epoch [5/10], Step [594/600], Loss: 0.5962, Train Acc:0.8111\n",
      "Epoch [5/10], Step [595/600], Loss: 0.5818, Train Acc:0.8111\n",
      "Epoch [5/10], Step [596/600], Loss: 0.5397, Train Acc:0.8111\n",
      "Epoch [5/10], Step [597/600], Loss: 0.5401, Train Acc:0.8111\n",
      "Epoch [5/10], Step [598/600], Loss: 0.6204, Train Acc:0.8111\n",
      "Epoch [5/10], Step [599/600], Loss: 0.5808, Train Acc:0.8111\n",
      "Epoch [5/10], Step [600/600], Loss: 0.4964, Train Acc:0.8111\n",
      "Epoch [6/10], Step [1/600], Loss: 0.4876, Train Acc:0.8600\n",
      "Epoch [6/10], Step [2/600], Loss: 0.5479, Train Acc:0.8250\n",
      "Epoch [6/10], Step [3/600], Loss: 0.3966, Train Acc:0.8467\n",
      "Epoch [6/10], Step [4/600], Loss: 0.6634, Train Acc:0.8325\n",
      "Epoch [6/10], Step [5/600], Loss: 0.5736, Train Acc:0.8340\n",
      "Epoch [6/10], Step [6/600], Loss: 0.4587, Train Acc:0.8367\n",
      "Epoch [6/10], Step [7/600], Loss: 0.5699, Train Acc:0.8257\n",
      "Epoch [6/10], Step [8/600], Loss: 0.5003, Train Acc:0.8275\n",
      "Epoch [6/10], Step [9/600], Loss: 0.6333, Train Acc:0.8222\n",
      "Epoch [6/10], Step [10/600], Loss: 0.5727, Train Acc:0.8230\n",
      "Epoch [6/10], Step [11/600], Loss: 0.6752, Train Acc:0.8209\n",
      "Epoch [6/10], Step [12/600], Loss: 0.7388, Train Acc:0.8142\n",
      "Epoch [6/10], Step [13/600], Loss: 0.6147, Train Acc:0.8092\n",
      "Epoch [6/10], Step [14/600], Loss: 0.4769, Train Acc:0.8114\n",
      "Epoch [6/10], Step [15/600], Loss: 0.5712, Train Acc:0.8107\n",
      "Epoch [6/10], Step [16/600], Loss: 0.5019, Train Acc:0.8144\n",
      "Epoch [6/10], Step [17/600], Loss: 0.4495, Train Acc:0.8153\n",
      "Epoch [6/10], Step [18/600], Loss: 0.5500, Train Acc:0.8156\n",
      "Epoch [6/10], Step [19/600], Loss: 0.4101, Train Acc:0.8189\n",
      "Epoch [6/10], Step [20/600], Loss: 0.5899, Train Acc:0.8185\n",
      "Epoch [6/10], Step [21/600], Loss: 0.5000, Train Acc:0.8195\n",
      "Epoch [6/10], Step [22/600], Loss: 0.6188, Train Acc:0.8186\n",
      "Epoch [6/10], Step [23/600], Loss: 0.6457, Train Acc:0.8170\n",
      "Epoch [6/10], Step [24/600], Loss: 0.6080, Train Acc:0.8183\n",
      "Epoch [6/10], Step [25/600], Loss: 0.5016, Train Acc:0.8204\n",
      "Epoch [6/10], Step [26/600], Loss: 0.5882, Train Acc:0.8177\n",
      "Epoch [6/10], Step [27/600], Loss: 0.5249, Train Acc:0.8185\n",
      "Epoch [6/10], Step [28/600], Loss: 0.5522, Train Acc:0.8193\n",
      "Epoch [6/10], Step [29/600], Loss: 0.6466, Train Acc:0.8179\n",
      "Epoch [6/10], Step [30/600], Loss: 0.5923, Train Acc:0.8173\n",
      "Epoch [6/10], Step [31/600], Loss: 0.5334, Train Acc:0.8174\n",
      "Epoch [6/10], Step [32/600], Loss: 0.4705, Train Acc:0.8172\n",
      "Epoch [6/10], Step [33/600], Loss: 0.5380, Train Acc:0.8182\n",
      "Epoch [6/10], Step [34/600], Loss: 0.5787, Train Acc:0.8176\n",
      "Epoch [6/10], Step [35/600], Loss: 0.6004, Train Acc:0.8169\n",
      "Epoch [6/10], Step [36/600], Loss: 0.7155, Train Acc:0.8150\n",
      "Epoch [6/10], Step [37/600], Loss: 0.7359, Train Acc:0.8143\n",
      "Epoch [6/10], Step [38/600], Loss: 0.5291, Train Acc:0.8150\n",
      "Epoch [6/10], Step [39/600], Loss: 0.5058, Train Acc:0.8151\n",
      "Epoch [6/10], Step [40/600], Loss: 0.6340, Train Acc:0.8147\n",
      "Epoch [6/10], Step [41/600], Loss: 0.5696, Train Acc:0.8146\n",
      "Epoch [6/10], Step [42/600], Loss: 0.5409, Train Acc:0.8152\n",
      "Epoch [6/10], Step [43/600], Loss: 0.6139, Train Acc:0.8142\n",
      "Epoch [6/10], Step [44/600], Loss: 0.3729, Train Acc:0.8161\n",
      "Epoch [6/10], Step [45/600], Loss: 0.7511, Train Acc:0.8140\n",
      "Epoch [6/10], Step [46/600], Loss: 0.5736, Train Acc:0.8139\n",
      "Epoch [6/10], Step [47/600], Loss: 0.7156, Train Acc:0.8123\n",
      "Epoch [6/10], Step [48/600], Loss: 0.6666, Train Acc:0.8106\n",
      "Epoch [6/10], Step [49/600], Loss: 0.6097, Train Acc:0.8096\n",
      "Epoch [6/10], Step [50/600], Loss: 0.4198, Train Acc:0.8110\n",
      "Epoch [6/10], Step [51/600], Loss: 0.6694, Train Acc:0.8092\n",
      "Epoch [6/10], Step [52/600], Loss: 0.6278, Train Acc:0.8090\n",
      "Epoch [6/10], Step [53/600], Loss: 0.4971, Train Acc:0.8089\n",
      "Epoch [6/10], Step [54/600], Loss: 0.4330, Train Acc:0.8107\n",
      "Epoch [6/10], Step [55/600], Loss: 0.5310, Train Acc:0.8111\n",
      "Epoch [6/10], Step [56/600], Loss: 0.5292, Train Acc:0.8116\n",
      "Epoch [6/10], Step [57/600], Loss: 0.5595, Train Acc:0.8111\n",
      "Epoch [6/10], Step [58/600], Loss: 0.6071, Train Acc:0.8109\n",
      "Epoch [6/10], Step [59/600], Loss: 0.5112, Train Acc:0.8117\n",
      "Epoch [6/10], Step [60/600], Loss: 0.4488, Train Acc:0.8125\n",
      "Epoch [6/10], Step [61/600], Loss: 0.5649, Train Acc:0.8125\n",
      "Epoch [6/10], Step [62/600], Loss: 0.6012, Train Acc:0.8126\n",
      "Epoch [6/10], Step [63/600], Loss: 0.5392, Train Acc:0.8127\n",
      "Epoch [6/10], Step [64/600], Loss: 0.4502, Train Acc:0.8127\n",
      "Epoch [6/10], Step [65/600], Loss: 0.4964, Train Acc:0.8132\n",
      "Epoch [6/10], Step [66/600], Loss: 0.5287, Train Acc:0.8135\n",
      "Epoch [6/10], Step [67/600], Loss: 0.6418, Train Acc:0.8127\n",
      "Epoch [6/10], Step [68/600], Loss: 0.6752, Train Acc:0.8124\n",
      "Epoch [6/10], Step [69/600], Loss: 0.6204, Train Acc:0.8120\n",
      "Epoch [6/10], Step [70/600], Loss: 0.7023, Train Acc:0.8111\n",
      "Epoch [6/10], Step [71/600], Loss: 0.4846, Train Acc:0.8115\n",
      "Epoch [6/10], Step [72/600], Loss: 0.4825, Train Acc:0.8119\n",
      "Epoch [6/10], Step [73/600], Loss: 0.5087, Train Acc:0.8122\n",
      "Epoch [6/10], Step [74/600], Loss: 0.6882, Train Acc:0.8120\n",
      "Epoch [6/10], Step [75/600], Loss: 0.6249, Train Acc:0.8120\n",
      "Epoch [6/10], Step [76/600], Loss: 0.4719, Train Acc:0.8122\n",
      "Epoch [6/10], Step [77/600], Loss: 0.6876, Train Acc:0.8114\n",
      "Epoch [6/10], Step [78/600], Loss: 0.4738, Train Acc:0.8118\n",
      "Epoch [6/10], Step [79/600], Loss: 0.6060, Train Acc:0.8120\n",
      "Epoch [6/10], Step [80/600], Loss: 0.5785, Train Acc:0.8115\n",
      "Epoch [6/10], Step [81/600], Loss: 0.6458, Train Acc:0.8114\n",
      "Epoch [6/10], Step [82/600], Loss: 0.6865, Train Acc:0.8111\n",
      "Epoch [6/10], Step [83/600], Loss: 0.6085, Train Acc:0.8113\n",
      "Epoch [6/10], Step [84/600], Loss: 0.6055, Train Acc:0.8112\n",
      "Epoch [6/10], Step [85/600], Loss: 0.5881, Train Acc:0.8106\n",
      "Epoch [6/10], Step [86/600], Loss: 0.5509, Train Acc:0.8108\n",
      "Epoch [6/10], Step [87/600], Loss: 0.6414, Train Acc:0.8108\n",
      "Epoch [6/10], Step [88/600], Loss: 0.5074, Train Acc:0.8108\n",
      "Epoch [6/10], Step [89/600], Loss: 0.4329, Train Acc:0.8115\n",
      "Epoch [6/10], Step [90/600], Loss: 0.5963, Train Acc:0.8113\n",
      "Epoch [6/10], Step [91/600], Loss: 0.6495, Train Acc:0.8113\n",
      "Epoch [6/10], Step [92/600], Loss: 0.6454, Train Acc:0.8111\n",
      "Epoch [6/10], Step [93/600], Loss: 0.4986, Train Acc:0.8114\n",
      "Epoch [6/10], Step [94/600], Loss: 0.5260, Train Acc:0.8116\n",
      "Epoch [6/10], Step [95/600], Loss: 0.6800, Train Acc:0.8108\n",
      "Epoch [6/10], Step [96/600], Loss: 0.5432, Train Acc:0.8111\n",
      "Epoch [6/10], Step [97/600], Loss: 0.6699, Train Acc:0.8109\n",
      "Epoch [6/10], Step [98/600], Loss: 0.4641, Train Acc:0.8111\n",
      "Epoch [6/10], Step [99/600], Loss: 0.5543, Train Acc:0.8116\n",
      "Epoch [6/10], Step [100/600], Loss: 0.5314, Train Acc:0.8120\n",
      "Epoch [6/10], Step [101/600], Loss: 0.6165, Train Acc:0.8116\n",
      "Epoch [6/10], Step [102/600], Loss: 0.5897, Train Acc:0.8113\n",
      "Epoch [6/10], Step [103/600], Loss: 0.5387, Train Acc:0.8115\n",
      "Epoch [6/10], Step [104/600], Loss: 0.6221, Train Acc:0.8112\n",
      "Epoch [6/10], Step [105/600], Loss: 0.6032, Train Acc:0.8112\n",
      "Epoch [6/10], Step [106/600], Loss: 0.5242, Train Acc:0.8110\n",
      "Epoch [6/10], Step [107/600], Loss: 0.6270, Train Acc:0.8108\n",
      "Epoch [6/10], Step [108/600], Loss: 0.4904, Train Acc:0.8112\n",
      "Epoch [6/10], Step [109/600], Loss: 0.4156, Train Acc:0.8122\n",
      "Epoch [6/10], Step [110/600], Loss: 0.5197, Train Acc:0.8125\n",
      "Epoch [6/10], Step [111/600], Loss: 0.5458, Train Acc:0.8125\n",
      "Epoch [6/10], Step [112/600], Loss: 0.4014, Train Acc:0.8133\n",
      "Epoch [6/10], Step [113/600], Loss: 0.6609, Train Acc:0.8127\n",
      "Epoch [6/10], Step [114/600], Loss: 0.5060, Train Acc:0.8132\n",
      "Epoch [6/10], Step [115/600], Loss: 0.5559, Train Acc:0.8135\n",
      "Epoch [6/10], Step [116/600], Loss: 0.5645, Train Acc:0.8134\n",
      "Epoch [6/10], Step [117/600], Loss: 0.5897, Train Acc:0.8132\n",
      "Epoch [6/10], Step [118/600], Loss: 0.5107, Train Acc:0.8134\n",
      "Epoch [6/10], Step [119/600], Loss: 0.6783, Train Acc:0.8131\n",
      "Epoch [6/10], Step [120/600], Loss: 0.5272, Train Acc:0.8129\n",
      "Epoch [6/10], Step [121/600], Loss: 0.6340, Train Acc:0.8126\n",
      "Epoch [6/10], Step [122/600], Loss: 0.5915, Train Acc:0.8129\n",
      "Epoch [6/10], Step [123/600], Loss: 0.7776, Train Acc:0.8122\n",
      "Epoch [6/10], Step [124/600], Loss: 0.4956, Train Acc:0.8128\n",
      "Epoch [6/10], Step [125/600], Loss: 0.4630, Train Acc:0.8131\n",
      "Epoch [6/10], Step [126/600], Loss: 0.5318, Train Acc:0.8136\n",
      "Epoch [6/10], Step [127/600], Loss: 0.5366, Train Acc:0.8135\n",
      "Epoch [6/10], Step [128/600], Loss: 0.7465, Train Acc:0.8133\n",
      "Epoch [6/10], Step [129/600], Loss: 0.4502, Train Acc:0.8137\n",
      "Epoch [6/10], Step [130/600], Loss: 0.4373, Train Acc:0.8142\n",
      "Epoch [6/10], Step [131/600], Loss: 0.4544, Train Acc:0.8147\n",
      "Epoch [6/10], Step [132/600], Loss: 0.5083, Train Acc:0.8148\n",
      "Epoch [6/10], Step [133/600], Loss: 0.5952, Train Acc:0.8144\n",
      "Epoch [6/10], Step [134/600], Loss: 0.5865, Train Acc:0.8146\n",
      "Epoch [6/10], Step [135/600], Loss: 0.4579, Train Acc:0.8152\n",
      "Epoch [6/10], Step [136/600], Loss: 0.5902, Train Acc:0.8153\n",
      "Epoch [6/10], Step [137/600], Loss: 0.6451, Train Acc:0.8151\n",
      "Epoch [6/10], Step [138/600], Loss: 0.5369, Train Acc:0.8151\n",
      "Epoch [6/10], Step [139/600], Loss: 0.6472, Train Acc:0.8148\n",
      "Epoch [6/10], Step [140/600], Loss: 0.6247, Train Acc:0.8147\n",
      "Epoch [6/10], Step [141/600], Loss: 0.6376, Train Acc:0.8142\n",
      "Epoch [6/10], Step [142/600], Loss: 0.5826, Train Acc:0.8142\n",
      "Epoch [6/10], Step [143/600], Loss: 0.4599, Train Acc:0.8144\n",
      "Epoch [6/10], Step [144/600], Loss: 0.5919, Train Acc:0.8144\n",
      "Epoch [6/10], Step [145/600], Loss: 0.6277, Train Acc:0.8143\n",
      "Epoch [6/10], Step [146/600], Loss: 0.6724, Train Acc:0.8140\n",
      "Epoch [6/10], Step [147/600], Loss: 0.6084, Train Acc:0.8135\n",
      "Epoch [6/10], Step [148/600], Loss: 0.7304, Train Acc:0.8129\n",
      "Epoch [6/10], Step [149/600], Loss: 0.5450, Train Acc:0.8130\n",
      "Epoch [6/10], Step [150/600], Loss: 0.6543, Train Acc:0.8125\n",
      "Epoch [6/10], Step [151/600], Loss: 0.5753, Train Acc:0.8126\n",
      "Epoch [6/10], Step [152/600], Loss: 0.6428, Train Acc:0.8122\n",
      "Epoch [6/10], Step [153/600], Loss: 0.5387, Train Acc:0.8121\n",
      "Epoch [6/10], Step [154/600], Loss: 0.6298, Train Acc:0.8118\n",
      "Epoch [6/10], Step [155/600], Loss: 0.5769, Train Acc:0.8119\n",
      "Epoch [6/10], Step [156/600], Loss: 0.5443, Train Acc:0.8117\n",
      "Epoch [6/10], Step [157/600], Loss: 0.6202, Train Acc:0.8116\n",
      "Epoch [6/10], Step [158/600], Loss: 0.6096, Train Acc:0.8114\n",
      "Epoch [6/10], Step [159/600], Loss: 0.5362, Train Acc:0.8111\n",
      "Epoch [6/10], Step [160/600], Loss: 0.6647, Train Acc:0.8108\n",
      "Epoch [6/10], Step [161/600], Loss: 0.5470, Train Acc:0.8108\n",
      "Epoch [6/10], Step [162/600], Loss: 0.5944, Train Acc:0.8107\n",
      "Epoch [6/10], Step [163/600], Loss: 0.5933, Train Acc:0.8107\n",
      "Epoch [6/10], Step [164/600], Loss: 0.5131, Train Acc:0.8109\n",
      "Epoch [6/10], Step [165/600], Loss: 0.6621, Train Acc:0.8105\n",
      "Epoch [6/10], Step [166/600], Loss: 0.4381, Train Acc:0.8107\n",
      "Epoch [6/10], Step [167/600], Loss: 0.4827, Train Acc:0.8109\n",
      "Epoch [6/10], Step [168/600], Loss: 0.6080, Train Acc:0.8106\n",
      "Epoch [6/10], Step [169/600], Loss: 0.5617, Train Acc:0.8106\n",
      "Epoch [6/10], Step [170/600], Loss: 0.5030, Train Acc:0.8111\n",
      "Epoch [6/10], Step [171/600], Loss: 0.5073, Train Acc:0.8115\n",
      "Epoch [6/10], Step [172/600], Loss: 0.4612, Train Acc:0.8120\n",
      "Epoch [6/10], Step [173/600], Loss: 0.7094, Train Acc:0.8113\n",
      "Epoch [6/10], Step [174/600], Loss: 0.5807, Train Acc:0.8113\n",
      "Epoch [6/10], Step [175/600], Loss: 0.5139, Train Acc:0.8115\n",
      "Epoch [6/10], Step [176/600], Loss: 0.4865, Train Acc:0.8118\n",
      "Epoch [6/10], Step [177/600], Loss: 0.5315, Train Acc:0.8118\n",
      "Epoch [6/10], Step [178/600], Loss: 0.5030, Train Acc:0.8121\n",
      "Epoch [6/10], Step [179/600], Loss: 0.6204, Train Acc:0.8120\n",
      "Epoch [6/10], Step [180/600], Loss: 0.5169, Train Acc:0.8122\n",
      "Epoch [6/10], Step [181/600], Loss: 0.6755, Train Acc:0.8121\n",
      "Epoch [6/10], Step [182/600], Loss: 0.4762, Train Acc:0.8123\n",
      "Epoch [6/10], Step [183/600], Loss: 0.5546, Train Acc:0.8124\n",
      "Epoch [6/10], Step [184/600], Loss: 0.6587, Train Acc:0.8121\n",
      "Epoch [6/10], Step [185/600], Loss: 0.4747, Train Acc:0.8123\n",
      "Epoch [6/10], Step [186/600], Loss: 0.6031, Train Acc:0.8122\n",
      "Epoch [6/10], Step [187/600], Loss: 0.4949, Train Acc:0.8126\n",
      "Epoch [6/10], Step [188/600], Loss: 0.4906, Train Acc:0.8126\n",
      "Epoch [6/10], Step [189/600], Loss: 0.5550, Train Acc:0.8125\n",
      "Epoch [6/10], Step [190/600], Loss: 0.5575, Train Acc:0.8125\n",
      "Epoch [6/10], Step [191/600], Loss: 0.6133, Train Acc:0.8123\n",
      "Epoch [6/10], Step [192/600], Loss: 0.5955, Train Acc:0.8122\n",
      "Epoch [6/10], Step [193/600], Loss: 0.4239, Train Acc:0.8126\n",
      "Epoch [6/10], Step [194/600], Loss: 0.6261, Train Acc:0.8128\n",
      "Epoch [6/10], Step [195/600], Loss: 0.5849, Train Acc:0.8127\n",
      "Epoch [6/10], Step [196/600], Loss: 0.5213, Train Acc:0.8128\n",
      "Epoch [6/10], Step [197/600], Loss: 0.7729, Train Acc:0.8121\n",
      "Epoch [6/10], Step [198/600], Loss: 0.6225, Train Acc:0.8119\n",
      "Epoch [6/10], Step [199/600], Loss: 0.5798, Train Acc:0.8118\n",
      "Epoch [6/10], Step [200/600], Loss: 0.4756, Train Acc:0.8119\n",
      "Epoch [6/10], Step [201/600], Loss: 0.6387, Train Acc:0.8116\n",
      "Epoch [6/10], Step [202/600], Loss: 0.6368, Train Acc:0.8114\n",
      "Epoch [6/10], Step [203/600], Loss: 0.5688, Train Acc:0.8113\n",
      "Epoch [6/10], Step [204/600], Loss: 0.4891, Train Acc:0.8114\n",
      "Epoch [6/10], Step [205/600], Loss: 0.5445, Train Acc:0.8115\n",
      "Epoch [6/10], Step [206/600], Loss: 0.5141, Train Acc:0.8116\n",
      "Epoch [6/10], Step [207/600], Loss: 0.5089, Train Acc:0.8118\n",
      "Epoch [6/10], Step [208/600], Loss: 0.5622, Train Acc:0.8116\n",
      "Epoch [6/10], Step [209/600], Loss: 0.4922, Train Acc:0.8117\n",
      "Epoch [6/10], Step [210/600], Loss: 0.5125, Train Acc:0.8120\n",
      "Epoch [6/10], Step [211/600], Loss: 0.4896, Train Acc:0.8124\n",
      "Epoch [6/10], Step [212/600], Loss: 0.4159, Train Acc:0.8127\n",
      "Epoch [6/10], Step [213/600], Loss: 0.6062, Train Acc:0.8124\n",
      "Epoch [6/10], Step [214/600], Loss: 0.5663, Train Acc:0.8126\n",
      "Epoch [6/10], Step [215/600], Loss: 0.5850, Train Acc:0.8126\n",
      "Epoch [6/10], Step [216/600], Loss: 0.4136, Train Acc:0.8130\n",
      "Epoch [6/10], Step [217/600], Loss: 0.5046, Train Acc:0.8131\n",
      "Epoch [6/10], Step [218/600], Loss: 0.6794, Train Acc:0.8130\n",
      "Epoch [6/10], Step [219/600], Loss: 0.7302, Train Acc:0.8128\n",
      "Epoch [6/10], Step [220/600], Loss: 0.5245, Train Acc:0.8129\n",
      "Epoch [6/10], Step [221/600], Loss: 0.5947, Train Acc:0.8129\n",
      "Epoch [6/10], Step [222/600], Loss: 0.5452, Train Acc:0.8129\n",
      "Epoch [6/10], Step [223/600], Loss: 0.6153, Train Acc:0.8128\n",
      "Epoch [6/10], Step [224/600], Loss: 0.4698, Train Acc:0.8131\n",
      "Epoch [6/10], Step [225/600], Loss: 0.6440, Train Acc:0.8130\n",
      "Epoch [6/10], Step [226/600], Loss: 0.5892, Train Acc:0.8131\n",
      "Epoch [6/10], Step [227/600], Loss: 0.5166, Train Acc:0.8133\n",
      "Epoch [6/10], Step [228/600], Loss: 0.5411, Train Acc:0.8134\n",
      "Epoch [6/10], Step [229/600], Loss: 0.4421, Train Acc:0.8136\n",
      "Epoch [6/10], Step [230/600], Loss: 0.4877, Train Acc:0.8137\n",
      "Epoch [6/10], Step [231/600], Loss: 0.5542, Train Acc:0.8138\n",
      "Epoch [6/10], Step [232/600], Loss: 0.6341, Train Acc:0.8137\n",
      "Epoch [6/10], Step [233/600], Loss: 0.4913, Train Acc:0.8138\n",
      "Epoch [6/10], Step [234/600], Loss: 0.5759, Train Acc:0.8138\n",
      "Epoch [6/10], Step [235/600], Loss: 0.5939, Train Acc:0.8139\n",
      "Epoch [6/10], Step [236/600], Loss: 0.5946, Train Acc:0.8139\n",
      "Epoch [6/10], Step [237/600], Loss: 0.5741, Train Acc:0.8140\n",
      "Epoch [6/10], Step [238/600], Loss: 0.4591, Train Acc:0.8141\n",
      "Epoch [6/10], Step [239/600], Loss: 0.8244, Train Acc:0.8139\n",
      "Epoch [6/10], Step [240/600], Loss: 0.6085, Train Acc:0.8138\n",
      "Epoch [6/10], Step [241/600], Loss: 0.4147, Train Acc:0.8141\n",
      "Epoch [6/10], Step [242/600], Loss: 0.5285, Train Acc:0.8142\n",
      "Epoch [6/10], Step [243/600], Loss: 0.6135, Train Acc:0.8140\n",
      "Epoch [6/10], Step [244/600], Loss: 0.6382, Train Acc:0.8139\n",
      "Epoch [6/10], Step [245/600], Loss: 0.6787, Train Acc:0.8139\n",
      "Epoch [6/10], Step [246/600], Loss: 0.5456, Train Acc:0.8139\n",
      "Epoch [6/10], Step [247/600], Loss: 0.4965, Train Acc:0.8141\n",
      "Epoch [6/10], Step [248/600], Loss: 0.6732, Train Acc:0.8141\n",
      "Epoch [6/10], Step [249/600], Loss: 0.5513, Train Acc:0.8141\n",
      "Epoch [6/10], Step [250/600], Loss: 0.5319, Train Acc:0.8142\n",
      "Epoch [6/10], Step [251/600], Loss: 0.4590, Train Acc:0.8146\n",
      "Epoch [6/10], Step [252/600], Loss: 0.5464, Train Acc:0.8148\n",
      "Epoch [6/10], Step [253/600], Loss: 0.4756, Train Acc:0.8150\n",
      "Epoch [6/10], Step [254/600], Loss: 0.6969, Train Acc:0.8148\n",
      "Epoch [6/10], Step [255/600], Loss: 0.6836, Train Acc:0.8148\n",
      "Epoch [6/10], Step [256/600], Loss: 0.4819, Train Acc:0.8148\n",
      "Epoch [6/10], Step [257/600], Loss: 0.4834, Train Acc:0.8149\n",
      "Epoch [6/10], Step [258/600], Loss: 0.6163, Train Acc:0.8148\n",
      "Epoch [6/10], Step [259/600], Loss: 0.5903, Train Acc:0.8148\n",
      "Epoch [6/10], Step [260/600], Loss: 0.5980, Train Acc:0.8149\n",
      "Epoch [6/10], Step [261/600], Loss: 0.6776, Train Acc:0.8150\n",
      "Epoch [6/10], Step [262/600], Loss: 0.7998, Train Acc:0.8148\n",
      "Epoch [6/10], Step [263/600], Loss: 0.5022, Train Acc:0.8148\n",
      "Epoch [6/10], Step [264/600], Loss: 0.6795, Train Acc:0.8145\n",
      "Epoch [6/10], Step [265/600], Loss: 0.5178, Train Acc:0.8146\n",
      "Epoch [6/10], Step [266/600], Loss: 0.6619, Train Acc:0.8145\n",
      "Epoch [6/10], Step [267/600], Loss: 0.5497, Train Acc:0.8146\n",
      "Epoch [6/10], Step [268/600], Loss: 0.7517, Train Acc:0.8145\n",
      "Epoch [6/10], Step [269/600], Loss: 0.6350, Train Acc:0.8143\n",
      "Epoch [6/10], Step [270/600], Loss: 0.7129, Train Acc:0.8142\n",
      "Epoch [6/10], Step [271/600], Loss: 0.6269, Train Acc:0.8141\n",
      "Epoch [6/10], Step [272/600], Loss: 0.5043, Train Acc:0.8142\n",
      "Epoch [6/10], Step [273/600], Loss: 0.5723, Train Acc:0.8142\n",
      "Epoch [6/10], Step [274/600], Loss: 0.7781, Train Acc:0.8140\n",
      "Epoch [6/10], Step [275/600], Loss: 0.6059, Train Acc:0.8140\n",
      "Epoch [6/10], Step [276/600], Loss: 0.5374, Train Acc:0.8141\n",
      "Epoch [6/10], Step [277/600], Loss: 0.5291, Train Acc:0.8140\n",
      "Epoch [6/10], Step [278/600], Loss: 0.5441, Train Acc:0.8140\n",
      "Epoch [6/10], Step [279/600], Loss: 0.5551, Train Acc:0.8141\n",
      "Epoch [6/10], Step [280/600], Loss: 0.6204, Train Acc:0.8140\n",
      "Epoch [6/10], Step [281/600], Loss: 0.5379, Train Acc:0.8143\n",
      "Epoch [6/10], Step [282/600], Loss: 0.4144, Train Acc:0.8143\n",
      "Epoch [6/10], Step [283/600], Loss: 0.5250, Train Acc:0.8145\n",
      "Epoch [6/10], Step [284/600], Loss: 0.5161, Train Acc:0.8145\n",
      "Epoch [6/10], Step [285/600], Loss: 0.6606, Train Acc:0.8143\n",
      "Epoch [6/10], Step [286/600], Loss: 0.5936, Train Acc:0.8143\n",
      "Epoch [6/10], Step [287/600], Loss: 0.4897, Train Acc:0.8145\n",
      "Epoch [6/10], Step [288/600], Loss: 0.6109, Train Acc:0.8145\n",
      "Epoch [6/10], Step [289/600], Loss: 0.6078, Train Acc:0.8144\n",
      "Epoch [6/10], Step [290/600], Loss: 0.5686, Train Acc:0.8142\n",
      "Epoch [6/10], Step [291/600], Loss: 0.6410, Train Acc:0.8142\n",
      "Epoch [6/10], Step [292/600], Loss: 0.4855, Train Acc:0.8144\n",
      "Epoch [6/10], Step [293/600], Loss: 0.5468, Train Acc:0.8144\n",
      "Epoch [6/10], Step [294/600], Loss: 0.4738, Train Acc:0.8146\n",
      "Epoch [6/10], Step [295/600], Loss: 0.5112, Train Acc:0.8147\n",
      "Epoch [6/10], Step [296/600], Loss: 0.5595, Train Acc:0.8148\n",
      "Epoch [6/10], Step [297/600], Loss: 0.5878, Train Acc:0.8147\n",
      "Epoch [6/10], Step [298/600], Loss: 0.5035, Train Acc:0.8149\n",
      "Epoch [6/10], Step [299/600], Loss: 0.5597, Train Acc:0.8149\n",
      "Epoch [6/10], Step [300/600], Loss: 0.5167, Train Acc:0.8152\n",
      "Epoch [6/10], Step [301/600], Loss: 0.6250, Train Acc:0.8151\n",
      "Epoch [6/10], Step [302/600], Loss: 0.5488, Train Acc:0.8151\n",
      "Epoch [6/10], Step [303/600], Loss: 0.5312, Train Acc:0.8150\n",
      "Epoch [6/10], Step [304/600], Loss: 0.4303, Train Acc:0.8154\n",
      "Epoch [6/10], Step [305/600], Loss: 0.6430, Train Acc:0.8153\n",
      "Epoch [6/10], Step [306/600], Loss: 0.8733, Train Acc:0.8149\n",
      "Epoch [6/10], Step [307/600], Loss: 0.4826, Train Acc:0.8151\n",
      "Epoch [6/10], Step [308/600], Loss: 0.5913, Train Acc:0.8149\n",
      "Epoch [6/10], Step [309/600], Loss: 0.6167, Train Acc:0.8149\n",
      "Epoch [6/10], Step [310/600], Loss: 0.5011, Train Acc:0.8150\n",
      "Epoch [6/10], Step [311/600], Loss: 0.5584, Train Acc:0.8149\n",
      "Epoch [6/10], Step [312/600], Loss: 0.5602, Train Acc:0.8148\n",
      "Epoch [6/10], Step [313/600], Loss: 0.5997, Train Acc:0.8148\n",
      "Epoch [6/10], Step [314/600], Loss: 0.5088, Train Acc:0.8149\n",
      "Epoch [6/10], Step [315/600], Loss: 0.7576, Train Acc:0.8148\n",
      "Epoch [6/10], Step [316/600], Loss: 0.4395, Train Acc:0.8150\n",
      "Epoch [6/10], Step [317/600], Loss: 0.5699, Train Acc:0.8149\n",
      "Epoch [6/10], Step [318/600], Loss: 0.5065, Train Acc:0.8149\n",
      "Epoch [6/10], Step [319/600], Loss: 0.7008, Train Acc:0.8146\n",
      "Epoch [6/10], Step [320/600], Loss: 0.4974, Train Acc:0.8147\n",
      "Epoch [6/10], Step [321/600], Loss: 0.6500, Train Acc:0.8145\n",
      "Epoch [6/10], Step [322/600], Loss: 0.4390, Train Acc:0.8148\n",
      "Epoch [6/10], Step [323/600], Loss: 0.4824, Train Acc:0.8150\n",
      "Epoch [6/10], Step [324/600], Loss: 0.6939, Train Acc:0.8147\n",
      "Epoch [6/10], Step [325/600], Loss: 0.5676, Train Acc:0.8147\n",
      "Epoch [6/10], Step [326/600], Loss: 0.4354, Train Acc:0.8149\n",
      "Epoch [6/10], Step [327/600], Loss: 0.6001, Train Acc:0.8150\n",
      "Epoch [6/10], Step [328/600], Loss: 0.6703, Train Acc:0.8149\n",
      "Epoch [6/10], Step [329/600], Loss: 0.4551, Train Acc:0.8151\n",
      "Epoch [6/10], Step [330/600], Loss: 0.6319, Train Acc:0.8150\n",
      "Epoch [6/10], Step [331/600], Loss: 0.6235, Train Acc:0.8149\n",
      "Epoch [6/10], Step [332/600], Loss: 0.4628, Train Acc:0.8150\n",
      "Epoch [6/10], Step [333/600], Loss: 0.5519, Train Acc:0.8150\n",
      "Epoch [6/10], Step [334/600], Loss: 0.6049, Train Acc:0.8150\n",
      "Epoch [6/10], Step [335/600], Loss: 0.5947, Train Acc:0.8150\n",
      "Epoch [6/10], Step [336/600], Loss: 0.5863, Train Acc:0.8150\n",
      "Epoch [6/10], Step [337/600], Loss: 0.4983, Train Acc:0.8152\n",
      "Epoch [6/10], Step [338/600], Loss: 0.4855, Train Acc:0.8153\n",
      "Epoch [6/10], Step [339/600], Loss: 0.4052, Train Acc:0.8154\n",
      "Epoch [6/10], Step [340/600], Loss: 0.6805, Train Acc:0.8154\n",
      "Epoch [6/10], Step [341/600], Loss: 0.6167, Train Acc:0.8152\n",
      "Epoch [6/10], Step [342/600], Loss: 0.5571, Train Acc:0.8153\n",
      "Epoch [6/10], Step [343/600], Loss: 0.5816, Train Acc:0.8153\n",
      "Epoch [6/10], Step [344/600], Loss: 0.4943, Train Acc:0.8155\n",
      "Epoch [6/10], Step [345/600], Loss: 0.6090, Train Acc:0.8154\n",
      "Epoch [6/10], Step [346/600], Loss: 0.5667, Train Acc:0.8154\n",
      "Epoch [6/10], Step [347/600], Loss: 0.7400, Train Acc:0.8152\n",
      "Epoch [6/10], Step [348/600], Loss: 0.6174, Train Acc:0.8153\n",
      "Epoch [6/10], Step [349/600], Loss: 0.5828, Train Acc:0.8152\n",
      "Epoch [6/10], Step [350/600], Loss: 0.5958, Train Acc:0.8153\n",
      "Epoch [6/10], Step [351/600], Loss: 0.5713, Train Acc:0.8152\n",
      "Epoch [6/10], Step [352/600], Loss: 0.5070, Train Acc:0.8153\n",
      "Epoch [6/10], Step [353/600], Loss: 0.7276, Train Acc:0.8150\n",
      "Epoch [6/10], Step [354/600], Loss: 0.7348, Train Acc:0.8149\n",
      "Epoch [6/10], Step [355/600], Loss: 0.5926, Train Acc:0.8147\n",
      "Epoch [6/10], Step [356/600], Loss: 0.5600, Train Acc:0.8147\n",
      "Epoch [6/10], Step [357/600], Loss: 0.5923, Train Acc:0.8148\n",
      "Epoch [6/10], Step [358/600], Loss: 0.5130, Train Acc:0.8149\n",
      "Epoch [6/10], Step [359/600], Loss: 0.5714, Train Acc:0.8149\n",
      "Epoch [6/10], Step [360/600], Loss: 0.5264, Train Acc:0.8149\n",
      "Epoch [6/10], Step [361/600], Loss: 0.5877, Train Acc:0.8148\n",
      "Epoch [6/10], Step [362/600], Loss: 0.5590, Train Acc:0.8149\n",
      "Epoch [6/10], Step [363/600], Loss: 0.5842, Train Acc:0.8150\n",
      "Epoch [6/10], Step [364/600], Loss: 0.5637, Train Acc:0.8151\n",
      "Epoch [6/10], Step [365/600], Loss: 0.5898, Train Acc:0.8151\n",
      "Epoch [6/10], Step [366/600], Loss: 0.6130, Train Acc:0.8151\n",
      "Epoch [6/10], Step [367/600], Loss: 0.5230, Train Acc:0.8151\n",
      "Epoch [6/10], Step [368/600], Loss: 0.6178, Train Acc:0.8151\n",
      "Epoch [6/10], Step [369/600], Loss: 0.6111, Train Acc:0.8151\n",
      "Epoch [6/10], Step [370/600], Loss: 0.6064, Train Acc:0.8150\n",
      "Epoch [6/10], Step [371/600], Loss: 0.4119, Train Acc:0.8152\n",
      "Epoch [6/10], Step [372/600], Loss: 0.4694, Train Acc:0.8152\n",
      "Epoch [6/10], Step [373/600], Loss: 0.4397, Train Acc:0.8154\n",
      "Epoch [6/10], Step [374/600], Loss: 0.5810, Train Acc:0.8153\n",
      "Epoch [6/10], Step [375/600], Loss: 0.5379, Train Acc:0.8153\n",
      "Epoch [6/10], Step [376/600], Loss: 0.5341, Train Acc:0.8154\n",
      "Epoch [6/10], Step [377/600], Loss: 0.4914, Train Acc:0.8156\n",
      "Epoch [6/10], Step [378/600], Loss: 0.5792, Train Acc:0.8155\n",
      "Epoch [6/10], Step [379/600], Loss: 0.6777, Train Acc:0.8155\n",
      "Epoch [6/10], Step [380/600], Loss: 0.5083, Train Acc:0.8155\n",
      "Epoch [6/10], Step [381/600], Loss: 0.5176, Train Acc:0.8156\n",
      "Epoch [6/10], Step [382/600], Loss: 0.6093, Train Acc:0.8155\n",
      "Epoch [6/10], Step [383/600], Loss: 0.6074, Train Acc:0.8156\n",
      "Epoch [6/10], Step [384/600], Loss: 0.5232, Train Acc:0.8157\n",
      "Epoch [6/10], Step [385/600], Loss: 0.5506, Train Acc:0.8157\n",
      "Epoch [6/10], Step [386/600], Loss: 0.4717, Train Acc:0.8158\n",
      "Epoch [6/10], Step [387/600], Loss: 0.4716, Train Acc:0.8159\n",
      "Epoch [6/10], Step [388/600], Loss: 0.6566, Train Acc:0.8158\n",
      "Epoch [6/10], Step [389/600], Loss: 0.7119, Train Acc:0.8157\n",
      "Epoch [6/10], Step [390/600], Loss: 0.5544, Train Acc:0.8159\n",
      "Epoch [6/10], Step [391/600], Loss: 0.7143, Train Acc:0.8156\n",
      "Epoch [6/10], Step [392/600], Loss: 0.4578, Train Acc:0.8157\n",
      "Epoch [6/10], Step [393/600], Loss: 0.4521, Train Acc:0.8159\n",
      "Epoch [6/10], Step [394/600], Loss: 0.7100, Train Acc:0.8158\n",
      "Epoch [6/10], Step [395/600], Loss: 0.6234, Train Acc:0.8158\n",
      "Epoch [6/10], Step [396/600], Loss: 0.5467, Train Acc:0.8159\n",
      "Epoch [6/10], Step [397/600], Loss: 0.5849, Train Acc:0.8160\n",
      "Epoch [6/10], Step [398/600], Loss: 0.5309, Train Acc:0.8160\n",
      "Epoch [6/10], Step [399/600], Loss: 0.6022, Train Acc:0.8160\n",
      "Epoch [6/10], Step [400/600], Loss: 0.6051, Train Acc:0.8159\n",
      "Epoch [6/10], Step [401/600], Loss: 0.5450, Train Acc:0.8158\n",
      "Epoch [6/10], Step [402/600], Loss: 0.5236, Train Acc:0.8158\n",
      "Epoch [6/10], Step [403/600], Loss: 0.4661, Train Acc:0.8160\n",
      "Epoch [6/10], Step [404/600], Loss: 0.5241, Train Acc:0.8162\n",
      "Epoch [6/10], Step [405/600], Loss: 0.5565, Train Acc:0.8162\n",
      "Epoch [6/10], Step [406/600], Loss: 0.6792, Train Acc:0.8162\n",
      "Epoch [6/10], Step [407/600], Loss: 0.5447, Train Acc:0.8162\n",
      "Epoch [6/10], Step [408/600], Loss: 0.6206, Train Acc:0.8162\n",
      "Epoch [6/10], Step [409/600], Loss: 0.4880, Train Acc:0.8163\n",
      "Epoch [6/10], Step [410/600], Loss: 0.6526, Train Acc:0.8162\n",
      "Epoch [6/10], Step [411/600], Loss: 0.5192, Train Acc:0.8164\n",
      "Epoch [6/10], Step [412/600], Loss: 0.6207, Train Acc:0.8163\n",
      "Epoch [6/10], Step [413/600], Loss: 0.6104, Train Acc:0.8162\n",
      "Epoch [6/10], Step [414/600], Loss: 0.4740, Train Acc:0.8163\n",
      "Epoch [6/10], Step [415/600], Loss: 0.6137, Train Acc:0.8163\n",
      "Epoch [6/10], Step [416/600], Loss: 0.5256, Train Acc:0.8163\n",
      "Epoch [6/10], Step [417/600], Loss: 0.5453, Train Acc:0.8163\n",
      "Epoch [6/10], Step [418/600], Loss: 0.5329, Train Acc:0.8164\n",
      "Epoch [6/10], Step [419/600], Loss: 0.5734, Train Acc:0.8164\n",
      "Epoch [6/10], Step [420/600], Loss: 0.6346, Train Acc:0.8163\n",
      "Epoch [6/10], Step [421/600], Loss: 0.5001, Train Acc:0.8163\n",
      "Epoch [6/10], Step [422/600], Loss: 0.6104, Train Acc:0.8162\n",
      "Epoch [6/10], Step [423/600], Loss: 0.5917, Train Acc:0.8161\n",
      "Epoch [6/10], Step [424/600], Loss: 0.4151, Train Acc:0.8163\n",
      "Epoch [6/10], Step [425/600], Loss: 0.6908, Train Acc:0.8162\n",
      "Epoch [6/10], Step [426/600], Loss: 0.6052, Train Acc:0.8161\n",
      "Epoch [6/10], Step [427/600], Loss: 0.5296, Train Acc:0.8161\n",
      "Epoch [6/10], Step [428/600], Loss: 0.5275, Train Acc:0.8161\n",
      "Epoch [6/10], Step [429/600], Loss: 0.6160, Train Acc:0.8161\n",
      "Epoch [6/10], Step [430/600], Loss: 0.6876, Train Acc:0.8159\n",
      "Epoch [6/10], Step [431/600], Loss: 0.4679, Train Acc:0.8160\n",
      "Epoch [6/10], Step [432/600], Loss: 0.6256, Train Acc:0.8159\n",
      "Epoch [6/10], Step [433/600], Loss: 0.5467, Train Acc:0.8160\n",
      "Epoch [6/10], Step [434/600], Loss: 0.6311, Train Acc:0.8159\n",
      "Epoch [6/10], Step [435/600], Loss: 0.7208, Train Acc:0.8158\n",
      "Epoch [6/10], Step [436/600], Loss: 0.6272, Train Acc:0.8157\n",
      "Epoch [6/10], Step [437/600], Loss: 0.6728, Train Acc:0.8157\n",
      "Epoch [6/10], Step [438/600], Loss: 0.4885, Train Acc:0.8158\n",
      "Epoch [6/10], Step [439/600], Loss: 0.5316, Train Acc:0.8159\n",
      "Epoch [6/10], Step [440/600], Loss: 0.4133, Train Acc:0.8161\n",
      "Epoch [6/10], Step [441/600], Loss: 0.5938, Train Acc:0.8160\n",
      "Epoch [6/10], Step [442/600], Loss: 0.5377, Train Acc:0.8160\n",
      "Epoch [6/10], Step [443/600], Loss: 0.5960, Train Acc:0.8159\n",
      "Epoch [6/10], Step [444/600], Loss: 0.6519, Train Acc:0.8158\n",
      "Epoch [6/10], Step [445/600], Loss: 0.5305, Train Acc:0.8159\n",
      "Epoch [6/10], Step [446/600], Loss: 0.5564, Train Acc:0.8159\n",
      "Epoch [6/10], Step [447/600], Loss: 0.4942, Train Acc:0.8158\n",
      "Epoch [6/10], Step [448/600], Loss: 0.5700, Train Acc:0.8158\n",
      "Epoch [6/10], Step [449/600], Loss: 0.6402, Train Acc:0.8158\n",
      "Epoch [6/10], Step [450/600], Loss: 0.4380, Train Acc:0.8159\n",
      "Epoch [6/10], Step [451/600], Loss: 0.4926, Train Acc:0.8160\n",
      "Epoch [6/10], Step [452/600], Loss: 0.6231, Train Acc:0.8160\n",
      "Epoch [6/10], Step [453/600], Loss: 0.5560, Train Acc:0.8160\n",
      "Epoch [6/10], Step [454/600], Loss: 0.4840, Train Acc:0.8160\n",
      "Epoch [6/10], Step [455/600], Loss: 0.7846, Train Acc:0.8157\n",
      "Epoch [6/10], Step [456/600], Loss: 0.6010, Train Acc:0.8157\n",
      "Epoch [6/10], Step [457/600], Loss: 0.5743, Train Acc:0.8157\n",
      "Epoch [6/10], Step [458/600], Loss: 0.4608, Train Acc:0.8158\n",
      "Epoch [6/10], Step [459/600], Loss: 0.4450, Train Acc:0.8159\n",
      "Epoch [6/10], Step [460/600], Loss: 0.4624, Train Acc:0.8160\n",
      "Epoch [6/10], Step [461/600], Loss: 0.5417, Train Acc:0.8161\n",
      "Epoch [6/10], Step [462/600], Loss: 0.6688, Train Acc:0.8160\n",
      "Epoch [6/10], Step [463/600], Loss: 0.7265, Train Acc:0.8158\n",
      "Epoch [6/10], Step [464/600], Loss: 0.5609, Train Acc:0.8158\n",
      "Epoch [6/10], Step [465/600], Loss: 0.5489, Train Acc:0.8159\n",
      "Epoch [6/10], Step [466/600], Loss: 0.6630, Train Acc:0.8158\n",
      "Epoch [6/10], Step [467/600], Loss: 0.6556, Train Acc:0.8158\n",
      "Epoch [6/10], Step [468/600], Loss: 0.5070, Train Acc:0.8158\n",
      "Epoch [6/10], Step [469/600], Loss: 0.5303, Train Acc:0.8158\n",
      "Epoch [6/10], Step [470/600], Loss: 0.5159, Train Acc:0.8158\n",
      "Epoch [6/10], Step [471/600], Loss: 0.7550, Train Acc:0.8157\n",
      "Epoch [6/10], Step [472/600], Loss: 0.5249, Train Acc:0.8158\n",
      "Epoch [6/10], Step [473/600], Loss: 0.7095, Train Acc:0.8158\n",
      "Epoch [6/10], Step [474/600], Loss: 0.6160, Train Acc:0.8157\n",
      "Epoch [6/10], Step [475/600], Loss: 0.4815, Train Acc:0.8158\n",
      "Epoch [6/10], Step [476/600], Loss: 0.5647, Train Acc:0.8158\n",
      "Epoch [6/10], Step [477/600], Loss: 0.6355, Train Acc:0.8158\n",
      "Epoch [6/10], Step [478/600], Loss: 0.5327, Train Acc:0.8159\n",
      "Epoch [6/10], Step [479/600], Loss: 0.6438, Train Acc:0.8158\n",
      "Epoch [6/10], Step [480/600], Loss: 0.5802, Train Acc:0.8158\n",
      "Epoch [6/10], Step [481/600], Loss: 0.5232, Train Acc:0.8159\n",
      "Epoch [6/10], Step [482/600], Loss: 0.6368, Train Acc:0.8158\n",
      "Epoch [6/10], Step [483/600], Loss: 0.6676, Train Acc:0.8156\n",
      "Epoch [6/10], Step [484/600], Loss: 0.6092, Train Acc:0.8156\n",
      "Epoch [6/10], Step [485/600], Loss: 0.5926, Train Acc:0.8156\n",
      "Epoch [6/10], Step [486/600], Loss: 0.5834, Train Acc:0.8156\n",
      "Epoch [6/10], Step [487/600], Loss: 0.5358, Train Acc:0.8157\n",
      "Epoch [6/10], Step [488/600], Loss: 0.5837, Train Acc:0.8156\n",
      "Epoch [6/10], Step [489/600], Loss: 0.5903, Train Acc:0.8155\n",
      "Epoch [6/10], Step [490/600], Loss: 0.4507, Train Acc:0.8157\n",
      "Epoch [6/10], Step [491/600], Loss: 0.4542, Train Acc:0.8158\n",
      "Epoch [6/10], Step [492/600], Loss: 0.6271, Train Acc:0.8158\n",
      "Epoch [6/10], Step [493/600], Loss: 0.5480, Train Acc:0.8158\n",
      "Epoch [6/10], Step [494/600], Loss: 0.5293, Train Acc:0.8158\n",
      "Epoch [6/10], Step [495/600], Loss: 0.4722, Train Acc:0.8159\n",
      "Epoch [6/10], Step [496/600], Loss: 0.5783, Train Acc:0.8158\n",
      "Epoch [6/10], Step [497/600], Loss: 0.6123, Train Acc:0.8159\n",
      "Epoch [6/10], Step [498/600], Loss: 0.4486, Train Acc:0.8160\n",
      "Epoch [6/10], Step [499/600], Loss: 0.4797, Train Acc:0.8161\n",
      "Epoch [6/10], Step [500/600], Loss: 0.4796, Train Acc:0.8162\n",
      "Epoch [6/10], Step [501/600], Loss: 0.7154, Train Acc:0.8161\n",
      "Epoch [6/10], Step [502/600], Loss: 0.6723, Train Acc:0.8159\n",
      "Epoch [6/10], Step [503/600], Loss: 0.6052, Train Acc:0.8159\n",
      "Epoch [6/10], Step [504/600], Loss: 0.4887, Train Acc:0.8160\n",
      "Epoch [6/10], Step [505/600], Loss: 0.5141, Train Acc:0.8160\n",
      "Epoch [6/10], Step [506/600], Loss: 0.6249, Train Acc:0.8160\n",
      "Epoch [6/10], Step [507/600], Loss: 0.4944, Train Acc:0.8161\n",
      "Epoch [6/10], Step [508/600], Loss: 0.5762, Train Acc:0.8160\n",
      "Epoch [6/10], Step [509/600], Loss: 0.5512, Train Acc:0.8159\n",
      "Epoch [6/10], Step [510/600], Loss: 0.4975, Train Acc:0.8160\n",
      "Epoch [6/10], Step [511/600], Loss: 0.6700, Train Acc:0.8159\n",
      "Epoch [6/10], Step [512/600], Loss: 0.5589, Train Acc:0.8159\n",
      "Epoch [6/10], Step [513/600], Loss: 0.4803, Train Acc:0.8160\n",
      "Epoch [6/10], Step [514/600], Loss: 0.4932, Train Acc:0.8160\n",
      "Epoch [6/10], Step [515/600], Loss: 0.5897, Train Acc:0.8160\n",
      "Epoch [6/10], Step [516/600], Loss: 0.5636, Train Acc:0.8161\n",
      "Epoch [6/10], Step [517/600], Loss: 0.5664, Train Acc:0.8161\n",
      "Epoch [6/10], Step [518/600], Loss: 0.5699, Train Acc:0.8160\n",
      "Epoch [6/10], Step [519/600], Loss: 0.6049, Train Acc:0.8160\n",
      "Epoch [6/10], Step [520/600], Loss: 0.4633, Train Acc:0.8160\n",
      "Epoch [6/10], Step [521/600], Loss: 0.6176, Train Acc:0.8159\n",
      "Epoch [6/10], Step [522/600], Loss: 0.4767, Train Acc:0.8160\n",
      "Epoch [6/10], Step [523/600], Loss: 0.5714, Train Acc:0.8160\n",
      "Epoch [6/10], Step [524/600], Loss: 0.5522, Train Acc:0.8160\n",
      "Epoch [6/10], Step [525/600], Loss: 0.5565, Train Acc:0.8160\n",
      "Epoch [6/10], Step [526/600], Loss: 0.6453, Train Acc:0.8160\n",
      "Epoch [6/10], Step [527/600], Loss: 0.6882, Train Acc:0.8159\n",
      "Epoch [6/10], Step [528/600], Loss: 0.5380, Train Acc:0.8159\n",
      "Epoch [6/10], Step [529/600], Loss: 0.4580, Train Acc:0.8160\n",
      "Epoch [6/10], Step [530/600], Loss: 0.4951, Train Acc:0.8161\n",
      "Epoch [6/10], Step [531/600], Loss: 0.6432, Train Acc:0.8161\n",
      "Epoch [6/10], Step [532/600], Loss: 0.5766, Train Acc:0.8161\n",
      "Epoch [6/10], Step [533/600], Loss: 0.6444, Train Acc:0.8160\n",
      "Epoch [6/10], Step [534/600], Loss: 0.4214, Train Acc:0.8161\n",
      "Epoch [6/10], Step [535/600], Loss: 0.5502, Train Acc:0.8160\n",
      "Epoch [6/10], Step [536/600], Loss: 0.6711, Train Acc:0.8160\n",
      "Epoch [6/10], Step [537/600], Loss: 0.5607, Train Acc:0.8159\n",
      "Epoch [6/10], Step [538/600], Loss: 0.5601, Train Acc:0.8158\n",
      "Epoch [6/10], Step [539/600], Loss: 0.4753, Train Acc:0.8159\n",
      "Epoch [6/10], Step [540/600], Loss: 0.5303, Train Acc:0.8159\n",
      "Epoch [6/10], Step [541/600], Loss: 0.6446, Train Acc:0.8158\n",
      "Epoch [6/10], Step [542/600], Loss: 0.6597, Train Acc:0.8157\n",
      "Epoch [6/10], Step [543/600], Loss: 0.5555, Train Acc:0.8157\n",
      "Epoch [6/10], Step [544/600], Loss: 0.4789, Train Acc:0.8159\n",
      "Epoch [6/10], Step [545/600], Loss: 0.6177, Train Acc:0.8159\n",
      "Epoch [6/10], Step [546/600], Loss: 0.5911, Train Acc:0.8158\n",
      "Epoch [6/10], Step [547/600], Loss: 0.5989, Train Acc:0.8157\n",
      "Epoch [6/10], Step [548/600], Loss: 0.4305, Train Acc:0.8158\n",
      "Epoch [6/10], Step [549/600], Loss: 0.4158, Train Acc:0.8160\n",
      "Epoch [6/10], Step [550/600], Loss: 0.5542, Train Acc:0.8160\n",
      "Epoch [6/10], Step [551/600], Loss: 0.7682, Train Acc:0.8160\n",
      "Epoch [6/10], Step [552/600], Loss: 0.4552, Train Acc:0.8161\n",
      "Epoch [6/10], Step [553/600], Loss: 0.7140, Train Acc:0.8160\n",
      "Epoch [6/10], Step [554/600], Loss: 0.4752, Train Acc:0.8160\n",
      "Epoch [6/10], Step [555/600], Loss: 0.5636, Train Acc:0.8160\n",
      "Epoch [6/10], Step [556/600], Loss: 0.5418, Train Acc:0.8160\n",
      "Epoch [6/10], Step [557/600], Loss: 0.5289, Train Acc:0.8160\n",
      "Epoch [6/10], Step [558/600], Loss: 0.7234, Train Acc:0.8159\n",
      "Epoch [6/10], Step [559/600], Loss: 0.5144, Train Acc:0.8159\n",
      "Epoch [6/10], Step [560/600], Loss: 0.5288, Train Acc:0.8160\n",
      "Epoch [6/10], Step [561/600], Loss: 0.5820, Train Acc:0.8160\n",
      "Epoch [6/10], Step [562/600], Loss: 0.5020, Train Acc:0.8160\n",
      "Epoch [6/10], Step [563/600], Loss: 0.5776, Train Acc:0.8160\n",
      "Epoch [6/10], Step [564/600], Loss: 0.5414, Train Acc:0.8160\n",
      "Epoch [6/10], Step [565/600], Loss: 0.5546, Train Acc:0.8160\n",
      "Epoch [6/10], Step [566/600], Loss: 0.5350, Train Acc:0.8160\n",
      "Epoch [6/10], Step [567/600], Loss: 0.6092, Train Acc:0.8159\n",
      "Epoch [6/10], Step [568/600], Loss: 0.5664, Train Acc:0.8160\n",
      "Epoch [6/10], Step [569/600], Loss: 0.4922, Train Acc:0.8160\n",
      "Epoch [6/10], Step [570/600], Loss: 0.5176, Train Acc:0.8160\n",
      "Epoch [6/10], Step [571/600], Loss: 0.5539, Train Acc:0.8160\n",
      "Epoch [6/10], Step [572/600], Loss: 0.6003, Train Acc:0.8159\n",
      "Epoch [6/10], Step [573/600], Loss: 0.5241, Train Acc:0.8159\n",
      "Epoch [6/10], Step [574/600], Loss: 0.5549, Train Acc:0.8159\n",
      "Epoch [6/10], Step [575/600], Loss: 0.6583, Train Acc:0.8159\n",
      "Epoch [6/10], Step [576/600], Loss: 0.5236, Train Acc:0.8159\n",
      "Epoch [6/10], Step [577/600], Loss: 0.6066, Train Acc:0.8159\n",
      "Epoch [6/10], Step [578/600], Loss: 0.4550, Train Acc:0.8160\n",
      "Epoch [6/10], Step [579/600], Loss: 0.6042, Train Acc:0.8159\n",
      "Epoch [6/10], Step [580/600], Loss: 0.6729, Train Acc:0.8159\n",
      "Epoch [6/10], Step [581/600], Loss: 0.4480, Train Acc:0.8160\n",
      "Epoch [6/10], Step [582/600], Loss: 0.4575, Train Acc:0.8161\n",
      "Epoch [6/10], Step [583/600], Loss: 0.5150, Train Acc:0.8161\n",
      "Epoch [6/10], Step [584/600], Loss: 0.5163, Train Acc:0.8162\n",
      "Epoch [6/10], Step [585/600], Loss: 0.5250, Train Acc:0.8163\n",
      "Epoch [6/10], Step [586/600], Loss: 0.6193, Train Acc:0.8163\n",
      "Epoch [6/10], Step [587/600], Loss: 0.4172, Train Acc:0.8164\n",
      "Epoch [6/10], Step [588/600], Loss: 0.7071, Train Acc:0.8163\n",
      "Epoch [6/10], Step [589/600], Loss: 0.6325, Train Acc:0.8162\n",
      "Epoch [6/10], Step [590/600], Loss: 0.5132, Train Acc:0.8163\n",
      "Epoch [6/10], Step [591/600], Loss: 0.5295, Train Acc:0.8163\n",
      "Epoch [6/10], Step [592/600], Loss: 0.5332, Train Acc:0.8163\n",
      "Epoch [6/10], Step [593/600], Loss: 0.5981, Train Acc:0.8163\n",
      "Epoch [6/10], Step [594/600], Loss: 0.5072, Train Acc:0.8163\n",
      "Epoch [6/10], Step [595/600], Loss: 0.6113, Train Acc:0.8163\n",
      "Epoch [6/10], Step [596/600], Loss: 0.5433, Train Acc:0.8163\n",
      "Epoch [6/10], Step [597/600], Loss: 0.5957, Train Acc:0.8163\n",
      "Epoch [6/10], Step [598/600], Loss: 0.5147, Train Acc:0.8164\n",
      "Epoch [6/10], Step [599/600], Loss: 0.5824, Train Acc:0.8164\n",
      "Epoch [6/10], Step [600/600], Loss: 0.5164, Train Acc:0.8164\n",
      "Epoch [7/10], Step [1/600], Loss: 0.5204, Train Acc:0.8600\n",
      "Epoch [7/10], Step [2/600], Loss: 0.6232, Train Acc:0.8100\n",
      "Epoch [7/10], Step [3/600], Loss: 0.6108, Train Acc:0.8133\n",
      "Epoch [7/10], Step [4/600], Loss: 0.5540, Train Acc:0.8150\n",
      "Epoch [7/10], Step [5/600], Loss: 0.5810, Train Acc:0.8060\n",
      "Epoch [7/10], Step [6/600], Loss: 0.5524, Train Acc:0.8100\n",
      "Epoch [7/10], Step [7/600], Loss: 0.5163, Train Acc:0.8143\n",
      "Epoch [7/10], Step [8/600], Loss: 0.5310, Train Acc:0.8163\n",
      "Epoch [7/10], Step [9/600], Loss: 0.5498, Train Acc:0.8122\n",
      "Epoch [7/10], Step [10/600], Loss: 0.5635, Train Acc:0.8150\n",
      "Epoch [7/10], Step [11/600], Loss: 0.5078, Train Acc:0.8136\n",
      "Epoch [7/10], Step [12/600], Loss: 0.4839, Train Acc:0.8150\n",
      "Epoch [7/10], Step [13/600], Loss: 0.4546, Train Acc:0.8185\n",
      "Epoch [7/10], Step [14/600], Loss: 0.6102, Train Acc:0.8171\n",
      "Epoch [7/10], Step [15/600], Loss: 0.7083, Train Acc:0.8187\n",
      "Epoch [7/10], Step [16/600], Loss: 0.5696, Train Acc:0.8200\n",
      "Epoch [7/10], Step [17/600], Loss: 0.6070, Train Acc:0.8165\n",
      "Epoch [7/10], Step [18/600], Loss: 0.5727, Train Acc:0.8183\n",
      "Epoch [7/10], Step [19/600], Loss: 0.6748, Train Acc:0.8137\n",
      "Epoch [7/10], Step [20/600], Loss: 0.5526, Train Acc:0.8150\n",
      "Epoch [7/10], Step [21/600], Loss: 0.4329, Train Acc:0.8171\n",
      "Epoch [7/10], Step [22/600], Loss: 0.5444, Train Acc:0.8168\n",
      "Epoch [7/10], Step [23/600], Loss: 0.6456, Train Acc:0.8143\n",
      "Epoch [7/10], Step [24/600], Loss: 0.5023, Train Acc:0.8154\n",
      "Epoch [7/10], Step [25/600], Loss: 0.4736, Train Acc:0.8176\n",
      "Epoch [7/10], Step [26/600], Loss: 0.6342, Train Acc:0.8181\n",
      "Epoch [7/10], Step [27/600], Loss: 0.6467, Train Acc:0.8178\n",
      "Epoch [7/10], Step [28/600], Loss: 0.6252, Train Acc:0.8168\n",
      "Epoch [7/10], Step [29/600], Loss: 0.5708, Train Acc:0.8169\n",
      "Epoch [7/10], Step [30/600], Loss: 0.5755, Train Acc:0.8160\n",
      "Epoch [7/10], Step [31/600], Loss: 0.4592, Train Acc:0.8177\n",
      "Epoch [7/10], Step [32/600], Loss: 0.5997, Train Acc:0.8169\n",
      "Epoch [7/10], Step [33/600], Loss: 0.4820, Train Acc:0.8173\n",
      "Epoch [7/10], Step [34/600], Loss: 0.5423, Train Acc:0.8171\n",
      "Epoch [7/10], Step [35/600], Loss: 0.5032, Train Acc:0.8186\n",
      "Epoch [7/10], Step [36/600], Loss: 0.5149, Train Acc:0.8194\n",
      "Epoch [7/10], Step [37/600], Loss: 0.5134, Train Acc:0.8195\n",
      "Epoch [7/10], Step [38/600], Loss: 0.4567, Train Acc:0.8208\n",
      "Epoch [7/10], Step [39/600], Loss: 0.4974, Train Acc:0.8221\n",
      "Epoch [7/10], Step [40/600], Loss: 0.6735, Train Acc:0.8213\n",
      "Epoch [7/10], Step [41/600], Loss: 0.6480, Train Acc:0.8205\n",
      "Epoch [7/10], Step [42/600], Loss: 0.5475, Train Acc:0.8207\n",
      "Epoch [7/10], Step [43/600], Loss: 0.4283, Train Acc:0.8219\n",
      "Epoch [7/10], Step [44/600], Loss: 0.4890, Train Acc:0.8209\n",
      "Epoch [7/10], Step [45/600], Loss: 0.5195, Train Acc:0.8207\n",
      "Epoch [7/10], Step [46/600], Loss: 0.6263, Train Acc:0.8209\n",
      "Epoch [7/10], Step [47/600], Loss: 0.5620, Train Acc:0.8211\n",
      "Epoch [7/10], Step [48/600], Loss: 0.5503, Train Acc:0.8206\n",
      "Epoch [7/10], Step [49/600], Loss: 0.5345, Train Acc:0.8212\n",
      "Epoch [7/10], Step [50/600], Loss: 0.5600, Train Acc:0.8210\n",
      "Epoch [7/10], Step [51/600], Loss: 0.5984, Train Acc:0.8212\n",
      "Epoch [7/10], Step [52/600], Loss: 0.4570, Train Acc:0.8225\n",
      "Epoch [7/10], Step [53/600], Loss: 0.5734, Train Acc:0.8230\n",
      "Epoch [7/10], Step [54/600], Loss: 0.6135, Train Acc:0.8224\n",
      "Epoch [7/10], Step [55/600], Loss: 0.6089, Train Acc:0.8216\n",
      "Epoch [7/10], Step [56/600], Loss: 0.5042, Train Acc:0.8225\n",
      "Epoch [7/10], Step [57/600], Loss: 0.6154, Train Acc:0.8218\n",
      "Epoch [7/10], Step [58/600], Loss: 0.5955, Train Acc:0.8216\n",
      "Epoch [7/10], Step [59/600], Loss: 0.4842, Train Acc:0.8222\n",
      "Epoch [7/10], Step [60/600], Loss: 0.4481, Train Acc:0.8230\n",
      "Epoch [7/10], Step [61/600], Loss: 0.4331, Train Acc:0.8238\n",
      "Epoch [7/10], Step [62/600], Loss: 0.5362, Train Acc:0.8237\n",
      "Epoch [7/10], Step [63/600], Loss: 0.6587, Train Acc:0.8233\n",
      "Epoch [7/10], Step [64/600], Loss: 0.5632, Train Acc:0.8233\n",
      "Epoch [7/10], Step [65/600], Loss: 0.5124, Train Acc:0.8237\n",
      "Epoch [7/10], Step [66/600], Loss: 0.5610, Train Acc:0.8230\n",
      "Epoch [7/10], Step [67/600], Loss: 0.6037, Train Acc:0.8224\n",
      "Epoch [7/10], Step [68/600], Loss: 0.5186, Train Acc:0.8231\n",
      "Epoch [7/10], Step [69/600], Loss: 0.5205, Train Acc:0.8233\n",
      "Epoch [7/10], Step [70/600], Loss: 0.4912, Train Acc:0.8236\n",
      "Epoch [7/10], Step [71/600], Loss: 0.4289, Train Acc:0.8244\n",
      "Epoch [7/10], Step [72/600], Loss: 0.5243, Train Acc:0.8240\n",
      "Epoch [7/10], Step [73/600], Loss: 0.5737, Train Acc:0.8237\n",
      "Epoch [7/10], Step [74/600], Loss: 0.5381, Train Acc:0.8236\n",
      "Epoch [7/10], Step [75/600], Loss: 0.5321, Train Acc:0.8235\n",
      "Epoch [7/10], Step [76/600], Loss: 0.4850, Train Acc:0.8241\n",
      "Epoch [7/10], Step [77/600], Loss: 0.5227, Train Acc:0.8248\n",
      "Epoch [7/10], Step [78/600], Loss: 0.7017, Train Acc:0.8245\n",
      "Epoch [7/10], Step [79/600], Loss: 0.6102, Train Acc:0.8237\n",
      "Epoch [7/10], Step [80/600], Loss: 0.5493, Train Acc:0.8235\n",
      "Epoch [7/10], Step [81/600], Loss: 0.5589, Train Acc:0.8233\n",
      "Epoch [7/10], Step [82/600], Loss: 0.5436, Train Acc:0.8237\n",
      "Epoch [7/10], Step [83/600], Loss: 0.4836, Train Acc:0.8241\n",
      "Epoch [7/10], Step [84/600], Loss: 0.6318, Train Acc:0.8242\n",
      "Epoch [7/10], Step [85/600], Loss: 0.5522, Train Acc:0.8245\n",
      "Epoch [7/10], Step [86/600], Loss: 0.4563, Train Acc:0.8250\n",
      "Epoch [7/10], Step [87/600], Loss: 0.6347, Train Acc:0.8245\n",
      "Epoch [7/10], Step [88/600], Loss: 0.5079, Train Acc:0.8249\n",
      "Epoch [7/10], Step [89/600], Loss: 0.5395, Train Acc:0.8249\n",
      "Epoch [7/10], Step [90/600], Loss: 0.5012, Train Acc:0.8256\n",
      "Epoch [7/10], Step [91/600], Loss: 0.6730, Train Acc:0.8248\n",
      "Epoch [7/10], Step [92/600], Loss: 0.5793, Train Acc:0.8243\n",
      "Epoch [7/10], Step [93/600], Loss: 0.6276, Train Acc:0.8246\n",
      "Epoch [7/10], Step [94/600], Loss: 0.5871, Train Acc:0.8245\n",
      "Epoch [7/10], Step [95/600], Loss: 0.4515, Train Acc:0.8247\n",
      "Epoch [7/10], Step [96/600], Loss: 0.6667, Train Acc:0.8239\n",
      "Epoch [7/10], Step [97/600], Loss: 0.5036, Train Acc:0.8236\n",
      "Epoch [7/10], Step [98/600], Loss: 0.4504, Train Acc:0.8242\n",
      "Epoch [7/10], Step [99/600], Loss: 0.4505, Train Acc:0.8244\n",
      "Epoch [7/10], Step [100/600], Loss: 0.5238, Train Acc:0.8244\n",
      "Epoch [7/10], Step [101/600], Loss: 0.6217, Train Acc:0.8244\n",
      "Epoch [7/10], Step [102/600], Loss: 0.5067, Train Acc:0.8246\n",
      "Epoch [7/10], Step [103/600], Loss: 0.6740, Train Acc:0.8245\n",
      "Epoch [7/10], Step [104/600], Loss: 0.4640, Train Acc:0.8246\n",
      "Epoch [7/10], Step [105/600], Loss: 0.5725, Train Acc:0.8242\n",
      "Epoch [7/10], Step [106/600], Loss: 0.5366, Train Acc:0.8243\n",
      "Epoch [7/10], Step [107/600], Loss: 0.5575, Train Acc:0.8243\n",
      "Epoch [7/10], Step [108/600], Loss: 0.4912, Train Acc:0.8244\n",
      "Epoch [7/10], Step [109/600], Loss: 0.5850, Train Acc:0.8245\n",
      "Epoch [7/10], Step [110/600], Loss: 0.5847, Train Acc:0.8244\n",
      "Epoch [7/10], Step [111/600], Loss: 0.5284, Train Acc:0.8243\n",
      "Epoch [7/10], Step [112/600], Loss: 0.5270, Train Acc:0.8245\n",
      "Epoch [7/10], Step [113/600], Loss: 0.5149, Train Acc:0.8248\n",
      "Epoch [7/10], Step [114/600], Loss: 0.5665, Train Acc:0.8246\n",
      "Epoch [7/10], Step [115/600], Loss: 0.5359, Train Acc:0.8246\n",
      "Epoch [7/10], Step [116/600], Loss: 0.7798, Train Acc:0.8240\n",
      "Epoch [7/10], Step [117/600], Loss: 0.6360, Train Acc:0.8240\n",
      "Epoch [7/10], Step [118/600], Loss: 0.4715, Train Acc:0.8242\n",
      "Epoch [7/10], Step [119/600], Loss: 0.5761, Train Acc:0.8239\n",
      "Epoch [7/10], Step [120/600], Loss: 0.5185, Train Acc:0.8237\n",
      "Epoch [7/10], Step [121/600], Loss: 0.5092, Train Acc:0.8237\n",
      "Epoch [7/10], Step [122/600], Loss: 0.5708, Train Acc:0.8234\n",
      "Epoch [7/10], Step [123/600], Loss: 0.5868, Train Acc:0.8234\n",
      "Epoch [7/10], Step [124/600], Loss: 0.4657, Train Acc:0.8237\n",
      "Epoch [7/10], Step [125/600], Loss: 0.4185, Train Acc:0.8240\n",
      "Epoch [7/10], Step [126/600], Loss: 0.5224, Train Acc:0.8244\n",
      "Epoch [7/10], Step [127/600], Loss: 0.7010, Train Acc:0.8239\n",
      "Epoch [7/10], Step [128/600], Loss: 0.5385, Train Acc:0.8239\n",
      "Epoch [7/10], Step [129/600], Loss: 0.5357, Train Acc:0.8237\n",
      "Epoch [7/10], Step [130/600], Loss: 0.5424, Train Acc:0.8242\n",
      "Epoch [7/10], Step [131/600], Loss: 0.5900, Train Acc:0.8241\n",
      "Epoch [7/10], Step [132/600], Loss: 0.5519, Train Acc:0.8239\n",
      "Epoch [7/10], Step [133/600], Loss: 0.4754, Train Acc:0.8241\n",
      "Epoch [7/10], Step [134/600], Loss: 0.4348, Train Acc:0.8242\n",
      "Epoch [7/10], Step [135/600], Loss: 0.5377, Train Acc:0.8244\n",
      "Epoch [7/10], Step [136/600], Loss: 0.4972, Train Acc:0.8241\n",
      "Epoch [7/10], Step [137/600], Loss: 0.6529, Train Acc:0.8235\n",
      "Epoch [7/10], Step [138/600], Loss: 0.5802, Train Acc:0.8237\n",
      "Epoch [7/10], Step [139/600], Loss: 0.6193, Train Acc:0.8237\n",
      "Epoch [7/10], Step [140/600], Loss: 0.5452, Train Acc:0.8238\n",
      "Epoch [7/10], Step [141/600], Loss: 0.5031, Train Acc:0.8238\n",
      "Epoch [7/10], Step [142/600], Loss: 0.5166, Train Acc:0.8237\n",
      "Epoch [7/10], Step [143/600], Loss: 0.4258, Train Acc:0.8239\n",
      "Epoch [7/10], Step [144/600], Loss: 0.5086, Train Acc:0.8240\n",
      "Epoch [7/10], Step [145/600], Loss: 0.5139, Train Acc:0.8241\n",
      "Epoch [7/10], Step [146/600], Loss: 0.5423, Train Acc:0.8244\n",
      "Epoch [7/10], Step [147/600], Loss: 0.5725, Train Acc:0.8243\n",
      "Epoch [7/10], Step [148/600], Loss: 0.4852, Train Acc:0.8244\n",
      "Epoch [7/10], Step [149/600], Loss: 0.6359, Train Acc:0.8241\n",
      "Epoch [7/10], Step [150/600], Loss: 0.6318, Train Acc:0.8238\n",
      "Epoch [7/10], Step [151/600], Loss: 0.6798, Train Acc:0.8239\n",
      "Epoch [7/10], Step [152/600], Loss: 0.5628, Train Acc:0.8239\n",
      "Epoch [7/10], Step [153/600], Loss: 0.5554, Train Acc:0.8241\n",
      "Epoch [7/10], Step [154/600], Loss: 0.5285, Train Acc:0.8242\n",
      "Epoch [7/10], Step [155/600], Loss: 0.5329, Train Acc:0.8243\n",
      "Epoch [7/10], Step [156/600], Loss: 0.5516, Train Acc:0.8246\n",
      "Epoch [7/10], Step [157/600], Loss: 0.5539, Train Acc:0.8245\n",
      "Epoch [7/10], Step [158/600], Loss: 0.4704, Train Acc:0.8248\n",
      "Epoch [7/10], Step [159/600], Loss: 0.4888, Train Acc:0.8248\n",
      "Epoch [7/10], Step [160/600], Loss: 0.4827, Train Acc:0.8246\n",
      "Epoch [7/10], Step [161/600], Loss: 0.7939, Train Acc:0.8242\n",
      "Epoch [7/10], Step [162/600], Loss: 0.5402, Train Acc:0.8242\n",
      "Epoch [7/10], Step [163/600], Loss: 0.5062, Train Acc:0.8245\n",
      "Epoch [7/10], Step [164/600], Loss: 0.5147, Train Acc:0.8245\n",
      "Epoch [7/10], Step [165/600], Loss: 0.4921, Train Acc:0.8245\n",
      "Epoch [7/10], Step [166/600], Loss: 0.5683, Train Acc:0.8246\n",
      "Epoch [7/10], Step [167/600], Loss: 0.5028, Train Acc:0.8247\n",
      "Epoch [7/10], Step [168/600], Loss: 0.5241, Train Acc:0.8246\n",
      "Epoch [7/10], Step [169/600], Loss: 0.5826, Train Acc:0.8243\n",
      "Epoch [7/10], Step [170/600], Loss: 0.4563, Train Acc:0.8244\n",
      "Epoch [7/10], Step [171/600], Loss: 0.4340, Train Acc:0.8244\n",
      "Epoch [7/10], Step [172/600], Loss: 0.4936, Train Acc:0.8248\n",
      "Epoch [7/10], Step [173/600], Loss: 0.5500, Train Acc:0.8249\n",
      "Epoch [7/10], Step [174/600], Loss: 0.4645, Train Acc:0.8250\n",
      "Epoch [7/10], Step [175/600], Loss: 0.6418, Train Acc:0.8249\n",
      "Epoch [7/10], Step [176/600], Loss: 0.5812, Train Acc:0.8246\n",
      "Epoch [7/10], Step [177/600], Loss: 0.4577, Train Acc:0.8247\n",
      "Epoch [7/10], Step [178/600], Loss: 0.6608, Train Acc:0.8242\n",
      "Epoch [7/10], Step [179/600], Loss: 0.5515, Train Acc:0.8240\n",
      "Epoch [7/10], Step [180/600], Loss: 0.4876, Train Acc:0.8241\n",
      "Epoch [7/10], Step [181/600], Loss: 0.5756, Train Acc:0.8239\n",
      "Epoch [7/10], Step [182/600], Loss: 0.7377, Train Acc:0.8233\n",
      "Epoch [7/10], Step [183/600], Loss: 0.5339, Train Acc:0.8232\n",
      "Epoch [7/10], Step [184/600], Loss: 0.5523, Train Acc:0.8230\n",
      "Epoch [7/10], Step [185/600], Loss: 0.5780, Train Acc:0.8229\n",
      "Epoch [7/10], Step [186/600], Loss: 0.4860, Train Acc:0.8232\n",
      "Epoch [7/10], Step [187/600], Loss: 0.4128, Train Acc:0.8234\n",
      "Epoch [7/10], Step [188/600], Loss: 0.5715, Train Acc:0.8234\n",
      "Epoch [7/10], Step [189/600], Loss: 0.4351, Train Acc:0.8237\n",
      "Epoch [7/10], Step [190/600], Loss: 0.5853, Train Acc:0.8237\n",
      "Epoch [7/10], Step [191/600], Loss: 0.5362, Train Acc:0.8238\n",
      "Epoch [7/10], Step [192/600], Loss: 0.5728, Train Acc:0.8239\n",
      "Epoch [7/10], Step [193/600], Loss: 0.4578, Train Acc:0.8240\n",
      "Epoch [7/10], Step [194/600], Loss: 0.6061, Train Acc:0.8239\n",
      "Epoch [7/10], Step [195/600], Loss: 0.5109, Train Acc:0.8239\n",
      "Epoch [7/10], Step [196/600], Loss: 0.4712, Train Acc:0.8241\n",
      "Epoch [7/10], Step [197/600], Loss: 0.5435, Train Acc:0.8240\n",
      "Epoch [7/10], Step [198/600], Loss: 0.4596, Train Acc:0.8243\n",
      "Epoch [7/10], Step [199/600], Loss: 0.6338, Train Acc:0.8242\n",
      "Epoch [7/10], Step [200/600], Loss: 0.6119, Train Acc:0.8238\n",
      "Epoch [7/10], Step [201/600], Loss: 0.5759, Train Acc:0.8237\n",
      "Epoch [7/10], Step [202/600], Loss: 0.5707, Train Acc:0.8237\n",
      "Epoch [7/10], Step [203/600], Loss: 0.5821, Train Acc:0.8237\n",
      "Epoch [7/10], Step [204/600], Loss: 0.4635, Train Acc:0.8239\n",
      "Epoch [7/10], Step [205/600], Loss: 0.4741, Train Acc:0.8238\n",
      "Epoch [7/10], Step [206/600], Loss: 0.4892, Train Acc:0.8240\n",
      "Epoch [7/10], Step [207/600], Loss: 0.4309, Train Acc:0.8241\n",
      "Epoch [7/10], Step [208/600], Loss: 0.5458, Train Acc:0.8240\n",
      "Epoch [7/10], Step [209/600], Loss: 0.6656, Train Acc:0.8238\n",
      "Epoch [7/10], Step [210/600], Loss: 0.4739, Train Acc:0.8239\n",
      "Epoch [7/10], Step [211/600], Loss: 0.4767, Train Acc:0.8241\n",
      "Epoch [7/10], Step [212/600], Loss: 0.5186, Train Acc:0.8241\n",
      "Epoch [7/10], Step [213/600], Loss: 0.5318, Train Acc:0.8241\n",
      "Epoch [7/10], Step [214/600], Loss: 0.5702, Train Acc:0.8241\n",
      "Epoch [7/10], Step [215/600], Loss: 0.6508, Train Acc:0.8240\n",
      "Epoch [7/10], Step [216/600], Loss: 0.4285, Train Acc:0.8242\n",
      "Epoch [7/10], Step [217/600], Loss: 0.7135, Train Acc:0.8237\n",
      "Epoch [7/10], Step [218/600], Loss: 0.6618, Train Acc:0.8234\n",
      "Epoch [7/10], Step [219/600], Loss: 0.5909, Train Acc:0.8233\n",
      "Epoch [7/10], Step [220/600], Loss: 0.5720, Train Acc:0.8233\n",
      "Epoch [7/10], Step [221/600], Loss: 0.4213, Train Acc:0.8236\n",
      "Epoch [7/10], Step [222/600], Loss: 0.5199, Train Acc:0.8237\n",
      "Epoch [7/10], Step [223/600], Loss: 0.5660, Train Acc:0.8237\n",
      "Epoch [7/10], Step [224/600], Loss: 0.6218, Train Acc:0.8233\n",
      "Epoch [7/10], Step [225/600], Loss: 0.5183, Train Acc:0.8236\n",
      "Epoch [7/10], Step [226/600], Loss: 0.5859, Train Acc:0.8235\n",
      "Epoch [7/10], Step [227/600], Loss: 0.5050, Train Acc:0.8238\n",
      "Epoch [7/10], Step [228/600], Loss: 0.7359, Train Acc:0.8236\n",
      "Epoch [7/10], Step [229/600], Loss: 0.5566, Train Acc:0.8236\n",
      "Epoch [7/10], Step [230/600], Loss: 0.6112, Train Acc:0.8234\n",
      "Epoch [7/10], Step [231/600], Loss: 0.5300, Train Acc:0.8233\n",
      "Epoch [7/10], Step [232/600], Loss: 0.5076, Train Acc:0.8235\n",
      "Epoch [7/10], Step [233/600], Loss: 0.6082, Train Acc:0.8232\n",
      "Epoch [7/10], Step [234/600], Loss: 0.5504, Train Acc:0.8232\n",
      "Epoch [7/10], Step [235/600], Loss: 0.6241, Train Acc:0.8229\n",
      "Epoch [7/10], Step [236/600], Loss: 0.6121, Train Acc:0.8228\n",
      "Epoch [7/10], Step [237/600], Loss: 0.4666, Train Acc:0.8228\n",
      "Epoch [7/10], Step [238/600], Loss: 0.5557, Train Acc:0.8228\n",
      "Epoch [7/10], Step [239/600], Loss: 0.5781, Train Acc:0.8227\n",
      "Epoch [7/10], Step [240/600], Loss: 0.5891, Train Acc:0.8225\n",
      "Epoch [7/10], Step [241/600], Loss: 0.6353, Train Acc:0.8223\n",
      "Epoch [7/10], Step [242/600], Loss: 0.4647, Train Acc:0.8224\n",
      "Epoch [7/10], Step [243/600], Loss: 0.5331, Train Acc:0.8223\n",
      "Epoch [7/10], Step [244/600], Loss: 0.4227, Train Acc:0.8225\n",
      "Epoch [7/10], Step [245/600], Loss: 0.5263, Train Acc:0.8226\n",
      "Epoch [7/10], Step [246/600], Loss: 0.5041, Train Acc:0.8226\n",
      "Epoch [7/10], Step [247/600], Loss: 0.6561, Train Acc:0.8224\n",
      "Epoch [7/10], Step [248/600], Loss: 0.4900, Train Acc:0.8226\n",
      "Epoch [7/10], Step [249/600], Loss: 0.6136, Train Acc:0.8224\n",
      "Epoch [7/10], Step [250/600], Loss: 0.4359, Train Acc:0.8227\n",
      "Epoch [7/10], Step [251/600], Loss: 0.5406, Train Acc:0.8228\n",
      "Epoch [7/10], Step [252/600], Loss: 0.4631, Train Acc:0.8231\n",
      "Epoch [7/10], Step [253/600], Loss: 0.5901, Train Acc:0.8230\n",
      "Epoch [7/10], Step [254/600], Loss: 0.5862, Train Acc:0.8230\n",
      "Epoch [7/10], Step [255/600], Loss: 0.4856, Train Acc:0.8230\n",
      "Epoch [7/10], Step [256/600], Loss: 0.6128, Train Acc:0.8229\n",
      "Epoch [7/10], Step [257/600], Loss: 0.4290, Train Acc:0.8230\n",
      "Epoch [7/10], Step [258/600], Loss: 0.5209, Train Acc:0.8231\n",
      "Epoch [7/10], Step [259/600], Loss: 0.5853, Train Acc:0.8232\n",
      "Epoch [7/10], Step [260/600], Loss: 0.5526, Train Acc:0.8231\n",
      "Epoch [7/10], Step [261/600], Loss: 0.6090, Train Acc:0.8230\n",
      "Epoch [7/10], Step [262/600], Loss: 0.4780, Train Acc:0.8230\n",
      "Epoch [7/10], Step [263/600], Loss: 0.4666, Train Acc:0.8230\n",
      "Epoch [7/10], Step [264/600], Loss: 0.4887, Train Acc:0.8231\n",
      "Epoch [7/10], Step [265/600], Loss: 0.5807, Train Acc:0.8229\n",
      "Epoch [7/10], Step [266/600], Loss: 0.5528, Train Acc:0.8230\n",
      "Epoch [7/10], Step [267/600], Loss: 0.5337, Train Acc:0.8229\n",
      "Epoch [7/10], Step [268/600], Loss: 0.4601, Train Acc:0.8229\n",
      "Epoch [7/10], Step [269/600], Loss: 0.5646, Train Acc:0.8229\n",
      "Epoch [7/10], Step [270/600], Loss: 0.5038, Train Acc:0.8231\n",
      "Epoch [7/10], Step [271/600], Loss: 0.6101, Train Acc:0.8230\n",
      "Epoch [7/10], Step [272/600], Loss: 0.6077, Train Acc:0.8228\n",
      "Epoch [7/10], Step [273/600], Loss: 0.5462, Train Acc:0.8229\n",
      "Epoch [7/10], Step [274/600], Loss: 0.5498, Train Acc:0.8229\n",
      "Epoch [7/10], Step [275/600], Loss: 0.5037, Train Acc:0.8229\n",
      "Epoch [7/10], Step [276/600], Loss: 0.5817, Train Acc:0.8227\n",
      "Epoch [7/10], Step [277/600], Loss: 0.5299, Train Acc:0.8226\n",
      "Epoch [7/10], Step [278/600], Loss: 0.6693, Train Acc:0.8224\n",
      "Epoch [7/10], Step [279/600], Loss: 0.5257, Train Acc:0.8225\n",
      "Epoch [7/10], Step [280/600], Loss: 0.4786, Train Acc:0.8226\n",
      "Epoch [7/10], Step [281/600], Loss: 0.5702, Train Acc:0.8224\n",
      "Epoch [7/10], Step [282/600], Loss: 0.5230, Train Acc:0.8224\n",
      "Epoch [7/10], Step [283/600], Loss: 0.5129, Train Acc:0.8223\n",
      "Epoch [7/10], Step [284/600], Loss: 0.5746, Train Acc:0.8225\n",
      "Epoch [7/10], Step [285/600], Loss: 0.6015, Train Acc:0.8224\n",
      "Epoch [7/10], Step [286/600], Loss: 0.6041, Train Acc:0.8224\n",
      "Epoch [7/10], Step [287/600], Loss: 0.5010, Train Acc:0.8224\n",
      "Epoch [7/10], Step [288/600], Loss: 0.5893, Train Acc:0.8224\n",
      "Epoch [7/10], Step [289/600], Loss: 0.6334, Train Acc:0.8223\n",
      "Epoch [7/10], Step [290/600], Loss: 0.4521, Train Acc:0.8226\n",
      "Epoch [7/10], Step [291/600], Loss: 0.6499, Train Acc:0.8225\n",
      "Epoch [7/10], Step [292/600], Loss: 0.5703, Train Acc:0.8225\n",
      "Epoch [7/10], Step [293/600], Loss: 0.5223, Train Acc:0.8226\n",
      "Epoch [7/10], Step [294/600], Loss: 0.5357, Train Acc:0.8227\n",
      "Epoch [7/10], Step [295/600], Loss: 0.6317, Train Acc:0.8225\n",
      "Epoch [7/10], Step [296/600], Loss: 0.5308, Train Acc:0.8226\n",
      "Epoch [7/10], Step [297/600], Loss: 0.5094, Train Acc:0.8228\n",
      "Epoch [7/10], Step [298/600], Loss: 0.5413, Train Acc:0.8229\n",
      "Epoch [7/10], Step [299/600], Loss: 0.5549, Train Acc:0.8229\n",
      "Epoch [7/10], Step [300/600], Loss: 0.6248, Train Acc:0.8229\n",
      "Epoch [7/10], Step [301/600], Loss: 0.5659, Train Acc:0.8229\n",
      "Epoch [7/10], Step [302/600], Loss: 0.5712, Train Acc:0.8226\n",
      "Epoch [7/10], Step [303/600], Loss: 0.4120, Train Acc:0.8228\n",
      "Epoch [7/10], Step [304/600], Loss: 0.6043, Train Acc:0.8227\n",
      "Epoch [7/10], Step [305/600], Loss: 0.5714, Train Acc:0.8226\n",
      "Epoch [7/10], Step [306/600], Loss: 0.4927, Train Acc:0.8226\n",
      "Epoch [7/10], Step [307/600], Loss: 0.4959, Train Acc:0.8227\n",
      "Epoch [7/10], Step [308/600], Loss: 0.3934, Train Acc:0.8230\n",
      "Epoch [7/10], Step [309/600], Loss: 0.6559, Train Acc:0.8229\n",
      "Epoch [7/10], Step [310/600], Loss: 0.4684, Train Acc:0.8231\n",
      "Epoch [7/10], Step [311/600], Loss: 0.4943, Train Acc:0.8230\n",
      "Epoch [7/10], Step [312/600], Loss: 0.5039, Train Acc:0.8231\n",
      "Epoch [7/10], Step [313/600], Loss: 0.6225, Train Acc:0.8230\n",
      "Epoch [7/10], Step [314/600], Loss: 0.4521, Train Acc:0.8230\n",
      "Epoch [7/10], Step [315/600], Loss: 0.5871, Train Acc:0.8230\n",
      "Epoch [7/10], Step [316/600], Loss: 0.5290, Train Acc:0.8231\n",
      "Epoch [7/10], Step [317/600], Loss: 0.8767, Train Acc:0.8227\n",
      "Epoch [7/10], Step [318/600], Loss: 0.4298, Train Acc:0.8228\n",
      "Epoch [7/10], Step [319/600], Loss: 0.5748, Train Acc:0.8227\n",
      "Epoch [7/10], Step [320/600], Loss: 0.5917, Train Acc:0.8228\n",
      "Epoch [7/10], Step [321/600], Loss: 0.5464, Train Acc:0.8229\n",
      "Epoch [7/10], Step [322/600], Loss: 0.5183, Train Acc:0.8230\n",
      "Epoch [7/10], Step [323/600], Loss: 0.4964, Train Acc:0.8231\n",
      "Epoch [7/10], Step [324/600], Loss: 0.5183, Train Acc:0.8231\n",
      "Epoch [7/10], Step [325/600], Loss: 0.4160, Train Acc:0.8234\n",
      "Epoch [7/10], Step [326/600], Loss: 0.5543, Train Acc:0.8233\n",
      "Epoch [7/10], Step [327/600], Loss: 0.4339, Train Acc:0.8235\n",
      "Epoch [7/10], Step [328/600], Loss: 0.5687, Train Acc:0.8234\n",
      "Epoch [7/10], Step [329/600], Loss: 0.4985, Train Acc:0.8233\n",
      "Epoch [7/10], Step [330/600], Loss: 0.5276, Train Acc:0.8234\n",
      "Epoch [7/10], Step [331/600], Loss: 0.4509, Train Acc:0.8234\n",
      "Epoch [7/10], Step [332/600], Loss: 0.6212, Train Acc:0.8232\n",
      "Epoch [7/10], Step [333/600], Loss: 0.6152, Train Acc:0.8232\n",
      "Epoch [7/10], Step [334/600], Loss: 0.5464, Train Acc:0.8231\n",
      "Epoch [7/10], Step [335/600], Loss: 0.5635, Train Acc:0.8231\n",
      "Epoch [7/10], Step [336/600], Loss: 0.5620, Train Acc:0.8230\n",
      "Epoch [7/10], Step [337/600], Loss: 0.3554, Train Acc:0.8233\n",
      "Epoch [7/10], Step [338/600], Loss: 0.5998, Train Acc:0.8233\n",
      "Epoch [7/10], Step [339/600], Loss: 0.7240, Train Acc:0.8232\n",
      "Epoch [7/10], Step [340/600], Loss: 0.5956, Train Acc:0.8231\n",
      "Epoch [7/10], Step [341/600], Loss: 0.4914, Train Acc:0.8231\n",
      "Epoch [7/10], Step [342/600], Loss: 0.4949, Train Acc:0.8231\n",
      "Epoch [7/10], Step [343/600], Loss: 0.6480, Train Acc:0.8229\n",
      "Epoch [7/10], Step [344/600], Loss: 0.6302, Train Acc:0.8228\n",
      "Epoch [7/10], Step [345/600], Loss: 0.5442, Train Acc:0.8228\n",
      "Epoch [7/10], Step [346/600], Loss: 0.5563, Train Acc:0.8229\n",
      "Epoch [7/10], Step [347/600], Loss: 0.6274, Train Acc:0.8228\n",
      "Epoch [7/10], Step [348/600], Loss: 0.5411, Train Acc:0.8228\n",
      "Epoch [7/10], Step [349/600], Loss: 0.6112, Train Acc:0.8228\n",
      "Epoch [7/10], Step [350/600], Loss: 0.5383, Train Acc:0.8229\n",
      "Epoch [7/10], Step [351/600], Loss: 0.6614, Train Acc:0.8227\n",
      "Epoch [7/10], Step [352/600], Loss: 0.6112, Train Acc:0.8226\n",
      "Epoch [7/10], Step [353/600], Loss: 0.4711, Train Acc:0.8227\n",
      "Epoch [7/10], Step [354/600], Loss: 0.5913, Train Acc:0.8227\n",
      "Epoch [7/10], Step [355/600], Loss: 0.5107, Train Acc:0.8228\n",
      "Epoch [7/10], Step [356/600], Loss: 0.5623, Train Acc:0.8228\n",
      "Epoch [7/10], Step [357/600], Loss: 0.4622, Train Acc:0.8229\n",
      "Epoch [7/10], Step [358/600], Loss: 0.5270, Train Acc:0.8230\n",
      "Epoch [7/10], Step [359/600], Loss: 0.4921, Train Acc:0.8230\n",
      "Epoch [7/10], Step [360/600], Loss: 0.5444, Train Acc:0.8230\n",
      "Epoch [7/10], Step [361/600], Loss: 0.5130, Train Acc:0.8231\n",
      "Epoch [7/10], Step [362/600], Loss: 0.5318, Train Acc:0.8231\n",
      "Epoch [7/10], Step [363/600], Loss: 0.4486, Train Acc:0.8232\n",
      "Epoch [7/10], Step [364/600], Loss: 0.6569, Train Acc:0.8232\n",
      "Epoch [7/10], Step [365/600], Loss: 0.5921, Train Acc:0.8230\n",
      "Epoch [7/10], Step [366/600], Loss: 0.6811, Train Acc:0.8230\n",
      "Epoch [7/10], Step [367/600], Loss: 0.4195, Train Acc:0.8231\n",
      "Epoch [7/10], Step [368/600], Loss: 0.3887, Train Acc:0.8233\n",
      "Epoch [7/10], Step [369/600], Loss: 0.6588, Train Acc:0.8231\n",
      "Epoch [7/10], Step [370/600], Loss: 0.4698, Train Acc:0.8232\n",
      "Epoch [7/10], Step [371/600], Loss: 0.3928, Train Acc:0.8234\n",
      "Epoch [7/10], Step [372/600], Loss: 0.5475, Train Acc:0.8234\n",
      "Epoch [7/10], Step [373/600], Loss: 0.6015, Train Acc:0.8232\n",
      "Epoch [7/10], Step [374/600], Loss: 0.5966, Train Acc:0.8232\n",
      "Epoch [7/10], Step [375/600], Loss: 0.6235, Train Acc:0.8231\n",
      "Epoch [7/10], Step [376/600], Loss: 0.5795, Train Acc:0.8231\n",
      "Epoch [7/10], Step [377/600], Loss: 0.5061, Train Acc:0.8232\n",
      "Epoch [7/10], Step [378/600], Loss: 0.5135, Train Acc:0.8233\n",
      "Epoch [7/10], Step [379/600], Loss: 0.5887, Train Acc:0.8233\n",
      "Epoch [7/10], Step [380/600], Loss: 0.3987, Train Acc:0.8233\n",
      "Epoch [7/10], Step [381/600], Loss: 0.6330, Train Acc:0.8233\n",
      "Epoch [7/10], Step [382/600], Loss: 0.5948, Train Acc:0.8231\n",
      "Epoch [7/10], Step [383/600], Loss: 0.5911, Train Acc:0.8231\n",
      "Epoch [7/10], Step [384/600], Loss: 0.6034, Train Acc:0.8231\n",
      "Epoch [7/10], Step [385/600], Loss: 0.4504, Train Acc:0.8232\n",
      "Epoch [7/10], Step [386/600], Loss: 0.6708, Train Acc:0.8230\n",
      "Epoch [7/10], Step [387/600], Loss: 0.6577, Train Acc:0.8228\n",
      "Epoch [7/10], Step [388/600], Loss: 0.7315, Train Acc:0.8227\n",
      "Epoch [7/10], Step [389/600], Loss: 0.5652, Train Acc:0.8226\n",
      "Epoch [7/10], Step [390/600], Loss: 0.5337, Train Acc:0.8226\n",
      "Epoch [7/10], Step [391/600], Loss: 0.4975, Train Acc:0.8226\n",
      "Epoch [7/10], Step [392/600], Loss: 0.5954, Train Acc:0.8226\n",
      "Epoch [7/10], Step [393/600], Loss: 0.5289, Train Acc:0.8225\n",
      "Epoch [7/10], Step [394/600], Loss: 0.5665, Train Acc:0.8225\n",
      "Epoch [7/10], Step [395/600], Loss: 0.6212, Train Acc:0.8225\n",
      "Epoch [7/10], Step [396/600], Loss: 0.5206, Train Acc:0.8224\n",
      "Epoch [7/10], Step [397/600], Loss: 0.5291, Train Acc:0.8224\n",
      "Epoch [7/10], Step [398/600], Loss: 0.4813, Train Acc:0.8226\n",
      "Epoch [7/10], Step [399/600], Loss: 0.5958, Train Acc:0.8225\n",
      "Epoch [7/10], Step [400/600], Loss: 0.5398, Train Acc:0.8225\n",
      "Epoch [7/10], Step [401/600], Loss: 0.5434, Train Acc:0.8224\n",
      "Epoch [7/10], Step [402/600], Loss: 0.4778, Train Acc:0.8224\n",
      "Epoch [7/10], Step [403/600], Loss: 0.5720, Train Acc:0.8223\n",
      "Epoch [7/10], Step [404/600], Loss: 0.4666, Train Acc:0.8223\n",
      "Epoch [7/10], Step [405/600], Loss: 0.5499, Train Acc:0.8222\n",
      "Epoch [7/10], Step [406/600], Loss: 0.6750, Train Acc:0.8221\n",
      "Epoch [7/10], Step [407/600], Loss: 0.4790, Train Acc:0.8223\n",
      "Epoch [7/10], Step [408/600], Loss: 0.6108, Train Acc:0.8224\n",
      "Epoch [7/10], Step [409/600], Loss: 0.5435, Train Acc:0.8222\n",
      "Epoch [7/10], Step [410/600], Loss: 0.5123, Train Acc:0.8222\n",
      "Epoch [7/10], Step [411/600], Loss: 0.6525, Train Acc:0.8222\n",
      "Epoch [7/10], Step [412/600], Loss: 0.6650, Train Acc:0.8221\n",
      "Epoch [7/10], Step [413/600], Loss: 0.5040, Train Acc:0.8221\n",
      "Epoch [7/10], Step [414/600], Loss: 0.6589, Train Acc:0.8221\n",
      "Epoch [7/10], Step [415/600], Loss: 0.6018, Train Acc:0.8221\n",
      "Epoch [7/10], Step [416/600], Loss: 0.5660, Train Acc:0.8221\n",
      "Epoch [7/10], Step [417/600], Loss: 0.4284, Train Acc:0.8222\n",
      "Epoch [7/10], Step [418/600], Loss: 0.6183, Train Acc:0.8222\n",
      "Epoch [7/10], Step [419/600], Loss: 0.4972, Train Acc:0.8223\n",
      "Epoch [7/10], Step [420/600], Loss: 0.5474, Train Acc:0.8223\n",
      "Epoch [7/10], Step [421/600], Loss: 0.5457, Train Acc:0.8224\n",
      "Epoch [7/10], Step [422/600], Loss: 0.4866, Train Acc:0.8224\n",
      "Epoch [7/10], Step [423/600], Loss: 0.4128, Train Acc:0.8225\n",
      "Epoch [7/10], Step [424/600], Loss: 0.5699, Train Acc:0.8225\n",
      "Epoch [7/10], Step [425/600], Loss: 0.5143, Train Acc:0.8225\n",
      "Epoch [7/10], Step [426/600], Loss: 0.6716, Train Acc:0.8223\n",
      "Epoch [7/10], Step [427/600], Loss: 0.5971, Train Acc:0.8223\n",
      "Epoch [7/10], Step [428/600], Loss: 0.5112, Train Acc:0.8224\n",
      "Epoch [7/10], Step [429/600], Loss: 0.4526, Train Acc:0.8224\n",
      "Epoch [7/10], Step [430/600], Loss: 0.5542, Train Acc:0.8225\n",
      "Epoch [7/10], Step [431/600], Loss: 0.6683, Train Acc:0.8223\n",
      "Epoch [7/10], Step [432/600], Loss: 0.5953, Train Acc:0.8223\n",
      "Epoch [7/10], Step [433/600], Loss: 0.4881, Train Acc:0.8223\n",
      "Epoch [7/10], Step [434/600], Loss: 0.5688, Train Acc:0.8224\n",
      "Epoch [7/10], Step [435/600], Loss: 0.5772, Train Acc:0.8223\n",
      "Epoch [7/10], Step [436/600], Loss: 0.4913, Train Acc:0.8224\n",
      "Epoch [7/10], Step [437/600], Loss: 0.4982, Train Acc:0.8224\n",
      "Epoch [7/10], Step [438/600], Loss: 0.5923, Train Acc:0.8223\n",
      "Epoch [7/10], Step [439/600], Loss: 0.4505, Train Acc:0.8223\n",
      "Epoch [7/10], Step [440/600], Loss: 0.6533, Train Acc:0.8222\n",
      "Epoch [7/10], Step [441/600], Loss: 0.4879, Train Acc:0.8223\n",
      "Epoch [7/10], Step [442/600], Loss: 0.4223, Train Acc:0.8224\n",
      "Epoch [7/10], Step [443/600], Loss: 0.5388, Train Acc:0.8224\n",
      "Epoch [7/10], Step [444/600], Loss: 0.5190, Train Acc:0.8224\n",
      "Epoch [7/10], Step [445/600], Loss: 0.5952, Train Acc:0.8223\n",
      "Epoch [7/10], Step [446/600], Loss: 0.5520, Train Acc:0.8223\n",
      "Epoch [7/10], Step [447/600], Loss: 0.8232, Train Acc:0.8221\n",
      "Epoch [7/10], Step [448/600], Loss: 0.6357, Train Acc:0.8221\n",
      "Epoch [7/10], Step [449/600], Loss: 0.5423, Train Acc:0.8223\n",
      "Epoch [7/10], Step [450/600], Loss: 0.5359, Train Acc:0.8222\n",
      "Epoch [7/10], Step [451/600], Loss: 0.4773, Train Acc:0.8223\n",
      "Epoch [7/10], Step [452/600], Loss: 0.7053, Train Acc:0.8223\n",
      "Epoch [7/10], Step [453/600], Loss: 0.6033, Train Acc:0.8223\n",
      "Epoch [7/10], Step [454/600], Loss: 0.6776, Train Acc:0.8221\n",
      "Epoch [7/10], Step [455/600], Loss: 0.6616, Train Acc:0.8221\n",
      "Epoch [7/10], Step [456/600], Loss: 0.4956, Train Acc:0.8221\n",
      "Epoch [7/10], Step [457/600], Loss: 0.5723, Train Acc:0.8221\n",
      "Epoch [7/10], Step [458/600], Loss: 0.4946, Train Acc:0.8222\n",
      "Epoch [7/10], Step [459/600], Loss: 0.6742, Train Acc:0.8222\n",
      "Epoch [7/10], Step [460/600], Loss: 0.5169, Train Acc:0.8222\n",
      "Epoch [7/10], Step [461/600], Loss: 0.5737, Train Acc:0.8222\n",
      "Epoch [7/10], Step [462/600], Loss: 0.5207, Train Acc:0.8222\n",
      "Epoch [7/10], Step [463/600], Loss: 0.4827, Train Acc:0.8222\n",
      "Epoch [7/10], Step [464/600], Loss: 0.6063, Train Acc:0.8221\n",
      "Epoch [7/10], Step [465/600], Loss: 0.5197, Train Acc:0.8222\n",
      "Epoch [7/10], Step [466/600], Loss: 0.5035, Train Acc:0.8222\n",
      "Epoch [7/10], Step [467/600], Loss: 0.4901, Train Acc:0.8222\n",
      "Epoch [7/10], Step [468/600], Loss: 0.5477, Train Acc:0.8222\n",
      "Epoch [7/10], Step [469/600], Loss: 0.6712, Train Acc:0.8221\n",
      "Epoch [7/10], Step [470/600], Loss: 0.5089, Train Acc:0.8222\n",
      "Epoch [7/10], Step [471/600], Loss: 0.5860, Train Acc:0.8221\n",
      "Epoch [7/10], Step [472/600], Loss: 0.5655, Train Acc:0.8222\n",
      "Epoch [7/10], Step [473/600], Loss: 0.5338, Train Acc:0.8221\n",
      "Epoch [7/10], Step [474/600], Loss: 0.5590, Train Acc:0.8222\n",
      "Epoch [7/10], Step [475/600], Loss: 0.5585, Train Acc:0.8222\n",
      "Epoch [7/10], Step [476/600], Loss: 0.5412, Train Acc:0.8221\n",
      "Epoch [7/10], Step [477/600], Loss: 0.5628, Train Acc:0.8221\n",
      "Epoch [7/10], Step [478/600], Loss: 0.4978, Train Acc:0.8221\n",
      "Epoch [7/10], Step [479/600], Loss: 0.4185, Train Acc:0.8223\n",
      "Epoch [7/10], Step [480/600], Loss: 0.5131, Train Acc:0.8223\n",
      "Epoch [7/10], Step [481/600], Loss: 0.5665, Train Acc:0.8224\n",
      "Epoch [7/10], Step [482/600], Loss: 0.5188, Train Acc:0.8224\n",
      "Epoch [7/10], Step [483/600], Loss: 0.5423, Train Acc:0.8224\n",
      "Epoch [7/10], Step [484/600], Loss: 0.5652, Train Acc:0.8224\n",
      "Epoch [7/10], Step [485/600], Loss: 0.4498, Train Acc:0.8225\n",
      "Epoch [7/10], Step [486/600], Loss: 0.5283, Train Acc:0.8225\n",
      "Epoch [7/10], Step [487/600], Loss: 0.6966, Train Acc:0.8223\n",
      "Epoch [7/10], Step [488/600], Loss: 0.7014, Train Acc:0.8222\n",
      "Epoch [7/10], Step [489/600], Loss: 0.6402, Train Acc:0.8221\n",
      "Epoch [7/10], Step [490/600], Loss: 0.5683, Train Acc:0.8221\n",
      "Epoch [7/10], Step [491/600], Loss: 0.5200, Train Acc:0.8221\n",
      "Epoch [7/10], Step [492/600], Loss: 0.4641, Train Acc:0.8222\n",
      "Epoch [7/10], Step [493/600], Loss: 0.6459, Train Acc:0.8222\n",
      "Epoch [7/10], Step [494/600], Loss: 0.3649, Train Acc:0.8222\n",
      "Epoch [7/10], Step [495/600], Loss: 0.6640, Train Acc:0.8222\n",
      "Epoch [7/10], Step [496/600], Loss: 0.5955, Train Acc:0.8221\n",
      "Epoch [7/10], Step [497/600], Loss: 0.4129, Train Acc:0.8221\n",
      "Epoch [7/10], Step [498/600], Loss: 0.5218, Train Acc:0.8222\n",
      "Epoch [7/10], Step [499/600], Loss: 0.4265, Train Acc:0.8222\n",
      "Epoch [7/10], Step [500/600], Loss: 0.5494, Train Acc:0.8222\n",
      "Epoch [7/10], Step [501/600], Loss: 0.5949, Train Acc:0.8223\n",
      "Epoch [7/10], Step [502/600], Loss: 0.5472, Train Acc:0.8222\n",
      "Epoch [7/10], Step [503/600], Loss: 0.5637, Train Acc:0.8222\n",
      "Epoch [7/10], Step [504/600], Loss: 0.4917, Train Acc:0.8223\n",
      "Epoch [7/10], Step [505/600], Loss: 0.6028, Train Acc:0.8222\n",
      "Epoch [7/10], Step [506/600], Loss: 0.6584, Train Acc:0.8221\n",
      "Epoch [7/10], Step [507/600], Loss: 0.4913, Train Acc:0.8222\n",
      "Epoch [7/10], Step [508/600], Loss: 0.6188, Train Acc:0.8220\n",
      "Epoch [7/10], Step [509/600], Loss: 0.6111, Train Acc:0.8219\n",
      "Epoch [7/10], Step [510/600], Loss: 0.5871, Train Acc:0.8220\n",
      "Epoch [7/10], Step [511/600], Loss: 0.4717, Train Acc:0.8220\n",
      "Epoch [7/10], Step [512/600], Loss: 0.4296, Train Acc:0.8221\n",
      "Epoch [7/10], Step [513/600], Loss: 0.6616, Train Acc:0.8221\n",
      "Epoch [7/10], Step [514/600], Loss: 0.4560, Train Acc:0.8222\n",
      "Epoch [7/10], Step [515/600], Loss: 0.7636, Train Acc:0.8220\n",
      "Epoch [7/10], Step [516/600], Loss: 0.6141, Train Acc:0.8219\n",
      "Epoch [7/10], Step [517/600], Loss: 0.6276, Train Acc:0.8219\n",
      "Epoch [7/10], Step [518/600], Loss: 0.5679, Train Acc:0.8219\n",
      "Epoch [7/10], Step [519/600], Loss: 0.5544, Train Acc:0.8219\n",
      "Epoch [7/10], Step [520/600], Loss: 0.4871, Train Acc:0.8219\n",
      "Epoch [7/10], Step [521/600], Loss: 0.5497, Train Acc:0.8219\n",
      "Epoch [7/10], Step [522/600], Loss: 0.4832, Train Acc:0.8220\n",
      "Epoch [7/10], Step [523/600], Loss: 0.6780, Train Acc:0.8218\n",
      "Epoch [7/10], Step [524/600], Loss: 0.4950, Train Acc:0.8218\n",
      "Epoch [7/10], Step [525/600], Loss: 0.4494, Train Acc:0.8218\n",
      "Epoch [7/10], Step [526/600], Loss: 0.5854, Train Acc:0.8218\n",
      "Epoch [7/10], Step [527/600], Loss: 0.5581, Train Acc:0.8218\n",
      "Epoch [7/10], Step [528/600], Loss: 0.5368, Train Acc:0.8219\n",
      "Epoch [7/10], Step [529/600], Loss: 0.5451, Train Acc:0.8218\n",
      "Epoch [7/10], Step [530/600], Loss: 0.6445, Train Acc:0.8217\n",
      "Epoch [7/10], Step [531/600], Loss: 0.4505, Train Acc:0.8217\n",
      "Epoch [7/10], Step [532/600], Loss: 0.4641, Train Acc:0.8218\n",
      "Epoch [7/10], Step [533/600], Loss: 0.6186, Train Acc:0.8217\n",
      "Epoch [7/10], Step [534/600], Loss: 0.6895, Train Acc:0.8216\n",
      "Epoch [7/10], Step [535/600], Loss: 0.4707, Train Acc:0.8216\n",
      "Epoch [7/10], Step [536/600], Loss: 0.5675, Train Acc:0.8216\n",
      "Epoch [7/10], Step [537/600], Loss: 0.5131, Train Acc:0.8216\n",
      "Epoch [7/10], Step [538/600], Loss: 0.6379, Train Acc:0.8215\n",
      "Epoch [7/10], Step [539/600], Loss: 0.5774, Train Acc:0.8214\n",
      "Epoch [7/10], Step [540/600], Loss: 0.6406, Train Acc:0.8214\n",
      "Epoch [7/10], Step [541/600], Loss: 0.6430, Train Acc:0.8213\n",
      "Epoch [7/10], Step [542/600], Loss: 0.5094, Train Acc:0.8213\n",
      "Epoch [7/10], Step [543/600], Loss: 0.5454, Train Acc:0.8214\n",
      "Epoch [7/10], Step [544/600], Loss: 0.4567, Train Acc:0.8215\n",
      "Epoch [7/10], Step [545/600], Loss: 0.6161, Train Acc:0.8214\n",
      "Epoch [7/10], Step [546/600], Loss: 0.5942, Train Acc:0.8214\n",
      "Epoch [7/10], Step [547/600], Loss: 0.6112, Train Acc:0.8212\n",
      "Epoch [7/10], Step [548/600], Loss: 0.4859, Train Acc:0.8212\n",
      "Epoch [7/10], Step [549/600], Loss: 0.5738, Train Acc:0.8212\n",
      "Epoch [7/10], Step [550/600], Loss: 0.4689, Train Acc:0.8213\n",
      "Epoch [7/10], Step [551/600], Loss: 0.6262, Train Acc:0.8212\n",
      "Epoch [7/10], Step [552/600], Loss: 0.5640, Train Acc:0.8211\n",
      "Epoch [7/10], Step [553/600], Loss: 0.6794, Train Acc:0.8211\n",
      "Epoch [7/10], Step [554/600], Loss: 0.7056, Train Acc:0.8210\n",
      "Epoch [7/10], Step [555/600], Loss: 0.5633, Train Acc:0.8211\n",
      "Epoch [7/10], Step [556/600], Loss: 0.4263, Train Acc:0.8211\n",
      "Epoch [7/10], Step [557/600], Loss: 0.5592, Train Acc:0.8211\n",
      "Epoch [7/10], Step [558/600], Loss: 0.5717, Train Acc:0.8211\n",
      "Epoch [7/10], Step [559/600], Loss: 0.4955, Train Acc:0.8211\n",
      "Epoch [7/10], Step [560/600], Loss: 0.4545, Train Acc:0.8211\n",
      "Epoch [7/10], Step [561/600], Loss: 0.5318, Train Acc:0.8211\n",
      "Epoch [7/10], Step [562/600], Loss: 0.5535, Train Acc:0.8210\n",
      "Epoch [7/10], Step [563/600], Loss: 0.6162, Train Acc:0.8210\n",
      "Epoch [7/10], Step [564/600], Loss: 0.5809, Train Acc:0.8210\n",
      "Epoch [7/10], Step [565/600], Loss: 0.7628, Train Acc:0.8209\n",
      "Epoch [7/10], Step [566/600], Loss: 0.4734, Train Acc:0.8210\n",
      "Epoch [7/10], Step [567/600], Loss: 0.6541, Train Acc:0.8209\n",
      "Epoch [7/10], Step [568/600], Loss: 0.5634, Train Acc:0.8209\n",
      "Epoch [7/10], Step [569/600], Loss: 0.5550, Train Acc:0.8209\n",
      "Epoch [7/10], Step [570/600], Loss: 0.4732, Train Acc:0.8209\n",
      "Epoch [7/10], Step [571/600], Loss: 0.5537, Train Acc:0.8210\n",
      "Epoch [7/10], Step [572/600], Loss: 0.4643, Train Acc:0.8211\n",
      "Epoch [7/10], Step [573/600], Loss: 0.4471, Train Acc:0.8211\n",
      "Epoch [7/10], Step [574/600], Loss: 0.6803, Train Acc:0.8211\n",
      "Epoch [7/10], Step [575/600], Loss: 0.4647, Train Acc:0.8212\n",
      "Epoch [7/10], Step [576/600], Loss: 0.6352, Train Acc:0.8211\n",
      "Epoch [7/10], Step [577/600], Loss: 0.4752, Train Acc:0.8212\n",
      "Epoch [7/10], Step [578/600], Loss: 0.5902, Train Acc:0.8212\n",
      "Epoch [7/10], Step [579/600], Loss: 0.7080, Train Acc:0.8210\n",
      "Epoch [7/10], Step [580/600], Loss: 0.5771, Train Acc:0.8210\n",
      "Epoch [7/10], Step [581/600], Loss: 0.4632, Train Acc:0.8211\n",
      "Epoch [7/10], Step [582/600], Loss: 0.6066, Train Acc:0.8210\n",
      "Epoch [7/10], Step [583/600], Loss: 0.4327, Train Acc:0.8211\n",
      "Epoch [7/10], Step [584/600], Loss: 0.7497, Train Acc:0.8209\n",
      "Epoch [7/10], Step [585/600], Loss: 0.6537, Train Acc:0.8208\n",
      "Epoch [7/10], Step [586/600], Loss: 0.4330, Train Acc:0.8209\n",
      "Epoch [7/10], Step [587/600], Loss: 0.5485, Train Acc:0.8208\n",
      "Epoch [7/10], Step [588/600], Loss: 0.7024, Train Acc:0.8207\n",
      "Epoch [7/10], Step [589/600], Loss: 0.5572, Train Acc:0.8207\n",
      "Epoch [7/10], Step [590/600], Loss: 0.6183, Train Acc:0.8207\n",
      "Epoch [7/10], Step [591/600], Loss: 0.5331, Train Acc:0.8207\n",
      "Epoch [7/10], Step [592/600], Loss: 0.5852, Train Acc:0.8207\n",
      "Epoch [7/10], Step [593/600], Loss: 0.5955, Train Acc:0.8207\n",
      "Epoch [7/10], Step [594/600], Loss: 0.5669, Train Acc:0.8208\n",
      "Epoch [7/10], Step [595/600], Loss: 0.5660, Train Acc:0.8207\n",
      "Epoch [7/10], Step [596/600], Loss: 0.4814, Train Acc:0.8208\n",
      "Epoch [7/10], Step [597/600], Loss: 0.6574, Train Acc:0.8207\n",
      "Epoch [7/10], Step [598/600], Loss: 0.5241, Train Acc:0.8207\n",
      "Epoch [7/10], Step [599/600], Loss: 0.4819, Train Acc:0.8207\n",
      "Epoch [7/10], Step [600/600], Loss: 0.6381, Train Acc:0.8206\n",
      "Epoch [8/10], Step [1/600], Loss: 0.4985, Train Acc:0.8200\n",
      "Epoch [8/10], Step [2/600], Loss: 0.6582, Train Acc:0.8250\n",
      "Epoch [8/10], Step [3/600], Loss: 0.4030, Train Acc:0.8500\n",
      "Epoch [8/10], Step [4/600], Loss: 0.4894, Train Acc:0.8500\n",
      "Epoch [8/10], Step [5/600], Loss: 0.5868, Train Acc:0.8360\n",
      "Epoch [8/10], Step [6/600], Loss: 0.4765, Train Acc:0.8450\n",
      "Epoch [8/10], Step [7/600], Loss: 0.6058, Train Acc:0.8471\n",
      "Epoch [8/10], Step [8/600], Loss: 0.5845, Train Acc:0.8387\n",
      "Epoch [8/10], Step [9/600], Loss: 0.5493, Train Acc:0.8400\n",
      "Epoch [8/10], Step [10/600], Loss: 0.4685, Train Acc:0.8390\n",
      "Epoch [8/10], Step [11/600], Loss: 0.5275, Train Acc:0.8382\n",
      "Epoch [8/10], Step [12/600], Loss: 0.7618, Train Acc:0.8308\n",
      "Epoch [8/10], Step [13/600], Loss: 0.4899, Train Acc:0.8354\n",
      "Epoch [8/10], Step [14/600], Loss: 0.4528, Train Acc:0.8364\n",
      "Epoch [8/10], Step [15/600], Loss: 0.4087, Train Acc:0.8413\n",
      "Epoch [8/10], Step [16/600], Loss: 0.5585, Train Acc:0.8387\n",
      "Epoch [8/10], Step [17/600], Loss: 0.3560, Train Acc:0.8429\n",
      "Epoch [8/10], Step [18/600], Loss: 0.5490, Train Acc:0.8417\n",
      "Epoch [8/10], Step [19/600], Loss: 0.5946, Train Acc:0.8400\n",
      "Epoch [8/10], Step [20/600], Loss: 0.5762, Train Acc:0.8375\n",
      "Epoch [8/10], Step [21/600], Loss: 0.6221, Train Acc:0.8357\n",
      "Epoch [8/10], Step [22/600], Loss: 0.3664, Train Acc:0.8391\n",
      "Epoch [8/10], Step [23/600], Loss: 0.5716, Train Acc:0.8378\n",
      "Epoch [8/10], Step [24/600], Loss: 0.5149, Train Acc:0.8392\n",
      "Epoch [8/10], Step [25/600], Loss: 0.4959, Train Acc:0.8388\n",
      "Epoch [8/10], Step [26/600], Loss: 0.5218, Train Acc:0.8396\n",
      "Epoch [8/10], Step [27/600], Loss: 0.5270, Train Acc:0.8393\n",
      "Epoch [8/10], Step [28/600], Loss: 0.4558, Train Acc:0.8400\n",
      "Epoch [8/10], Step [29/600], Loss: 0.6770, Train Acc:0.8390\n",
      "Epoch [8/10], Step [30/600], Loss: 0.5781, Train Acc:0.8377\n",
      "Epoch [8/10], Step [31/600], Loss: 0.5978, Train Acc:0.8358\n",
      "Epoch [8/10], Step [32/600], Loss: 0.6317, Train Acc:0.8325\n",
      "Epoch [8/10], Step [33/600], Loss: 0.3903, Train Acc:0.8333\n",
      "Epoch [8/10], Step [34/600], Loss: 0.4642, Train Acc:0.8341\n",
      "Epoch [8/10], Step [35/600], Loss: 0.5828, Train Acc:0.8346\n",
      "Epoch [8/10], Step [36/600], Loss: 0.4885, Train Acc:0.8347\n",
      "Epoch [8/10], Step [37/600], Loss: 0.5166, Train Acc:0.8346\n",
      "Epoch [8/10], Step [38/600], Loss: 0.4730, Train Acc:0.8350\n",
      "Epoch [8/10], Step [39/600], Loss: 0.5066, Train Acc:0.8349\n",
      "Epoch [8/10], Step [40/600], Loss: 0.6409, Train Acc:0.8333\n",
      "Epoch [8/10], Step [41/600], Loss: 0.6440, Train Acc:0.8312\n",
      "Epoch [8/10], Step [42/600], Loss: 0.5731, Train Acc:0.8300\n",
      "Epoch [8/10], Step [43/600], Loss: 0.5008, Train Acc:0.8300\n",
      "Epoch [8/10], Step [44/600], Loss: 0.5661, Train Acc:0.8309\n",
      "Epoch [8/10], Step [45/600], Loss: 0.5974, Train Acc:0.8304\n",
      "Epoch [8/10], Step [46/600], Loss: 0.4792, Train Acc:0.8304\n",
      "Epoch [8/10], Step [47/600], Loss: 0.6605, Train Acc:0.8302\n",
      "Epoch [8/10], Step [48/600], Loss: 0.4465, Train Acc:0.8306\n",
      "Epoch [8/10], Step [49/600], Loss: 0.4814, Train Acc:0.8314\n",
      "Epoch [8/10], Step [50/600], Loss: 0.5317, Train Acc:0.8316\n",
      "Epoch [8/10], Step [51/600], Loss: 0.6028, Train Acc:0.8316\n",
      "Epoch [8/10], Step [52/600], Loss: 0.6212, Train Acc:0.8308\n",
      "Epoch [8/10], Step [53/600], Loss: 0.5368, Train Acc:0.8304\n",
      "Epoch [8/10], Step [54/600], Loss: 0.5861, Train Acc:0.8300\n",
      "Epoch [8/10], Step [55/600], Loss: 0.4505, Train Acc:0.8305\n",
      "Epoch [8/10], Step [56/600], Loss: 0.6169, Train Acc:0.8293\n",
      "Epoch [8/10], Step [57/600], Loss: 0.6273, Train Acc:0.8291\n",
      "Epoch [8/10], Step [58/600], Loss: 0.5375, Train Acc:0.8283\n",
      "Epoch [8/10], Step [59/600], Loss: 0.5619, Train Acc:0.8280\n",
      "Epoch [8/10], Step [60/600], Loss: 0.5654, Train Acc:0.8273\n",
      "Epoch [8/10], Step [61/600], Loss: 0.4133, Train Acc:0.8279\n",
      "Epoch [8/10], Step [62/600], Loss: 0.4646, Train Acc:0.8287\n",
      "Epoch [8/10], Step [63/600], Loss: 0.6107, Train Acc:0.8281\n",
      "Epoch [8/10], Step [64/600], Loss: 0.5069, Train Acc:0.8277\n",
      "Epoch [8/10], Step [65/600], Loss: 0.5112, Train Acc:0.8282\n",
      "Epoch [8/10], Step [66/600], Loss: 0.5320, Train Acc:0.8283\n",
      "Epoch [8/10], Step [67/600], Loss: 0.6711, Train Acc:0.8276\n",
      "Epoch [8/10], Step [68/600], Loss: 0.5100, Train Acc:0.8269\n",
      "Epoch [8/10], Step [69/600], Loss: 0.5308, Train Acc:0.8268\n",
      "Epoch [8/10], Step [70/600], Loss: 0.4240, Train Acc:0.8270\n",
      "Epoch [8/10], Step [71/600], Loss: 0.5339, Train Acc:0.8259\n",
      "Epoch [8/10], Step [72/600], Loss: 0.4504, Train Acc:0.8264\n",
      "Epoch [8/10], Step [73/600], Loss: 0.4602, Train Acc:0.8268\n",
      "Epoch [8/10], Step [74/600], Loss: 0.4557, Train Acc:0.8269\n",
      "Epoch [8/10], Step [75/600], Loss: 0.6340, Train Acc:0.8265\n",
      "Epoch [8/10], Step [76/600], Loss: 0.5314, Train Acc:0.8264\n",
      "Epoch [8/10], Step [77/600], Loss: 0.5201, Train Acc:0.8268\n",
      "Epoch [8/10], Step [78/600], Loss: 0.4497, Train Acc:0.8273\n",
      "Epoch [8/10], Step [79/600], Loss: 0.5379, Train Acc:0.8270\n",
      "Epoch [8/10], Step [80/600], Loss: 0.5278, Train Acc:0.8265\n",
      "Epoch [8/10], Step [81/600], Loss: 0.5196, Train Acc:0.8268\n",
      "Epoch [8/10], Step [82/600], Loss: 0.5582, Train Acc:0.8266\n",
      "Epoch [8/10], Step [83/600], Loss: 0.4224, Train Acc:0.8271\n",
      "Epoch [8/10], Step [84/600], Loss: 0.6253, Train Acc:0.8262\n",
      "Epoch [8/10], Step [85/600], Loss: 0.5526, Train Acc:0.8262\n",
      "Epoch [8/10], Step [86/600], Loss: 0.5402, Train Acc:0.8264\n",
      "Epoch [8/10], Step [87/600], Loss: 0.5213, Train Acc:0.8263\n",
      "Epoch [8/10], Step [88/600], Loss: 0.5799, Train Acc:0.8264\n",
      "Epoch [8/10], Step [89/600], Loss: 0.5341, Train Acc:0.8262\n",
      "Epoch [8/10], Step [90/600], Loss: 0.5732, Train Acc:0.8257\n",
      "Epoch [8/10], Step [91/600], Loss: 0.4352, Train Acc:0.8263\n",
      "Epoch [8/10], Step [92/600], Loss: 0.5533, Train Acc:0.8264\n",
      "Epoch [8/10], Step [93/600], Loss: 0.4275, Train Acc:0.8269\n",
      "Epoch [8/10], Step [94/600], Loss: 0.3974, Train Acc:0.8272\n",
      "Epoch [8/10], Step [95/600], Loss: 0.5335, Train Acc:0.8274\n",
      "Epoch [8/10], Step [96/600], Loss: 0.5918, Train Acc:0.8273\n",
      "Epoch [8/10], Step [97/600], Loss: 0.5658, Train Acc:0.8270\n",
      "Epoch [8/10], Step [98/600], Loss: 0.6830, Train Acc:0.8264\n",
      "Epoch [8/10], Step [99/600], Loss: 0.4180, Train Acc:0.8272\n",
      "Epoch [8/10], Step [100/600], Loss: 0.5665, Train Acc:0.8271\n",
      "Epoch [8/10], Step [101/600], Loss: 0.4919, Train Acc:0.8275\n",
      "Epoch [8/10], Step [102/600], Loss: 0.5231, Train Acc:0.8277\n",
      "Epoch [8/10], Step [103/600], Loss: 0.4517, Train Acc:0.8283\n",
      "Epoch [8/10], Step [104/600], Loss: 0.5053, Train Acc:0.8282\n",
      "Epoch [8/10], Step [105/600], Loss: 0.5653, Train Acc:0.8277\n",
      "Epoch [8/10], Step [106/600], Loss: 0.4571, Train Acc:0.8280\n",
      "Epoch [8/10], Step [107/600], Loss: 0.6397, Train Acc:0.8276\n",
      "Epoch [8/10], Step [108/600], Loss: 0.5239, Train Acc:0.8276\n",
      "Epoch [8/10], Step [109/600], Loss: 0.4963, Train Acc:0.8275\n",
      "Epoch [8/10], Step [110/600], Loss: 0.4520, Train Acc:0.8275\n",
      "Epoch [8/10], Step [111/600], Loss: 0.6095, Train Acc:0.8274\n",
      "Epoch [8/10], Step [112/600], Loss: 0.6294, Train Acc:0.8271\n",
      "Epoch [8/10], Step [113/600], Loss: 0.5360, Train Acc:0.8265\n",
      "Epoch [8/10], Step [114/600], Loss: 0.6053, Train Acc:0.8258\n",
      "Epoch [8/10], Step [115/600], Loss: 0.4906, Train Acc:0.8257\n",
      "Epoch [8/10], Step [116/600], Loss: 0.5458, Train Acc:0.8258\n",
      "Epoch [8/10], Step [117/600], Loss: 0.5406, Train Acc:0.8257\n",
      "Epoch [8/10], Step [118/600], Loss: 0.4867, Train Acc:0.8258\n",
      "Epoch [8/10], Step [119/600], Loss: 0.6281, Train Acc:0.8255\n",
      "Epoch [8/10], Step [120/600], Loss: 0.5170, Train Acc:0.8255\n",
      "Epoch [8/10], Step [121/600], Loss: 0.5051, Train Acc:0.8254\n",
      "Epoch [8/10], Step [122/600], Loss: 0.5213, Train Acc:0.8254\n",
      "Epoch [8/10], Step [123/600], Loss: 0.6513, Train Acc:0.8250\n",
      "Epoch [8/10], Step [124/600], Loss: 0.5823, Train Acc:0.8245\n",
      "Epoch [8/10], Step [125/600], Loss: 0.6586, Train Acc:0.8243\n",
      "Epoch [8/10], Step [126/600], Loss: 0.4089, Train Acc:0.8248\n",
      "Epoch [8/10], Step [127/600], Loss: 0.4897, Train Acc:0.8246\n",
      "Epoch [8/10], Step [128/600], Loss: 0.4849, Train Acc:0.8247\n",
      "Epoch [8/10], Step [129/600], Loss: 0.4450, Train Acc:0.8250\n",
      "Epoch [8/10], Step [130/600], Loss: 0.5324, Train Acc:0.8250\n",
      "Epoch [8/10], Step [131/600], Loss: 0.5706, Train Acc:0.8249\n",
      "Epoch [8/10], Step [132/600], Loss: 0.5989, Train Acc:0.8247\n",
      "Epoch [8/10], Step [133/600], Loss: 0.5045, Train Acc:0.8247\n",
      "Epoch [8/10], Step [134/600], Loss: 0.5858, Train Acc:0.8249\n",
      "Epoch [8/10], Step [135/600], Loss: 0.5652, Train Acc:0.8250\n",
      "Epoch [8/10], Step [136/600], Loss: 0.4299, Train Acc:0.8254\n",
      "Epoch [8/10], Step [137/600], Loss: 0.7462, Train Acc:0.8248\n",
      "Epoch [8/10], Step [138/600], Loss: 0.5285, Train Acc:0.8248\n",
      "Epoch [8/10], Step [139/600], Loss: 0.5120, Train Acc:0.8250\n",
      "Epoch [8/10], Step [140/600], Loss: 0.3917, Train Acc:0.8251\n",
      "Epoch [8/10], Step [141/600], Loss: 0.5856, Train Acc:0.8248\n",
      "Epoch [8/10], Step [142/600], Loss: 0.4759, Train Acc:0.8250\n",
      "Epoch [8/10], Step [143/600], Loss: 0.5200, Train Acc:0.8253\n",
      "Epoch [8/10], Step [144/600], Loss: 0.6666, Train Acc:0.8248\n",
      "Epoch [8/10], Step [145/600], Loss: 0.4890, Train Acc:0.8250\n",
      "Epoch [8/10], Step [146/600], Loss: 0.5265, Train Acc:0.8249\n",
      "Epoch [8/10], Step [147/600], Loss: 0.4843, Train Acc:0.8250\n",
      "Epoch [8/10], Step [148/600], Loss: 0.5604, Train Acc:0.8250\n",
      "Epoch [8/10], Step [149/600], Loss: 0.4741, Train Acc:0.8250\n",
      "Epoch [8/10], Step [150/600], Loss: 0.4935, Train Acc:0.8252\n",
      "Epoch [8/10], Step [151/600], Loss: 0.7523, Train Acc:0.8246\n",
      "Epoch [8/10], Step [152/600], Loss: 0.6636, Train Acc:0.8246\n",
      "Epoch [8/10], Step [153/600], Loss: 0.3857, Train Acc:0.8250\n",
      "Epoch [8/10], Step [154/600], Loss: 0.4703, Train Acc:0.8251\n",
      "Epoch [8/10], Step [155/600], Loss: 0.5723, Train Acc:0.8252\n",
      "Epoch [8/10], Step [156/600], Loss: 0.5646, Train Acc:0.8252\n",
      "Epoch [8/10], Step [157/600], Loss: 0.5168, Train Acc:0.8251\n",
      "Epoch [8/10], Step [158/600], Loss: 0.4593, Train Acc:0.8254\n",
      "Epoch [8/10], Step [159/600], Loss: 0.5132, Train Acc:0.8252\n",
      "Epoch [8/10], Step [160/600], Loss: 0.5467, Train Acc:0.8253\n",
      "Epoch [8/10], Step [161/600], Loss: 0.4138, Train Acc:0.8255\n",
      "Epoch [8/10], Step [162/600], Loss: 0.5036, Train Acc:0.8254\n",
      "Epoch [8/10], Step [163/600], Loss: 0.5132, Train Acc:0.8253\n",
      "Epoch [8/10], Step [164/600], Loss: 0.5223, Train Acc:0.8254\n",
      "Epoch [8/10], Step [165/600], Loss: 0.4774, Train Acc:0.8253\n",
      "Epoch [8/10], Step [166/600], Loss: 0.4840, Train Acc:0.8251\n",
      "Epoch [8/10], Step [167/600], Loss: 0.7159, Train Acc:0.8247\n",
      "Epoch [8/10], Step [168/600], Loss: 0.6926, Train Acc:0.8243\n",
      "Epoch [8/10], Step [169/600], Loss: 0.5579, Train Acc:0.8245\n",
      "Epoch [8/10], Step [170/600], Loss: 0.4325, Train Acc:0.8249\n",
      "Epoch [8/10], Step [171/600], Loss: 0.5621, Train Acc:0.8250\n",
      "Epoch [8/10], Step [172/600], Loss: 0.4329, Train Acc:0.8251\n",
      "Epoch [8/10], Step [173/600], Loss: 0.5475, Train Acc:0.8252\n",
      "Epoch [8/10], Step [174/600], Loss: 0.4629, Train Acc:0.8252\n",
      "Epoch [8/10], Step [175/600], Loss: 0.4315, Train Acc:0.8254\n",
      "Epoch [8/10], Step [176/600], Loss: 0.6563, Train Acc:0.8250\n",
      "Epoch [8/10], Step [177/600], Loss: 0.4238, Train Acc:0.8251\n",
      "Epoch [8/10], Step [178/600], Loss: 0.6669, Train Acc:0.8249\n",
      "Epoch [8/10], Step [179/600], Loss: 0.4800, Train Acc:0.8250\n",
      "Epoch [8/10], Step [180/600], Loss: 0.4582, Train Acc:0.8252\n",
      "Epoch [8/10], Step [181/600], Loss: 0.4070, Train Acc:0.8255\n",
      "Epoch [8/10], Step [182/600], Loss: 0.5740, Train Acc:0.8254\n",
      "Epoch [8/10], Step [183/600], Loss: 0.5235, Train Acc:0.8253\n",
      "Epoch [8/10], Step [184/600], Loss: 0.5645, Train Acc:0.8251\n",
      "Epoch [8/10], Step [185/600], Loss: 0.6174, Train Acc:0.8249\n",
      "Epoch [8/10], Step [186/600], Loss: 0.5817, Train Acc:0.8247\n",
      "Epoch [8/10], Step [187/600], Loss: 0.6144, Train Acc:0.8247\n",
      "Epoch [8/10], Step [188/600], Loss: 0.6031, Train Acc:0.8246\n",
      "Epoch [8/10], Step [189/600], Loss: 0.4688, Train Acc:0.8247\n",
      "Epoch [8/10], Step [190/600], Loss: 0.6700, Train Acc:0.8245\n",
      "Epoch [8/10], Step [191/600], Loss: 0.5017, Train Acc:0.8245\n",
      "Epoch [8/10], Step [192/600], Loss: 0.5967, Train Acc:0.8246\n",
      "Epoch [8/10], Step [193/600], Loss: 0.4953, Train Acc:0.8247\n",
      "Epoch [8/10], Step [194/600], Loss: 0.6381, Train Acc:0.8243\n",
      "Epoch [8/10], Step [195/600], Loss: 0.5329, Train Acc:0.8242\n",
      "Epoch [8/10], Step [196/600], Loss: 0.5136, Train Acc:0.8241\n",
      "Epoch [8/10], Step [197/600], Loss: 0.5263, Train Acc:0.8242\n",
      "Epoch [8/10], Step [198/600], Loss: 0.8216, Train Acc:0.8237\n",
      "Epoch [8/10], Step [199/600], Loss: 0.4999, Train Acc:0.8238\n",
      "Epoch [8/10], Step [200/600], Loss: 0.5890, Train Acc:0.8239\n",
      "Epoch [8/10], Step [201/600], Loss: 0.4821, Train Acc:0.8240\n",
      "Epoch [8/10], Step [202/600], Loss: 0.4362, Train Acc:0.8242\n",
      "Epoch [8/10], Step [203/600], Loss: 0.6033, Train Acc:0.8240\n",
      "Epoch [8/10], Step [204/600], Loss: 0.4530, Train Acc:0.8242\n",
      "Epoch [8/10], Step [205/600], Loss: 0.4192, Train Acc:0.8243\n",
      "Epoch [8/10], Step [206/600], Loss: 0.5022, Train Acc:0.8246\n",
      "Epoch [8/10], Step [207/600], Loss: 0.5470, Train Acc:0.8244\n",
      "Epoch [8/10], Step [208/600], Loss: 0.5932, Train Acc:0.8245\n",
      "Epoch [8/10], Step [209/600], Loss: 0.6198, Train Acc:0.8243\n",
      "Epoch [8/10], Step [210/600], Loss: 0.5407, Train Acc:0.8243\n",
      "Epoch [8/10], Step [211/600], Loss: 0.6465, Train Acc:0.8241\n",
      "Epoch [8/10], Step [212/600], Loss: 0.4502, Train Acc:0.8242\n",
      "Epoch [8/10], Step [213/600], Loss: 0.4670, Train Acc:0.8243\n",
      "Epoch [8/10], Step [214/600], Loss: 0.6112, Train Acc:0.8243\n",
      "Epoch [8/10], Step [215/600], Loss: 0.5234, Train Acc:0.8244\n",
      "Epoch [8/10], Step [216/600], Loss: 0.4283, Train Acc:0.8247\n",
      "Epoch [8/10], Step [217/600], Loss: 0.5891, Train Acc:0.8247\n",
      "Epoch [8/10], Step [218/600], Loss: 0.5618, Train Acc:0.8245\n",
      "Epoch [8/10], Step [219/600], Loss: 0.4082, Train Acc:0.8248\n",
      "Epoch [8/10], Step [220/600], Loss: 0.4988, Train Acc:0.8248\n",
      "Epoch [8/10], Step [221/600], Loss: 0.5420, Train Acc:0.8248\n",
      "Epoch [8/10], Step [222/600], Loss: 0.4703, Train Acc:0.8250\n",
      "Epoch [8/10], Step [223/600], Loss: 0.4524, Train Acc:0.8252\n",
      "Epoch [8/10], Step [224/600], Loss: 0.5748, Train Acc:0.8248\n",
      "Epoch [8/10], Step [225/600], Loss: 0.5433, Train Acc:0.8247\n",
      "Epoch [8/10], Step [226/600], Loss: 0.4714, Train Acc:0.8249\n",
      "Epoch [8/10], Step [227/600], Loss: 0.6330, Train Acc:0.8246\n",
      "Epoch [8/10], Step [228/600], Loss: 0.5373, Train Acc:0.8246\n",
      "Epoch [8/10], Step [229/600], Loss: 0.4924, Train Acc:0.8248\n",
      "Epoch [8/10], Step [230/600], Loss: 0.5325, Train Acc:0.8248\n",
      "Epoch [8/10], Step [231/600], Loss: 0.4798, Train Acc:0.8248\n",
      "Epoch [8/10], Step [232/600], Loss: 0.6256, Train Acc:0.8247\n",
      "Epoch [8/10], Step [233/600], Loss: 0.5373, Train Acc:0.8246\n",
      "Epoch [8/10], Step [234/600], Loss: 0.5357, Train Acc:0.8246\n",
      "Epoch [8/10], Step [235/600], Loss: 0.6243, Train Acc:0.8244\n",
      "Epoch [8/10], Step [236/600], Loss: 0.5831, Train Acc:0.8243\n",
      "Epoch [8/10], Step [237/600], Loss: 0.5976, Train Acc:0.8243\n",
      "Epoch [8/10], Step [238/600], Loss: 0.5014, Train Acc:0.8244\n",
      "Epoch [8/10], Step [239/600], Loss: 0.4426, Train Acc:0.8246\n",
      "Epoch [8/10], Step [240/600], Loss: 0.4326, Train Acc:0.8247\n",
      "Epoch [8/10], Step [241/600], Loss: 0.4938, Train Acc:0.8247\n",
      "Epoch [8/10], Step [242/600], Loss: 0.6833, Train Acc:0.8246\n",
      "Epoch [8/10], Step [243/600], Loss: 0.5800, Train Acc:0.8244\n",
      "Epoch [8/10], Step [244/600], Loss: 0.4842, Train Acc:0.8245\n",
      "Epoch [8/10], Step [245/600], Loss: 0.5824, Train Acc:0.8243\n",
      "Epoch [8/10], Step [246/600], Loss: 0.4730, Train Acc:0.8243\n",
      "Epoch [8/10], Step [247/600], Loss: 0.4202, Train Acc:0.8244\n",
      "Epoch [8/10], Step [248/600], Loss: 0.5311, Train Acc:0.8244\n",
      "Epoch [8/10], Step [249/600], Loss: 0.5016, Train Acc:0.8244\n",
      "Epoch [8/10], Step [250/600], Loss: 0.5733, Train Acc:0.8244\n",
      "Epoch [8/10], Step [251/600], Loss: 0.3940, Train Acc:0.8246\n",
      "Epoch [8/10], Step [252/600], Loss: 0.4568, Train Acc:0.8248\n",
      "Epoch [8/10], Step [253/600], Loss: 0.7146, Train Acc:0.8245\n",
      "Epoch [8/10], Step [254/600], Loss: 0.5754, Train Acc:0.8246\n",
      "Epoch [8/10], Step [255/600], Loss: 0.4753, Train Acc:0.8247\n",
      "Epoch [8/10], Step [256/600], Loss: 0.5404, Train Acc:0.8246\n",
      "Epoch [8/10], Step [257/600], Loss: 0.6638, Train Acc:0.8245\n",
      "Epoch [8/10], Step [258/600], Loss: 0.4595, Train Acc:0.8247\n",
      "Epoch [8/10], Step [259/600], Loss: 0.4691, Train Acc:0.8248\n",
      "Epoch [8/10], Step [260/600], Loss: 0.5105, Train Acc:0.8249\n",
      "Epoch [8/10], Step [261/600], Loss: 0.6271, Train Acc:0.8248\n",
      "Epoch [8/10], Step [262/600], Loss: 0.5247, Train Acc:0.8249\n",
      "Epoch [8/10], Step [263/600], Loss: 0.5277, Train Acc:0.8249\n",
      "Epoch [8/10], Step [264/600], Loss: 0.4235, Train Acc:0.8251\n",
      "Epoch [8/10], Step [265/600], Loss: 0.6708, Train Acc:0.8249\n",
      "Epoch [8/10], Step [266/600], Loss: 0.6158, Train Acc:0.8248\n",
      "Epoch [8/10], Step [267/600], Loss: 0.5634, Train Acc:0.8246\n",
      "Epoch [8/10], Step [268/600], Loss: 0.5409, Train Acc:0.8245\n",
      "Epoch [8/10], Step [269/600], Loss: 0.5084, Train Acc:0.8245\n",
      "Epoch [8/10], Step [270/600], Loss: 0.4120, Train Acc:0.8247\n",
      "Epoch [8/10], Step [271/600], Loss: 0.6045, Train Acc:0.8246\n",
      "Epoch [8/10], Step [272/600], Loss: 0.5060, Train Acc:0.8247\n",
      "Epoch [8/10], Step [273/600], Loss: 0.5915, Train Acc:0.8245\n",
      "Epoch [8/10], Step [274/600], Loss: 0.5995, Train Acc:0.8245\n",
      "Epoch [8/10], Step [275/600], Loss: 0.6371, Train Acc:0.8244\n",
      "Epoch [8/10], Step [276/600], Loss: 0.6169, Train Acc:0.8242\n",
      "Epoch [8/10], Step [277/600], Loss: 0.4492, Train Acc:0.8243\n",
      "Epoch [8/10], Step [278/600], Loss: 0.4698, Train Acc:0.8245\n",
      "Epoch [8/10], Step [279/600], Loss: 0.4713, Train Acc:0.8246\n",
      "Epoch [8/10], Step [280/600], Loss: 0.6355, Train Acc:0.8245\n",
      "Epoch [8/10], Step [281/600], Loss: 0.5053, Train Acc:0.8244\n",
      "Epoch [8/10], Step [282/600], Loss: 0.5155, Train Acc:0.8246\n",
      "Epoch [8/10], Step [283/600], Loss: 0.4912, Train Acc:0.8246\n",
      "Epoch [8/10], Step [284/600], Loss: 0.4556, Train Acc:0.8246\n",
      "Epoch [8/10], Step [285/600], Loss: 0.5030, Train Acc:0.8245\n",
      "Epoch [8/10], Step [286/600], Loss: 0.4636, Train Acc:0.8246\n",
      "Epoch [8/10], Step [287/600], Loss: 0.7738, Train Acc:0.8244\n",
      "Epoch [8/10], Step [288/600], Loss: 0.4862, Train Acc:0.8244\n",
      "Epoch [8/10], Step [289/600], Loss: 0.5996, Train Acc:0.8246\n",
      "Epoch [8/10], Step [290/600], Loss: 0.6478, Train Acc:0.8245\n",
      "Epoch [8/10], Step [291/600], Loss: 0.6722, Train Acc:0.8242\n",
      "Epoch [8/10], Step [292/600], Loss: 0.4525, Train Acc:0.8245\n",
      "Epoch [8/10], Step [293/600], Loss: 0.6070, Train Acc:0.8245\n",
      "Epoch [8/10], Step [294/600], Loss: 0.4746, Train Acc:0.8246\n",
      "Epoch [8/10], Step [295/600], Loss: 0.5196, Train Acc:0.8246\n",
      "Epoch [8/10], Step [296/600], Loss: 0.4723, Train Acc:0.8246\n",
      "Epoch [8/10], Step [297/600], Loss: 0.6053, Train Acc:0.8244\n",
      "Epoch [8/10], Step [298/600], Loss: 0.4946, Train Acc:0.8246\n",
      "Epoch [8/10], Step [299/600], Loss: 0.4475, Train Acc:0.8246\n",
      "Epoch [8/10], Step [300/600], Loss: 0.6184, Train Acc:0.8244\n",
      "Epoch [8/10], Step [301/600], Loss: 0.5051, Train Acc:0.8246\n",
      "Epoch [8/10], Step [302/600], Loss: 0.5459, Train Acc:0.8246\n",
      "Epoch [8/10], Step [303/600], Loss: 0.6300, Train Acc:0.8246\n",
      "Epoch [8/10], Step [304/600], Loss: 0.6657, Train Acc:0.8245\n",
      "Epoch [8/10], Step [305/600], Loss: 0.4389, Train Acc:0.8248\n",
      "Epoch [8/10], Step [306/600], Loss: 0.5534, Train Acc:0.8248\n",
      "Epoch [8/10], Step [307/600], Loss: 0.5177, Train Acc:0.8248\n",
      "Epoch [8/10], Step [308/600], Loss: 0.4707, Train Acc:0.8248\n",
      "Epoch [8/10], Step [309/600], Loss: 0.6219, Train Acc:0.8247\n",
      "Epoch [8/10], Step [310/600], Loss: 0.5728, Train Acc:0.8248\n",
      "Epoch [8/10], Step [311/600], Loss: 0.5575, Train Acc:0.8247\n",
      "Epoch [8/10], Step [312/600], Loss: 0.4990, Train Acc:0.8247\n",
      "Epoch [8/10], Step [313/600], Loss: 0.4161, Train Acc:0.8248\n",
      "Epoch [8/10], Step [314/600], Loss: 0.5887, Train Acc:0.8249\n",
      "Epoch [8/10], Step [315/600], Loss: 0.5016, Train Acc:0.8251\n",
      "Epoch [8/10], Step [316/600], Loss: 0.5894, Train Acc:0.8249\n",
      "Epoch [8/10], Step [317/600], Loss: 0.5389, Train Acc:0.8249\n",
      "Epoch [8/10], Step [318/600], Loss: 0.5229, Train Acc:0.8248\n",
      "Epoch [8/10], Step [319/600], Loss: 0.4987, Train Acc:0.8248\n",
      "Epoch [8/10], Step [320/600], Loss: 0.5558, Train Acc:0.8248\n",
      "Epoch [8/10], Step [321/600], Loss: 0.5530, Train Acc:0.8247\n",
      "Epoch [8/10], Step [322/600], Loss: 0.4570, Train Acc:0.8247\n",
      "Epoch [8/10], Step [323/600], Loss: 0.5189, Train Acc:0.8247\n",
      "Epoch [8/10], Step [324/600], Loss: 0.4341, Train Acc:0.8248\n",
      "Epoch [8/10], Step [325/600], Loss: 0.6401, Train Acc:0.8247\n",
      "Epoch [8/10], Step [326/600], Loss: 0.5847, Train Acc:0.8248\n",
      "Epoch [8/10], Step [327/600], Loss: 0.5324, Train Acc:0.8247\n",
      "Epoch [8/10], Step [328/600], Loss: 0.5867, Train Acc:0.8247\n",
      "Epoch [8/10], Step [329/600], Loss: 0.6903, Train Acc:0.8245\n",
      "Epoch [8/10], Step [330/600], Loss: 0.4866, Train Acc:0.8245\n",
      "Epoch [8/10], Step [331/600], Loss: 0.5479, Train Acc:0.8246\n",
      "Epoch [8/10], Step [332/600], Loss: 0.6294, Train Acc:0.8245\n",
      "Epoch [8/10], Step [333/600], Loss: 0.6288, Train Acc:0.8243\n",
      "Epoch [8/10], Step [334/600], Loss: 0.6066, Train Acc:0.8243\n",
      "Epoch [8/10], Step [335/600], Loss: 0.6492, Train Acc:0.8241\n",
      "Epoch [8/10], Step [336/600], Loss: 0.5132, Train Acc:0.8241\n",
      "Epoch [8/10], Step [337/600], Loss: 0.4747, Train Acc:0.8242\n",
      "Epoch [8/10], Step [338/600], Loss: 0.5164, Train Acc:0.8243\n",
      "Epoch [8/10], Step [339/600], Loss: 0.3730, Train Acc:0.8245\n",
      "Epoch [8/10], Step [340/600], Loss: 0.6158, Train Acc:0.8245\n",
      "Epoch [8/10], Step [341/600], Loss: 0.5408, Train Acc:0.8245\n",
      "Epoch [8/10], Step [342/600], Loss: 0.6741, Train Acc:0.8244\n",
      "Epoch [8/10], Step [343/600], Loss: 0.4961, Train Acc:0.8245\n",
      "Epoch [8/10], Step [344/600], Loss: 0.6319, Train Acc:0.8244\n",
      "Epoch [8/10], Step [345/600], Loss: 0.5221, Train Acc:0.8245\n",
      "Epoch [8/10], Step [346/600], Loss: 0.5622, Train Acc:0.8243\n",
      "Epoch [8/10], Step [347/600], Loss: 0.4634, Train Acc:0.8244\n",
      "Epoch [8/10], Step [348/600], Loss: 0.8039, Train Acc:0.8240\n",
      "Epoch [8/10], Step [349/600], Loss: 0.5461, Train Acc:0.8241\n",
      "Epoch [8/10], Step [350/600], Loss: 0.6173, Train Acc:0.8240\n",
      "Epoch [8/10], Step [351/600], Loss: 0.5691, Train Acc:0.8240\n",
      "Epoch [8/10], Step [352/600], Loss: 0.4265, Train Acc:0.8241\n",
      "Epoch [8/10], Step [353/600], Loss: 0.5742, Train Acc:0.8240\n",
      "Epoch [8/10], Step [354/600], Loss: 0.5070, Train Acc:0.8240\n",
      "Epoch [8/10], Step [355/600], Loss: 0.5462, Train Acc:0.8240\n",
      "Epoch [8/10], Step [356/600], Loss: 0.5335, Train Acc:0.8242\n",
      "Epoch [8/10], Step [357/600], Loss: 0.4851, Train Acc:0.8242\n",
      "Epoch [8/10], Step [358/600], Loss: 0.5579, Train Acc:0.8242\n",
      "Epoch [8/10], Step [359/600], Loss: 0.5890, Train Acc:0.8241\n",
      "Epoch [8/10], Step [360/600], Loss: 0.4452, Train Acc:0.8242\n",
      "Epoch [8/10], Step [361/600], Loss: 0.3742, Train Acc:0.8244\n",
      "Epoch [8/10], Step [362/600], Loss: 0.5230, Train Acc:0.8244\n",
      "Epoch [8/10], Step [363/600], Loss: 0.6067, Train Acc:0.8244\n",
      "Epoch [8/10], Step [364/600], Loss: 0.6557, Train Acc:0.8243\n",
      "Epoch [8/10], Step [365/600], Loss: 0.5309, Train Acc:0.8243\n",
      "Epoch [8/10], Step [366/600], Loss: 0.5717, Train Acc:0.8243\n",
      "Epoch [8/10], Step [367/600], Loss: 0.4747, Train Acc:0.8243\n",
      "Epoch [8/10], Step [368/600], Loss: 0.5755, Train Acc:0.8241\n",
      "Epoch [8/10], Step [369/600], Loss: 0.3484, Train Acc:0.8243\n",
      "Epoch [8/10], Step [370/600], Loss: 0.4848, Train Acc:0.8243\n",
      "Epoch [8/10], Step [371/600], Loss: 0.4708, Train Acc:0.8245\n",
      "Epoch [8/10], Step [372/600], Loss: 0.6738, Train Acc:0.8244\n",
      "Epoch [8/10], Step [373/600], Loss: 0.4808, Train Acc:0.8244\n",
      "Epoch [8/10], Step [374/600], Loss: 0.5085, Train Acc:0.8245\n",
      "Epoch [8/10], Step [375/600], Loss: 0.5471, Train Acc:0.8246\n",
      "Epoch [8/10], Step [376/600], Loss: 0.5455, Train Acc:0.8245\n",
      "Epoch [8/10], Step [377/600], Loss: 0.5360, Train Acc:0.8246\n",
      "Epoch [8/10], Step [378/600], Loss: 0.4953, Train Acc:0.8245\n",
      "Epoch [8/10], Step [379/600], Loss: 0.4910, Train Acc:0.8246\n",
      "Epoch [8/10], Step [380/600], Loss: 0.4788, Train Acc:0.8246\n",
      "Epoch [8/10], Step [381/600], Loss: 0.5293, Train Acc:0.8245\n",
      "Epoch [8/10], Step [382/600], Loss: 0.4874, Train Acc:0.8247\n",
      "Epoch [8/10], Step [383/600], Loss: 0.4995, Train Acc:0.8247\n",
      "Epoch [8/10], Step [384/600], Loss: 0.6608, Train Acc:0.8248\n",
      "Epoch [8/10], Step [385/600], Loss: 0.5297, Train Acc:0.8248\n",
      "Epoch [8/10], Step [386/600], Loss: 0.6708, Train Acc:0.8247\n",
      "Epoch [8/10], Step [387/600], Loss: 0.5165, Train Acc:0.8247\n",
      "Epoch [8/10], Step [388/600], Loss: 0.5675, Train Acc:0.8247\n",
      "Epoch [8/10], Step [389/600], Loss: 0.4587, Train Acc:0.8247\n",
      "Epoch [8/10], Step [390/600], Loss: 0.6578, Train Acc:0.8246\n",
      "Epoch [8/10], Step [391/600], Loss: 0.6910, Train Acc:0.8245\n",
      "Epoch [8/10], Step [392/600], Loss: 0.5627, Train Acc:0.8244\n",
      "Epoch [8/10], Step [393/600], Loss: 0.5734, Train Acc:0.8243\n",
      "Epoch [8/10], Step [394/600], Loss: 0.4737, Train Acc:0.8244\n",
      "Epoch [8/10], Step [395/600], Loss: 0.5464, Train Acc:0.8244\n",
      "Epoch [8/10], Step [396/600], Loss: 0.5663, Train Acc:0.8243\n",
      "Epoch [8/10], Step [397/600], Loss: 0.5663, Train Acc:0.8244\n",
      "Epoch [8/10], Step [398/600], Loss: 0.6454, Train Acc:0.8242\n",
      "Epoch [8/10], Step [399/600], Loss: 0.6818, Train Acc:0.8241\n",
      "Epoch [8/10], Step [400/600], Loss: 0.5117, Train Acc:0.8241\n",
      "Epoch [8/10], Step [401/600], Loss: 0.6372, Train Acc:0.8240\n",
      "Epoch [8/10], Step [402/600], Loss: 0.4823, Train Acc:0.8240\n",
      "Epoch [8/10], Step [403/600], Loss: 0.6012, Train Acc:0.8240\n",
      "Epoch [8/10], Step [404/600], Loss: 0.5250, Train Acc:0.8239\n",
      "Epoch [8/10], Step [405/600], Loss: 0.3987, Train Acc:0.8240\n",
      "Epoch [8/10], Step [406/600], Loss: 0.4642, Train Acc:0.8240\n",
      "Epoch [8/10], Step [407/600], Loss: 0.4733, Train Acc:0.8240\n",
      "Epoch [8/10], Step [408/600], Loss: 0.5748, Train Acc:0.8240\n",
      "Epoch [8/10], Step [409/600], Loss: 0.5658, Train Acc:0.8239\n",
      "Epoch [8/10], Step [410/600], Loss: 0.4196, Train Acc:0.8241\n",
      "Epoch [8/10], Step [411/600], Loss: 0.7016, Train Acc:0.8240\n",
      "Epoch [8/10], Step [412/600], Loss: 0.5942, Train Acc:0.8240\n",
      "Epoch [8/10], Step [413/600], Loss: 0.5430, Train Acc:0.8240\n",
      "Epoch [8/10], Step [414/600], Loss: 0.5322, Train Acc:0.8240\n",
      "Epoch [8/10], Step [415/600], Loss: 0.6662, Train Acc:0.8239\n",
      "Epoch [8/10], Step [416/600], Loss: 0.6173, Train Acc:0.8238\n",
      "Epoch [8/10], Step [417/600], Loss: 0.5438, Train Acc:0.8238\n",
      "Epoch [8/10], Step [418/600], Loss: 0.4769, Train Acc:0.8238\n",
      "Epoch [8/10], Step [419/600], Loss: 0.5897, Train Acc:0.8238\n",
      "Epoch [8/10], Step [420/600], Loss: 0.4987, Train Acc:0.8239\n",
      "Epoch [8/10], Step [421/600], Loss: 0.5872, Train Acc:0.8238\n",
      "Epoch [8/10], Step [422/600], Loss: 0.4681, Train Acc:0.8238\n",
      "Epoch [8/10], Step [423/600], Loss: 0.4433, Train Acc:0.8239\n",
      "Epoch [8/10], Step [424/600], Loss: 0.4028, Train Acc:0.8241\n",
      "Epoch [8/10], Step [425/600], Loss: 0.5553, Train Acc:0.8241\n",
      "Epoch [8/10], Step [426/600], Loss: 0.6242, Train Acc:0.8239\n",
      "Epoch [8/10], Step [427/600], Loss: 0.4723, Train Acc:0.8240\n",
      "Epoch [8/10], Step [428/600], Loss: 0.4707, Train Acc:0.8241\n",
      "Epoch [8/10], Step [429/600], Loss: 0.3699, Train Acc:0.8243\n",
      "Epoch [8/10], Step [430/600], Loss: 0.5360, Train Acc:0.8242\n",
      "Epoch [8/10], Step [431/600], Loss: 0.4984, Train Acc:0.8244\n",
      "Epoch [8/10], Step [432/600], Loss: 0.5625, Train Acc:0.8244\n",
      "Epoch [8/10], Step [433/600], Loss: 0.5949, Train Acc:0.8243\n",
      "Epoch [8/10], Step [434/600], Loss: 0.5732, Train Acc:0.8243\n",
      "Epoch [8/10], Step [435/600], Loss: 0.5080, Train Acc:0.8243\n",
      "Epoch [8/10], Step [436/600], Loss: 0.6536, Train Acc:0.8243\n",
      "Epoch [8/10], Step [437/600], Loss: 0.3949, Train Acc:0.8245\n",
      "Epoch [8/10], Step [438/600], Loss: 0.5831, Train Acc:0.8244\n",
      "Epoch [8/10], Step [439/600], Loss: 0.6570, Train Acc:0.8243\n",
      "Epoch [8/10], Step [440/600], Loss: 0.5964, Train Acc:0.8242\n",
      "Epoch [8/10], Step [441/600], Loss: 0.6050, Train Acc:0.8241\n",
      "Epoch [8/10], Step [442/600], Loss: 0.4788, Train Acc:0.8242\n",
      "Epoch [8/10], Step [443/600], Loss: 0.5822, Train Acc:0.8241\n",
      "Epoch [8/10], Step [444/600], Loss: 0.5982, Train Acc:0.8241\n",
      "Epoch [8/10], Step [445/600], Loss: 0.5315, Train Acc:0.8242\n",
      "Epoch [8/10], Step [446/600], Loss: 0.3898, Train Acc:0.8243\n",
      "Epoch [8/10], Step [447/600], Loss: 0.5024, Train Acc:0.8243\n",
      "Epoch [8/10], Step [448/600], Loss: 0.5723, Train Acc:0.8242\n",
      "Epoch [8/10], Step [449/600], Loss: 0.5118, Train Acc:0.8241\n",
      "Epoch [8/10], Step [450/600], Loss: 0.5719, Train Acc:0.8241\n",
      "Epoch [8/10], Step [451/600], Loss: 0.5145, Train Acc:0.8241\n",
      "Epoch [8/10], Step [452/600], Loss: 0.5846, Train Acc:0.8241\n",
      "Epoch [8/10], Step [453/600], Loss: 0.4786, Train Acc:0.8241\n",
      "Epoch [8/10], Step [454/600], Loss: 0.5061, Train Acc:0.8241\n",
      "Epoch [8/10], Step [455/600], Loss: 0.4678, Train Acc:0.8241\n",
      "Epoch [8/10], Step [456/600], Loss: 0.4367, Train Acc:0.8242\n",
      "Epoch [8/10], Step [457/600], Loss: 0.4862, Train Acc:0.8241\n",
      "Epoch [8/10], Step [458/600], Loss: 0.5983, Train Acc:0.8240\n",
      "Epoch [8/10], Step [459/600], Loss: 0.5790, Train Acc:0.8239\n",
      "Epoch [8/10], Step [460/600], Loss: 0.4543, Train Acc:0.8240\n",
      "Epoch [8/10], Step [461/600], Loss: 0.5508, Train Acc:0.8240\n",
      "Epoch [8/10], Step [462/600], Loss: 0.4517, Train Acc:0.8240\n",
      "Epoch [8/10], Step [463/600], Loss: 0.4468, Train Acc:0.8241\n",
      "Epoch [8/10], Step [464/600], Loss: 0.7001, Train Acc:0.8241\n",
      "Epoch [8/10], Step [465/600], Loss: 0.4966, Train Acc:0.8241\n",
      "Epoch [8/10], Step [466/600], Loss: 0.5154, Train Acc:0.8241\n",
      "Epoch [8/10], Step [467/600], Loss: 0.6188, Train Acc:0.8239\n",
      "Epoch [8/10], Step [468/600], Loss: 0.5077, Train Acc:0.8239\n",
      "Epoch [8/10], Step [469/600], Loss: 0.4293, Train Acc:0.8239\n",
      "Epoch [8/10], Step [470/600], Loss: 0.6054, Train Acc:0.8239\n",
      "Epoch [8/10], Step [471/600], Loss: 0.5319, Train Acc:0.8238\n",
      "Epoch [8/10], Step [472/600], Loss: 0.5850, Train Acc:0.8237\n",
      "Epoch [8/10], Step [473/600], Loss: 0.5180, Train Acc:0.8237\n",
      "Epoch [8/10], Step [474/600], Loss: 0.5474, Train Acc:0.8238\n",
      "Epoch [8/10], Step [475/600], Loss: 0.6292, Train Acc:0.8238\n",
      "Epoch [8/10], Step [476/600], Loss: 0.5450, Train Acc:0.8237\n",
      "Epoch [8/10], Step [477/600], Loss: 0.4687, Train Acc:0.8237\n",
      "Epoch [8/10], Step [478/600], Loss: 0.7305, Train Acc:0.8236\n",
      "Epoch [8/10], Step [479/600], Loss: 0.5060, Train Acc:0.8235\n",
      "Epoch [8/10], Step [480/600], Loss: 0.6273, Train Acc:0.8236\n",
      "Epoch [8/10], Step [481/600], Loss: 0.6232, Train Acc:0.8235\n",
      "Epoch [8/10], Step [482/600], Loss: 0.4467, Train Acc:0.8236\n",
      "Epoch [8/10], Step [483/600], Loss: 0.5361, Train Acc:0.8236\n",
      "Epoch [8/10], Step [484/600], Loss: 0.5656, Train Acc:0.8235\n",
      "Epoch [8/10], Step [485/600], Loss: 0.4987, Train Acc:0.8235\n",
      "Epoch [8/10], Step [486/600], Loss: 0.5104, Train Acc:0.8235\n",
      "Epoch [8/10], Step [487/600], Loss: 0.5308, Train Acc:0.8236\n",
      "Epoch [8/10], Step [488/600], Loss: 0.5284, Train Acc:0.8235\n",
      "Epoch [8/10], Step [489/600], Loss: 0.4875, Train Acc:0.8236\n",
      "Epoch [8/10], Step [490/600], Loss: 0.6431, Train Acc:0.8235\n",
      "Epoch [8/10], Step [491/600], Loss: 0.5697, Train Acc:0.8235\n",
      "Epoch [8/10], Step [492/600], Loss: 0.5207, Train Acc:0.8235\n",
      "Epoch [8/10], Step [493/600], Loss: 0.5380, Train Acc:0.8236\n",
      "Epoch [8/10], Step [494/600], Loss: 0.7257, Train Acc:0.8234\n",
      "Epoch [8/10], Step [495/600], Loss: 0.4499, Train Acc:0.8235\n",
      "Epoch [8/10], Step [496/600], Loss: 0.4866, Train Acc:0.8235\n",
      "Epoch [8/10], Step [497/600], Loss: 0.4330, Train Acc:0.8237\n",
      "Epoch [8/10], Step [498/600], Loss: 0.5729, Train Acc:0.8236\n",
      "Epoch [8/10], Step [499/600], Loss: 0.4472, Train Acc:0.8236\n",
      "Epoch [8/10], Step [500/600], Loss: 0.5678, Train Acc:0.8237\n",
      "Epoch [8/10], Step [501/600], Loss: 0.6555, Train Acc:0.8236\n",
      "Epoch [8/10], Step [502/600], Loss: 0.4672, Train Acc:0.8237\n",
      "Epoch [8/10], Step [503/600], Loss: 0.5777, Train Acc:0.8237\n",
      "Epoch [8/10], Step [504/600], Loss: 0.6189, Train Acc:0.8237\n",
      "Epoch [8/10], Step [505/600], Loss: 0.5926, Train Acc:0.8237\n",
      "Epoch [8/10], Step [506/600], Loss: 0.7153, Train Acc:0.8236\n",
      "Epoch [8/10], Step [507/600], Loss: 0.4817, Train Acc:0.8236\n",
      "Epoch [8/10], Step [508/600], Loss: 0.4958, Train Acc:0.8237\n",
      "Epoch [8/10], Step [509/600], Loss: 0.5877, Train Acc:0.8237\n",
      "Epoch [8/10], Step [510/600], Loss: 0.7023, Train Acc:0.8236\n",
      "Epoch [8/10], Step [511/600], Loss: 0.5913, Train Acc:0.8236\n",
      "Epoch [8/10], Step [512/600], Loss: 0.6045, Train Acc:0.8236\n",
      "Epoch [8/10], Step [513/600], Loss: 0.5104, Train Acc:0.8236\n",
      "Epoch [8/10], Step [514/600], Loss: 0.5600, Train Acc:0.8236\n",
      "Epoch [8/10], Step [515/600], Loss: 0.5995, Train Acc:0.8235\n",
      "Epoch [8/10], Step [516/600], Loss: 0.6890, Train Acc:0.8234\n",
      "Epoch [8/10], Step [517/600], Loss: 0.5561, Train Acc:0.8234\n",
      "Epoch [8/10], Step [518/600], Loss: 0.5259, Train Acc:0.8234\n",
      "Epoch [8/10], Step [519/600], Loss: 0.4947, Train Acc:0.8234\n",
      "Epoch [8/10], Step [520/600], Loss: 0.6026, Train Acc:0.8235\n",
      "Epoch [8/10], Step [521/600], Loss: 0.4673, Train Acc:0.8235\n",
      "Epoch [8/10], Step [522/600], Loss: 0.5791, Train Acc:0.8235\n",
      "Epoch [8/10], Step [523/600], Loss: 0.4916, Train Acc:0.8236\n",
      "Epoch [8/10], Step [524/600], Loss: 0.5327, Train Acc:0.8236\n",
      "Epoch [8/10], Step [525/600], Loss: 0.5076, Train Acc:0.8236\n",
      "Epoch [8/10], Step [526/600], Loss: 0.5513, Train Acc:0.8236\n",
      "Epoch [8/10], Step [527/600], Loss: 0.5853, Train Acc:0.8235\n",
      "Epoch [8/10], Step [528/600], Loss: 0.7138, Train Acc:0.8234\n",
      "Epoch [8/10], Step [529/600], Loss: 0.4988, Train Acc:0.8234\n",
      "Epoch [8/10], Step [530/600], Loss: 0.7137, Train Acc:0.8232\n",
      "Epoch [8/10], Step [531/600], Loss: 0.5619, Train Acc:0.8232\n",
      "Epoch [8/10], Step [532/600], Loss: 0.5000, Train Acc:0.8233\n",
      "Epoch [8/10], Step [533/600], Loss: 0.4913, Train Acc:0.8232\n",
      "Epoch [8/10], Step [534/600], Loss: 0.6095, Train Acc:0.8232\n",
      "Epoch [8/10], Step [535/600], Loss: 0.6123, Train Acc:0.8232\n",
      "Epoch [8/10], Step [536/600], Loss: 0.4848, Train Acc:0.8233\n",
      "Epoch [8/10], Step [537/600], Loss: 0.5037, Train Acc:0.8233\n",
      "Epoch [8/10], Step [538/600], Loss: 0.4922, Train Acc:0.8234\n",
      "Epoch [8/10], Step [539/600], Loss: 0.5165, Train Acc:0.8234\n",
      "Epoch [8/10], Step [540/600], Loss: 0.4857, Train Acc:0.8235\n",
      "Epoch [8/10], Step [541/600], Loss: 0.6023, Train Acc:0.8234\n",
      "Epoch [8/10], Step [542/600], Loss: 0.5745, Train Acc:0.8234\n",
      "Epoch [8/10], Step [543/600], Loss: 0.4459, Train Acc:0.8235\n",
      "Epoch [8/10], Step [544/600], Loss: 0.6021, Train Acc:0.8234\n",
      "Epoch [8/10], Step [545/600], Loss: 0.4761, Train Acc:0.8234\n",
      "Epoch [8/10], Step [546/600], Loss: 0.6053, Train Acc:0.8234\n",
      "Epoch [8/10], Step [547/600], Loss: 0.5855, Train Acc:0.8234\n",
      "Epoch [8/10], Step [548/600], Loss: 0.5629, Train Acc:0.8233\n",
      "Epoch [8/10], Step [549/600], Loss: 0.6013, Train Acc:0.8233\n",
      "Epoch [8/10], Step [550/600], Loss: 0.3318, Train Acc:0.8234\n",
      "Epoch [8/10], Step [551/600], Loss: 0.5843, Train Acc:0.8234\n",
      "Epoch [8/10], Step [552/600], Loss: 0.6374, Train Acc:0.8233\n",
      "Epoch [8/10], Step [553/600], Loss: 0.6499, Train Acc:0.8232\n",
      "Epoch [8/10], Step [554/600], Loss: 0.4893, Train Acc:0.8232\n",
      "Epoch [8/10], Step [555/600], Loss: 0.3765, Train Acc:0.8234\n",
      "Epoch [8/10], Step [556/600], Loss: 0.5016, Train Acc:0.8235\n",
      "Epoch [8/10], Step [557/600], Loss: 0.6094, Train Acc:0.8234\n",
      "Epoch [8/10], Step [558/600], Loss: 0.5006, Train Acc:0.8235\n",
      "Epoch [8/10], Step [559/600], Loss: 0.4826, Train Acc:0.8235\n",
      "Epoch [8/10], Step [560/600], Loss: 0.4932, Train Acc:0.8236\n",
      "Epoch [8/10], Step [561/600], Loss: 0.6082, Train Acc:0.8235\n",
      "Epoch [8/10], Step [562/600], Loss: 0.4051, Train Acc:0.8236\n",
      "Epoch [8/10], Step [563/600], Loss: 0.4268, Train Acc:0.8238\n",
      "Epoch [8/10], Step [564/600], Loss: 0.6805, Train Acc:0.8237\n",
      "Epoch [8/10], Step [565/600], Loss: 0.6606, Train Acc:0.8236\n",
      "Epoch [8/10], Step [566/600], Loss: 0.4068, Train Acc:0.8237\n",
      "Epoch [8/10], Step [567/600], Loss: 0.5495, Train Acc:0.8237\n",
      "Epoch [8/10], Step [568/600], Loss: 0.5106, Train Acc:0.8238\n",
      "Epoch [8/10], Step [569/600], Loss: 0.5814, Train Acc:0.8238\n",
      "Epoch [8/10], Step [570/600], Loss: 0.5666, Train Acc:0.8238\n",
      "Epoch [8/10], Step [571/600], Loss: 0.4721, Train Acc:0.8238\n",
      "Epoch [8/10], Step [572/600], Loss: 0.5154, Train Acc:0.8238\n",
      "Epoch [8/10], Step [573/600], Loss: 0.5781, Train Acc:0.8238\n",
      "Epoch [8/10], Step [574/600], Loss: 0.5426, Train Acc:0.8239\n",
      "Epoch [8/10], Step [575/600], Loss: 0.5702, Train Acc:0.8239\n",
      "Epoch [8/10], Step [576/600], Loss: 0.4118, Train Acc:0.8240\n",
      "Epoch [8/10], Step [577/600], Loss: 0.4847, Train Acc:0.8240\n",
      "Epoch [8/10], Step [578/600], Loss: 0.6058, Train Acc:0.8240\n",
      "Epoch [8/10], Step [579/600], Loss: 0.5677, Train Acc:0.8240\n",
      "Epoch [8/10], Step [580/600], Loss: 0.6224, Train Acc:0.8239\n",
      "Epoch [8/10], Step [581/600], Loss: 0.6199, Train Acc:0.8239\n",
      "Epoch [8/10], Step [582/600], Loss: 0.4626, Train Acc:0.8239\n",
      "Epoch [8/10], Step [583/600], Loss: 0.5224, Train Acc:0.8240\n",
      "Epoch [8/10], Step [584/600], Loss: 0.5868, Train Acc:0.8240\n",
      "Epoch [8/10], Step [585/600], Loss: 0.5682, Train Acc:0.8240\n",
      "Epoch [8/10], Step [586/600], Loss: 0.5688, Train Acc:0.8241\n",
      "Epoch [8/10], Step [587/600], Loss: 0.6578, Train Acc:0.8240\n",
      "Epoch [8/10], Step [588/600], Loss: 0.6506, Train Acc:0.8239\n",
      "Epoch [8/10], Step [589/600], Loss: 0.5668, Train Acc:0.8240\n",
      "Epoch [8/10], Step [590/600], Loss: 0.6041, Train Acc:0.8239\n",
      "Epoch [8/10], Step [591/600], Loss: 0.5181, Train Acc:0.8239\n",
      "Epoch [8/10], Step [592/600], Loss: 0.4519, Train Acc:0.8240\n",
      "Epoch [8/10], Step [593/600], Loss: 0.4221, Train Acc:0.8241\n",
      "Epoch [8/10], Step [594/600], Loss: 0.5570, Train Acc:0.8241\n",
      "Epoch [8/10], Step [595/600], Loss: 0.5728, Train Acc:0.8240\n",
      "Epoch [8/10], Step [596/600], Loss: 0.5911, Train Acc:0.8240\n",
      "Epoch [8/10], Step [597/600], Loss: 0.4563, Train Acc:0.8241\n",
      "Epoch [8/10], Step [598/600], Loss: 0.5646, Train Acc:0.8240\n",
      "Epoch [8/10], Step [599/600], Loss: 0.5669, Train Acc:0.8240\n",
      "Epoch [8/10], Step [600/600], Loss: 0.5075, Train Acc:0.8240\n",
      "Epoch [9/10], Step [1/600], Loss: 0.6397, Train Acc:0.7800\n",
      "Epoch [9/10], Step [2/600], Loss: 0.5054, Train Acc:0.7800\n",
      "Epoch [9/10], Step [3/600], Loss: 0.5352, Train Acc:0.7967\n",
      "Epoch [9/10], Step [4/600], Loss: 0.5559, Train Acc:0.8000\n",
      "Epoch [9/10], Step [5/600], Loss: 0.6105, Train Acc:0.7960\n",
      "Epoch [9/10], Step [6/600], Loss: 0.6039, Train Acc:0.7967\n",
      "Epoch [9/10], Step [7/600], Loss: 0.5844, Train Acc:0.7986\n",
      "Epoch [9/10], Step [8/600], Loss: 0.5957, Train Acc:0.7937\n",
      "Epoch [9/10], Step [9/600], Loss: 0.4450, Train Acc:0.8056\n",
      "Epoch [9/10], Step [10/600], Loss: 0.4335, Train Acc:0.8100\n",
      "Epoch [9/10], Step [11/600], Loss: 0.5133, Train Acc:0.8100\n",
      "Epoch [9/10], Step [12/600], Loss: 0.4687, Train Acc:0.8117\n",
      "Epoch [9/10], Step [13/600], Loss: 0.5733, Train Acc:0.8115\n",
      "Epoch [9/10], Step [14/600], Loss: 0.4243, Train Acc:0.8157\n",
      "Epoch [9/10], Step [15/600], Loss: 0.4006, Train Acc:0.8207\n",
      "Epoch [9/10], Step [16/600], Loss: 0.4242, Train Acc:0.8244\n",
      "Epoch [9/10], Step [17/600], Loss: 0.3926, Train Acc:0.8294\n",
      "Epoch [9/10], Step [18/600], Loss: 0.5267, Train Acc:0.8294\n",
      "Epoch [9/10], Step [19/600], Loss: 0.4753, Train Acc:0.8305\n",
      "Epoch [9/10], Step [20/600], Loss: 0.6478, Train Acc:0.8290\n",
      "Epoch [9/10], Step [21/600], Loss: 0.4754, Train Acc:0.8314\n",
      "Epoch [9/10], Step [22/600], Loss: 0.5330, Train Acc:0.8309\n",
      "Epoch [9/10], Step [23/600], Loss: 0.4607, Train Acc:0.8313\n",
      "Epoch [9/10], Step [24/600], Loss: 0.4565, Train Acc:0.8313\n",
      "Epoch [9/10], Step [25/600], Loss: 0.4595, Train Acc:0.8312\n",
      "Epoch [9/10], Step [26/600], Loss: 0.4734, Train Acc:0.8331\n",
      "Epoch [9/10], Step [27/600], Loss: 0.4741, Train Acc:0.8337\n",
      "Epoch [9/10], Step [28/600], Loss: 0.4494, Train Acc:0.8350\n",
      "Epoch [9/10], Step [29/600], Loss: 0.5748, Train Acc:0.8324\n",
      "Epoch [9/10], Step [30/600], Loss: 0.5643, Train Acc:0.8327\n",
      "Epoch [9/10], Step [31/600], Loss: 0.4656, Train Acc:0.8339\n",
      "Epoch [9/10], Step [32/600], Loss: 0.5316, Train Acc:0.8334\n",
      "Epoch [9/10], Step [33/600], Loss: 0.5926, Train Acc:0.8339\n",
      "Epoch [9/10], Step [34/600], Loss: 0.5450, Train Acc:0.8329\n",
      "Epoch [9/10], Step [35/600], Loss: 0.4902, Train Acc:0.8326\n",
      "Epoch [9/10], Step [36/600], Loss: 0.5500, Train Acc:0.8306\n",
      "Epoch [9/10], Step [37/600], Loss: 0.5354, Train Acc:0.8292\n",
      "Epoch [9/10], Step [38/600], Loss: 0.4840, Train Acc:0.8289\n",
      "Epoch [9/10], Step [39/600], Loss: 0.6992, Train Acc:0.8290\n",
      "Epoch [9/10], Step [40/600], Loss: 0.5027, Train Acc:0.8297\n",
      "Epoch [9/10], Step [41/600], Loss: 0.4902, Train Acc:0.8293\n",
      "Epoch [9/10], Step [42/600], Loss: 0.5568, Train Acc:0.8288\n",
      "Epoch [9/10], Step [43/600], Loss: 0.5740, Train Acc:0.8286\n",
      "Epoch [9/10], Step [44/600], Loss: 0.5593, Train Acc:0.8275\n",
      "Epoch [9/10], Step [45/600], Loss: 0.5490, Train Acc:0.8269\n",
      "Epoch [9/10], Step [46/600], Loss: 0.6306, Train Acc:0.8263\n",
      "Epoch [9/10], Step [47/600], Loss: 0.5632, Train Acc:0.8268\n",
      "Epoch [9/10], Step [48/600], Loss: 0.4494, Train Acc:0.8271\n",
      "Epoch [9/10], Step [49/600], Loss: 0.5699, Train Acc:0.8271\n",
      "Epoch [9/10], Step [50/600], Loss: 0.4322, Train Acc:0.8278\n",
      "Epoch [9/10], Step [51/600], Loss: 0.6549, Train Acc:0.8269\n",
      "Epoch [9/10], Step [52/600], Loss: 0.4645, Train Acc:0.8279\n",
      "Epoch [9/10], Step [53/600], Loss: 0.5628, Train Acc:0.8275\n",
      "Epoch [9/10], Step [54/600], Loss: 0.6137, Train Acc:0.8263\n",
      "Epoch [9/10], Step [55/600], Loss: 0.6262, Train Acc:0.8253\n",
      "Epoch [9/10], Step [56/600], Loss: 0.6146, Train Acc:0.8257\n",
      "Epoch [9/10], Step [57/600], Loss: 0.5512, Train Acc:0.8258\n",
      "Epoch [9/10], Step [58/600], Loss: 0.6303, Train Acc:0.8250\n",
      "Epoch [9/10], Step [59/600], Loss: 0.4832, Train Acc:0.8253\n",
      "Epoch [9/10], Step [60/600], Loss: 0.6030, Train Acc:0.8250\n",
      "Epoch [9/10], Step [61/600], Loss: 0.5976, Train Acc:0.8252\n",
      "Epoch [9/10], Step [62/600], Loss: 0.5504, Train Acc:0.8250\n",
      "Epoch [9/10], Step [63/600], Loss: 0.5043, Train Acc:0.8251\n",
      "Epoch [9/10], Step [64/600], Loss: 0.4900, Train Acc:0.8256\n",
      "Epoch [9/10], Step [65/600], Loss: 0.4493, Train Acc:0.8263\n",
      "Epoch [9/10], Step [66/600], Loss: 0.6444, Train Acc:0.8256\n",
      "Epoch [9/10], Step [67/600], Loss: 0.4297, Train Acc:0.8263\n",
      "Epoch [9/10], Step [68/600], Loss: 0.4733, Train Acc:0.8263\n",
      "Epoch [9/10], Step [69/600], Loss: 0.5141, Train Acc:0.8267\n",
      "Epoch [9/10], Step [70/600], Loss: 0.4895, Train Acc:0.8266\n",
      "Epoch [9/10], Step [71/600], Loss: 0.6885, Train Acc:0.8248\n",
      "Epoch [9/10], Step [72/600], Loss: 0.5244, Train Acc:0.8249\n",
      "Epoch [9/10], Step [73/600], Loss: 0.5073, Train Acc:0.8252\n",
      "Epoch [9/10], Step [74/600], Loss: 0.6378, Train Acc:0.8247\n",
      "Epoch [9/10], Step [75/600], Loss: 0.5166, Train Acc:0.8252\n",
      "Epoch [9/10], Step [76/600], Loss: 0.4861, Train Acc:0.8253\n",
      "Epoch [9/10], Step [77/600], Loss: 0.5387, Train Acc:0.8253\n",
      "Epoch [9/10], Step [78/600], Loss: 0.6256, Train Acc:0.8247\n",
      "Epoch [9/10], Step [79/600], Loss: 0.5399, Train Acc:0.8248\n",
      "Epoch [9/10], Step [80/600], Loss: 0.5797, Train Acc:0.8253\n",
      "Epoch [9/10], Step [81/600], Loss: 0.4901, Train Acc:0.8258\n",
      "Epoch [9/10], Step [82/600], Loss: 0.4886, Train Acc:0.8260\n",
      "Epoch [9/10], Step [83/600], Loss: 0.6878, Train Acc:0.8249\n",
      "Epoch [9/10], Step [84/600], Loss: 0.5420, Train Acc:0.8244\n",
      "Epoch [9/10], Step [85/600], Loss: 0.6830, Train Acc:0.8238\n",
      "Epoch [9/10], Step [86/600], Loss: 0.5139, Train Acc:0.8241\n",
      "Epoch [9/10], Step [87/600], Loss: 0.5524, Train Acc:0.8238\n",
      "Epoch [9/10], Step [88/600], Loss: 0.4282, Train Acc:0.8244\n",
      "Epoch [9/10], Step [89/600], Loss: 0.5974, Train Acc:0.8243\n",
      "Epoch [9/10], Step [90/600], Loss: 0.6096, Train Acc:0.8236\n",
      "Epoch [9/10], Step [91/600], Loss: 0.5414, Train Acc:0.8236\n",
      "Epoch [9/10], Step [92/600], Loss: 0.4124, Train Acc:0.8240\n",
      "Epoch [9/10], Step [93/600], Loss: 0.3536, Train Acc:0.8248\n",
      "Epoch [9/10], Step [94/600], Loss: 0.6176, Train Acc:0.8244\n",
      "Epoch [9/10], Step [95/600], Loss: 0.5799, Train Acc:0.8240\n",
      "Epoch [9/10], Step [96/600], Loss: 0.7122, Train Acc:0.8232\n",
      "Epoch [9/10], Step [97/600], Loss: 0.5219, Train Acc:0.8232\n",
      "Epoch [9/10], Step [98/600], Loss: 0.5147, Train Acc:0.8232\n",
      "Epoch [9/10], Step [99/600], Loss: 0.5007, Train Acc:0.8230\n",
      "Epoch [9/10], Step [100/600], Loss: 0.5184, Train Acc:0.8232\n",
      "Epoch [9/10], Step [101/600], Loss: 0.5248, Train Acc:0.8233\n",
      "Epoch [9/10], Step [102/600], Loss: 0.5175, Train Acc:0.8229\n",
      "Epoch [9/10], Step [103/600], Loss: 0.4917, Train Acc:0.8234\n",
      "Epoch [9/10], Step [104/600], Loss: 0.5467, Train Acc:0.8235\n",
      "Epoch [9/10], Step [105/600], Loss: 0.5261, Train Acc:0.8230\n",
      "Epoch [9/10], Step [106/600], Loss: 0.4026, Train Acc:0.8234\n",
      "Epoch [9/10], Step [107/600], Loss: 0.4721, Train Acc:0.8239\n",
      "Epoch [9/10], Step [108/600], Loss: 0.5031, Train Acc:0.8240\n",
      "Epoch [9/10], Step [109/600], Loss: 0.6498, Train Acc:0.8236\n",
      "Epoch [9/10], Step [110/600], Loss: 0.4597, Train Acc:0.8238\n",
      "Epoch [9/10], Step [111/600], Loss: 0.6347, Train Acc:0.8235\n",
      "Epoch [9/10], Step [112/600], Loss: 0.4258, Train Acc:0.8237\n",
      "Epoch [9/10], Step [113/600], Loss: 0.5224, Train Acc:0.8235\n",
      "Epoch [9/10], Step [114/600], Loss: 0.4621, Train Acc:0.8232\n",
      "Epoch [9/10], Step [115/600], Loss: 0.6314, Train Acc:0.8231\n",
      "Epoch [9/10], Step [116/600], Loss: 0.5352, Train Acc:0.8233\n",
      "Epoch [9/10], Step [117/600], Loss: 0.5764, Train Acc:0.8232\n",
      "Epoch [9/10], Step [118/600], Loss: 0.4933, Train Acc:0.8231\n",
      "Epoch [9/10], Step [119/600], Loss: 0.5580, Train Acc:0.8229\n",
      "Epoch [9/10], Step [120/600], Loss: 0.3372, Train Acc:0.8235\n",
      "Epoch [9/10], Step [121/600], Loss: 0.5404, Train Acc:0.8238\n",
      "Epoch [9/10], Step [122/600], Loss: 0.5920, Train Acc:0.8239\n",
      "Epoch [9/10], Step [123/600], Loss: 0.5054, Train Acc:0.8240\n",
      "Epoch [9/10], Step [124/600], Loss: 0.4512, Train Acc:0.8244\n",
      "Epoch [9/10], Step [125/600], Loss: 0.5313, Train Acc:0.8245\n",
      "Epoch [9/10], Step [126/600], Loss: 0.6239, Train Acc:0.8244\n",
      "Epoch [9/10], Step [127/600], Loss: 0.6152, Train Acc:0.8242\n",
      "Epoch [9/10], Step [128/600], Loss: 0.5584, Train Acc:0.8239\n",
      "Epoch [9/10], Step [129/600], Loss: 0.6059, Train Acc:0.8240\n",
      "Epoch [9/10], Step [130/600], Loss: 0.5302, Train Acc:0.8243\n",
      "Epoch [9/10], Step [131/600], Loss: 0.4512, Train Acc:0.8244\n",
      "Epoch [9/10], Step [132/600], Loss: 0.5498, Train Acc:0.8242\n",
      "Epoch [9/10], Step [133/600], Loss: 0.5609, Train Acc:0.8238\n",
      "Epoch [9/10], Step [134/600], Loss: 0.5434, Train Acc:0.8237\n",
      "Epoch [9/10], Step [135/600], Loss: 0.5748, Train Acc:0.8235\n",
      "Epoch [9/10], Step [136/600], Loss: 0.4894, Train Acc:0.8235\n",
      "Epoch [9/10], Step [137/600], Loss: 0.7546, Train Acc:0.8228\n",
      "Epoch [9/10], Step [138/600], Loss: 0.5537, Train Acc:0.8230\n",
      "Epoch [9/10], Step [139/600], Loss: 0.5767, Train Acc:0.8228\n",
      "Epoch [9/10], Step [140/600], Loss: 0.4708, Train Acc:0.8232\n",
      "Epoch [9/10], Step [141/600], Loss: 0.4755, Train Acc:0.8233\n",
      "Epoch [9/10], Step [142/600], Loss: 0.6052, Train Acc:0.8232\n",
      "Epoch [9/10], Step [143/600], Loss: 0.5966, Train Acc:0.8231\n",
      "Epoch [9/10], Step [144/600], Loss: 0.5840, Train Acc:0.8228\n",
      "Epoch [9/10], Step [145/600], Loss: 0.4675, Train Acc:0.8231\n",
      "Epoch [9/10], Step [146/600], Loss: 0.5346, Train Acc:0.8229\n",
      "Epoch [9/10], Step [147/600], Loss: 0.5333, Train Acc:0.8229\n",
      "Epoch [9/10], Step [148/600], Loss: 0.5299, Train Acc:0.8232\n",
      "Epoch [9/10], Step [149/600], Loss: 0.4155, Train Acc:0.8236\n",
      "Epoch [9/10], Step [150/600], Loss: 0.5458, Train Acc:0.8236\n",
      "Epoch [9/10], Step [151/600], Loss: 0.5329, Train Acc:0.8235\n",
      "Epoch [9/10], Step [152/600], Loss: 0.6462, Train Acc:0.8232\n",
      "Epoch [9/10], Step [153/600], Loss: 0.5513, Train Acc:0.8231\n",
      "Epoch [9/10], Step [154/600], Loss: 0.5096, Train Acc:0.8232\n",
      "Epoch [9/10], Step [155/600], Loss: 0.5029, Train Acc:0.8234\n",
      "Epoch [9/10], Step [156/600], Loss: 0.4988, Train Acc:0.8235\n",
      "Epoch [9/10], Step [157/600], Loss: 0.6793, Train Acc:0.8231\n",
      "Epoch [9/10], Step [158/600], Loss: 0.4305, Train Acc:0.8234\n",
      "Epoch [9/10], Step [159/600], Loss: 0.5728, Train Acc:0.8233\n",
      "Epoch [9/10], Step [160/600], Loss: 0.5596, Train Acc:0.8231\n",
      "Epoch [9/10], Step [161/600], Loss: 0.4895, Train Acc:0.8230\n",
      "Epoch [9/10], Step [162/600], Loss: 0.4889, Train Acc:0.8228\n",
      "Epoch [9/10], Step [163/600], Loss: 0.5367, Train Acc:0.8231\n",
      "Epoch [9/10], Step [164/600], Loss: 0.5535, Train Acc:0.8230\n",
      "Epoch [9/10], Step [165/600], Loss: 0.5944, Train Acc:0.8228\n",
      "Epoch [9/10], Step [166/600], Loss: 0.5823, Train Acc:0.8229\n",
      "Epoch [9/10], Step [167/600], Loss: 0.4826, Train Acc:0.8232\n",
      "Epoch [9/10], Step [168/600], Loss: 0.5773, Train Acc:0.8230\n",
      "Epoch [9/10], Step [169/600], Loss: 0.5604, Train Acc:0.8228\n",
      "Epoch [9/10], Step [170/600], Loss: 0.6016, Train Acc:0.8227\n",
      "Epoch [9/10], Step [171/600], Loss: 0.5200, Train Acc:0.8228\n",
      "Epoch [9/10], Step [172/600], Loss: 0.6474, Train Acc:0.8226\n",
      "Epoch [9/10], Step [173/600], Loss: 0.5626, Train Acc:0.8226\n",
      "Epoch [9/10], Step [174/600], Loss: 0.5681, Train Acc:0.8225\n",
      "Epoch [9/10], Step [175/600], Loss: 0.5594, Train Acc:0.8227\n",
      "Epoch [9/10], Step [176/600], Loss: 0.4049, Train Acc:0.8230\n",
      "Epoch [9/10], Step [177/600], Loss: 0.3566, Train Acc:0.8237\n",
      "Epoch [9/10], Step [178/600], Loss: 0.4240, Train Acc:0.8240\n",
      "Epoch [9/10], Step [179/600], Loss: 0.4180, Train Acc:0.8241\n",
      "Epoch [9/10], Step [180/600], Loss: 0.4420, Train Acc:0.8242\n",
      "Epoch [9/10], Step [181/600], Loss: 0.6402, Train Acc:0.8242\n",
      "Epoch [9/10], Step [182/600], Loss: 0.6296, Train Acc:0.8242\n",
      "Epoch [9/10], Step [183/600], Loss: 0.6678, Train Acc:0.8241\n",
      "Epoch [9/10], Step [184/600], Loss: 0.5789, Train Acc:0.8239\n",
      "Epoch [9/10], Step [185/600], Loss: 0.6693, Train Acc:0.8237\n",
      "Epoch [9/10], Step [186/600], Loss: 0.5489, Train Acc:0.8236\n",
      "Epoch [9/10], Step [187/600], Loss: 0.4764, Train Acc:0.8240\n",
      "Epoch [9/10], Step [188/600], Loss: 0.4303, Train Acc:0.8242\n",
      "Epoch [9/10], Step [189/600], Loss: 0.4382, Train Acc:0.8244\n",
      "Epoch [9/10], Step [190/600], Loss: 0.6068, Train Acc:0.8243\n",
      "Epoch [9/10], Step [191/600], Loss: 0.4585, Train Acc:0.8248\n",
      "Epoch [9/10], Step [192/600], Loss: 0.6274, Train Acc:0.8244\n",
      "Epoch [9/10], Step [193/600], Loss: 0.4793, Train Acc:0.8246\n",
      "Epoch [9/10], Step [194/600], Loss: 0.5014, Train Acc:0.8247\n",
      "Epoch [9/10], Step [195/600], Loss: 0.5565, Train Acc:0.8247\n",
      "Epoch [9/10], Step [196/600], Loss: 0.6450, Train Acc:0.8245\n",
      "Epoch [9/10], Step [197/600], Loss: 0.6387, Train Acc:0.8242\n",
      "Epoch [9/10], Step [198/600], Loss: 0.5073, Train Acc:0.8244\n",
      "Epoch [9/10], Step [199/600], Loss: 0.5445, Train Acc:0.8243\n",
      "Epoch [9/10], Step [200/600], Loss: 0.3506, Train Acc:0.8248\n",
      "Epoch [9/10], Step [201/600], Loss: 0.6539, Train Acc:0.8246\n",
      "Epoch [9/10], Step [202/600], Loss: 0.4844, Train Acc:0.8249\n",
      "Epoch [9/10], Step [203/600], Loss: 0.5000, Train Acc:0.8249\n",
      "Epoch [9/10], Step [204/600], Loss: 0.5146, Train Acc:0.8249\n",
      "Epoch [9/10], Step [205/600], Loss: 0.6954, Train Acc:0.8246\n",
      "Epoch [9/10], Step [206/600], Loss: 0.5293, Train Acc:0.8245\n",
      "Epoch [9/10], Step [207/600], Loss: 0.6310, Train Acc:0.8244\n",
      "Epoch [9/10], Step [208/600], Loss: 0.5444, Train Acc:0.8246\n",
      "Epoch [9/10], Step [209/600], Loss: 0.4652, Train Acc:0.8246\n",
      "Epoch [9/10], Step [210/600], Loss: 0.5585, Train Acc:0.8247\n",
      "Epoch [9/10], Step [211/600], Loss: 0.5096, Train Acc:0.8245\n",
      "Epoch [9/10], Step [212/600], Loss: 0.5063, Train Acc:0.8245\n",
      "Epoch [9/10], Step [213/600], Loss: 0.5845, Train Acc:0.8244\n",
      "Epoch [9/10], Step [214/600], Loss: 0.5118, Train Acc:0.8243\n",
      "Epoch [9/10], Step [215/600], Loss: 0.4637, Train Acc:0.8242\n",
      "Epoch [9/10], Step [216/600], Loss: 0.5488, Train Acc:0.8243\n",
      "Epoch [9/10], Step [217/600], Loss: 0.5291, Train Acc:0.8243\n",
      "Epoch [9/10], Step [218/600], Loss: 0.5673, Train Acc:0.8242\n",
      "Epoch [9/10], Step [219/600], Loss: 0.4039, Train Acc:0.8245\n",
      "Epoch [9/10], Step [220/600], Loss: 0.6037, Train Acc:0.8242\n",
      "Epoch [9/10], Step [221/600], Loss: 0.4949, Train Acc:0.8241\n",
      "Epoch [9/10], Step [222/600], Loss: 0.5005, Train Acc:0.8243\n",
      "Epoch [9/10], Step [223/600], Loss: 0.4106, Train Acc:0.8243\n",
      "Epoch [9/10], Step [224/600], Loss: 0.4019, Train Acc:0.8247\n",
      "Epoch [9/10], Step [225/600], Loss: 0.6235, Train Acc:0.8247\n",
      "Epoch [9/10], Step [226/600], Loss: 0.6083, Train Acc:0.8247\n",
      "Epoch [9/10], Step [227/600], Loss: 0.5318, Train Acc:0.8248\n",
      "Epoch [9/10], Step [228/600], Loss: 0.6330, Train Acc:0.8245\n",
      "Epoch [9/10], Step [229/600], Loss: 0.4300, Train Acc:0.8248\n",
      "Epoch [9/10], Step [230/600], Loss: 0.4796, Train Acc:0.8249\n",
      "Epoch [9/10], Step [231/600], Loss: 0.5220, Train Acc:0.8249\n",
      "Epoch [9/10], Step [232/600], Loss: 0.4486, Train Acc:0.8249\n",
      "Epoch [9/10], Step [233/600], Loss: 0.5198, Train Acc:0.8250\n",
      "Epoch [9/10], Step [234/600], Loss: 0.4136, Train Acc:0.8251\n",
      "Epoch [9/10], Step [235/600], Loss: 0.6898, Train Acc:0.8249\n",
      "Epoch [9/10], Step [236/600], Loss: 0.6057, Train Acc:0.8248\n",
      "Epoch [9/10], Step [237/600], Loss: 0.4831, Train Acc:0.8247\n",
      "Epoch [9/10], Step [238/600], Loss: 0.4657, Train Acc:0.8246\n",
      "Epoch [9/10], Step [239/600], Loss: 0.6064, Train Acc:0.8242\n",
      "Epoch [9/10], Step [240/600], Loss: 0.5743, Train Acc:0.8240\n",
      "Epoch [9/10], Step [241/600], Loss: 0.4136, Train Acc:0.8242\n",
      "Epoch [9/10], Step [242/600], Loss: 0.4637, Train Acc:0.8242\n",
      "Epoch [9/10], Step [243/600], Loss: 0.5605, Train Acc:0.8241\n",
      "Epoch [9/10], Step [244/600], Loss: 0.5730, Train Acc:0.8241\n",
      "Epoch [9/10], Step [245/600], Loss: 0.5716, Train Acc:0.8241\n",
      "Epoch [9/10], Step [246/600], Loss: 0.5003, Train Acc:0.8240\n",
      "Epoch [9/10], Step [247/600], Loss: 0.4348, Train Acc:0.8242\n",
      "Epoch [9/10], Step [248/600], Loss: 0.4521, Train Acc:0.8244\n",
      "Epoch [9/10], Step [249/600], Loss: 0.5145, Train Acc:0.8245\n",
      "Epoch [9/10], Step [250/600], Loss: 0.5350, Train Acc:0.8244\n",
      "Epoch [9/10], Step [251/600], Loss: 0.4950, Train Acc:0.8243\n",
      "Epoch [9/10], Step [252/600], Loss: 0.5238, Train Acc:0.8243\n",
      "Epoch [9/10], Step [253/600], Loss: 0.5964, Train Acc:0.8240\n",
      "Epoch [9/10], Step [254/600], Loss: 0.4467, Train Acc:0.8241\n",
      "Epoch [9/10], Step [255/600], Loss: 0.4439, Train Acc:0.8242\n",
      "Epoch [9/10], Step [256/600], Loss: 0.4414, Train Acc:0.8244\n",
      "Epoch [9/10], Step [257/600], Loss: 0.4984, Train Acc:0.8244\n",
      "Epoch [9/10], Step [258/600], Loss: 0.4665, Train Acc:0.8245\n",
      "Epoch [9/10], Step [259/600], Loss: 0.5539, Train Acc:0.8244\n",
      "Epoch [9/10], Step [260/600], Loss: 0.5901, Train Acc:0.8245\n",
      "Epoch [9/10], Step [261/600], Loss: 0.7226, Train Acc:0.8243\n",
      "Epoch [9/10], Step [262/600], Loss: 0.5270, Train Acc:0.8244\n",
      "Epoch [9/10], Step [263/600], Loss: 0.4705, Train Acc:0.8243\n",
      "Epoch [9/10], Step [264/600], Loss: 0.4101, Train Acc:0.8244\n",
      "Epoch [9/10], Step [265/600], Loss: 0.4674, Train Acc:0.8246\n",
      "Epoch [9/10], Step [266/600], Loss: 0.4126, Train Acc:0.8246\n",
      "Epoch [9/10], Step [267/600], Loss: 0.5580, Train Acc:0.8245\n",
      "Epoch [9/10], Step [268/600], Loss: 0.3925, Train Acc:0.8247\n",
      "Epoch [9/10], Step [269/600], Loss: 0.5411, Train Acc:0.8246\n",
      "Epoch [9/10], Step [270/600], Loss: 0.3876, Train Acc:0.8248\n",
      "Epoch [9/10], Step [271/600], Loss: 0.5122, Train Acc:0.8248\n",
      "Epoch [9/10], Step [272/600], Loss: 0.5660, Train Acc:0.8248\n",
      "Epoch [9/10], Step [273/600], Loss: 0.5646, Train Acc:0.8247\n",
      "Epoch [9/10], Step [274/600], Loss: 0.4082, Train Acc:0.8249\n",
      "Epoch [9/10], Step [275/600], Loss: 0.5488, Train Acc:0.8248\n",
      "Epoch [9/10], Step [276/600], Loss: 0.4969, Train Acc:0.8247\n",
      "Epoch [9/10], Step [277/600], Loss: 0.3619, Train Acc:0.8250\n",
      "Epoch [9/10], Step [278/600], Loss: 0.5541, Train Acc:0.8250\n",
      "Epoch [9/10], Step [279/600], Loss: 0.5836, Train Acc:0.8251\n",
      "Epoch [9/10], Step [280/600], Loss: 0.5451, Train Acc:0.8250\n",
      "Epoch [9/10], Step [281/600], Loss: 0.4770, Train Acc:0.8251\n",
      "Epoch [9/10], Step [282/600], Loss: 0.5696, Train Acc:0.8252\n",
      "Epoch [9/10], Step [283/600], Loss: 0.6826, Train Acc:0.8251\n",
      "Epoch [9/10], Step [284/600], Loss: 0.4774, Train Acc:0.8252\n",
      "Epoch [9/10], Step [285/600], Loss: 0.5193, Train Acc:0.8252\n",
      "Epoch [9/10], Step [286/600], Loss: 0.4453, Train Acc:0.8255\n",
      "Epoch [9/10], Step [287/600], Loss: 0.5609, Train Acc:0.8255\n",
      "Epoch [9/10], Step [288/600], Loss: 0.7318, Train Acc:0.8253\n",
      "Epoch [9/10], Step [289/600], Loss: 0.5234, Train Acc:0.8253\n",
      "Epoch [9/10], Step [290/600], Loss: 0.5940, Train Acc:0.8251\n",
      "Epoch [9/10], Step [291/600], Loss: 0.5459, Train Acc:0.8250\n",
      "Epoch [9/10], Step [292/600], Loss: 0.6656, Train Acc:0.8248\n",
      "Epoch [9/10], Step [293/600], Loss: 0.5173, Train Acc:0.8248\n",
      "Epoch [9/10], Step [294/600], Loss: 0.5899, Train Acc:0.8247\n",
      "Epoch [9/10], Step [295/600], Loss: 0.7728, Train Acc:0.8246\n",
      "Epoch [9/10], Step [296/600], Loss: 0.5222, Train Acc:0.8245\n",
      "Epoch [9/10], Step [297/600], Loss: 0.5140, Train Acc:0.8246\n",
      "Epoch [9/10], Step [298/600], Loss: 0.6819, Train Acc:0.8244\n",
      "Epoch [9/10], Step [299/600], Loss: 0.5471, Train Acc:0.8243\n",
      "Epoch [9/10], Step [300/600], Loss: 0.3848, Train Acc:0.8246\n",
      "Epoch [9/10], Step [301/600], Loss: 0.5174, Train Acc:0.8245\n",
      "Epoch [9/10], Step [302/600], Loss: 0.5265, Train Acc:0.8245\n",
      "Epoch [9/10], Step [303/600], Loss: 0.6368, Train Acc:0.8244\n",
      "Epoch [9/10], Step [304/600], Loss: 0.6032, Train Acc:0.8244\n",
      "Epoch [9/10], Step [305/600], Loss: 0.6429, Train Acc:0.8243\n",
      "Epoch [9/10], Step [306/600], Loss: 0.5315, Train Acc:0.8243\n",
      "Epoch [9/10], Step [307/600], Loss: 0.3631, Train Acc:0.8245\n",
      "Epoch [9/10], Step [308/600], Loss: 0.4954, Train Acc:0.8245\n",
      "Epoch [9/10], Step [309/600], Loss: 0.5172, Train Acc:0.8244\n",
      "Epoch [9/10], Step [310/600], Loss: 0.5843, Train Acc:0.8244\n",
      "Epoch [9/10], Step [311/600], Loss: 0.5208, Train Acc:0.8243\n",
      "Epoch [9/10], Step [312/600], Loss: 0.5340, Train Acc:0.8244\n",
      "Epoch [9/10], Step [313/600], Loss: 0.5122, Train Acc:0.8243\n",
      "Epoch [9/10], Step [314/600], Loss: 0.6371, Train Acc:0.8242\n",
      "Epoch [9/10], Step [315/600], Loss: 0.5373, Train Acc:0.8242\n",
      "Epoch [9/10], Step [316/600], Loss: 0.4932, Train Acc:0.8242\n",
      "Epoch [9/10], Step [317/600], Loss: 0.4147, Train Acc:0.8245\n",
      "Epoch [9/10], Step [318/600], Loss: 0.5882, Train Acc:0.8245\n",
      "Epoch [9/10], Step [319/600], Loss: 0.4516, Train Acc:0.8246\n",
      "Epoch [9/10], Step [320/600], Loss: 0.5112, Train Acc:0.8247\n",
      "Epoch [9/10], Step [321/600], Loss: 0.5733, Train Acc:0.8246\n",
      "Epoch [9/10], Step [322/600], Loss: 0.4767, Train Acc:0.8246\n",
      "Epoch [9/10], Step [323/600], Loss: 0.5446, Train Acc:0.8248\n",
      "Epoch [9/10], Step [324/600], Loss: 0.4727, Train Acc:0.8247\n",
      "Epoch [9/10], Step [325/600], Loss: 0.5720, Train Acc:0.8247\n",
      "Epoch [9/10], Step [326/600], Loss: 0.5672, Train Acc:0.8246\n",
      "Epoch [9/10], Step [327/600], Loss: 0.3885, Train Acc:0.8248\n",
      "Epoch [9/10], Step [328/600], Loss: 0.6692, Train Acc:0.8247\n",
      "Epoch [9/10], Step [329/600], Loss: 0.5407, Train Acc:0.8246\n",
      "Epoch [9/10], Step [330/600], Loss: 0.5620, Train Acc:0.8244\n",
      "Epoch [9/10], Step [331/600], Loss: 0.4508, Train Acc:0.8245\n",
      "Epoch [9/10], Step [332/600], Loss: 0.5637, Train Acc:0.8246\n",
      "Epoch [9/10], Step [333/600], Loss: 0.6556, Train Acc:0.8243\n",
      "Epoch [9/10], Step [334/600], Loss: 0.5548, Train Acc:0.8243\n",
      "Epoch [9/10], Step [335/600], Loss: 0.6072, Train Acc:0.8243\n",
      "Epoch [9/10], Step [336/600], Loss: 0.4744, Train Acc:0.8244\n",
      "Epoch [9/10], Step [337/600], Loss: 0.5011, Train Acc:0.8245\n",
      "Epoch [9/10], Step [338/600], Loss: 0.4909, Train Acc:0.8246\n",
      "Epoch [9/10], Step [339/600], Loss: 0.5518, Train Acc:0.8247\n",
      "Epoch [9/10], Step [340/600], Loss: 0.6711, Train Acc:0.8246\n",
      "Epoch [9/10], Step [341/600], Loss: 0.4575, Train Acc:0.8246\n",
      "Epoch [9/10], Step [342/600], Loss: 0.4368, Train Acc:0.8246\n",
      "Epoch [9/10], Step [343/600], Loss: 0.4871, Train Acc:0.8246\n",
      "Epoch [9/10], Step [344/600], Loss: 0.5466, Train Acc:0.8248\n",
      "Epoch [9/10], Step [345/600], Loss: 0.5753, Train Acc:0.8246\n",
      "Epoch [9/10], Step [346/600], Loss: 0.5063, Train Acc:0.8246\n",
      "Epoch [9/10], Step [347/600], Loss: 0.4875, Train Acc:0.8247\n",
      "Epoch [9/10], Step [348/600], Loss: 0.5684, Train Acc:0.8247\n",
      "Epoch [9/10], Step [349/600], Loss: 0.4858, Train Acc:0.8248\n",
      "Epoch [9/10], Step [350/600], Loss: 0.6248, Train Acc:0.8246\n",
      "Epoch [9/10], Step [351/600], Loss: 0.5268, Train Acc:0.8245\n",
      "Epoch [9/10], Step [352/600], Loss: 0.5453, Train Acc:0.8244\n",
      "Epoch [9/10], Step [353/600], Loss: 0.4610, Train Acc:0.8245\n",
      "Epoch [9/10], Step [354/600], Loss: 0.6205, Train Acc:0.8242\n",
      "Epoch [9/10], Step [355/600], Loss: 0.5874, Train Acc:0.8244\n",
      "Epoch [9/10], Step [356/600], Loss: 0.4982, Train Acc:0.8245\n",
      "Epoch [9/10], Step [357/600], Loss: 0.5876, Train Acc:0.8245\n",
      "Epoch [9/10], Step [358/600], Loss: 0.5618, Train Acc:0.8246\n",
      "Epoch [9/10], Step [359/600], Loss: 0.5937, Train Acc:0.8245\n",
      "Epoch [9/10], Step [360/600], Loss: 0.4873, Train Acc:0.8246\n",
      "Epoch [9/10], Step [361/600], Loss: 0.5870, Train Acc:0.8244\n",
      "Epoch [9/10], Step [362/600], Loss: 0.3822, Train Acc:0.8246\n",
      "Epoch [9/10], Step [363/600], Loss: 0.5224, Train Acc:0.8246\n",
      "Epoch [9/10], Step [364/600], Loss: 0.4877, Train Acc:0.8247\n",
      "Epoch [9/10], Step [365/600], Loss: 0.5227, Train Acc:0.8246\n",
      "Epoch [9/10], Step [366/600], Loss: 0.5497, Train Acc:0.8245\n",
      "Epoch [9/10], Step [367/600], Loss: 0.6212, Train Acc:0.8244\n",
      "Epoch [9/10], Step [368/600], Loss: 0.5997, Train Acc:0.8242\n",
      "Epoch [9/10], Step [369/600], Loss: 0.3375, Train Acc:0.8245\n",
      "Epoch [9/10], Step [370/600], Loss: 0.5832, Train Acc:0.8244\n",
      "Epoch [9/10], Step [371/600], Loss: 0.4477, Train Acc:0.8243\n",
      "Epoch [9/10], Step [372/600], Loss: 0.5449, Train Acc:0.8243\n",
      "Epoch [9/10], Step [373/600], Loss: 0.6533, Train Acc:0.8243\n",
      "Epoch [9/10], Step [374/600], Loss: 0.5872, Train Acc:0.8243\n",
      "Epoch [9/10], Step [375/600], Loss: 0.5446, Train Acc:0.8242\n",
      "Epoch [9/10], Step [376/600], Loss: 0.5569, Train Acc:0.8242\n",
      "Epoch [9/10], Step [377/600], Loss: 0.6326, Train Acc:0.8241\n",
      "Epoch [9/10], Step [378/600], Loss: 0.5530, Train Acc:0.8242\n",
      "Epoch [9/10], Step [379/600], Loss: 0.6384, Train Acc:0.8242\n",
      "Epoch [9/10], Step [380/600], Loss: 0.6441, Train Acc:0.8242\n",
      "Epoch [9/10], Step [381/600], Loss: 0.5949, Train Acc:0.8242\n",
      "Epoch [9/10], Step [382/600], Loss: 0.5455, Train Acc:0.8242\n",
      "Epoch [9/10], Step [383/600], Loss: 0.5381, Train Acc:0.8241\n",
      "Epoch [9/10], Step [384/600], Loss: 0.5357, Train Acc:0.8242\n",
      "Epoch [9/10], Step [385/600], Loss: 0.3852, Train Acc:0.8244\n",
      "Epoch [9/10], Step [386/600], Loss: 0.5369, Train Acc:0.8244\n",
      "Epoch [9/10], Step [387/600], Loss: 0.5205, Train Acc:0.8244\n",
      "Epoch [9/10], Step [388/600], Loss: 0.5249, Train Acc:0.8245\n",
      "Epoch [9/10], Step [389/600], Loss: 0.5097, Train Acc:0.8245\n",
      "Epoch [9/10], Step [390/600], Loss: 0.5898, Train Acc:0.8245\n",
      "Epoch [9/10], Step [391/600], Loss: 0.4261, Train Acc:0.8246\n",
      "Epoch [9/10], Step [392/600], Loss: 0.5148, Train Acc:0.8246\n",
      "Epoch [9/10], Step [393/600], Loss: 0.4511, Train Acc:0.8247\n",
      "Epoch [9/10], Step [394/600], Loss: 0.4548, Train Acc:0.8248\n",
      "Epoch [9/10], Step [395/600], Loss: 0.5303, Train Acc:0.8248\n",
      "Epoch [9/10], Step [396/600], Loss: 0.3907, Train Acc:0.8249\n",
      "Epoch [9/10], Step [397/600], Loss: 0.4592, Train Acc:0.8251\n",
      "Epoch [9/10], Step [398/600], Loss: 0.4716, Train Acc:0.8251\n",
      "Epoch [9/10], Step [399/600], Loss: 0.5809, Train Acc:0.8252\n",
      "Epoch [9/10], Step [400/600], Loss: 0.5919, Train Acc:0.8250\n",
      "Epoch [9/10], Step [401/600], Loss: 0.5396, Train Acc:0.8251\n",
      "Epoch [9/10], Step [402/600], Loss: 0.5265, Train Acc:0.8251\n",
      "Epoch [9/10], Step [403/600], Loss: 0.6306, Train Acc:0.8250\n",
      "Epoch [9/10], Step [404/600], Loss: 0.5305, Train Acc:0.8250\n",
      "Epoch [9/10], Step [405/600], Loss: 0.3875, Train Acc:0.8252\n",
      "Epoch [9/10], Step [406/600], Loss: 0.4938, Train Acc:0.8252\n",
      "Epoch [9/10], Step [407/600], Loss: 0.5691, Train Acc:0.8251\n",
      "Epoch [9/10], Step [408/600], Loss: 0.5742, Train Acc:0.8250\n",
      "Epoch [9/10], Step [409/600], Loss: 0.6444, Train Acc:0.8250\n",
      "Epoch [9/10], Step [410/600], Loss: 0.5610, Train Acc:0.8251\n",
      "Epoch [9/10], Step [411/600], Loss: 0.6219, Train Acc:0.8249\n",
      "Epoch [9/10], Step [412/600], Loss: 0.6427, Train Acc:0.8249\n",
      "Epoch [9/10], Step [413/600], Loss: 0.6170, Train Acc:0.8247\n",
      "Epoch [9/10], Step [414/600], Loss: 0.5311, Train Acc:0.8247\n",
      "Epoch [9/10], Step [415/600], Loss: 0.5329, Train Acc:0.8247\n",
      "Epoch [9/10], Step [416/600], Loss: 0.5070, Train Acc:0.8246\n",
      "Epoch [9/10], Step [417/600], Loss: 0.5372, Train Acc:0.8247\n",
      "Epoch [9/10], Step [418/600], Loss: 0.5011, Train Acc:0.8248\n",
      "Epoch [9/10], Step [419/600], Loss: 0.5902, Train Acc:0.8247\n",
      "Epoch [9/10], Step [420/600], Loss: 0.4839, Train Acc:0.8247\n",
      "Epoch [9/10], Step [421/600], Loss: 0.3655, Train Acc:0.8249\n",
      "Epoch [9/10], Step [422/600], Loss: 0.5826, Train Acc:0.8248\n",
      "Epoch [9/10], Step [423/600], Loss: 0.4964, Train Acc:0.8248\n",
      "Epoch [9/10], Step [424/600], Loss: 0.4568, Train Acc:0.8248\n",
      "Epoch [9/10], Step [425/600], Loss: 0.4755, Train Acc:0.8249\n",
      "Epoch [9/10], Step [426/600], Loss: 0.5367, Train Acc:0.8250\n",
      "Epoch [9/10], Step [427/600], Loss: 0.5364, Train Acc:0.8249\n",
      "Epoch [9/10], Step [428/600], Loss: 0.4191, Train Acc:0.8251\n",
      "Epoch [9/10], Step [429/600], Loss: 0.6212, Train Acc:0.8250\n",
      "Epoch [9/10], Step [430/600], Loss: 0.6291, Train Acc:0.8250\n",
      "Epoch [9/10], Step [431/600], Loss: 0.5561, Train Acc:0.8251\n",
      "Epoch [9/10], Step [432/600], Loss: 0.5961, Train Acc:0.8250\n",
      "Epoch [9/10], Step [433/600], Loss: 0.4222, Train Acc:0.8252\n",
      "Epoch [9/10], Step [434/600], Loss: 0.5475, Train Acc:0.8250\n",
      "Epoch [9/10], Step [435/600], Loss: 0.4184, Train Acc:0.8251\n",
      "Epoch [9/10], Step [436/600], Loss: 0.5653, Train Acc:0.8251\n",
      "Epoch [9/10], Step [437/600], Loss: 0.5600, Train Acc:0.8251\n",
      "Epoch [9/10], Step [438/600], Loss: 0.5418, Train Acc:0.8252\n",
      "Epoch [9/10], Step [439/600], Loss: 0.5192, Train Acc:0.8252\n",
      "Epoch [9/10], Step [440/600], Loss: 0.6437, Train Acc:0.8251\n",
      "Epoch [9/10], Step [441/600], Loss: 0.4100, Train Acc:0.8252\n",
      "Epoch [9/10], Step [442/600], Loss: 0.5325, Train Acc:0.8251\n",
      "Epoch [9/10], Step [443/600], Loss: 0.5792, Train Acc:0.8250\n",
      "Epoch [9/10], Step [444/600], Loss: 0.4554, Train Acc:0.8251\n",
      "Epoch [9/10], Step [445/600], Loss: 0.7861, Train Acc:0.8249\n",
      "Epoch [9/10], Step [446/600], Loss: 0.6444, Train Acc:0.8248\n",
      "Epoch [9/10], Step [447/600], Loss: 0.4964, Train Acc:0.8248\n",
      "Epoch [9/10], Step [448/600], Loss: 0.4587, Train Acc:0.8249\n",
      "Epoch [9/10], Step [449/600], Loss: 0.6116, Train Acc:0.8248\n",
      "Epoch [9/10], Step [450/600], Loss: 0.5093, Train Acc:0.8249\n",
      "Epoch [9/10], Step [451/600], Loss: 0.5309, Train Acc:0.8249\n",
      "Epoch [9/10], Step [452/600], Loss: 0.5908, Train Acc:0.8248\n",
      "Epoch [9/10], Step [453/600], Loss: 0.4213, Train Acc:0.8248\n",
      "Epoch [9/10], Step [454/600], Loss: 0.5443, Train Acc:0.8248\n",
      "Epoch [9/10], Step [455/600], Loss: 0.5639, Train Acc:0.8248\n",
      "Epoch [9/10], Step [456/600], Loss: 0.4056, Train Acc:0.8249\n",
      "Epoch [9/10], Step [457/600], Loss: 0.4494, Train Acc:0.8250\n",
      "Epoch [9/10], Step [458/600], Loss: 0.4323, Train Acc:0.8250\n",
      "Epoch [9/10], Step [459/600], Loss: 0.5486, Train Acc:0.8249\n",
      "Epoch [9/10], Step [460/600], Loss: 0.4054, Train Acc:0.8251\n",
      "Epoch [9/10], Step [461/600], Loss: 0.4500, Train Acc:0.8252\n",
      "Epoch [9/10], Step [462/600], Loss: 0.4147, Train Acc:0.8252\n",
      "Epoch [9/10], Step [463/600], Loss: 0.6476, Train Acc:0.8251\n",
      "Epoch [9/10], Step [464/600], Loss: 0.5833, Train Acc:0.8251\n",
      "Epoch [9/10], Step [465/600], Loss: 0.4549, Train Acc:0.8251\n",
      "Epoch [9/10], Step [466/600], Loss: 0.6049, Train Acc:0.8251\n",
      "Epoch [9/10], Step [467/600], Loss: 0.6151, Train Acc:0.8250\n",
      "Epoch [9/10], Step [468/600], Loss: 0.6380, Train Acc:0.8249\n",
      "Epoch [9/10], Step [469/600], Loss: 0.4212, Train Acc:0.8249\n",
      "Epoch [9/10], Step [470/600], Loss: 0.6260, Train Acc:0.8249\n",
      "Epoch [9/10], Step [471/600], Loss: 0.5610, Train Acc:0.8248\n",
      "Epoch [9/10], Step [472/600], Loss: 0.6496, Train Acc:0.8248\n",
      "Epoch [9/10], Step [473/600], Loss: 0.6072, Train Acc:0.8248\n",
      "Epoch [9/10], Step [474/600], Loss: 0.4497, Train Acc:0.8249\n",
      "Epoch [9/10], Step [475/600], Loss: 0.5256, Train Acc:0.8249\n",
      "Epoch [9/10], Step [476/600], Loss: 0.6621, Train Acc:0.8249\n",
      "Epoch [9/10], Step [477/600], Loss: 0.6026, Train Acc:0.8247\n",
      "Epoch [9/10], Step [478/600], Loss: 0.3841, Train Acc:0.8248\n",
      "Epoch [9/10], Step [479/600], Loss: 0.5767, Train Acc:0.8247\n",
      "Epoch [9/10], Step [480/600], Loss: 0.6021, Train Acc:0.8246\n",
      "Epoch [9/10], Step [481/600], Loss: 0.5363, Train Acc:0.8246\n",
      "Epoch [9/10], Step [482/600], Loss: 0.4887, Train Acc:0.8247\n",
      "Epoch [9/10], Step [483/600], Loss: 0.5619, Train Acc:0.8247\n",
      "Epoch [9/10], Step [484/600], Loss: 0.5485, Train Acc:0.8247\n",
      "Epoch [9/10], Step [485/600], Loss: 0.4871, Train Acc:0.8247\n",
      "Epoch [9/10], Step [486/600], Loss: 0.4462, Train Acc:0.8248\n",
      "Epoch [9/10], Step [487/600], Loss: 0.5207, Train Acc:0.8247\n",
      "Epoch [9/10], Step [488/600], Loss: 0.7454, Train Acc:0.8245\n",
      "Epoch [9/10], Step [489/600], Loss: 0.3673, Train Acc:0.8246\n",
      "Epoch [9/10], Step [490/600], Loss: 0.4713, Train Acc:0.8246\n",
      "Epoch [9/10], Step [491/600], Loss: 0.4689, Train Acc:0.8246\n",
      "Epoch [9/10], Step [492/600], Loss: 0.5304, Train Acc:0.8246\n",
      "Epoch [9/10], Step [493/600], Loss: 0.4293, Train Acc:0.8247\n",
      "Epoch [9/10], Step [494/600], Loss: 0.4988, Train Acc:0.8247\n",
      "Epoch [9/10], Step [495/600], Loss: 0.3600, Train Acc:0.8249\n",
      "Epoch [9/10], Step [496/600], Loss: 0.5420, Train Acc:0.8249\n",
      "Epoch [9/10], Step [497/600], Loss: 0.5126, Train Acc:0.8248\n",
      "Epoch [9/10], Step [498/600], Loss: 0.5708, Train Acc:0.8248\n",
      "Epoch [9/10], Step [499/600], Loss: 0.5387, Train Acc:0.8248\n",
      "Epoch [9/10], Step [500/600], Loss: 0.5067, Train Acc:0.8249\n",
      "Epoch [9/10], Step [501/600], Loss: 0.4545, Train Acc:0.8250\n",
      "Epoch [9/10], Step [502/600], Loss: 0.4014, Train Acc:0.8252\n",
      "Epoch [9/10], Step [503/600], Loss: 0.4703, Train Acc:0.8253\n",
      "Epoch [9/10], Step [504/600], Loss: 0.4956, Train Acc:0.8253\n",
      "Epoch [9/10], Step [505/600], Loss: 0.5134, Train Acc:0.8253\n",
      "Epoch [9/10], Step [506/600], Loss: 0.4456, Train Acc:0.8254\n",
      "Epoch [9/10], Step [507/600], Loss: 0.4896, Train Acc:0.8255\n",
      "Epoch [9/10], Step [508/600], Loss: 0.5742, Train Acc:0.8255\n",
      "Epoch [9/10], Step [509/600], Loss: 0.4649, Train Acc:0.8255\n",
      "Epoch [9/10], Step [510/600], Loss: 0.5404, Train Acc:0.8256\n",
      "Epoch [9/10], Step [511/600], Loss: 0.6476, Train Acc:0.8254\n",
      "Epoch [9/10], Step [512/600], Loss: 0.6874, Train Acc:0.8253\n",
      "Epoch [9/10], Step [513/600], Loss: 0.4358, Train Acc:0.8254\n",
      "Epoch [9/10], Step [514/600], Loss: 0.5606, Train Acc:0.8254\n",
      "Epoch [9/10], Step [515/600], Loss: 0.4082, Train Acc:0.8255\n",
      "Epoch [9/10], Step [516/600], Loss: 0.4948, Train Acc:0.8255\n",
      "Epoch [9/10], Step [517/600], Loss: 0.6421, Train Acc:0.8254\n",
      "Epoch [9/10], Step [518/600], Loss: 0.6048, Train Acc:0.8253\n",
      "Epoch [9/10], Step [519/600], Loss: 0.6521, Train Acc:0.8253\n",
      "Epoch [9/10], Step [520/600], Loss: 0.4414, Train Acc:0.8254\n",
      "Epoch [9/10], Step [521/600], Loss: 0.5041, Train Acc:0.8254\n",
      "Epoch [9/10], Step [522/600], Loss: 0.5499, Train Acc:0.8253\n",
      "Epoch [9/10], Step [523/600], Loss: 0.6305, Train Acc:0.8253\n",
      "Epoch [9/10], Step [524/600], Loss: 0.4003, Train Acc:0.8254\n",
      "Epoch [9/10], Step [525/600], Loss: 0.4030, Train Acc:0.8255\n",
      "Epoch [9/10], Step [526/600], Loss: 0.4780, Train Acc:0.8255\n",
      "Epoch [9/10], Step [527/600], Loss: 0.4809, Train Acc:0.8256\n",
      "Epoch [9/10], Step [528/600], Loss: 0.5602, Train Acc:0.8255\n",
      "Epoch [9/10], Step [529/600], Loss: 0.5645, Train Acc:0.8255\n",
      "Epoch [9/10], Step [530/600], Loss: 0.5259, Train Acc:0.8255\n",
      "Epoch [9/10], Step [531/600], Loss: 0.4332, Train Acc:0.8256\n",
      "Epoch [9/10], Step [532/600], Loss: 0.6649, Train Acc:0.8254\n",
      "Epoch [9/10], Step [533/600], Loss: 0.6210, Train Acc:0.8253\n",
      "Epoch [9/10], Step [534/600], Loss: 0.5306, Train Acc:0.8252\n",
      "Epoch [9/10], Step [535/600], Loss: 0.5967, Train Acc:0.8252\n",
      "Epoch [9/10], Step [536/600], Loss: 0.5232, Train Acc:0.8252\n",
      "Epoch [9/10], Step [537/600], Loss: 0.4028, Train Acc:0.8253\n",
      "Epoch [9/10], Step [538/600], Loss: 0.4736, Train Acc:0.8253\n",
      "Epoch [9/10], Step [539/600], Loss: 0.5122, Train Acc:0.8253\n",
      "Epoch [9/10], Step [540/600], Loss: 0.5013, Train Acc:0.8253\n",
      "Epoch [9/10], Step [541/600], Loss: 0.4859, Train Acc:0.8253\n",
      "Epoch [9/10], Step [542/600], Loss: 0.4986, Train Acc:0.8254\n",
      "Epoch [9/10], Step [543/600], Loss: 0.5797, Train Acc:0.8254\n",
      "Epoch [9/10], Step [544/600], Loss: 0.5553, Train Acc:0.8254\n",
      "Epoch [9/10], Step [545/600], Loss: 0.5526, Train Acc:0.8254\n",
      "Epoch [9/10], Step [546/600], Loss: 0.5095, Train Acc:0.8254\n",
      "Epoch [9/10], Step [547/600], Loss: 0.5979, Train Acc:0.8254\n",
      "Epoch [9/10], Step [548/600], Loss: 0.4873, Train Acc:0.8255\n",
      "Epoch [9/10], Step [549/600], Loss: 0.4944, Train Acc:0.8255\n",
      "Epoch [9/10], Step [550/600], Loss: 0.5841, Train Acc:0.8256\n",
      "Epoch [9/10], Step [551/600], Loss: 0.4815, Train Acc:0.8257\n",
      "Epoch [9/10], Step [552/600], Loss: 0.5136, Train Acc:0.8257\n",
      "Epoch [9/10], Step [553/600], Loss: 0.4584, Train Acc:0.8257\n",
      "Epoch [9/10], Step [554/600], Loss: 0.6256, Train Acc:0.8255\n",
      "Epoch [9/10], Step [555/600], Loss: 0.5177, Train Acc:0.8256\n",
      "Epoch [9/10], Step [556/600], Loss: 0.5029, Train Acc:0.8257\n",
      "Epoch [9/10], Step [557/600], Loss: 0.6213, Train Acc:0.8257\n",
      "Epoch [9/10], Step [558/600], Loss: 0.5261, Train Acc:0.8258\n",
      "Epoch [9/10], Step [559/600], Loss: 0.6079, Train Acc:0.8257\n",
      "Epoch [9/10], Step [560/600], Loss: 0.6785, Train Acc:0.8257\n",
      "Epoch [9/10], Step [561/600], Loss: 0.4857, Train Acc:0.8257\n",
      "Epoch [9/10], Step [562/600], Loss: 0.6016, Train Acc:0.8256\n",
      "Epoch [9/10], Step [563/600], Loss: 0.4660, Train Acc:0.8257\n",
      "Epoch [9/10], Step [564/600], Loss: 0.4279, Train Acc:0.8257\n",
      "Epoch [9/10], Step [565/600], Loss: 0.3576, Train Acc:0.8259\n",
      "Epoch [9/10], Step [566/600], Loss: 0.4496, Train Acc:0.8259\n",
      "Epoch [9/10], Step [567/600], Loss: 0.4550, Train Acc:0.8259\n",
      "Epoch [9/10], Step [568/600], Loss: 0.4889, Train Acc:0.8258\n",
      "Epoch [9/10], Step [569/600], Loss: 0.4379, Train Acc:0.8259\n",
      "Epoch [9/10], Step [570/600], Loss: 0.5754, Train Acc:0.8259\n",
      "Epoch [9/10], Step [571/600], Loss: 0.5758, Train Acc:0.8259\n",
      "Epoch [9/10], Step [572/600], Loss: 0.5234, Train Acc:0.8260\n",
      "Epoch [9/10], Step [573/600], Loss: 0.5607, Train Acc:0.8259\n",
      "Epoch [9/10], Step [574/600], Loss: 0.4638, Train Acc:0.8260\n",
      "Epoch [9/10], Step [575/600], Loss: 0.5298, Train Acc:0.8260\n",
      "Epoch [9/10], Step [576/600], Loss: 0.4321, Train Acc:0.8260\n",
      "Epoch [9/10], Step [577/600], Loss: 0.4772, Train Acc:0.8260\n",
      "Epoch [9/10], Step [578/600], Loss: 0.4904, Train Acc:0.8260\n",
      "Epoch [9/10], Step [579/600], Loss: 0.4678, Train Acc:0.8259\n",
      "Epoch [9/10], Step [580/600], Loss: 0.5571, Train Acc:0.8259\n",
      "Epoch [9/10], Step [581/600], Loss: 0.4959, Train Acc:0.8259\n",
      "Epoch [9/10], Step [582/600], Loss: 0.6090, Train Acc:0.8259\n",
      "Epoch [9/10], Step [583/600], Loss: 0.5583, Train Acc:0.8259\n",
      "Epoch [9/10], Step [584/600], Loss: 0.4484, Train Acc:0.8259\n",
      "Epoch [9/10], Step [585/600], Loss: 0.4788, Train Acc:0.8260\n",
      "Epoch [9/10], Step [586/600], Loss: 0.5254, Train Acc:0.8260\n",
      "Epoch [9/10], Step [587/600], Loss: 0.4245, Train Acc:0.8260\n",
      "Epoch [9/10], Step [588/600], Loss: 0.3526, Train Acc:0.8261\n",
      "Epoch [9/10], Step [589/600], Loss: 0.4364, Train Acc:0.8262\n",
      "Epoch [9/10], Step [590/600], Loss: 0.4600, Train Acc:0.8262\n",
      "Epoch [9/10], Step [591/600], Loss: 0.5340, Train Acc:0.8262\n",
      "Epoch [9/10], Step [592/600], Loss: 0.4300, Train Acc:0.8263\n",
      "Epoch [9/10], Step [593/600], Loss: 0.5410, Train Acc:0.8263\n",
      "Epoch [9/10], Step [594/600], Loss: 0.5203, Train Acc:0.8263\n",
      "Epoch [9/10], Step [595/600], Loss: 0.4703, Train Acc:0.8263\n",
      "Epoch [9/10], Step [596/600], Loss: 0.6739, Train Acc:0.8262\n",
      "Epoch [9/10], Step [597/600], Loss: 0.5379, Train Acc:0.8262\n",
      "Epoch [9/10], Step [598/600], Loss: 0.6418, Train Acc:0.8261\n",
      "Epoch [9/10], Step [599/600], Loss: 0.6035, Train Acc:0.8261\n",
      "Epoch [9/10], Step [600/600], Loss: 0.5170, Train Acc:0.8261\n",
      "Epoch [10/10], Step [1/600], Loss: 0.4881, Train Acc:0.8200\n",
      "Epoch [10/10], Step [2/600], Loss: 0.4353, Train Acc:0.8200\n",
      "Epoch [10/10], Step [3/600], Loss: 0.5551, Train Acc:0.8033\n",
      "Epoch [10/10], Step [4/600], Loss: 0.4236, Train Acc:0.8150\n",
      "Epoch [10/10], Step [5/600], Loss: 0.5542, Train Acc:0.8080\n",
      "Epoch [10/10], Step [6/600], Loss: 0.6104, Train Acc:0.8050\n",
      "Epoch [10/10], Step [7/600], Loss: 0.5069, Train Acc:0.8071\n",
      "Epoch [10/10], Step [8/600], Loss: 0.7119, Train Acc:0.8050\n",
      "Epoch [10/10], Step [9/600], Loss: 0.4421, Train Acc:0.8133\n",
      "Epoch [10/10], Step [10/600], Loss: 0.4019, Train Acc:0.8210\n",
      "Epoch [10/10], Step [11/600], Loss: 0.4417, Train Acc:0.8245\n",
      "Epoch [10/10], Step [12/600], Loss: 0.4583, Train Acc:0.8267\n",
      "Epoch [10/10], Step [13/600], Loss: 0.6167, Train Acc:0.8223\n",
      "Epoch [10/10], Step [14/600], Loss: 0.4964, Train Acc:0.8221\n",
      "Epoch [10/10], Step [15/600], Loss: 0.4871, Train Acc:0.8207\n",
      "Epoch [10/10], Step [16/600], Loss: 0.5625, Train Acc:0.8175\n",
      "Epoch [10/10], Step [17/600], Loss: 0.6066, Train Acc:0.8171\n",
      "Epoch [10/10], Step [18/600], Loss: 0.3765, Train Acc:0.8211\n",
      "Epoch [10/10], Step [19/600], Loss: 0.6081, Train Acc:0.8195\n",
      "Epoch [10/10], Step [20/600], Loss: 0.5259, Train Acc:0.8195\n",
      "Epoch [10/10], Step [21/600], Loss: 0.6028, Train Acc:0.8186\n",
      "Epoch [10/10], Step [22/600], Loss: 0.5636, Train Acc:0.8191\n",
      "Epoch [10/10], Step [23/600], Loss: 0.4272, Train Acc:0.8209\n",
      "Epoch [10/10], Step [24/600], Loss: 0.4215, Train Acc:0.8237\n",
      "Epoch [10/10], Step [25/600], Loss: 0.5510, Train Acc:0.8248\n",
      "Epoch [10/10], Step [26/600], Loss: 0.5758, Train Acc:0.8238\n",
      "Epoch [10/10], Step [27/600], Loss: 0.6375, Train Acc:0.8230\n",
      "Epoch [10/10], Step [28/600], Loss: 0.4857, Train Acc:0.8229\n",
      "Epoch [10/10], Step [29/600], Loss: 0.4764, Train Acc:0.8231\n",
      "Epoch [10/10], Step [30/600], Loss: 0.5536, Train Acc:0.8210\n",
      "Epoch [10/10], Step [31/600], Loss: 0.5288, Train Acc:0.8210\n",
      "Epoch [10/10], Step [32/600], Loss: 0.5625, Train Acc:0.8209\n",
      "Epoch [10/10], Step [33/600], Loss: 0.5232, Train Acc:0.8203\n",
      "Epoch [10/10], Step [34/600], Loss: 0.5201, Train Acc:0.8203\n",
      "Epoch [10/10], Step [35/600], Loss: 0.3892, Train Acc:0.8217\n",
      "Epoch [10/10], Step [36/600], Loss: 0.7408, Train Acc:0.8214\n",
      "Epoch [10/10], Step [37/600], Loss: 0.6232, Train Acc:0.8208\n",
      "Epoch [10/10], Step [38/600], Loss: 0.4507, Train Acc:0.8205\n",
      "Epoch [10/10], Step [39/600], Loss: 0.6006, Train Acc:0.8190\n",
      "Epoch [10/10], Step [40/600], Loss: 0.4363, Train Acc:0.8197\n",
      "Epoch [10/10], Step [41/600], Loss: 0.4814, Train Acc:0.8198\n",
      "Epoch [10/10], Step [42/600], Loss: 0.6579, Train Acc:0.8193\n",
      "Epoch [10/10], Step [43/600], Loss: 0.5124, Train Acc:0.8195\n",
      "Epoch [10/10], Step [44/600], Loss: 0.5893, Train Acc:0.8177\n",
      "Epoch [10/10], Step [45/600], Loss: 0.5402, Train Acc:0.8171\n",
      "Epoch [10/10], Step [46/600], Loss: 0.5138, Train Acc:0.8172\n",
      "Epoch [10/10], Step [47/600], Loss: 0.5160, Train Acc:0.8168\n",
      "Epoch [10/10], Step [48/600], Loss: 0.5094, Train Acc:0.8171\n",
      "Epoch [10/10], Step [49/600], Loss: 0.4845, Train Acc:0.8180\n",
      "Epoch [10/10], Step [50/600], Loss: 0.4130, Train Acc:0.8192\n",
      "Epoch [10/10], Step [51/600], Loss: 0.5604, Train Acc:0.8192\n",
      "Epoch [10/10], Step [52/600], Loss: 0.6492, Train Acc:0.8198\n",
      "Epoch [10/10], Step [53/600], Loss: 0.4678, Train Acc:0.8206\n",
      "Epoch [10/10], Step [54/600], Loss: 0.5634, Train Acc:0.8206\n",
      "Epoch [10/10], Step [55/600], Loss: 0.4403, Train Acc:0.8216\n",
      "Epoch [10/10], Step [56/600], Loss: 0.4250, Train Acc:0.8223\n",
      "Epoch [10/10], Step [57/600], Loss: 0.6098, Train Acc:0.8221\n",
      "Epoch [10/10], Step [58/600], Loss: 0.4732, Train Acc:0.8226\n",
      "Epoch [10/10], Step [59/600], Loss: 0.5487, Train Acc:0.8227\n",
      "Epoch [10/10], Step [60/600], Loss: 0.4198, Train Acc:0.8235\n",
      "Epoch [10/10], Step [61/600], Loss: 0.5485, Train Acc:0.8231\n",
      "Epoch [10/10], Step [62/600], Loss: 0.4930, Train Acc:0.8239\n",
      "Epoch [10/10], Step [63/600], Loss: 0.5243, Train Acc:0.8240\n",
      "Epoch [10/10], Step [64/600], Loss: 0.5321, Train Acc:0.8245\n",
      "Epoch [10/10], Step [65/600], Loss: 0.5745, Train Acc:0.8242\n",
      "Epoch [10/10], Step [66/600], Loss: 0.5276, Train Acc:0.8238\n",
      "Epoch [10/10], Step [67/600], Loss: 0.4022, Train Acc:0.8246\n",
      "Epoch [10/10], Step [68/600], Loss: 0.5092, Train Acc:0.8246\n",
      "Epoch [10/10], Step [69/600], Loss: 0.4482, Train Acc:0.8248\n",
      "Epoch [10/10], Step [70/600], Loss: 0.4447, Train Acc:0.8254\n",
      "Epoch [10/10], Step [71/600], Loss: 0.6118, Train Acc:0.8252\n",
      "Epoch [10/10], Step [72/600], Loss: 0.5132, Train Acc:0.8254\n",
      "Epoch [10/10], Step [73/600], Loss: 0.5716, Train Acc:0.8249\n",
      "Epoch [10/10], Step [74/600], Loss: 0.4951, Train Acc:0.8251\n",
      "Epoch [10/10], Step [75/600], Loss: 0.4060, Train Acc:0.8260\n",
      "Epoch [10/10], Step [76/600], Loss: 0.5829, Train Acc:0.8259\n",
      "Epoch [10/10], Step [77/600], Loss: 0.4842, Train Acc:0.8258\n",
      "Epoch [10/10], Step [78/600], Loss: 0.4242, Train Acc:0.8262\n",
      "Epoch [10/10], Step [79/600], Loss: 0.6361, Train Acc:0.8252\n",
      "Epoch [10/10], Step [80/600], Loss: 0.5112, Train Acc:0.8254\n",
      "Epoch [10/10], Step [81/600], Loss: 0.5550, Train Acc:0.8254\n",
      "Epoch [10/10], Step [82/600], Loss: 0.5620, Train Acc:0.8251\n",
      "Epoch [10/10], Step [83/600], Loss: 0.6316, Train Acc:0.8246\n",
      "Epoch [10/10], Step [84/600], Loss: 0.4673, Train Acc:0.8248\n",
      "Epoch [10/10], Step [85/600], Loss: 0.4313, Train Acc:0.8249\n",
      "Epoch [10/10], Step [86/600], Loss: 0.4227, Train Acc:0.8251\n",
      "Epoch [10/10], Step [87/600], Loss: 0.4942, Train Acc:0.8254\n",
      "Epoch [10/10], Step [88/600], Loss: 0.4942, Train Acc:0.8255\n",
      "Epoch [10/10], Step [89/600], Loss: 0.5335, Train Acc:0.8249\n",
      "Epoch [10/10], Step [90/600], Loss: 0.4690, Train Acc:0.8253\n",
      "Epoch [10/10], Step [91/600], Loss: 0.6696, Train Acc:0.8252\n",
      "Epoch [10/10], Step [92/600], Loss: 0.5460, Train Acc:0.8252\n",
      "Epoch [10/10], Step [93/600], Loss: 0.5431, Train Acc:0.8255\n",
      "Epoch [10/10], Step [94/600], Loss: 0.5342, Train Acc:0.8251\n",
      "Epoch [10/10], Step [95/600], Loss: 0.6139, Train Acc:0.8248\n",
      "Epoch [10/10], Step [96/600], Loss: 0.5145, Train Acc:0.8247\n",
      "Epoch [10/10], Step [97/600], Loss: 0.5186, Train Acc:0.8245\n",
      "Epoch [10/10], Step [98/600], Loss: 0.5571, Train Acc:0.8246\n",
      "Epoch [10/10], Step [99/600], Loss: 0.4463, Train Acc:0.8246\n",
      "Epoch [10/10], Step [100/600], Loss: 0.6086, Train Acc:0.8242\n",
      "Epoch [10/10], Step [101/600], Loss: 0.6197, Train Acc:0.8239\n",
      "Epoch [10/10], Step [102/600], Loss: 0.6807, Train Acc:0.8233\n",
      "Epoch [10/10], Step [103/600], Loss: 0.6702, Train Acc:0.8227\n",
      "Epoch [10/10], Step [104/600], Loss: 0.4148, Train Acc:0.8230\n",
      "Epoch [10/10], Step [105/600], Loss: 0.5985, Train Acc:0.8227\n",
      "Epoch [10/10], Step [106/600], Loss: 0.6008, Train Acc:0.8225\n",
      "Epoch [10/10], Step [107/600], Loss: 0.4874, Train Acc:0.8226\n",
      "Epoch [10/10], Step [108/600], Loss: 0.4826, Train Acc:0.8225\n",
      "Epoch [10/10], Step [109/600], Loss: 0.4670, Train Acc:0.8224\n",
      "Epoch [10/10], Step [110/600], Loss: 0.6443, Train Acc:0.8220\n",
      "Epoch [10/10], Step [111/600], Loss: 0.5772, Train Acc:0.8218\n",
      "Epoch [10/10], Step [112/600], Loss: 0.5590, Train Acc:0.8217\n",
      "Epoch [10/10], Step [113/600], Loss: 0.4548, Train Acc:0.8219\n",
      "Epoch [10/10], Step [114/600], Loss: 0.7103, Train Acc:0.8214\n",
      "Epoch [10/10], Step [115/600], Loss: 0.4186, Train Acc:0.8219\n",
      "Epoch [10/10], Step [116/600], Loss: 0.6476, Train Acc:0.8215\n",
      "Epoch [10/10], Step [117/600], Loss: 0.4951, Train Acc:0.8221\n",
      "Epoch [10/10], Step [118/600], Loss: 0.5621, Train Acc:0.8220\n",
      "Epoch [10/10], Step [119/600], Loss: 0.4482, Train Acc:0.8222\n",
      "Epoch [10/10], Step [120/600], Loss: 0.6258, Train Acc:0.8217\n",
      "Epoch [10/10], Step [121/600], Loss: 0.5482, Train Acc:0.8219\n",
      "Epoch [10/10], Step [122/600], Loss: 0.4955, Train Acc:0.8217\n",
      "Epoch [10/10], Step [123/600], Loss: 0.5429, Train Acc:0.8218\n",
      "Epoch [10/10], Step [124/600], Loss: 0.4506, Train Acc:0.8222\n",
      "Epoch [10/10], Step [125/600], Loss: 0.4530, Train Acc:0.8224\n",
      "Epoch [10/10], Step [126/600], Loss: 0.5359, Train Acc:0.8225\n",
      "Epoch [10/10], Step [127/600], Loss: 0.4875, Train Acc:0.8228\n",
      "Epoch [10/10], Step [128/600], Loss: 0.5716, Train Acc:0.8230\n",
      "Epoch [10/10], Step [129/600], Loss: 0.7507, Train Acc:0.8229\n",
      "Epoch [10/10], Step [130/600], Loss: 0.5168, Train Acc:0.8231\n",
      "Epoch [10/10], Step [131/600], Loss: 0.5040, Train Acc:0.8232\n",
      "Epoch [10/10], Step [132/600], Loss: 0.4969, Train Acc:0.8234\n",
      "Epoch [10/10], Step [133/600], Loss: 0.5958, Train Acc:0.8232\n",
      "Epoch [10/10], Step [134/600], Loss: 0.5414, Train Acc:0.8231\n",
      "Epoch [10/10], Step [135/600], Loss: 0.5127, Train Acc:0.8235\n",
      "Epoch [10/10], Step [136/600], Loss: 0.5187, Train Acc:0.8234\n",
      "Epoch [10/10], Step [137/600], Loss: 0.4829, Train Acc:0.8239\n",
      "Epoch [10/10], Step [138/600], Loss: 0.4265, Train Acc:0.8241\n",
      "Epoch [10/10], Step [139/600], Loss: 0.6411, Train Acc:0.8240\n",
      "Epoch [10/10], Step [140/600], Loss: 0.4808, Train Acc:0.8240\n",
      "Epoch [10/10], Step [141/600], Loss: 0.5006, Train Acc:0.8240\n",
      "Epoch [10/10], Step [142/600], Loss: 0.5173, Train Acc:0.8243\n",
      "Epoch [10/10], Step [143/600], Loss: 0.4205, Train Acc:0.8245\n",
      "Epoch [10/10], Step [144/600], Loss: 0.6266, Train Acc:0.8242\n",
      "Epoch [10/10], Step [145/600], Loss: 0.4041, Train Acc:0.8244\n",
      "Epoch [10/10], Step [146/600], Loss: 0.6073, Train Acc:0.8242\n",
      "Epoch [10/10], Step [147/600], Loss: 0.4011, Train Acc:0.8247\n",
      "Epoch [10/10], Step [148/600], Loss: 0.3786, Train Acc:0.8252\n",
      "Epoch [10/10], Step [149/600], Loss: 0.4676, Train Acc:0.8256\n",
      "Epoch [10/10], Step [150/600], Loss: 0.4123, Train Acc:0.8259\n",
      "Epoch [10/10], Step [151/600], Loss: 0.5668, Train Acc:0.8257\n",
      "Epoch [10/10], Step [152/600], Loss: 0.5754, Train Acc:0.8257\n",
      "Epoch [10/10], Step [153/600], Loss: 0.4622, Train Acc:0.8261\n",
      "Epoch [10/10], Step [154/600], Loss: 0.5094, Train Acc:0.8261\n",
      "Epoch [10/10], Step [155/600], Loss: 0.3730, Train Acc:0.8266\n",
      "Epoch [10/10], Step [156/600], Loss: 0.4764, Train Acc:0.8267\n",
      "Epoch [10/10], Step [157/600], Loss: 0.4542, Train Acc:0.8270\n",
      "Epoch [10/10], Step [158/600], Loss: 0.4006, Train Acc:0.8272\n",
      "Epoch [10/10], Step [159/600], Loss: 0.4674, Train Acc:0.8275\n",
      "Epoch [10/10], Step [160/600], Loss: 0.6231, Train Acc:0.8271\n",
      "Epoch [10/10], Step [161/600], Loss: 0.6510, Train Acc:0.8271\n",
      "Epoch [10/10], Step [162/600], Loss: 0.5529, Train Acc:0.8267\n",
      "Epoch [10/10], Step [163/600], Loss: 0.4668, Train Acc:0.8269\n",
      "Epoch [10/10], Step [164/600], Loss: 0.4922, Train Acc:0.8272\n",
      "Epoch [10/10], Step [165/600], Loss: 0.5076, Train Acc:0.8271\n",
      "Epoch [10/10], Step [166/600], Loss: 0.5012, Train Acc:0.8271\n",
      "Epoch [10/10], Step [167/600], Loss: 0.4886, Train Acc:0.8272\n",
      "Epoch [10/10], Step [168/600], Loss: 0.5278, Train Acc:0.8271\n",
      "Epoch [10/10], Step [169/600], Loss: 0.4571, Train Acc:0.8272\n",
      "Epoch [10/10], Step [170/600], Loss: 0.6407, Train Acc:0.8269\n",
      "Epoch [10/10], Step [171/600], Loss: 0.4247, Train Acc:0.8273\n",
      "Epoch [10/10], Step [172/600], Loss: 0.4858, Train Acc:0.8272\n",
      "Epoch [10/10], Step [173/600], Loss: 0.4838, Train Acc:0.8271\n",
      "Epoch [10/10], Step [174/600], Loss: 0.4788, Train Acc:0.8274\n",
      "Epoch [10/10], Step [175/600], Loss: 0.4857, Train Acc:0.8274\n",
      "Epoch [10/10], Step [176/600], Loss: 0.5011, Train Acc:0.8273\n",
      "Epoch [10/10], Step [177/600], Loss: 0.5099, Train Acc:0.8272\n",
      "Epoch [10/10], Step [178/600], Loss: 0.6023, Train Acc:0.8270\n",
      "Epoch [10/10], Step [179/600], Loss: 0.6179, Train Acc:0.8268\n",
      "Epoch [10/10], Step [180/600], Loss: 0.4692, Train Acc:0.8268\n",
      "Epoch [10/10], Step [181/600], Loss: 0.6414, Train Acc:0.8268\n",
      "Epoch [10/10], Step [182/600], Loss: 0.4269, Train Acc:0.8271\n",
      "Epoch [10/10], Step [183/600], Loss: 0.4229, Train Acc:0.8273\n",
      "Epoch [10/10], Step [184/600], Loss: 0.5060, Train Acc:0.8272\n",
      "Epoch [10/10], Step [185/600], Loss: 0.5720, Train Acc:0.8268\n",
      "Epoch [10/10], Step [186/600], Loss: 0.5221, Train Acc:0.8268\n",
      "Epoch [10/10], Step [187/600], Loss: 0.5178, Train Acc:0.8269\n",
      "Epoch [10/10], Step [188/600], Loss: 0.4058, Train Acc:0.8270\n",
      "Epoch [10/10], Step [189/600], Loss: 0.6260, Train Acc:0.8269\n",
      "Epoch [10/10], Step [190/600], Loss: 0.5149, Train Acc:0.8270\n",
      "Epoch [10/10], Step [191/600], Loss: 0.4464, Train Acc:0.8271\n",
      "Epoch [10/10], Step [192/600], Loss: 0.4993, Train Acc:0.8271\n",
      "Epoch [10/10], Step [193/600], Loss: 0.6665, Train Acc:0.8268\n",
      "Epoch [10/10], Step [194/600], Loss: 0.5119, Train Acc:0.8268\n",
      "Epoch [10/10], Step [195/600], Loss: 0.4585, Train Acc:0.8269\n",
      "Epoch [10/10], Step [196/600], Loss: 0.3740, Train Acc:0.8274\n",
      "Epoch [10/10], Step [197/600], Loss: 0.5394, Train Acc:0.8275\n",
      "Epoch [10/10], Step [198/600], Loss: 0.5863, Train Acc:0.8273\n",
      "Epoch [10/10], Step [199/600], Loss: 0.4708, Train Acc:0.8274\n",
      "Epoch [10/10], Step [200/600], Loss: 0.4706, Train Acc:0.8278\n",
      "Epoch [10/10], Step [201/600], Loss: 0.4725, Train Acc:0.8279\n",
      "Epoch [10/10], Step [202/600], Loss: 0.6151, Train Acc:0.8279\n",
      "Epoch [10/10], Step [203/600], Loss: 0.5949, Train Acc:0.8278\n",
      "Epoch [10/10], Step [204/600], Loss: 0.5141, Train Acc:0.8278\n",
      "Epoch [10/10], Step [205/600], Loss: 0.6066, Train Acc:0.8276\n",
      "Epoch [10/10], Step [206/600], Loss: 0.5717, Train Acc:0.8274\n",
      "Epoch [10/10], Step [207/600], Loss: 0.4705, Train Acc:0.8276\n",
      "Epoch [10/10], Step [208/600], Loss: 0.5153, Train Acc:0.8278\n",
      "Epoch [10/10], Step [209/600], Loss: 0.6090, Train Acc:0.8276\n",
      "Epoch [10/10], Step [210/600], Loss: 0.4197, Train Acc:0.8278\n",
      "Epoch [10/10], Step [211/600], Loss: 0.4850, Train Acc:0.8277\n",
      "Epoch [10/10], Step [212/600], Loss: 0.4932, Train Acc:0.8277\n",
      "Epoch [10/10], Step [213/600], Loss: 0.4280, Train Acc:0.8279\n",
      "Epoch [10/10], Step [214/600], Loss: 0.5946, Train Acc:0.8278\n",
      "Epoch [10/10], Step [215/600], Loss: 0.5779, Train Acc:0.8278\n",
      "Epoch [10/10], Step [216/600], Loss: 0.3792, Train Acc:0.8280\n",
      "Epoch [10/10], Step [217/600], Loss: 0.5854, Train Acc:0.8280\n",
      "Epoch [10/10], Step [218/600], Loss: 0.4635, Train Acc:0.8281\n",
      "Epoch [10/10], Step [219/600], Loss: 0.6520, Train Acc:0.8279\n",
      "Epoch [10/10], Step [220/600], Loss: 0.4816, Train Acc:0.8280\n",
      "Epoch [10/10], Step [221/600], Loss: 0.4529, Train Acc:0.8281\n",
      "Epoch [10/10], Step [222/600], Loss: 0.5901, Train Acc:0.8281\n",
      "Epoch [10/10], Step [223/600], Loss: 0.5717, Train Acc:0.8281\n",
      "Epoch [10/10], Step [224/600], Loss: 0.4787, Train Acc:0.8281\n",
      "Epoch [10/10], Step [225/600], Loss: 0.3725, Train Acc:0.8284\n",
      "Epoch [10/10], Step [226/600], Loss: 0.4238, Train Acc:0.8288\n",
      "Epoch [10/10], Step [227/600], Loss: 0.5922, Train Acc:0.8289\n",
      "Epoch [10/10], Step [228/600], Loss: 0.4796, Train Acc:0.8289\n",
      "Epoch [10/10], Step [229/600], Loss: 0.5125, Train Acc:0.8290\n",
      "Epoch [10/10], Step [230/600], Loss: 0.6741, Train Acc:0.8287\n",
      "Epoch [10/10], Step [231/600], Loss: 0.5465, Train Acc:0.8286\n",
      "Epoch [10/10], Step [232/600], Loss: 0.5061, Train Acc:0.8285\n",
      "Epoch [10/10], Step [233/600], Loss: 0.6997, Train Acc:0.8283\n",
      "Epoch [10/10], Step [234/600], Loss: 0.5751, Train Acc:0.8282\n",
      "Epoch [10/10], Step [235/600], Loss: 0.5217, Train Acc:0.8282\n",
      "Epoch [10/10], Step [236/600], Loss: 0.5391, Train Acc:0.8282\n",
      "Epoch [10/10], Step [237/600], Loss: 0.5433, Train Acc:0.8281\n",
      "Epoch [10/10], Step [238/600], Loss: 0.4963, Train Acc:0.8281\n",
      "Epoch [10/10], Step [239/600], Loss: 0.6212, Train Acc:0.8280\n",
      "Epoch [10/10], Step [240/600], Loss: 0.5988, Train Acc:0.8279\n",
      "Epoch [10/10], Step [241/600], Loss: 0.4842, Train Acc:0.8282\n",
      "Epoch [10/10], Step [242/600], Loss: 0.5359, Train Acc:0.8281\n",
      "Epoch [10/10], Step [243/600], Loss: 0.5585, Train Acc:0.8280\n",
      "Epoch [10/10], Step [244/600], Loss: 0.4255, Train Acc:0.8282\n",
      "Epoch [10/10], Step [245/600], Loss: 0.4886, Train Acc:0.8280\n",
      "Epoch [10/10], Step [246/600], Loss: 0.4632, Train Acc:0.8282\n",
      "Epoch [10/10], Step [247/600], Loss: 0.5050, Train Acc:0.8283\n",
      "Epoch [10/10], Step [248/600], Loss: 0.4950, Train Acc:0.8282\n",
      "Epoch [10/10], Step [249/600], Loss: 0.6484, Train Acc:0.8281\n",
      "Epoch [10/10], Step [250/600], Loss: 0.6767, Train Acc:0.8279\n",
      "Epoch [10/10], Step [251/600], Loss: 0.5239, Train Acc:0.8278\n",
      "Epoch [10/10], Step [252/600], Loss: 0.6520, Train Acc:0.8276\n",
      "Epoch [10/10], Step [253/600], Loss: 0.4499, Train Acc:0.8276\n",
      "Epoch [10/10], Step [254/600], Loss: 0.5167, Train Acc:0.8277\n",
      "Epoch [10/10], Step [255/600], Loss: 0.4782, Train Acc:0.8278\n",
      "Epoch [10/10], Step [256/600], Loss: 0.5867, Train Acc:0.8276\n",
      "Epoch [10/10], Step [257/600], Loss: 0.4131, Train Acc:0.8279\n",
      "Epoch [10/10], Step [258/600], Loss: 0.6177, Train Acc:0.8278\n",
      "Epoch [10/10], Step [259/600], Loss: 0.6299, Train Acc:0.8277\n",
      "Epoch [10/10], Step [260/600], Loss: 0.4266, Train Acc:0.8277\n",
      "Epoch [10/10], Step [261/600], Loss: 0.4670, Train Acc:0.8277\n",
      "Epoch [10/10], Step [262/600], Loss: 0.4374, Train Acc:0.8279\n",
      "Epoch [10/10], Step [263/600], Loss: 0.4472, Train Acc:0.8279\n",
      "Epoch [10/10], Step [264/600], Loss: 0.6240, Train Acc:0.8277\n",
      "Epoch [10/10], Step [265/600], Loss: 0.7046, Train Acc:0.8275\n",
      "Epoch [10/10], Step [266/600], Loss: 0.4550, Train Acc:0.8276\n",
      "Epoch [10/10], Step [267/600], Loss: 0.5416, Train Acc:0.8277\n",
      "Epoch [10/10], Step [268/600], Loss: 0.5340, Train Acc:0.8277\n",
      "Epoch [10/10], Step [269/600], Loss: 0.5768, Train Acc:0.8275\n",
      "Epoch [10/10], Step [270/600], Loss: 0.4693, Train Acc:0.8276\n",
      "Epoch [10/10], Step [271/600], Loss: 0.4326, Train Acc:0.8278\n",
      "Epoch [10/10], Step [272/600], Loss: 0.4986, Train Acc:0.8278\n",
      "Epoch [10/10], Step [273/600], Loss: 0.4776, Train Acc:0.8278\n",
      "Epoch [10/10], Step [274/600], Loss: 0.6030, Train Acc:0.8276\n",
      "Epoch [10/10], Step [275/600], Loss: 0.4108, Train Acc:0.8277\n",
      "Epoch [10/10], Step [276/600], Loss: 0.6292, Train Acc:0.8278\n",
      "Epoch [10/10], Step [277/600], Loss: 0.4766, Train Acc:0.8279\n",
      "Epoch [10/10], Step [278/600], Loss: 0.4620, Train Acc:0.8279\n",
      "Epoch [10/10], Step [279/600], Loss: 0.3568, Train Acc:0.8283\n",
      "Epoch [10/10], Step [280/600], Loss: 0.4511, Train Acc:0.8284\n",
      "Epoch [10/10], Step [281/600], Loss: 0.4827, Train Acc:0.8285\n",
      "Epoch [10/10], Step [282/600], Loss: 0.4603, Train Acc:0.8286\n",
      "Epoch [10/10], Step [283/600], Loss: 0.5422, Train Acc:0.8285\n",
      "Epoch [10/10], Step [284/600], Loss: 0.4388, Train Acc:0.8287\n",
      "Epoch [10/10], Step [285/600], Loss: 0.4168, Train Acc:0.8287\n",
      "Epoch [10/10], Step [286/600], Loss: 0.6409, Train Acc:0.8284\n",
      "Epoch [10/10], Step [287/600], Loss: 0.5948, Train Acc:0.8283\n",
      "Epoch [10/10], Step [288/600], Loss: 0.4323, Train Acc:0.8285\n",
      "Epoch [10/10], Step [289/600], Loss: 0.5530, Train Acc:0.8283\n",
      "Epoch [10/10], Step [290/600], Loss: 0.5756, Train Acc:0.8284\n",
      "Epoch [10/10], Step [291/600], Loss: 0.5975, Train Acc:0.8283\n",
      "Epoch [10/10], Step [292/600], Loss: 0.5000, Train Acc:0.8283\n",
      "Epoch [10/10], Step [293/600], Loss: 0.5728, Train Acc:0.8282\n",
      "Epoch [10/10], Step [294/600], Loss: 0.4695, Train Acc:0.8283\n",
      "Epoch [10/10], Step [295/600], Loss: 0.5401, Train Acc:0.8282\n",
      "Epoch [10/10], Step [296/600], Loss: 0.6381, Train Acc:0.8279\n",
      "Epoch [10/10], Step [297/600], Loss: 0.4413, Train Acc:0.8280\n",
      "Epoch [10/10], Step [298/600], Loss: 0.5601, Train Acc:0.8280\n",
      "Epoch [10/10], Step [299/600], Loss: 0.4193, Train Acc:0.8280\n",
      "Epoch [10/10], Step [300/600], Loss: 0.4767, Train Acc:0.8280\n",
      "Epoch [10/10], Step [301/600], Loss: 0.4725, Train Acc:0.8279\n",
      "Epoch [10/10], Step [302/600], Loss: 0.4949, Train Acc:0.8278\n",
      "Epoch [10/10], Step [303/600], Loss: 0.3303, Train Acc:0.8282\n",
      "Epoch [10/10], Step [304/600], Loss: 0.5026, Train Acc:0.8281\n",
      "Epoch [10/10], Step [305/600], Loss: 0.4519, Train Acc:0.8282\n",
      "Epoch [10/10], Step [306/600], Loss: 0.6240, Train Acc:0.8281\n",
      "Epoch [10/10], Step [307/600], Loss: 0.4582, Train Acc:0.8283\n",
      "Epoch [10/10], Step [308/600], Loss: 0.3945, Train Acc:0.8285\n",
      "Epoch [10/10], Step [309/600], Loss: 0.6314, Train Acc:0.8283\n",
      "Epoch [10/10], Step [310/600], Loss: 0.5653, Train Acc:0.8285\n",
      "Epoch [10/10], Step [311/600], Loss: 0.5050, Train Acc:0.8284\n",
      "Epoch [10/10], Step [312/600], Loss: 0.4496, Train Acc:0.8284\n",
      "Epoch [10/10], Step [313/600], Loss: 0.4905, Train Acc:0.8284\n",
      "Epoch [10/10], Step [314/600], Loss: 0.4704, Train Acc:0.8285\n",
      "Epoch [10/10], Step [315/600], Loss: 0.5948, Train Acc:0.8285\n",
      "Epoch [10/10], Step [316/600], Loss: 0.6310, Train Acc:0.8284\n",
      "Epoch [10/10], Step [317/600], Loss: 0.5699, Train Acc:0.8283\n",
      "Epoch [10/10], Step [318/600], Loss: 0.4649, Train Acc:0.8285\n",
      "Epoch [10/10], Step [319/600], Loss: 0.5298, Train Acc:0.8285\n",
      "Epoch [10/10], Step [320/600], Loss: 0.5428, Train Acc:0.8285\n",
      "Epoch [10/10], Step [321/600], Loss: 0.5361, Train Acc:0.8284\n",
      "Epoch [10/10], Step [322/600], Loss: 0.4644, Train Acc:0.8285\n",
      "Epoch [10/10], Step [323/600], Loss: 0.5764, Train Acc:0.8284\n",
      "Epoch [10/10], Step [324/600], Loss: 0.4559, Train Acc:0.8286\n",
      "Epoch [10/10], Step [325/600], Loss: 0.4341, Train Acc:0.8287\n",
      "Epoch [10/10], Step [326/600], Loss: 0.5988, Train Acc:0.8285\n",
      "Epoch [10/10], Step [327/600], Loss: 0.4028, Train Acc:0.8287\n",
      "Epoch [10/10], Step [328/600], Loss: 0.5233, Train Acc:0.8286\n",
      "Epoch [10/10], Step [329/600], Loss: 0.6062, Train Acc:0.8284\n",
      "Epoch [10/10], Step [330/600], Loss: 0.4070, Train Acc:0.8286\n",
      "Epoch [10/10], Step [331/600], Loss: 0.5568, Train Acc:0.8286\n",
      "Epoch [10/10], Step [332/600], Loss: 0.4844, Train Acc:0.8286\n",
      "Epoch [10/10], Step [333/600], Loss: 0.4190, Train Acc:0.8287\n",
      "Epoch [10/10], Step [334/600], Loss: 0.4548, Train Acc:0.8288\n",
      "Epoch [10/10], Step [335/600], Loss: 0.6204, Train Acc:0.8285\n",
      "Epoch [10/10], Step [336/600], Loss: 0.4241, Train Acc:0.8286\n",
      "Epoch [10/10], Step [337/600], Loss: 0.5392, Train Acc:0.8287\n",
      "Epoch [10/10], Step [338/600], Loss: 0.3935, Train Acc:0.8288\n",
      "Epoch [10/10], Step [339/600], Loss: 0.6720, Train Acc:0.8286\n",
      "Epoch [10/10], Step [340/600], Loss: 0.4749, Train Acc:0.8287\n",
      "Epoch [10/10], Step [341/600], Loss: 0.5451, Train Acc:0.8287\n",
      "Epoch [10/10], Step [342/600], Loss: 0.5661, Train Acc:0.8287\n",
      "Epoch [10/10], Step [343/600], Loss: 0.4404, Train Acc:0.8288\n",
      "Epoch [10/10], Step [344/600], Loss: 0.5788, Train Acc:0.8289\n",
      "Epoch [10/10], Step [345/600], Loss: 0.5403, Train Acc:0.8289\n",
      "Epoch [10/10], Step [346/600], Loss: 0.6453, Train Acc:0.8288\n",
      "Epoch [10/10], Step [347/600], Loss: 0.5704, Train Acc:0.8288\n",
      "Epoch [10/10], Step [348/600], Loss: 0.5335, Train Acc:0.8287\n",
      "Epoch [10/10], Step [349/600], Loss: 0.4649, Train Acc:0.8289\n",
      "Epoch [10/10], Step [350/600], Loss: 0.5024, Train Acc:0.8288\n",
      "Epoch [10/10], Step [351/600], Loss: 0.5680, Train Acc:0.8288\n",
      "Epoch [10/10], Step [352/600], Loss: 0.5657, Train Acc:0.8287\n",
      "Epoch [10/10], Step [353/600], Loss: 0.6190, Train Acc:0.8286\n",
      "Epoch [10/10], Step [354/600], Loss: 0.5280, Train Acc:0.8287\n",
      "Epoch [10/10], Step [355/600], Loss: 0.4262, Train Acc:0.8288\n",
      "Epoch [10/10], Step [356/600], Loss: 0.4828, Train Acc:0.8286\n",
      "Epoch [10/10], Step [357/600], Loss: 0.4298, Train Acc:0.8288\n",
      "Epoch [10/10], Step [358/600], Loss: 0.4674, Train Acc:0.8288\n",
      "Epoch [10/10], Step [359/600], Loss: 0.4406, Train Acc:0.8289\n",
      "Epoch [10/10], Step [360/600], Loss: 0.3993, Train Acc:0.8290\n",
      "Epoch [10/10], Step [361/600], Loss: 0.6435, Train Acc:0.8289\n",
      "Epoch [10/10], Step [362/600], Loss: 0.5184, Train Acc:0.8289\n",
      "Epoch [10/10], Step [363/600], Loss: 0.7115, Train Acc:0.8288\n",
      "Epoch [10/10], Step [364/600], Loss: 0.5259, Train Acc:0.8287\n",
      "Epoch [10/10], Step [365/600], Loss: 0.5128, Train Acc:0.8287\n",
      "Epoch [10/10], Step [366/600], Loss: 0.3685, Train Acc:0.8289\n",
      "Epoch [10/10], Step [367/600], Loss: 0.5595, Train Acc:0.8289\n",
      "Epoch [10/10], Step [368/600], Loss: 0.5181, Train Acc:0.8289\n",
      "Epoch [10/10], Step [369/600], Loss: 0.4023, Train Acc:0.8290\n",
      "Epoch [10/10], Step [370/600], Loss: 0.5152, Train Acc:0.8291\n",
      "Epoch [10/10], Step [371/600], Loss: 0.6242, Train Acc:0.8290\n",
      "Epoch [10/10], Step [372/600], Loss: 0.4707, Train Acc:0.8291\n",
      "Epoch [10/10], Step [373/600], Loss: 0.5645, Train Acc:0.8290\n",
      "Epoch [10/10], Step [374/600], Loss: 0.5954, Train Acc:0.8289\n",
      "Epoch [10/10], Step [375/600], Loss: 0.4739, Train Acc:0.8289\n",
      "Epoch [10/10], Step [376/600], Loss: 0.5290, Train Acc:0.8287\n",
      "Epoch [10/10], Step [377/600], Loss: 0.4577, Train Acc:0.8288\n",
      "Epoch [10/10], Step [378/600], Loss: 0.4763, Train Acc:0.8288\n",
      "Epoch [10/10], Step [379/600], Loss: 0.4718, Train Acc:0.8287\n",
      "Epoch [10/10], Step [380/600], Loss: 0.4477, Train Acc:0.8288\n",
      "Epoch [10/10], Step [381/600], Loss: 0.5403, Train Acc:0.8288\n",
      "Epoch [10/10], Step [382/600], Loss: 0.5746, Train Acc:0.8288\n",
      "Epoch [10/10], Step [383/600], Loss: 0.5035, Train Acc:0.8289\n",
      "Epoch [10/10], Step [384/600], Loss: 0.5774, Train Acc:0.8287\n",
      "Epoch [10/10], Step [385/600], Loss: 0.5396, Train Acc:0.8287\n",
      "Epoch [10/10], Step [386/600], Loss: 0.5746, Train Acc:0.8285\n",
      "Epoch [10/10], Step [387/600], Loss: 0.5391, Train Acc:0.8286\n",
      "Epoch [10/10], Step [388/600], Loss: 0.5628, Train Acc:0.8285\n",
      "Epoch [10/10], Step [389/600], Loss: 0.5696, Train Acc:0.8283\n",
      "Epoch [10/10], Step [390/600], Loss: 0.6510, Train Acc:0.8282\n",
      "Epoch [10/10], Step [391/600], Loss: 0.5962, Train Acc:0.8281\n",
      "Epoch [10/10], Step [392/600], Loss: 0.6103, Train Acc:0.8281\n",
      "Epoch [10/10], Step [393/600], Loss: 0.4748, Train Acc:0.8281\n",
      "Epoch [10/10], Step [394/600], Loss: 0.5328, Train Acc:0.8280\n",
      "Epoch [10/10], Step [395/600], Loss: 0.5474, Train Acc:0.8279\n",
      "Epoch [10/10], Step [396/600], Loss: 0.6186, Train Acc:0.8278\n",
      "Epoch [10/10], Step [397/600], Loss: 0.4602, Train Acc:0.8278\n",
      "Epoch [10/10], Step [398/600], Loss: 0.6634, Train Acc:0.8277\n",
      "Epoch [10/10], Step [399/600], Loss: 0.4509, Train Acc:0.8277\n",
      "Epoch [10/10], Step [400/600], Loss: 0.4977, Train Acc:0.8278\n",
      "Epoch [10/10], Step [401/600], Loss: 0.5946, Train Acc:0.8277\n",
      "Epoch [10/10], Step [402/600], Loss: 0.5836, Train Acc:0.8277\n",
      "Epoch [10/10], Step [403/600], Loss: 0.5267, Train Acc:0.8277\n",
      "Epoch [10/10], Step [404/600], Loss: 0.5243, Train Acc:0.8277\n",
      "Epoch [10/10], Step [405/600], Loss: 0.4832, Train Acc:0.8278\n",
      "Epoch [10/10], Step [406/600], Loss: 0.4168, Train Acc:0.8279\n",
      "Epoch [10/10], Step [407/600], Loss: 0.4719, Train Acc:0.8281\n",
      "Epoch [10/10], Step [408/600], Loss: 0.5593, Train Acc:0.8281\n",
      "Epoch [10/10], Step [409/600], Loss: 0.7282, Train Acc:0.8280\n",
      "Epoch [10/10], Step [410/600], Loss: 0.5498, Train Acc:0.8280\n",
      "Epoch [10/10], Step [411/600], Loss: 0.5607, Train Acc:0.8281\n",
      "Epoch [10/10], Step [412/600], Loss: 0.6207, Train Acc:0.8279\n",
      "Epoch [10/10], Step [413/600], Loss: 0.5403, Train Acc:0.8279\n",
      "Epoch [10/10], Step [414/600], Loss: 0.4144, Train Acc:0.8280\n",
      "Epoch [10/10], Step [415/600], Loss: 0.4974, Train Acc:0.8280\n",
      "Epoch [10/10], Step [416/600], Loss: 0.5924, Train Acc:0.8278\n",
      "Epoch [10/10], Step [417/600], Loss: 0.4647, Train Acc:0.8278\n",
      "Epoch [10/10], Step [418/600], Loss: 0.5348, Train Acc:0.8278\n",
      "Epoch [10/10], Step [419/600], Loss: 0.5502, Train Acc:0.8278\n",
      "Epoch [10/10], Step [420/600], Loss: 0.5248, Train Acc:0.8278\n",
      "Epoch [10/10], Step [421/600], Loss: 0.5303, Train Acc:0.8278\n",
      "Epoch [10/10], Step [422/600], Loss: 0.4194, Train Acc:0.8279\n",
      "Epoch [10/10], Step [423/600], Loss: 0.4672, Train Acc:0.8280\n",
      "Epoch [10/10], Step [424/600], Loss: 0.5285, Train Acc:0.8281\n",
      "Epoch [10/10], Step [425/600], Loss: 0.3889, Train Acc:0.8282\n",
      "Epoch [10/10], Step [426/600], Loss: 0.7359, Train Acc:0.8281\n",
      "Epoch [10/10], Step [427/600], Loss: 0.4071, Train Acc:0.8283\n",
      "Epoch [10/10], Step [428/600], Loss: 0.5810, Train Acc:0.8282\n",
      "Epoch [10/10], Step [429/600], Loss: 0.5681, Train Acc:0.8281\n",
      "Epoch [10/10], Step [430/600], Loss: 0.5139, Train Acc:0.8281\n",
      "Epoch [10/10], Step [431/600], Loss: 0.4728, Train Acc:0.8281\n",
      "Epoch [10/10], Step [432/600], Loss: 0.5549, Train Acc:0.8281\n",
      "Epoch [10/10], Step [433/600], Loss: 0.4926, Train Acc:0.8282\n",
      "Epoch [10/10], Step [434/600], Loss: 0.5036, Train Acc:0.8282\n",
      "Epoch [10/10], Step [435/600], Loss: 0.5270, Train Acc:0.8283\n",
      "Epoch [10/10], Step [436/600], Loss: 0.4359, Train Acc:0.8283\n",
      "Epoch [10/10], Step [437/600], Loss: 0.4933, Train Acc:0.8284\n",
      "Epoch [10/10], Step [438/600], Loss: 0.5856, Train Acc:0.8284\n",
      "Epoch [10/10], Step [439/600], Loss: 0.5283, Train Acc:0.8283\n",
      "Epoch [10/10], Step [440/600], Loss: 0.3909, Train Acc:0.8284\n",
      "Epoch [10/10], Step [441/600], Loss: 0.5283, Train Acc:0.8285\n",
      "Epoch [10/10], Step [442/600], Loss: 0.4608, Train Acc:0.8285\n",
      "Epoch [10/10], Step [443/600], Loss: 0.4816, Train Acc:0.8286\n",
      "Epoch [10/10], Step [444/600], Loss: 0.4239, Train Acc:0.8287\n",
      "Epoch [10/10], Step [445/600], Loss: 0.5962, Train Acc:0.8287\n",
      "Epoch [10/10], Step [446/600], Loss: 0.5013, Train Acc:0.8287\n",
      "Epoch [10/10], Step [447/600], Loss: 0.5952, Train Acc:0.8286\n",
      "Epoch [10/10], Step [448/600], Loss: 0.4862, Train Acc:0.8286\n",
      "Epoch [10/10], Step [449/600], Loss: 0.3088, Train Acc:0.8288\n",
      "Epoch [10/10], Step [450/600], Loss: 0.4790, Train Acc:0.8289\n",
      "Epoch [10/10], Step [451/600], Loss: 0.4958, Train Acc:0.8289\n",
      "Epoch [10/10], Step [452/600], Loss: 0.5610, Train Acc:0.8289\n",
      "Epoch [10/10], Step [453/600], Loss: 0.4470, Train Acc:0.8289\n",
      "Epoch [10/10], Step [454/600], Loss: 0.5723, Train Acc:0.8288\n",
      "Epoch [10/10], Step [455/600], Loss: 0.5499, Train Acc:0.8287\n",
      "Epoch [10/10], Step [456/600], Loss: 0.3918, Train Acc:0.8289\n",
      "Epoch [10/10], Step [457/600], Loss: 0.5279, Train Acc:0.8289\n",
      "Epoch [10/10], Step [458/600], Loss: 0.5904, Train Acc:0.8288\n",
      "Epoch [10/10], Step [459/600], Loss: 0.5451, Train Acc:0.8289\n",
      "Epoch [10/10], Step [460/600], Loss: 0.4785, Train Acc:0.8289\n",
      "Epoch [10/10], Step [461/600], Loss: 0.4510, Train Acc:0.8290\n",
      "Epoch [10/10], Step [462/600], Loss: 0.5243, Train Acc:0.8289\n",
      "Epoch [10/10], Step [463/600], Loss: 0.4514, Train Acc:0.8290\n",
      "Epoch [10/10], Step [464/600], Loss: 0.3885, Train Acc:0.8291\n",
      "Epoch [10/10], Step [465/600], Loss: 0.4442, Train Acc:0.8292\n",
      "Epoch [10/10], Step [466/600], Loss: 0.5037, Train Acc:0.8292\n",
      "Epoch [10/10], Step [467/600], Loss: 0.6131, Train Acc:0.8293\n",
      "Epoch [10/10], Step [468/600], Loss: 0.5456, Train Acc:0.8293\n",
      "Epoch [10/10], Step [469/600], Loss: 0.4614, Train Acc:0.8293\n",
      "Epoch [10/10], Step [470/600], Loss: 0.6732, Train Acc:0.8291\n",
      "Epoch [10/10], Step [471/600], Loss: 0.4748, Train Acc:0.8292\n",
      "Epoch [10/10], Step [472/600], Loss: 0.4529, Train Acc:0.8293\n",
      "Epoch [10/10], Step [473/600], Loss: 0.5520, Train Acc:0.8292\n",
      "Epoch [10/10], Step [474/600], Loss: 0.6247, Train Acc:0.8292\n",
      "Epoch [10/10], Step [475/600], Loss: 0.4895, Train Acc:0.8292\n",
      "Epoch [10/10], Step [476/600], Loss: 0.4587, Train Acc:0.8291\n",
      "Epoch [10/10], Step [477/600], Loss: 0.5507, Train Acc:0.8292\n",
      "Epoch [10/10], Step [478/600], Loss: 0.5795, Train Acc:0.8291\n",
      "Epoch [10/10], Step [479/600], Loss: 0.5361, Train Acc:0.8291\n",
      "Epoch [10/10], Step [480/600], Loss: 0.6832, Train Acc:0.8290\n",
      "Epoch [10/10], Step [481/600], Loss: 0.4660, Train Acc:0.8289\n",
      "Epoch [10/10], Step [482/600], Loss: 0.5893, Train Acc:0.8290\n",
      "Epoch [10/10], Step [483/600], Loss: 0.5063, Train Acc:0.8290\n",
      "Epoch [10/10], Step [484/600], Loss: 0.5016, Train Acc:0.8290\n",
      "Epoch [10/10], Step [485/600], Loss: 0.7377, Train Acc:0.8287\n",
      "Epoch [10/10], Step [486/600], Loss: 0.6188, Train Acc:0.8285\n",
      "Epoch [10/10], Step [487/600], Loss: 0.5968, Train Acc:0.8286\n",
      "Epoch [10/10], Step [488/600], Loss: 0.5198, Train Acc:0.8286\n",
      "Epoch [10/10], Step [489/600], Loss: 0.5874, Train Acc:0.8286\n",
      "Epoch [10/10], Step [490/600], Loss: 0.5597, Train Acc:0.8285\n",
      "Epoch [10/10], Step [491/600], Loss: 0.4738, Train Acc:0.8286\n",
      "Epoch [10/10], Step [492/600], Loss: 0.6087, Train Acc:0.8285\n",
      "Epoch [10/10], Step [493/600], Loss: 0.6167, Train Acc:0.8284\n",
      "Epoch [10/10], Step [494/600], Loss: 0.4628, Train Acc:0.8284\n",
      "Epoch [10/10], Step [495/600], Loss: 0.4866, Train Acc:0.8285\n",
      "Epoch [10/10], Step [496/600], Loss: 0.5209, Train Acc:0.8286\n",
      "Epoch [10/10], Step [497/600], Loss: 0.5970, Train Acc:0.8285\n",
      "Epoch [10/10], Step [498/600], Loss: 0.4785, Train Acc:0.8286\n",
      "Epoch [10/10], Step [499/600], Loss: 0.5049, Train Acc:0.8286\n",
      "Epoch [10/10], Step [500/600], Loss: 0.4472, Train Acc:0.8286\n",
      "Epoch [10/10], Step [501/600], Loss: 0.5632, Train Acc:0.8286\n",
      "Epoch [10/10], Step [502/600], Loss: 0.5741, Train Acc:0.8285\n",
      "Epoch [10/10], Step [503/600], Loss: 0.3437, Train Acc:0.8286\n",
      "Epoch [10/10], Step [504/600], Loss: 0.5130, Train Acc:0.8286\n",
      "Epoch [10/10], Step [505/600], Loss: 0.4676, Train Acc:0.8287\n",
      "Epoch [10/10], Step [506/600], Loss: 0.5030, Train Acc:0.8287\n",
      "Epoch [10/10], Step [507/600], Loss: 0.4466, Train Acc:0.8287\n",
      "Epoch [10/10], Step [508/600], Loss: 0.5345, Train Acc:0.8286\n",
      "Epoch [10/10], Step [509/600], Loss: 0.4529, Train Acc:0.8287\n",
      "Epoch [10/10], Step [510/600], Loss: 0.7527, Train Acc:0.8285\n",
      "Epoch [10/10], Step [511/600], Loss: 0.4461, Train Acc:0.8285\n",
      "Epoch [10/10], Step [512/600], Loss: 0.5685, Train Acc:0.8285\n",
      "Epoch [10/10], Step [513/600], Loss: 0.5267, Train Acc:0.8285\n",
      "Epoch [10/10], Step [514/600], Loss: 0.5727, Train Acc:0.8285\n",
      "Epoch [10/10], Step [515/600], Loss: 0.5603, Train Acc:0.8284\n",
      "Epoch [10/10], Step [516/600], Loss: 0.4439, Train Acc:0.8284\n",
      "Epoch [10/10], Step [517/600], Loss: 0.3949, Train Acc:0.8285\n",
      "Epoch [10/10], Step [518/600], Loss: 0.4414, Train Acc:0.8286\n",
      "Epoch [10/10], Step [519/600], Loss: 0.5955, Train Acc:0.8285\n",
      "Epoch [10/10], Step [520/600], Loss: 0.6010, Train Acc:0.8284\n",
      "Epoch [10/10], Step [521/600], Loss: 0.5648, Train Acc:0.8283\n",
      "Epoch [10/10], Step [522/600], Loss: 0.6248, Train Acc:0.8282\n",
      "Epoch [10/10], Step [523/600], Loss: 0.5123, Train Acc:0.8282\n",
      "Epoch [10/10], Step [524/600], Loss: 0.5160, Train Acc:0.8282\n",
      "Epoch [10/10], Step [525/600], Loss: 0.4515, Train Acc:0.8283\n",
      "Epoch [10/10], Step [526/600], Loss: 0.5407, Train Acc:0.8282\n",
      "Epoch [10/10], Step [527/600], Loss: 0.4378, Train Acc:0.8283\n",
      "Epoch [10/10], Step [528/600], Loss: 0.5263, Train Acc:0.8283\n",
      "Epoch [10/10], Step [529/600], Loss: 0.5235, Train Acc:0.8283\n",
      "Epoch [10/10], Step [530/600], Loss: 0.6196, Train Acc:0.8283\n",
      "Epoch [10/10], Step [531/600], Loss: 0.5441, Train Acc:0.8283\n",
      "Epoch [10/10], Step [532/600], Loss: 0.6007, Train Acc:0.8283\n",
      "Epoch [10/10], Step [533/600], Loss: 0.6214, Train Acc:0.8282\n",
      "Epoch [10/10], Step [534/600], Loss: 0.5439, Train Acc:0.8282\n",
      "Epoch [10/10], Step [535/600], Loss: 0.6010, Train Acc:0.8281\n",
      "Epoch [10/10], Step [536/600], Loss: 0.4449, Train Acc:0.8282\n",
      "Epoch [10/10], Step [537/600], Loss: 0.5547, Train Acc:0.8281\n",
      "Epoch [10/10], Step [538/600], Loss: 0.4781, Train Acc:0.8281\n",
      "Epoch [10/10], Step [539/600], Loss: 0.4930, Train Acc:0.8281\n",
      "Epoch [10/10], Step [540/600], Loss: 0.6614, Train Acc:0.8279\n",
      "Epoch [10/10], Step [541/600], Loss: 0.5029, Train Acc:0.8279\n",
      "Epoch [10/10], Step [542/600], Loss: 0.5696, Train Acc:0.8278\n",
      "Epoch [10/10], Step [543/600], Loss: 0.5490, Train Acc:0.8277\n",
      "Epoch [10/10], Step [544/600], Loss: 0.5483, Train Acc:0.8277\n",
      "Epoch [10/10], Step [545/600], Loss: 0.5462, Train Acc:0.8277\n",
      "Epoch [10/10], Step [546/600], Loss: 0.4775, Train Acc:0.8278\n",
      "Epoch [10/10], Step [547/600], Loss: 0.4619, Train Acc:0.8278\n",
      "Epoch [10/10], Step [548/600], Loss: 0.7197, Train Acc:0.8277\n",
      "Epoch [10/10], Step [549/600], Loss: 0.7044, Train Acc:0.8276\n",
      "Epoch [10/10], Step [550/600], Loss: 0.3893, Train Acc:0.8277\n",
      "Epoch [10/10], Step [551/600], Loss: 0.4681, Train Acc:0.8277\n",
      "Epoch [10/10], Step [552/600], Loss: 0.3786, Train Acc:0.8278\n",
      "Epoch [10/10], Step [553/600], Loss: 0.4787, Train Acc:0.8278\n",
      "Epoch [10/10], Step [554/600], Loss: 0.4962, Train Acc:0.8279\n",
      "Epoch [10/10], Step [555/600], Loss: 0.6058, Train Acc:0.8277\n",
      "Epoch [10/10], Step [556/600], Loss: 0.4663, Train Acc:0.8278\n",
      "Epoch [10/10], Step [557/600], Loss: 0.6045, Train Acc:0.8279\n",
      "Epoch [10/10], Step [558/600], Loss: 0.4713, Train Acc:0.8278\n",
      "Epoch [10/10], Step [559/600], Loss: 0.6898, Train Acc:0.8277\n",
      "Epoch [10/10], Step [560/600], Loss: 0.6180, Train Acc:0.8276\n",
      "Epoch [10/10], Step [561/600], Loss: 0.6284, Train Acc:0.8276\n",
      "Epoch [10/10], Step [562/600], Loss: 0.4315, Train Acc:0.8276\n",
      "Epoch [10/10], Step [563/600], Loss: 0.4469, Train Acc:0.8277\n",
      "Epoch [10/10], Step [564/600], Loss: 0.4572, Train Acc:0.8277\n",
      "Epoch [10/10], Step [565/600], Loss: 0.4960, Train Acc:0.8278\n",
      "Epoch [10/10], Step [566/600], Loss: 0.4481, Train Acc:0.8278\n",
      "Epoch [10/10], Step [567/600], Loss: 0.3818, Train Acc:0.8280\n",
      "Epoch [10/10], Step [568/600], Loss: 0.4601, Train Acc:0.8280\n",
      "Epoch [10/10], Step [569/600], Loss: 0.4534, Train Acc:0.8280\n",
      "Epoch [10/10], Step [570/600], Loss: 0.4274, Train Acc:0.8281\n",
      "Epoch [10/10], Step [571/600], Loss: 0.4650, Train Acc:0.8281\n",
      "Epoch [10/10], Step [572/600], Loss: 0.5171, Train Acc:0.8281\n",
      "Epoch [10/10], Step [573/600], Loss: 0.5639, Train Acc:0.8281\n",
      "Epoch [10/10], Step [574/600], Loss: 0.4530, Train Acc:0.8282\n",
      "Epoch [10/10], Step [575/600], Loss: 0.5187, Train Acc:0.8282\n",
      "Epoch [10/10], Step [576/600], Loss: 0.5375, Train Acc:0.8283\n",
      "Epoch [10/10], Step [577/600], Loss: 0.3976, Train Acc:0.8284\n",
      "Epoch [10/10], Step [578/600], Loss: 0.6796, Train Acc:0.8282\n",
      "Epoch [10/10], Step [579/600], Loss: 0.5512, Train Acc:0.8282\n",
      "Epoch [10/10], Step [580/600], Loss: 0.4095, Train Acc:0.8283\n",
      "Epoch [10/10], Step [581/600], Loss: 0.4880, Train Acc:0.8283\n",
      "Epoch [10/10], Step [582/600], Loss: 0.5715, Train Acc:0.8282\n",
      "Epoch [10/10], Step [583/600], Loss: 0.5113, Train Acc:0.8281\n",
      "Epoch [10/10], Step [584/600], Loss: 0.5613, Train Acc:0.8281\n",
      "Epoch [10/10], Step [585/600], Loss: 0.4451, Train Acc:0.8282\n",
      "Epoch [10/10], Step [586/600], Loss: 0.4940, Train Acc:0.8282\n",
      "Epoch [10/10], Step [587/600], Loss: 0.4879, Train Acc:0.8282\n",
      "Epoch [10/10], Step [588/600], Loss: 0.5157, Train Acc:0.8282\n",
      "Epoch [10/10], Step [589/600], Loss: 0.5694, Train Acc:0.8281\n",
      "Epoch [10/10], Step [590/600], Loss: 0.5470, Train Acc:0.8282\n",
      "Epoch [10/10], Step [591/600], Loss: 0.5375, Train Acc:0.8282\n",
      "Epoch [10/10], Step [592/600], Loss: 0.3924, Train Acc:0.8283\n",
      "Epoch [10/10], Step [593/600], Loss: 0.6237, Train Acc:0.8283\n",
      "Epoch [10/10], Step [594/600], Loss: 0.4631, Train Acc:0.8282\n",
      "Epoch [10/10], Step [595/600], Loss: 0.4600, Train Acc:0.8283\n",
      "Epoch [10/10], Step [596/600], Loss: 0.6020, Train Acc:0.8281\n",
      "Epoch [10/10], Step [597/600], Loss: 0.4526, Train Acc:0.8282\n",
      "Epoch [10/10], Step [598/600], Loss: 0.5116, Train Acc:0.8281\n",
      "Epoch [10/10], Step [599/600], Loss: 0.5322, Train Acc:0.8281\n",
      "Epoch [10/10], Step [600/600], Loss: 0.4864, Train Acc:0.8282\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #load data to device (dataloader했기 때문에)\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward, loss 계산\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward, 가중치 업데이트\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #로그 출력, acc \n",
    "        if (i+1)%1 ==0:\n",
    "            loss_list.append(loss.item())\n",
    "            _, preodicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preodicted == labels).sum().item()\n",
    "            acc_list.append(correct/total)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}, Train Acc:{acc_list[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss, acc visualize\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.plot(acc_list, label='Training Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이미지 가져오기\n",
    "detaiter = iter(test_loader)\n",
    "images, labels = detaiter.__next__()\n",
    "images = images.reshape(-1, 28*28)\n",
    "\n",
    "# 모델 예측\n",
    "outputs = model(images)\n",
    "_, preodicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8,8),\n",
    "                        subplot_kw = {'xticks':[], 'yticks':[]})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.squeeze(images[i].reshape(28,28)), cmap='gray')\n",
    "    ax.set_title(f\"preodicted: {preodicted[i].item()}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mspytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
