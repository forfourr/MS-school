{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "\n",
    "- 입력 변수 (Input variable) : 분류에 사용되는 데이터의 특징(feature)\n",
    "- 가중치 (Weights) : 각 입력 변수의 영향력을 나타내는 값\n",
    "- 편향 (Bias) : 모델의 적합도를 조정하는 상수항\n",
    "- 시그모이드 함수 (Sigmoid function) : 입력 값을 0과 1사이로 변환하는 함수\n",
    "- 로그 손실 함수 (Log loss function) : 모델의 오차를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1000개의 데이터 feature 5개, label 0/1\n",
    "x, y = make_classification(\n",
    "    n_samples = 1000,         #생성할 데이터 수\n",
    "    n_features = 5,           #독립변수 수(입력변수에 사용)/종속변수 라벨\n",
    "    n_informative = 2,        #독립 변수 중에 유의미한 변수 계수\n",
    "    n_redundant = 0,          #독립 변수 중에 불필요한 독립변수 계수\n",
    "    n_clusters_per_class = 1,    #클래스당 클러스터 계수\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "#print(x,y)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasetc 클래스로 to tensor\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        #to tensor\n",
    "        self.x = torch.tensor(x, dtype = torch.float32)\n",
    "        self.y = torch.tensor(y, dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 정의\n",
    "train_dataset = MyCustomDataset(X_train, y_train)\n",
    "test_dataset = MyCustomDataset(X_test, y_test)\n",
    "\n",
    "#데이터 로드 정의\n",
    "# batch 기반 딥러닝 학습을 위해 data slice한다\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size =32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#모델 정의\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(input_dim=5)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Loss function, optimizer 선언\n",
    "criterion = nn.BCELoss()        # 0/1 이진분류 이기 때문에 BCELoss사용()\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                    lr = 0.01,\n",
    "                    weight_decay = 1e-5,\n",
    "                    momentum = 0.9,\n",
    "                    nesterov = True)\n",
    "print(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] , Loss: 0.7278\n",
      "Epoch [1/100] , Loss: 0.7259\n",
      "Epoch [1/100] , Loss: 0.7669\n",
      "Epoch [1/100] , Loss: 0.6200\n",
      "Epoch [1/100] , Loss: 0.6972\n",
      "Epoch [1/100] , Loss: 0.5687\n",
      "Epoch [1/100] , Loss: 0.5330\n",
      "Epoch [1/100] , Loss: 0.7146\n",
      "Epoch [1/100] , Loss: 0.7395\n",
      "Epoch [1/100] , Loss: 0.5930\n",
      "Epoch [1/100] , Loss: 0.5436\n",
      "Epoch [1/100] , Loss: 0.5552\n",
      "Epoch [1/100] , Loss: 0.5919\n",
      "Epoch [1/100] , Loss: 0.5179\n",
      "Epoch [1/100] , Loss: 0.5466\n",
      "Epoch [1/100] , Loss: 0.5887\n",
      "Epoch [1/100] , Loss: 0.5438\n",
      "Epoch [1/100] , Loss: 0.5124\n",
      "Epoch [1/100] , Loss: 0.4412\n",
      "Epoch [1/100] , Loss: 0.4183\n",
      "Epoch [1/100] , Loss: 0.4344\n",
      "Epoch [1/100] , Loss: 0.4866\n",
      "Epoch [1/100] , Loss: 0.5040\n",
      "Epoch [1/100] , Loss: 0.4791\n",
      "Epoch [1/100] , Loss: 0.4014\n",
      "Epoch [11/100] , Loss: 0.1920\n",
      "Epoch [11/100] , Loss: 0.3994\n",
      "Epoch [11/100] , Loss: 0.4082\n",
      "Epoch [11/100] , Loss: 0.2118\n",
      "Epoch [11/100] , Loss: 0.2706\n",
      "Epoch [11/100] , Loss: 0.1783\n",
      "Epoch [11/100] , Loss: 0.3148\n",
      "Epoch [11/100] , Loss: 0.3735\n",
      "Epoch [11/100] , Loss: 0.1974\n",
      "Epoch [11/100] , Loss: 0.1357\n",
      "Epoch [11/100] , Loss: 0.1487\n",
      "Epoch [11/100] , Loss: 0.2571\n",
      "Epoch [11/100] , Loss: 0.2740\n",
      "Epoch [11/100] , Loss: 0.2586\n",
      "Epoch [11/100] , Loss: 0.2100\n",
      "Epoch [11/100] , Loss: 0.2129\n",
      "Epoch [11/100] , Loss: 0.2167\n",
      "Epoch [11/100] , Loss: 0.2751\n",
      "Epoch [11/100] , Loss: 0.3298\n",
      "Epoch [11/100] , Loss: 0.4254\n",
      "Epoch [11/100] , Loss: 0.0959\n",
      "Epoch [11/100] , Loss: 0.2196\n",
      "Epoch [11/100] , Loss: 0.4696\n",
      "Epoch [11/100] , Loss: 0.2350\n",
      "Epoch [11/100] , Loss: 0.2775\n",
      "Epoch [21/100] , Loss: 0.2837\n",
      "Epoch [21/100] , Loss: 0.2529\n",
      "Epoch [21/100] , Loss: 0.2639\n",
      "Epoch [21/100] , Loss: 0.3129\n",
      "Epoch [21/100] , Loss: 0.1581\n",
      "Epoch [21/100] , Loss: 0.2052\n",
      "Epoch [21/100] , Loss: 0.2399\n",
      "Epoch [21/100] , Loss: 0.1901\n",
      "Epoch [21/100] , Loss: 0.2385\n",
      "Epoch [21/100] , Loss: 0.3176\n",
      "Epoch [21/100] , Loss: 0.2173\n",
      "Epoch [21/100] , Loss: 0.2248\n",
      "Epoch [21/100] , Loss: 0.1862\n",
      "Epoch [21/100] , Loss: 0.2726\n",
      "Epoch [21/100] , Loss: 0.1905\n",
      "Epoch [21/100] , Loss: 0.3996\n",
      "Epoch [21/100] , Loss: 0.4447\n",
      "Epoch [21/100] , Loss: 0.2443\n",
      "Epoch [21/100] , Loss: 0.2516\n",
      "Epoch [21/100] , Loss: 0.2528\n",
      "Epoch [21/100] , Loss: 0.3124\n",
      "Epoch [21/100] , Loss: 0.2550\n",
      "Epoch [21/100] , Loss: 0.2193\n",
      "Epoch [21/100] , Loss: 0.0943\n",
      "Epoch [21/100] , Loss: 0.3476\n",
      "Epoch [31/100] , Loss: 0.1971\n",
      "Epoch [31/100] , Loss: 0.2509\n",
      "Epoch [31/100] , Loss: 0.2040\n",
      "Epoch [31/100] , Loss: 0.3505\n",
      "Epoch [31/100] , Loss: 0.3148\n",
      "Epoch [31/100] , Loss: 0.3240\n",
      "Epoch [31/100] , Loss: 0.2203\n",
      "Epoch [31/100] , Loss: 0.3660\n",
      "Epoch [31/100] , Loss: 0.3445\n",
      "Epoch [31/100] , Loss: 0.1969\n",
      "Epoch [31/100] , Loss: 0.2466\n",
      "Epoch [31/100] , Loss: 0.2546\n",
      "Epoch [31/100] , Loss: 0.1869\n",
      "Epoch [31/100] , Loss: 0.2027\n",
      "Epoch [31/100] , Loss: 0.2065\n",
      "Epoch [31/100] , Loss: 0.4177\n",
      "Epoch [31/100] , Loss: 0.1975\n",
      "Epoch [31/100] , Loss: 0.1278\n",
      "Epoch [31/100] , Loss: 0.2478\n",
      "Epoch [31/100] , Loss: 0.3142\n",
      "Epoch [31/100] , Loss: 0.2250\n",
      "Epoch [31/100] , Loss: 0.1100\n",
      "Epoch [31/100] , Loss: 0.2648\n",
      "Epoch [31/100] , Loss: 0.2512\n",
      "Epoch [31/100] , Loss: 0.2936\n",
      "Epoch [41/100] , Loss: 0.3021\n",
      "Epoch [41/100] , Loss: 0.2134\n",
      "Epoch [41/100] , Loss: 0.2468\n",
      "Epoch [41/100] , Loss: 0.2324\n",
      "Epoch [41/100] , Loss: 0.3531\n",
      "Epoch [41/100] , Loss: 0.3374\n",
      "Epoch [41/100] , Loss: 0.1605\n",
      "Epoch [41/100] , Loss: 0.3076\n",
      "Epoch [41/100] , Loss: 0.2913\n",
      "Epoch [41/100] , Loss: 0.3674\n",
      "Epoch [41/100] , Loss: 0.2132\n",
      "Epoch [41/100] , Loss: 0.2067\n",
      "Epoch [41/100] , Loss: 0.1455\n",
      "Epoch [41/100] , Loss: 0.3706\n",
      "Epoch [41/100] , Loss: 0.3860\n",
      "Epoch [41/100] , Loss: 0.1455\n",
      "Epoch [41/100] , Loss: 0.3130\n",
      "Epoch [41/100] , Loss: 0.1694\n",
      "Epoch [41/100] , Loss: 0.2263\n",
      "Epoch [41/100] , Loss: 0.3605\n",
      "Epoch [41/100] , Loss: 0.1700\n",
      "Epoch [41/100] , Loss: 0.1479\n",
      "Epoch [41/100] , Loss: 0.2089\n",
      "Epoch [41/100] , Loss: 0.2338\n",
      "Epoch [41/100] , Loss: 0.2103\n",
      "Epoch [51/100] , Loss: 0.2129\n",
      "Epoch [51/100] , Loss: 0.1542\n",
      "Epoch [51/100] , Loss: 0.2652\n",
      "Epoch [51/100] , Loss: 0.2307\n",
      "Epoch [51/100] , Loss: 0.2526\n",
      "Epoch [51/100] , Loss: 0.3746\n",
      "Epoch [51/100] , Loss: 0.2475\n",
      "Epoch [51/100] , Loss: 0.4440\n",
      "Epoch [51/100] , Loss: 0.3244\n",
      "Epoch [51/100] , Loss: 0.1455\n",
      "Epoch [51/100] , Loss: 0.1980\n",
      "Epoch [51/100] , Loss: 0.1970\n",
      "Epoch [51/100] , Loss: 0.1994\n",
      "Epoch [51/100] , Loss: 0.4225\n",
      "Epoch [51/100] , Loss: 0.1402\n",
      "Epoch [51/100] , Loss: 0.2769\n",
      "Epoch [51/100] , Loss: 0.3102\n",
      "Epoch [51/100] , Loss: 0.3585\n",
      "Epoch [51/100] , Loss: 0.1159\n",
      "Epoch [51/100] , Loss: 0.2400\n",
      "Epoch [51/100] , Loss: 0.2389\n",
      "Epoch [51/100] , Loss: 0.2083\n",
      "Epoch [51/100] , Loss: 0.1652\n",
      "Epoch [51/100] , Loss: 0.2656\n",
      "Epoch [51/100] , Loss: 0.2944\n",
      "Epoch [61/100] , Loss: 0.2722\n",
      "Epoch [61/100] , Loss: 0.2119\n",
      "Epoch [61/100] , Loss: 0.2161\n",
      "Epoch [61/100] , Loss: 0.1366\n",
      "Epoch [61/100] , Loss: 0.4388\n",
      "Epoch [61/100] , Loss: 0.2259\n",
      "Epoch [61/100] , Loss: 0.4465\n",
      "Epoch [61/100] , Loss: 0.3370\n",
      "Epoch [61/100] , Loss: 0.2738\n",
      "Epoch [61/100] , Loss: 0.2892\n",
      "Epoch [61/100] , Loss: 0.2239\n",
      "Epoch [61/100] , Loss: 0.2104\n",
      "Epoch [61/100] , Loss: 0.2928\n",
      "Epoch [61/100] , Loss: 0.1528\n",
      "Epoch [61/100] , Loss: 0.2321\n",
      "Epoch [61/100] , Loss: 0.2448\n",
      "Epoch [61/100] , Loss: 0.3137\n",
      "Epoch [61/100] , Loss: 0.2475\n",
      "Epoch [61/100] , Loss: 0.2270\n",
      "Epoch [61/100] , Loss: 0.2132\n",
      "Epoch [61/100] , Loss: 0.1051\n",
      "Epoch [61/100] , Loss: 0.2667\n",
      "Epoch [61/100] , Loss: 0.1924\n",
      "Epoch [61/100] , Loss: 0.2672\n",
      "Epoch [61/100] , Loss: 0.2333\n",
      "Epoch [71/100] , Loss: 0.3392\n",
      "Epoch [71/100] , Loss: 0.1734\n",
      "Epoch [71/100] , Loss: 0.2765\n",
      "Epoch [71/100] , Loss: 0.2634\n",
      "Epoch [71/100] , Loss: 0.3384\n",
      "Epoch [71/100] , Loss: 0.1666\n",
      "Epoch [71/100] , Loss: 0.1141\n",
      "Epoch [71/100] , Loss: 0.1314\n",
      "Epoch [71/100] , Loss: 0.1243\n",
      "Epoch [71/100] , Loss: 0.2318\n",
      "Epoch [71/100] , Loss: 0.4310\n",
      "Epoch [71/100] , Loss: 0.3006\n",
      "Epoch [71/100] , Loss: 0.2672\n",
      "Epoch [71/100] , Loss: 0.2669\n",
      "Epoch [71/100] , Loss: 0.1198\n",
      "Epoch [71/100] , Loss: 0.2768\n",
      "Epoch [71/100] , Loss: 0.1781\n",
      "Epoch [71/100] , Loss: 0.2783\n",
      "Epoch [71/100] , Loss: 0.2566\n",
      "Epoch [71/100] , Loss: 0.4867\n",
      "Epoch [71/100] , Loss: 0.1922\n",
      "Epoch [71/100] , Loss: 0.4847\n",
      "Epoch [71/100] , Loss: 0.1751\n",
      "Epoch [71/100] , Loss: 0.1358\n",
      "Epoch [71/100] , Loss: 0.2564\n",
      "Epoch [81/100] , Loss: 0.3612\n",
      "Epoch [81/100] , Loss: 0.2524\n",
      "Epoch [81/100] , Loss: 0.3477\n",
      "Epoch [81/100] , Loss: 0.4318\n",
      "Epoch [81/100] , Loss: 0.2125\n",
      "Epoch [81/100] , Loss: 0.2697\n",
      "Epoch [81/100] , Loss: 0.1072\n",
      "Epoch [81/100] , Loss: 0.1330\n",
      "Epoch [81/100] , Loss: 0.2791\n",
      "Epoch [81/100] , Loss: 0.2049\n",
      "Epoch [81/100] , Loss: 0.3138\n",
      "Epoch [81/100] , Loss: 0.2071\n",
      "Epoch [81/100] , Loss: 0.3920\n",
      "Epoch [81/100] , Loss: 0.2009\n",
      "Epoch [81/100] , Loss: 0.1018\n",
      "Epoch [81/100] , Loss: 0.3050\n",
      "Epoch [81/100] , Loss: 0.1805\n",
      "Epoch [81/100] , Loss: 0.1274\n",
      "Epoch [81/100] , Loss: 0.3332\n",
      "Epoch [81/100] , Loss: 0.3770\n",
      "Epoch [81/100] , Loss: 0.1335\n",
      "Epoch [81/100] , Loss: 0.3827\n",
      "Epoch [81/100] , Loss: 0.0937\n",
      "Epoch [81/100] , Loss: 0.2403\n",
      "Epoch [81/100] , Loss: 0.2769\n",
      "Epoch [91/100] , Loss: 0.4196\n",
      "Epoch [91/100] , Loss: 0.3246\n",
      "Epoch [91/100] , Loss: 0.2192\n",
      "Epoch [91/100] , Loss: 0.3249\n",
      "Epoch [91/100] , Loss: 0.1253\n",
      "Epoch [91/100] , Loss: 0.2573\n",
      "Epoch [91/100] , Loss: 0.2229\n",
      "Epoch [91/100] , Loss: 0.1610\n",
      "Epoch [91/100] , Loss: 0.2044\n",
      "Epoch [91/100] , Loss: 0.2069\n",
      "Epoch [91/100] , Loss: 0.2099\n",
      "Epoch [91/100] , Loss: 0.2794\n",
      "Epoch [91/100] , Loss: 0.1444\n",
      "Epoch [91/100] , Loss: 0.3332\n",
      "Epoch [91/100] , Loss: 0.5038\n",
      "Epoch [91/100] , Loss: 0.1490\n",
      "Epoch [91/100] , Loss: 0.3057\n",
      "Epoch [91/100] , Loss: 0.2683\n",
      "Epoch [91/100] , Loss: 0.4248\n",
      "Epoch [91/100] , Loss: 0.2028\n",
      "Epoch [91/100] , Loss: 0.2516\n",
      "Epoch [91/100] , Loss: 0.1179\n",
      "Epoch [91/100] , Loss: 0.2266\n",
      "Epoch [91/100] , Loss: 0.1458\n",
      "Epoch [91/100] , Loss: 0.2310\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        \n",
    "        #optimizer 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch%10 ==0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] , Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Accuary 51%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct =0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    print(\"Accuary %d%%\"% (100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mspytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
